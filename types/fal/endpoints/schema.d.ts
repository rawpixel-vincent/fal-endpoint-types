import type * as Components from './components.js';

export interface XaiGrokImagineVideoTextToVideoInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16';
    /**
     * Duration
     * @description Video duration in seconds.
     * @default 6
     */
    duration?: number;
    /**
     * Prompt
     * @description Text description of the desired video.
     * @example Anime schoolgirl bursting out of house door, cherry blossoms blowing, morning light, speed lines indicating rush, chibi-ready expressions, classic shojo aesthetic, vibrant colors
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the output video.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
}

export interface XaiGrokImagineVideoTextToVideoOutput {
    /**
     * Video
     * @description The generated video.
     * @example {
     *       "height": 720,
     *       "duration": 6.041667,
     *       "url": "https://v3b.fal.media/files/b/0a8b90e4/RUAbFYlssdqnbjNLmE8qP_IX7BNYGP.mp4",
     *       "fps": 24,
     *       "width": 1280,
     *       "file_name": "RUAbFYlssdqnbjNLmE8qP_IX7BNYGP.mp4",
     *       "num_frames": 145,
     *       "content_type": "video/mp4"
     *     }
     */
    video: Components.VideoFile_1;
}

export interface XaiGrokImagineVideoImageToVideoInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16';
    /**
     * Duration
     * @description Video duration in seconds.
     * @default 6
     */
    duration?: number;
    /**
     * Image URL
     * @description URL of the input image for video generation.
     * @example https://v3b.fal.media/files/b/0a8b90e0/BFLE9VDlZqsryU-UA3BoD_image_004.png
     */
    image_url: string;
    /**
     * Prompt
     * @description Text description of desired changes or motion in the video.
     * @example Medieval knight in ornate armor walking through a mystical forest, bioluminescent plants pulsing with light, ancient stone ruins overgrown with glowing vines, over-the-shoulder camera, dark fantasy aesthetic, volumetric fog and Lumen lighting
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the output video.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
}

export interface XaiGrokImagineVideoImageToVideoOutput {
    /**
     * Video
     * @description The generated video.
     * @example {
     *       "height": 720,
     *       "duration": 6.041667,
     *       "url": "https://v3b.fal.media/files/b/0a8b90e0/0Ci1dviuSnEyUZzBUq-_5_nu7MrAAa.mp4",
     *       "fps": 24,
     *       "width": 1280,
     *       "file_name": "0Ci1dviuSnEyUZzBUq-_5_nu7MrAAa.mp4",
     *       "num_frames": 145,
     *       "content_type": "video/mp4"
     *     }
     */
    video: Components.VideoFile_1;
}

export interface XaiGrokImagineVideoEditVideoInput {
    /**
     * Prompt
     * @description Text description of the desired edit.
     * @example Colorize the video
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the output video.
     * @default auto
     * @enum {string}
     */
    resolution?: 'auto' | '480p' | '720p';
    /**
     * Video URL
     * @description URL of the input video to edit. The video will be resized to a maximum area of 854x480 pixels and truncated to 8 seconds.
     * @example https://v3b.fal.media/files/b/0a8b9112/V5Z_NIPE3ppMDWivNo6_q_video_019.mp4
     */
    video_url: string;
}

export interface XaiGrokImagineVideoEditVideoOutput {
    /**
     * Video
     * @description The generated video.
     * @example {
     *       "height": 720,
     *       "duration": 5.041667,
     *       "url": "https://v3b.fal.media/files/b/0a8b9113/EuDrZuQTW9m1phBXOsauz_EpJH3s8X.mp4",
     *       "fps": 24,
     *       "width": 1280,
     *       "file_name": "EuDrZuQTW9m1phBXOsauz_EpJH3s8X.mp4",
     *       "num_frames": 121,
     *       "content_type": "video/mp4"
     *     }
     */
    video: Components.VideoFile_1;
}

export interface XaiGrokImagineImageEditInput {
    /**
     * Image URL
     * @description URL of the image to edit.
     * @example https://v3b.fal.media/files/b/0a8b911d/Abk8vStrvmSPlzUqI_NN3_image_043.png
     */
    image_url: string;
    /**
     * Number of Images
     * @description Number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description Text description of the desired image.
     * @example Make this scene more realistic but still keep the game vibes
     */
    prompt: string;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface XaiGrokImagineImageEditOutput {
    /**
     * Images
     * @description The URL of the edited image.
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a8b911d/XMqiVoO2ECXUZEUYmPl2l.jpg"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Revised Prompt
     * @description The enhanced prompt that was used to generate the image.
     */
    revised_prompt: string;
}

export interface XaiGrokImagineImageInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated image.
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?:
        | '2:1'
        | '20:9'
        | '19.5:9'
        | '16:9'
        | '4:3'
        | '3:2'
        | '1:1'
        | '2:3'
        | '3:4'
        | '9:16'
        | '9:19.5'
        | '9:20'
        | '1:2';
    /**
     * Number of Images
     * @description Number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description Text description of the desired image.
     * @example Abstract human silhouette, golden particles ready to burst outward representing joy, data visualization style, emotional expression through particles, artistic scientific
     */
    prompt: string;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface XaiGrokImagineImageOutput {
    /**
     * Images
     * @description The URL of the generated image.
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a8b90b7/9avg_nKJmcVinjQHJR_Ja.jpg"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Revised Prompt
     * @description The enhanced prompt that was used to generate the image.
     */
    revised_prompt: string;
}

export interface WanV26TextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video. Wan 2.6 supports additional ratios.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1' | '4:3' | '3:4';
    /**
     * Audio Url
     * @description URL of the audio to use as the background music. Must be publicly accessible.
     *     Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds),
     *     the audio is truncated to the first N seconds, and the rest is discarded. If
     *     the audio is shorter than the video, the remaining part of the video will be silent.
     *     For example, if the audio is 3 seconds long and the video duration is 5 seconds, the
     *     first 3 seconds of the output video will have sound, and the last 2 seconds will be silent.
     *     - Format: WAV, MP3.
     *     - Duration: 3 to 30 s.
     *     - File size: Up to 15 MB.
     */
    audio_url?: string;
    /**
     * Duration
     * @description Duration of the generated video in seconds. Choose between 5, 10, or 15 seconds.
     * @default 5
     * @example 5
     * @example 10
     * @example 15
     * @enum {string}
     */
    duration?: '5' | '10' | '15';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Multi Shots
     * @description When true, enables intelligent multi-shot segmentation for coherent narrative videos. Only active when enable_prompt_expansion is True. Set to false for single-shot generation.
     * @default true
     */
    multi_shots?: boolean;
    /**
     * Negative Prompt
     * @description Negative prompt to describe content to avoid. Max 500 characters.
     * @default
     * @example low resolution, error, worst quality, low quality, defects
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The text prompt for video generation. Supports Chinese and English, max 800 characters. For multi-shot videos, use format: 'Overall description. First shot [0-3s] content. Second shot [3-5s] content.'
     * @example Humorous but premium mini-trailer: a tiny fox 3D director proves "multi-scene" by calling simple commands that instantly change the set. Extreme photoreal 4K, cinematic lighting, subtle film grain, smooth camera. No subtitles, no UI, no watermark.
     *
     *     Shot 1 [0-3s] Macro close-up on the fox snapping a clapboard labeled "fal". the fox says : "Action."
     *     Shot 2 [3-6s] Hard cut: Wild West street at sunset. Wide shot, dust in the air. The Fox (in frame) points forward: "Make it wide."
     *     Shot 3 [6-10s] Hard cut: jungle river. The fox stands on a small boat. The camera pushes forward through vines and mist. Fox saying: "Now… adventure."
     *     Shot 4 [10-15s] Hard cut: space station window. Slow orbit around the fox with stars outside. Fox nods: "Done. Next movie."
     */
    prompt: string;
    /**
     * Resolution
     * @description Video resolution tier. Wan 2.6 T2V only supports 720p and 1080p (no 480p).
     * @default 1080p
     * @enum {string}
     */
    resolution?: '720p' | '1080p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface WanV26TextToVideoOutput {
    /**
     * Actual Prompt
     * @description The actual prompt used if prompt rewriting was enabled
     * @example Humorous but premium mini-trailer: a tiny fox 3D director proves "multi-scene" by calling simple commands that instantly change the set. Extreme photoreal 4K, cinematic lighting, subtle film grain, smooth camera. No subtitles, no UI, no watermark.
     *
     *     Shot 1 [0-3s] Macro close-up on the fox snapping a clapboard labeled "fal". the fox says : "Action."
     *     Shot 2 [3-6s] Hard cut: Wild West street at sunset. Wide shot, dust in the air. The Fox (in frame) points forward: "Make it wide."
     *     Shot 3 [6-10s] Hard cut: jungle river. The fox stands on a small boat. The camera pushes forward through vines and mist. Fox saying: "Now… adventure."
     *     Shot 4 [10-15s] Hard cut: space station window. Slow orbit around the fox with stars outside. Fox nods: "Done. Next movie."
     */
    actual_prompt?: string;
    /**
     * Seed
     * @description The seed used for generation
     * @example 175932751
     */
    seed: number;
    /**
     * Video
     * @description The generated video file
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/0a867564/PsHtrg623uJuI7DdRqXvb_etx4d0Un.mp4"
     *     }
     */
    video: Components.VideoFile_1;
}

export interface WanV26TextToImageInput {
    /**
     * Enable Safety Checker
     * @description Enable content moderation for input and output.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description Output image size. If not set: matches input image size (up to 1280*1280). Use presets like 'square_hd', 'landscape_16_9', or specify exact dimensions.
     * @example square_hd
     * @example landscape_16_9
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description Optional reference image (0 or 1). When provided, can be used for style guidance. Resolution: 384-5000px each dimension. Max size: 10MB. Formats: JPEG, JPG, PNG (no alpha), BMP, WEBP.
     */
    image_url?: string;
    /**
     * Max Images
     * @description Maximum number of images to generate (1-5). Actual count may be less depending on model inference.
     * @default 1
     */
    max_images?: number;
    /**
     * Negative Prompt
     * @description Content to avoid in the generated image. Max 500 characters.
     * @default
     * @example low resolution, error, worst quality, low quality, deformed
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description Text prompt describing the desired image. Supports Chinese and English. Max 2000 characters.
     * @example An ancient library floating among clouds, golden hour light streaming through massive windows, photorealistic
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility (0-2147483647).
     */
    seed?: number;
}

export interface WanV26TextToImageOutput {
    /**
     * Generated Text
     * @description Generated text content (in mixed text-and-image mode). May be None if only images were generated.
     */
    generated_text?: string;
    /**
     * Images
     * @description Generated images in PNG format
     * @example [
     *       {
     *         "file_name": "output_1.png",
     *         "content_type": "image/png",
     *         "url": "https://v3b.fal.media/files/b/0a86d6b0/cBXGSUEl3DkTcBnf9IEM0_output_1.png"
     *       }
     *     ]
     */
    images: Components.File[];
    /**
     * Seed
     * @description The seed used for generation
     * @example 175932751
     */
    seed: number;
}

export interface WanV26ReferenceToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1' | '4:3' | '3:4';
    /**
     * Duration
     * @description Duration of the generated video in seconds. R2V supports only 5 or 10 seconds (no 15s).
     * @default 5
     * @example 5
     * @example 10
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt rewriting using LLM.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Multi Shots
     * @description When true (default), enables intelligent multi-shot segmentation for coherent narrative videos with multiple shots. When false, generates single continuous shot. Only active when enable_prompt_expansion is True.
     * @default true
     */
    multi_shots?: boolean;
    /**
     * Negative Prompt
     * @description Negative prompt to describe content to avoid. Max 500 characters.
     * @default
     * @example low resolution, error, worst quality, low quality, defects
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description Use @Video1, @Video2, @Video3 to reference subjects from your videos. Works for people, animals, or objects. For multi-shot prompts: '[0-3s] Shot 1. [3-6s] Shot 2.' Max 800 characters.
     * @example Dance battle between @Video1 and @Video2.
     */
    prompt: string;
    /**
     * Resolution
     * @description Video resolution tier. R2V only supports 720p and 1080p (no 480p).
     * @default 1080p
     * @enum {string}
     */
    resolution?: '720p' | '1080p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Video Urls
     * @description Reference videos for subject consistency (1-3 videos). Videos' FPS must be at least 16 FPS.Reference in prompt as @Video1, @Video2, @Video3. Works for people, animals, or objects.
     * @example [
     *       "https://v3b.fal.media/files/b/0a86742f/9rVJtQ2ukp9cid8lheutF_output.mp4",
     *       "https://v3b.fal.media/files/b/0a867424/30OqWXFgHWqOwcP2OUwRx_output.mp4"
     *     ]
     */
    video_urls: string[];
}

export interface WanV26ReferenceToVideoOutput {
    /**
     * Actual Prompt
     * @description The actual prompt used if prompt rewriting was enabled
     * @example Dance battle between Character1 and Character2, cinematic lighting, dynamic camera movement.
     */
    actual_prompt?: string;
    /**
     * Seed
     * @description The seed used for generation
     * @example 175932751
     */
    seed: number;
    /**
     * Video
     * @description The generated video file
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/0a86762b/iDknfPkLFSFwWkyMgJi0U_QIzjwBDQ.mp4"
     *     }
     */
    video: Components.VideoFile_1;
}

export interface WanV26ImageToVideoFlashInput extends SharedType_d35 {}

export interface WanV26ImageToVideoFlashOutput extends SharedType_51a {}

export interface WanV26ImageToVideoInput extends SharedType_d35 {}

export interface WanV26ImageToVideoOutput extends SharedType_51a {}

export interface WanV26ImageToImageInput {
    /**
     * Enable Prompt Expansion
     * @description Enable LLM prompt optimization. Significantly improves results for simple prompts but adds 3-4 seconds processing time.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Enable content moderation for input and output.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description Output image size. Use presets like 'square_hd', 'landscape_16_9', 'portrait_9_16', or specify exact dimensions with ImageSize(width=1280, height=720). Total pixels must be between 768*768 and 1280*1280.
     * @default square_hd
     * @example square_hd
     * @example landscape_16_9
     * @example portrait_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Urls
     * @description Reference images for editing (1-3 images required). Order matters: reference as 'image 1', 'image 2', 'image 3' in prompt. Resolution: 384-5000px each dimension. Max size: 10MB each. Formats: JPEG, JPG, PNG (no alpha), BMP, WEBP.
     * @example [
     *       "https://v3b.fal.media/files/b/0a86d6a7/6smIczyPbvAU3IJ1F5Ok3.png",
     *       "https://v3b.fal.media/files/b/0a86d6a7/nTYVlOfKLD1FqHAGy7KS3.png",
     *       "https://v3b.fal.media/files/b/0a86d6ae/6JA70jOe0-pbDtXLF2roV.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Negative Prompt
     * @description Content to avoid in the generated image. Max 500 characters.
     * @default
     * @example low resolution, error, worst quality, low quality, deformed, extra fingers
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate (1-4). Directly affects billing cost.
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description Text prompt describing the desired image. Supports Chinese and English. Max 2000 characters. Example: 'Generate an image using the style of image 1 and background of image 2'.
     * @example Place the wizard from image 2 in the ancient library from image 3, holding and studying the magical crystal orb from image 1. The orb's glow illuminates his face with purple and blue light. Floating candles around him, ancient books visible in the background. Mystical, dramatic lighting, fantasy art style, highly detailed.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility (0-2147483647). Same seed produces more consistent results.
     */
    seed?: number;
}

export interface WanV26ImageToImageOutput {
    /**
     * Images
     * @description Generated images in PNG format
     * @example [
     *       {
     *         "file_name": "output_1.png",
     *         "content_type": "image/png",
     *         "url": "https://v3b.fal.media/files/b/0a86d6bb/iSEuXzi3kDy1jnlMCwYuH_output_3.png"
     *       }
     *     ]
     */
    images: Components.File[];
    /**
     * Seed
     * @description The seed used for generation
     * @example 175932751
     */
    seed: number;
}

export interface VeedVideoBackgroundRemovalGreenScreenInput {
    /**
     * Output Codec
     * @description Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.
     * @default vp9
     * @enum {string}
     */
    output_codec?: 'vp9' | 'h264';
    /**
     * Spill Suppression Strength
     * @description Increase the value if green spots remain in the video, decrease if color changes are noticed on the extracted subject.
     * @default 0.8
     */
    spill_suppression_strength?: number;
    /**
     * Video Url
     * Format: uri
     * @example https://v3b.fal.media/files/b/0a849c38/zoA0frujpZUkj07NtXytv_jessica.mp4
     */
    video_url: string;
}

export interface VeedVideoBackgroundRemovalGreenScreenOutput {
    /**
     * Video
     * @example [
     *       {
     *         "content_type": "video/webm",
     *         "url": "https://v3b.fal.media/files/b/0a849c48/MFOmvAhK4vvUsFsMVmw0P_output.webm"
     *       }
     *     ]
     */
    video: Components.File_1[];
}

export interface VeedVideoBackgroundRemovalFastInput {
    /**
     * Output Codec
     * @description Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.
     * @default vp9
     * @enum {string}
     */
    output_codec?: 'vp9' | 'h264';
    /**
     * Refine Foreground Edges
     * @description Improves the quality of the extracted object's edges.
     * @default true
     */
    refine_foreground_edges?: boolean;
    /**
     * Subject Is Person
     * @description Set to False if the subject is not a person.
     * @default true
     */
    subject_is_person?: boolean;
    /**
     * Video Url
     * Format: uri
     * @example https://v3b.fal.media/files/b/0a8479a0/3ngPszofrNpEvPaL9P5xR_generated.mp4
     */
    video_url: string;
}

export interface VeedVideoBackgroundRemovalFastOutput {
    /**
     * Video
     * @example [
     *       {
     *         "content_type": "video/webm",
     *         "url": "https://v3b.fal.media/files/b/0a8479a7/mdr5_b7CqeDmZROCLkp7i_output.webm"
     *       }
     *     ]
     */
    video: Components.File_1[];
}

export interface VeedVideoBackgroundRemovalInput {
    /**
     * Output Codec
     * @description Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.
     * @default vp9
     * @enum {string}
     */
    output_codec?: 'vp9' | 'h264';
    /**
     * Refine Foreground Edges
     * @description Improves the quality of the extracted object's edges.
     * @default true
     */
    refine_foreground_edges?: boolean;
    /**
     * Subject Is Person
     * @description Set to False if the subject is not a person.
     * @default true
     */
    subject_is_person?: boolean;
    /**
     * Video Url
     * Format: uri
     * @example https://v3b.fal.media/files/b/0a847700/NYnvFeP13ehgSgqMDV_PL_stock.mp4
     */
    video_url: string;
}

export interface VeedVideoBackgroundRemovalOutput {
    /**
     * Video
     * @example [
     *       {
     *         "content_type": "video/webm",
     *         "url": "https://v3b.fal.media/files/b/0a847713/C7g1UaT46yKhOPt6KRrgg_output.webm"
     *       }
     *     ]
     */
    video: Components.File_1[];
}

export interface VeedLipsyncInput {
    /**
     * Audio Url
     * Format: uri
     * @example https://v3.fal.media/files/rabbit/Ql3ade3wEKlZXRQLRbhxm_tts.mp3
     */
    audio_url: string;
    /**
     * Video Url
     * Format: uri
     * @example https://v3.fal.media/files/monkey/q1fDPhrpfjfsaRmbhTed4_influencer.mp4
     */
    video_url: string;
}

export interface VeedLipsyncOutput {
    /**
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://v3.fal.media/files/penguin/PsA4BJPGAojXKW2QGztm4_tmpe_e1cgbq.mp4"
     *     }
     */
    video: Components.File_1;
}

export interface VeedFabric10TextInput {
    /**
     * Image Url
     * Format: uri
     * @example https://v3.fal.media/files/koala/NLVPfOI4XL1cWT2PmmqT3_Hope.png
     */
    image_url: string;
    /**
     * Resolution
     * @description Resolution
     * @enum {string}
     */
    resolution: '720p' | '480p';
    /**
     * Text
     * @example Create talking videos with VEED Fabric-One API.
     */
    text: string;
    /**
     * Voice Description
     * @description Optional additional voice description. The primary voice description is auto-generated from the image. You can use simple descriptors like 'British accent' or 'Confident' or provide a detailed description like 'Confident male voice, mid-20s, with notes of...'
     */
    voice_description?: string;
}

export interface VeedFabric10TextOutput {
    /**
     * @example {
     *       "content_type": "audio/mp4",
     *       "url": "https://v3b.fal.media/files/b/0a8604be/zVkoAB4hTa8g6Fyl6V733_tmpy1fslwp2.mp4"
     *     }
     */
    video: Components.File_1;
}

export interface VeedFabric10FastInput extends SharedType_c45 {}

export interface VeedFabric10FastOutput extends SharedType_9d8 {}

export interface VeedFabric10Input extends SharedType_c45 {}

export interface VeedFabric10Output extends SharedType_9d8 {}

export interface VeedAvatarsTextToVideoInput {
    /**
     * Avatar Id
     * @description The avatar to use for the video
     * @enum {string}
     */
    avatar_id:
        | 'emily_vertical_primary'
        | 'emily_vertical_secondary'
        | 'marcus_vertical_primary'
        | 'marcus_vertical_secondary'
        | 'mira_vertical_primary'
        | 'mira_vertical_secondary'
        | 'jasmine_vertical_primary'
        | 'jasmine_vertical_secondary'
        | 'jasmine_vertical_walking'
        | 'aisha_vertical_walking'
        | 'elena_vertical_primary'
        | 'elena_vertical_secondary'
        | 'any_male_vertical_primary'
        | 'any_female_vertical_primary'
        | 'any_male_vertical_secondary'
        | 'any_female_vertical_secondary'
        | 'any_female_vertical_walking'
        | 'emily_primary'
        | 'emily_side'
        | 'marcus_primary'
        | 'marcus_side'
        | 'aisha_walking'
        | 'elena_primary'
        | 'elena_side'
        | 'any_male_primary'
        | 'any_female_primary'
        | 'any_male_side'
        | 'any_female_side';
    /**
     * Text
     * @example Ever wondered how to get that flawless glow?
     *     Introducing our new skincare line, designed for real life.
     *     Step one: Cleanse with our gentle, nourishing formula.
     *     Step two: Apply our hydrating serum for that dewy look.
     *     Step three: Lock it in with our lightweight moisturizer.
     *     Feel the difference with every application.
     *     See the glow? That's the magic of our skincare.
     *     Use code 'GLOW20' for an exclusive discount.
     *     Join the skincare revolution today!
     */
    text: string;
}

export interface VeedAvatarsTextToVideoOutput extends SharedType_78a {}

export interface VeedAvatarsAudioToVideoInput {
    /**
     * Audio Url
     * Format: uri
     * @example https://v3.fal.media/files/lion/OXiM5_Cve4kQ0ZcXmVzq4_product_presentation.mp3
     */
    audio_url: string;
    /**
     * Avatar Id
     * @description The avatar to use for the video
     * @enum {string}
     */
    avatar_id:
        | 'emily_vertical_primary'
        | 'emily_vertical_secondary'
        | 'marcus_vertical_primary'
        | 'marcus_vertical_secondary'
        | 'mira_vertical_primary'
        | 'mira_vertical_secondary'
        | 'jasmine_vertical_primary'
        | 'jasmine_vertical_secondary'
        | 'jasmine_vertical_walking'
        | 'aisha_vertical_walking'
        | 'elena_vertical_primary'
        | 'elena_vertical_secondary'
        | 'any_male_vertical_primary'
        | 'any_female_vertical_primary'
        | 'any_male_vertical_secondary'
        | 'any_female_vertical_secondary'
        | 'any_female_vertical_walking'
        | 'emily_primary'
        | 'emily_side'
        | 'marcus_primary'
        | 'marcus_side'
        | 'aisha_walking'
        | 'elena_primary'
        | 'elena_side'
        | 'any_male_primary'
        | 'any_female_primary'
        | 'any_male_side'
        | 'any_female_side';
}

export interface VeedAvatarsAudioToVideoOutput extends SharedType_78a {}

export interface Tripo3dTripoV25MultiviewTo3dInput {
    /**
     * Auto Size
     * @description Automatically scale the model to real-world dimensions, with the unit in meters. The default value is False.
     * @default false
     */
    auto_size?: boolean;
    /**
     * Back Image Url
     * @description Back view image of the object.
     * @example https://platform.tripo3d.ai/assets/back-6vq1a8L4.jpg
     */
    back_image_url?: string;
    /**
     * Face Limit
     * @description Limits the number of faces on the output model. If this option is not set, the face limit will be adaptively determined.
     */
    face_limit?: number;
    /**
     * Front Image Url
     * @description Front view image of the object.
     * @example https://platform.tripo3d.ai/assets/front-235queJB.jpg
     */
    front_image_url: string;
    /**
     * Left Image Url
     * @description Left view image of the object.
     * @example https://platform.tripo3d.ai/assets/left-Nfdj2U8P.jpg
     */
    left_image_url?: string;
    /**
     * Orientation
     * @description Set orientation=align_image to automatically rotate the model to align the original image. The default value is default.
     * @default default
     * @enum {string}
     */
    orientation?: 'default' | 'align_image';
    /**
     * Pbr
     * @description A boolean option to enable pbr. The default value is True, set False to get a model without pbr. If this option is set to True, texture will be ignored and used as True.
     * @default false
     */
    pbr?: boolean;
    /**
     * Quad
     * @description Set True to enable quad mesh output (extra $0.05 per generation). If quad=True and face_limit is not set, the default face_limit will be 10000. Note: Enabling this option will force the output to be an FBX model.
     * @default false
     */
    quad?: boolean;
    /**
     * Right Image Url
     * @description Right view image of the object.
     * @example https://platform.tripo3d.ai/assets/right-hj57H4if.jpg
     */
    right_image_url?: string;
    /**
     * Seed
     * @description This is the random seed for model generation. The seed controls the geometry generation process, ensuring identical models when the same seed is used. This parameter is an integer and is randomly chosen if not set.
     */
    seed?: number;
    /**
     * Style
     * @deprecated
     * @description [DEPRECATED] Defines the artistic style or transformation to be applied to the 3D model, altering its appearance according to preset options (extra $0.05 per generation). Omit this option to keep the original style and apperance.
     * @enum {string}
     */
    style?:
        | 'person:person2cartoon'
        | 'object:clay'
        | 'object:steampunk'
        | 'animal:venom'
        | 'object:barbie'
        | 'object:christmas'
        | 'gold'
        | 'ancient_bronze';
    /**
     * Texture
     * @description An option to enable texturing. Default is 'standard', set 'no' to get a model without any textures, and set 'HD' to get a model with hd quality textures.
     * @default standard
     * @enum {string}
     */
    texture?: 'no' | 'standard' | 'HD';
    /**
     * Texture Alignment
     * @description Determines the prioritization of texture alignment in the 3D model. The default value is original_image.
     * @default original_image
     * @enum {string}
     */
    texture_alignment?: 'original_image' | 'geometry';
    /**
     * Texture Seed
     * @description This is the random seed for texture generation. Using the same seed will produce identical textures. This parameter is an integer and is randomly chosen if not set. If you want a model with different textures, please use same seed and different texture_seed.
     */
    texture_seed?: number;
}

export interface Tripo3dTripoV25MultiviewTo3dOutput extends SharedType_819 {}

export interface Tripo3dTripoV25ImageTo3dInput {
    /**
     * Auto Size
     * @description Automatically scale the model to real-world dimensions, with the unit in meters. The default value is False.
     * @default false
     */
    auto_size?: boolean;
    /**
     * Face Limit
     * @description Limits the number of faces on the output model. If this option is not set, the face limit will be adaptively determined.
     */
    face_limit?: number;
    /**
     * Image Url
     * @description URL of the image to use for model generation.
     * @example https://platform.tripo3d.ai/assets/front-235queJB.jpg
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/hamburger.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/poly_fox.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/robot.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/teapot.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/tiger_girl.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/horse.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/flamingo.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/unicorn.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/chair.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/iso_house.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/marble.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/police_woman.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/captured_p.png
     */
    image_url: string;
    /**
     * Orientation
     * @description Set orientation=align_image to automatically rotate the model to align the original image. The default value is default.
     * @default default
     * @enum {string}
     */
    orientation?: 'default' | 'align_image';
    /**
     * Pbr
     * @description A boolean option to enable pbr. The default value is True, set False to get a model without pbr. If this option is set to True, texture will be ignored and used as True.
     * @default false
     */
    pbr?: boolean;
    /**
     * Quad
     * @description Set True to enable quad mesh output (extra $0.05 per generation). If quad=True and face_limit is not set, the default face_limit will be 10000. Note: Enabling this option will force the output to be an FBX model.
     * @default false
     */
    quad?: boolean;
    /**
     * Seed
     * @description This is the random seed for model generation. The seed controls the geometry generation process, ensuring identical models when the same seed is used. This parameter is an integer and is randomly chosen if not set.
     */
    seed?: number;
    /**
     * Style
     * @deprecated
     * @description [DEPRECATED] Defines the artistic style or transformation to be applied to the 3D model, altering its appearance according to preset options (extra $0.05 per generation). Omit this option to keep the original style and apperance.
     * @enum {string}
     */
    style?:
        | 'person:person2cartoon'
        | 'object:clay'
        | 'object:steampunk'
        | 'animal:venom'
        | 'object:barbie'
        | 'object:christmas'
        | 'gold'
        | 'ancient_bronze';
    /**
     * Texture
     * @description An option to enable texturing. Default is 'standard', set 'no' to get a model without any textures, and set 'HD' to get a model with hd quality textures.
     * @default standard
     * @enum {string}
     */
    texture?: 'no' | 'standard' | 'HD';
    /**
     * Texture Alignment
     * @description Determines the prioritization of texture alignment in the 3D model. The default value is original_image.
     * @default original_image
     * @enum {string}
     */
    texture_alignment?: 'original_image' | 'geometry';
    /**
     * Texture Seed
     * @description This is the random seed for texture generation. Using the same seed will produce identical textures. This parameter is an integer and is randomly chosen if not set. If you want a model with different textures, please use same seed and different texture_seed.
     */
    texture_seed?: number;
}

export interface Tripo3dTripoV25ImageTo3dOutput extends SharedType_819 {}

export interface SonautoV2TextToMusicInput {
    /**
     * Balance Strength
     * @description Greater means more natural vocals. Lower means sharper instrumentals. We recommend 0.7.
     * @default 0.7
     */
    balance_strength?: number;
    /**
     * Bpm
     * @description The beats per minute of the song. This can be set to an integer or the literal string "auto" to pick a suitable bpm based on the tags. Set bpm to null to not condition the model on bpm information.
     * @default auto
     */
    bpm?: number | 'auto';
    /**
     * Lyrics Prompt
     * @description The lyrics sung in the generated song. An empty string will generate an instrumental track.
     */
    lyrics_prompt?: string;
    /**
     * Num Songs
     * @description Generating 2 songs costs 1.5x the price of generating 1 song. Also, note that using the same seed may not result in identical songs if the number of songs generated is changed.
     * @default 1
     */
    num_songs?: number;
    /**
     * Output Bit Rate
     * @description The bit rate to use for mp3 and m4a formats. Not available for other formats.
     */
    output_bit_rate?: 128 | 192 | 256 | 320;
    /**
     * Output Format
     * @default wav
     * @enum {string}
     */
    output_format?: 'flac' | 'mp3' | 'wav' | 'ogg' | 'm4a';
    /**
     * Prompt
     * @description A description of the track you want to generate. This prompt will be used to automatically generate the tags and lyrics unless you manually set them. For example, if you set prompt and tags, then the prompt will be used to generate only the lyrics.
     * @example A pop song about turtles flying
     */
    prompt?: string;
    /**
     * Prompt Strength
     * @description Controls how strongly your prompt influences the output. Greater values adhere more to the prompt but sound less natural. (This is CFG.)
     * @default 2
     */
    prompt_strength?: number;
    /**
     * Seed
     * @description The seed to use for generation. Will pick a random seed if not provided. Repeating a request with identical parameters (must use lyrics and tags, not prompt) and the same seed will generate the same song.
     */
    seed?: number;
    /**
     * Tags
     * @description Tags/styles of the music to generate. You can view a list of all available tags at https://sonauto.ai/tag-explorer.
     */
    tags?: string[];
}

export interface SonautoV2TextToMusicOutput {
    /**
     * Audio
     * @description The generated audio files.
     * @example {
     *       "file_size": 16777294,
     *       "file_name": "sonauto.wav",
     *       "content_type": "audio/wav",
     *       "url": "https://cdn.sonauto.ai/generations2_altformats/audio_c5e63f7c-fc79-4322-808d-c09911af4713.wav"
     *     }
     */
    audio: Components.File_1[];
    /**
     * Lyrics
     * @description The lyrics used for generation.
     */
    lyrics?: string;
    /**
     * Seed
     * @description The seed used for generation. This can be used to generate an identical song by passing the same parameters with this seed in a future request.
     * @example 42
     */
    seed: number;
    /**
     * Tags
     * @description The style tags used for generation.
     */
    tags?: string[];
}

export interface SonautoV2InpaintInput {
    /**
     * Audio Url
     * Format: uri
     * @description The URL of the audio file to alter. Must be a valid publicly accessible URL.
     * @example https://cdn.sonauto.ai/generations2_altformats/audio_c5e63f7c-fc79-4322-808d-c09911af4713.wav
     */
    audio_url: string;
    /**
     * Balance Strength
     * @description Greater means more natural vocals. Lower means sharper instrumentals. We recommend 0.7.
     * @default 0.7
     */
    balance_strength?: number;
    /**
     * Lyrics Prompt
     * @description The lyrics sung in the generated song. An empty string will generate an instrumental track.
     * @example [Chorus]
     *     Pigs are soaring in the sky
     *     Wings of bacon flying by
     */
    lyrics_prompt: string;
    /**
     * Num Songs
     * @description Generating 2 songs costs 1.5x the price of generating 1 song. Also, note that using the same seed may not result in identical songs if the number of songs generated is changed.
     * @default 1
     */
    num_songs?: number;
    /**
     * Output Bit Rate
     * @description The bit rate to use for mp3 and m4a formats. Not available for other formats.
     */
    output_bit_rate?: 128 | 192 | 256 | 320;
    /**
     * Output Format
     * @default wav
     * @enum {string}
     */
    output_format?: 'flac' | 'mp3' | 'wav' | 'ogg' | 'm4a';
    /**
     * Prompt Strength
     * @description Controls how strongly your prompt influences the output. Greater values adhere more to the prompt but sound less natural. (This is CFG.)
     * @default 2
     */
    prompt_strength?: number;
    /**
     * Sections
     * @description List of sections to inpaint. Currently, only one section is supported so the list length must be 1.
     * @example [
     *       {
     *         "end": 9.45,
     *         "start": 0
     *       }
     *     ]
     */
    sections: Components.InpaintSection[];
    /**
     * Seed
     * @description The seed to use for generation. Will pick a random seed if not provided. Repeating a request with identical parameters (must use lyrics and tags, not prompt) and the same seed will generate the same song.
     */
    seed?: number;
    /**
     * Selection Crop
     * @description Crop to the selected region
     * @default false
     */
    selection_crop?: boolean;
    /**
     * Tags
     * @description Tags/styles of the music to generate. You can view a list of all available tags at https://sonauto.ai/tag-explorer.
     * @example [
     *       "2020s",
     *       "dance pop",
     *       "pop rock",
     *       "indie pop",
     *       "bubblegum pop",
     *       "synthpop",
     *       "teen pop",
     *       "electropop"
     *     ]
     */
    tags?: string[];
}

export interface SonautoV2InpaintOutput {
    /**
     * Audio
     * @description The generated audio files.
     * @example {
     *       "file_size": 16777294,
     *       "file_name": "sonauto.wav",
     *       "content_type": "audio/wav",
     *       "url": "https://cdn.sonauto.ai/generations2_altformats/audio_9a480c86-a3c0-46e5-bfb0-c0cd6e2fdbc6.wav"
     *     }
     */
    audio: Components.File_1[];
    /**
     * Seed
     * @description The seed used for generation. This can be used to generate an identical song by passing the same parameters with this seed in a future request.
     * @example 42
     */
    seed: number;
}

export interface SonautoV2ExtendInput {
    /**
     * Audio Url
     * Format: uri
     * @description The URL of the audio file to alter. Must be a valid publicly accessible URL.
     * @example https://cdn.sonauto.ai/generations2_altformats/audio_c5e63f7c-fc79-4322-808d-c09911af4713.wav
     */
    audio_url: string;
    /**
     * Balance Strength
     * @description Greater means more natural vocals. Lower means sharper instrumentals. We recommend 0.7.
     * @default 0.7
     */
    balance_strength?: number;
    /**
     * Crop Duration
     * @description Duration in seconds to crop from the selected side before extending from that side.
     * @default 0
     */
    crop_duration?: number;
    /**
     * Extend Duration
     * @description Duration in seconds to extend the song. If not provided, will attempt to automatically determine.
     */
    extend_duration?: number;
    /**
     * Lyrics Prompt
     * @description The lyrics sung in the generated song. An empty string will generate an instrumental track.
     */
    lyrics_prompt?: string;
    /**
     * Num Songs
     * @description Generating 2 songs costs 1.5x the price of generating 1 song. Also, note that using the same seed may not result in identical songs if the number of songs generated is changed.
     * @default 1
     */
    num_songs?: number;
    /**
     * Output Bit Rate
     * @description The bit rate to use for mp3 and m4a formats. Not available for other formats.
     */
    output_bit_rate?: 128 | 192 | 256 | 320;
    /**
     * Output Format
     * @default wav
     * @enum {string}
     */
    output_format?: 'flac' | 'mp3' | 'wav' | 'ogg' | 'm4a';
    /**
     * Prompt
     * @description A description of the track you want to generate. This prompt will be used to automatically generate the tags and lyrics unless you manually set them. For example, if you set prompt and tags, then the prompt will be used to generate only the lyrics.
     * @example Add a beginning to the song
     */
    prompt?: string;
    /**
     * Prompt Strength
     * @description Controls how strongly your prompt influences the output. Greater values adhere more to the prompt but sound less natural. (This is CFG.)
     * @default 1.8
     */
    prompt_strength?: number;
    /**
     * Seed
     * @description The seed to use for generation. Will pick a random seed if not provided. Repeating a request with identical parameters (must use lyrics and tags, not prompt) and the same seed will generate the same song.
     */
    seed?: number;
    /**
     * Side
     * @description Add more to the beginning (left) or end (right) of the song
     * @enum {string}
     */
    side: 'left' | 'right';
    /**
     * Tags
     * @description Tags/styles of the music to generate. You can view a list of all available tags at https://sonauto.ai/tag-explorer.
     */
    tags?: string[];
}

export interface SonautoV2ExtendOutput {
    /**
     * Audio
     * @description The generated audio files.
     * @example {
     *       "file_size": 22069326,
     *       "file_name": "sonauto.wav",
     *       "content_type": "audio/wav",
     *       "url": "https://cdn.sonauto.ai/generations2_altformats/audio_47337412-e577-42af-ae60-01a798e680ec.wav"
     *     }
     */
    audio: Components.File_1[];
    /**
     * Extend Duration
     * @description The duration in seconds that the song was extended by.
     */
    extend_duration: number;
    /**
     * Lyrics
     * @description The lyrics used for generation.
     */
    lyrics?: string;
    /**
     * Seed
     * @description The seed used for generation. This can be used to generate an identical song by passing the same parameters with this seed in a future request.
     * @example 42
     */
    seed: number;
    /**
     * Tags
     * @description The style tags used for generation.
     */
    tags?: string[];
}

export interface SmoretalkaiRembgEnhanceInput {
    /**
     * Image Url
     * @description URL of the input image
     * @example https://fal.media/files/kangaroo/SOF3bLF7b1kJ2-N9dTg-c.png
     */
    image_url: string;
}

export interface SmoretalkaiRembgEnhanceOutput {
    /** @description The segmented output image */
    image: Components.File_1;
}

export interface SharedType_fda {
    /**
     * Masks
     * @description Dictionary of label: mask image
     * @example [
     *       {
     *         "height": 1200,
     *         "file_size": 15724,
     *         "file_name": "019c3c1e3c50446e9996f709d36debb4.png",
     *         "content_type": "image/png",
     *         "url": "https://v3.fal.media/files/monkey/6ITmhHQJ-69s-UxajrY5T_019c3c1e3c50446e9996f709d36debb4.png",
     *         "width": 1800
     *       },
     *       {
     *         "height": 1200,
     *         "file_size": 14905,
     *         "file_name": "0a1522ca410942c7ad6c73efa15b3549.png",
     *         "content_type": "image/png",
     *         "url": "https://v3.fal.media/files/monkey/IljtMxahoo9-7SUpx0fth_0a1522ca410942c7ad6c73efa15b3549.png",
     *         "width": 1800
     *       }
     *     ]
     */
    masks: Components.Image[];
    /**
     * Output
     * @description Generated output
     * @example <p>  A white pickup truck  </p>   [SEG]  is parked on the side of  <p>  the red building  </p>   [SEG] , creating a unique and eye-catching contrast.<|im_end|>
     */
    output: string;
}

export interface SharedType_fd1 {
    /**
     * Prompt
     * @description The prompt used for generation.
     * @example The astronaut gets up and walks away
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/ltxv-image-to-video-output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_fc4 {
    /**
     * Back Image Url
     * @description URL of image to use while generating the 3D model.
     * @example https://storage.googleapis.com/falserverless/model_tests/video_models/back.png
     */
    back_image_url: string;
    /**
     * Front Image Url
     * @description URL of image to use while generating the 3D model.
     * @example https://storage.googleapis.com/falserverless/model_tests/video_models/front.png
     */
    front_image_url: string;
    /**
     * Guidance Scale
     * @description Guidance scale for the model.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Left Image Url
     * @description URL of image to use while generating the 3D model.
     * @example https://storage.googleapis.com/falserverless/model_tests/video_models/left.png
     */
    left_image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps to perform.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Octree Resolution
     * @description Octree resolution for the model.
     * @default 256
     */
    octree_resolution?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Textured Mesh
     * @description If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.
     * @default false
     */
    textured_mesh?: boolean;
}

export interface SharedType_fbd {
    /**
     * Seed
     * @description The seed used for generating the video.
     */
    seed: number;
    /**
     * Video
     * @example {
     *       "url": "https://v3.fal.media/files/kangaroo/y5-1YTGpun17eSeggZMzX_video-1733468228.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_fb3 {
    /**
     * Duration
     * @description Motion duration in seconds (0.5-12.0).
     * @default 5
     * @example 3
     * @example 5
     * @example 10
     */
    duration?: number;
    /**
     * Guidance Scale
     * @description Classifier-free guidance scale. Higher = more faithful to prompt.
     * @default 5
     * @example 3
     * @example 5
     * @example 7.5
     */
    guidance_scale?: number;
    /**
     * Output Format
     * @description Output format: 'fbx' for animation files, 'dict' for raw JSON.
     * @default fbx
     * @enum {string}
     */
    output_format?: 'fbx' | 'dict';
    /**
     * Prompt
     * @description Text prompt describing the motion to generate.
     * @example A person is running then takes a big leap.
     * @example Someone waves hello with their right hand.
     * @example A dancer performs a spinning pirouette.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducible generation.
     * @example 42
     * @example 12345
     */
    seed?: number;
}

export interface SharedType_faf {
    /**
     * Acceleration
     * @description Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URLs of the images to edit.
     * @example [
     *       "https://v3.fal.media/files/monkey/i3saq4bAPXSIl08nZtq9P_ec535747aefc4e31943136a6d8587075.png",
     *       "https://v3.fal.media/files/penguin/BCOZp6teRhSQFuOXpbBOa_da8ef9b4982347a2a62a516b737d4f21.png",
     *       "https://v3.fal.media/files/tiger/sCoZhBksx9DvwSR4_U3_C_3d1f581441874005908addeae9c10d0f.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate the image with
     * @example Close shot of a woman standing in next to this car on this highway
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_f89 {
    /**
     * Camera Movement
     * @description The type of camera movement to apply to the video
     * @enum {string}
     */
    camera_movement?:
        | 'horizontal_left'
        | 'horizontal_right'
        | 'vertical_up'
        | 'vertical_down'
        | 'zoom_in'
        | 'zoom_out'
        | 'crane_up'
        | 'quickly_zoom_in'
        | 'quickly_zoom_out'
        | 'smooth_zoom_in'
        | 'camera_rotation'
        | 'robo_arm'
        | 'super_dolly_out'
        | 'whip_pan'
        | 'hitchcock'
        | 'left_follow'
        | 'right_follow'
        | 'pan_left'
        | 'pan_right'
        | 'fix_bg';
    /**
     * Duration
     * @description The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '8';
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://v3.fal.media/files/zebra/qL93Je8ezvzQgDOEzTjKF_KhGKZTEebZcDw6T5rwQPK_output.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     * @example blurry, low quality, low resolution, pixelated, noisy, grainy, out of focus, poorly lit, poorly exposed, poorly composed, poorly framed, poorly cropped, poorly color corrected, poorly color graded
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example A woman warrior with her hammer walking with his glacier wolf.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p' | '1080p';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
    /**
     * Style
     * @description The style of the generated video
     * @enum {string}
     */
    style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk';
}

export interface SharedType_f74 {
    /**
     * Images
     * @description The generated/edited images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a860a37/ct1JcapCdZTzfNhI0-GM5.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface SharedType_f67 {
    /**
     * Auto Trim
     * @description auto trim the video, to working duration ( 5s )
     * @default true
     */
    auto_trim?: boolean;
    /**
     * Keypoints
     * @description Input keypoints [x,y] to erase or keep from the video. Format like so: {'x':100, 'y':100, 'type':'positive/negative'}
     * @example [
     *       "{'x': 765, 'y': 344, 'type': 'positive'}",
     *       "{'x': 200, 'y': 200, 'type': 'negative'}"
     *     ]
     */
    keypoints: string[];
    /**
     * Output Container And Codec
     * @description Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
     * @default mp4_h264
     * @enum {string}
     */
    output_container_and_codec?:
        | 'mp4_h265'
        | 'mp4_h264'
        | 'webm_vp9'
        | 'gif'
        | 'mov_h264'
        | 'mov_h265'
        | 'mov_proresks'
        | 'mkv_h264'
        | 'mkv_h265'
        | 'mkv_vp9'
        | 'mkv_mpeg4';
    /**
     * Preserve Audio
     * @description If true, audio will be preserved in the output video.
     * @default true
     */
    preserve_audio?: boolean;
    /**
     * Video Url
     * @description Input video to erase object from. duration must be less than 5s.
     * @example https://bria-test-images.s3.us-east-1.amazonaws.com/videos/eraser_mask/woman_right_side.mov
     */
    video_url: string;
}

export interface SharedType_f63 {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the final input image will be used.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URL of the cropped face image. Provide a close-up face photo.
     * @example [
     *       "https://v3b.fal.media/files/b/kangaroo/Tl9BsbouyruyrEJtXWYOz_ef4270d3ff4d47f18883c70cfdf07c27.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Lora Scale
     * @description The scale factor for the LoRA model. Controls the strength of the LoRA effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 6
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description Describe the full portrait you want to generate from the face. Include clothing, setting, pose, and style details.
     * @default Photography. A portrait of the person in professional attire with natural lighting
     * @example Photography. A young woman wearing a yellow dress stands in a flower field
     * @example Professional headshot with business suit and office background
     * @example Casual portrait outdoors with natural sunlight and bokeh background
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_f62 {
    /**
     * Image
     * @description The generated image file info.
     * @example {
     *       "height": 512,
     *       "file_size": 423052,
     *       "file_name": "36d3ca4791a647678b2ff01a35c87f5a.png",
     *       "content_type": "image/png",
     *       "url": "https://storage.googleapis.com/falserverless/docres_ckpt/Xssvg5K39QiD6mn9K5toF_f4942abeef8d4c7bbe236b59aed5e382.png",
     *       "width": 512
     *     }
     */
    image: Components.Image;
}

export interface SharedType_f5e {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16';
    /**
     * Auto Fix
     * @description Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
     * @default false
     */
    auto_fix?: boolean;
    /**
     * Duration
     * @description The duration of the generated video.
     * @default 8s
     * @enum {string}
     */
    duration?: '4s' | '6s' | '8s';
    /**
     * First Frame URL
     * @description URL of the first frame of the video
     * @example https://storage.googleapis.com/falserverless/example_inputs/veo31-flf2v-input-1.jpeg
     */
    first_frame_url: string;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Last Frame URL
     * @description URL of the last frame of the video
     * @example https://storage.googleapis.com/falserverless/example_inputs/veo31-flf2v-input-2.jpeg
     */
    last_frame_url: string;
    /**
     * Negative Prompt
     * @description A negative prompt to guide the video generation.
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The text prompt describing the video you want to generate
     * @example A woman looks into the camera, breathes in, then exclaims energetically, "have you guys checked out Veo3.1 First-Last-Frame-to-Video on Fal? It's incredible!"
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video.
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p' | '1080p' | '4k';
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
}

export interface SharedType_f51 {
    /**
     * Image Url
     * @description Url for the Input image.
     * @example https://raw.githubusercontent.com/facebookresearch/segment-anything-2/main/notebooks/images/truck.jpg
     */
    image_url: string;
    /**
     * Prompt
     * @description Prompt to be used for the chat completion
     * @example Could you please give me a brief description of the image? Please respond with interleaved segmentation masks for the corresponding parts of the answer.
     */
    prompt: string;
}

export interface SharedType_f15 {
    /**
     * Seed
     * @description The seed used for generating the video.
     */
    seed: number;
    /**
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/framepack/TfJPbwm6_D60dcWEv9LVX_output_video.mp4"
     *     }
     */
    video: Components.File_1;
}

export interface SharedType_f12 {
    /**
     * Video
     * @description URL of the generated video
     */
    video: Components.File;
}

export interface SharedType_f11 {
    /**
     * Create Masks
     * @description If True segmentation masks will be used in the weight the training loss. For people a face mask is used if possible.
     * @default true
     */
    create_masks?: boolean;
    /**
     * Data Archive Format
     * @description The format of the archive. If not specified, the format will be inferred from the URL.
     */
    data_archive_format?: string;
    /**
     * Images Data Url
     * @description URL to zip archive with images. Try to use at least 4 images in general the more the better.
     *
     *             In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.
     */
    images_data_url: string;
    /**
     * Is Input Format Already Preprocessed
     * @description Specifies whether the input data is already in a processed format. When set to False (default), the system expects raw input where image files and their corresponding caption files share the same name (e.g., 'photo.jpg' and 'photo.txt'). Set to True if your data is already in a preprocessed format.
     * @default false
     */
    is_input_format_already_preprocessed?: boolean;
    /**
     * Is Style
     * @description If True, the training will be for a style. This will deactivate segmentation, captioning and will use trigger word instead. Use the trigger word to specify the style.
     * @default false
     */
    is_style?: boolean;
    /**
     * Steps
     * @description Number of steps to train the LoRA on.
     * @example 1000
     */
    steps?: number;
    /**
     * Trigger Word
     * @description Trigger word to be used in the captions. If None, a trigger word will not be used.
     *             If no captions are provide the trigger_word will be used instead of captions. If captions are the trigger word will not be used.
     */
    trigger_word?: string;
}

export interface SharedType_f0b {
    /**
     * Image
     * @description Image with background removed
     * @example {
     *       "height": 1024,
     *       "file_name": "birefnet-output.png",
     *       "content_type": "image/png",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/birefnet-output.png",
     *       "width": 1024
     *     }
     */
    image: Components.ImageFile;
    /**
     * Mask Image
     * @description Mask used to remove the background
     */
    mask_image?: Components.ImageFile;
}

export interface SharedType_ef5 {
    /**
     * Apply Mask
     * @description Apply the mask on the image.
     * @default true
     */
    apply_mask?: boolean;
    /**
     * Box Prompts
     * @description Box prompt coordinates (x_min, y_min, x_max, y_max). Multiple boxes supported - use object_id to group boxes for the same object or leave empty for separate objects.
     * @default []
     */
    box_prompts?: Components.BoxPrompt[];
    /**
     * Image Url
     * @description URL of the image to be segmented
     * @example https://raw.githubusercontent.com/facebookresearch/segment-anything-2/main/notebooks/images/truck.jpg
     */
    image_url: string;
    /**
     * Include Boxes
     * @description Whether to include bounding boxes for each mask (when available).
     * @default false
     */
    include_boxes?: boolean;
    /**
     * Include Scores
     * @description Whether to include mask confidence scores.
     * @default false
     */
    include_scores?: boolean;
    /**
     * Max Masks
     * @description Maximum number of masks to return when `return_multiple_masks` is enabled.
     * @default 3
     */
    max_masks?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Point Prompts
     * @description List of point prompts
     * @default []
     */
    point_prompts?: Components.PointPrompt[];
    /**
     * Prompt
     * @description Text prompt for segmentation
     * @default wheel
     */
    prompt?: string;
    /**
     * Return Multiple Masks
     * @description If True, upload and return multiple generated masks as defined by `max_masks`.
     * @default false
     */
    return_multiple_masks?: boolean;
    /**
     * Sync Mode
     * @description If True, the media will be returned as a data URI.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Text Prompt
     * @deprecated
     * @description [DEPRECATED] Use 'prompt' instead. Kept for backward compatibility.
     */
    text_prompt?: string;
}

export interface SharedType_ee0 {
    /**
     * Speaker Embedding
     * @description The generated speaker embedding file in safetensors format.
     * @example {
     *       "file_size": 16288,
     *       "file_name": "tmpe71u7t4j.safetensors",
     *       "content_type": "application/octet-stream",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/qwen3-tts/clone_out.safetensors"
     *     }
     */
    speaker_embedding: Components.File;
}

export interface SharedType_ec2 {
    /**
     * Prompt
     * @description The text prompt used for video generation.
     * @default
     * @example A close-up of a young woman smiling gently in the rain, raindrops glistening on her face and eyelashes. The video captures the delicate details of her expression and the water droplets, with soft light reflecting off her skin in the rainy atmosphere.
     */
    prompt?: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/wan/v2.2-woman-output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_eac {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @default auto
     */
    aspect_ratio?:
        | 'auto'
        | '21:9'
        | '16:9'
        | '3:2'
        | '4:3'
        | '5:4'
        | '1:1'
        | '4:5'
        | '3:4'
        | '2:3'
        | '9:16';
    /**
     * Enable Web Search
     * @description Enable web search for the image generation task. This will allow the model to use the latest information from the web to generate the image.
     * @default false
     */
    enable_web_search?: boolean;
    /**
     * Image URLs
     * @description The URLs of the images to use for image-to-image generation or image editing.
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_inputs/nano-banana-edit-input.png",
     *       "https://storage.googleapis.com/falserverless/example_inputs/nano-banana-edit-input-2.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Limit Generations
     * @description Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.
     * @default false
     */
    limit_generations?: boolean;
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt for image editing.
     * @example make a photo of the man driving the car down the california coastline
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the image to generate.
     * @default 1K
     * @enum {string}
     */
    resolution?: '1K' | '2K' | '4K';
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_e74 {
    /**
     * Expand Prompt
     * @description Whether to expand the prompt with MagicPrompt functionality.
     * @default true
     */
    expand_prompt?: boolean;
    /**
     * Image URL
     * @description The image URL to generate an image from. Needs to match the dimensions of the mask.
     * @example https://storage.googleapis.com/falserverless/flux-lora/example-images/knight.jpeg
     */
    image_url: string;
    /**
     * Mask URL
     * @description The mask URL to inpaint the image. Needs to match the dimensions of the input image.
     * @example https://storage.googleapis.com/falserverless/flux-lora/example-images/mask_knight.jpeg
     */
    mask_url: string;
    /**
     * Prompt
     * @description The prompt to fill the masked part of the image.
     * @example A knight in shining armour holding a greatshield with "FAL" on it
     */
    prompt: string;
    /**
     * Seed
     * @description Seed for the random number generator
     */
    seed?: number;
    /**
     * Style
     * @description The style of the generated image
     * @default auto
     * @enum {string}
     */
    style?: 'auto' | 'general' | 'realistic' | 'design' | 'render_3D' | 'anime';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_e70 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1' | '4:3' | '3:4' | '21:9' | '9:21';
    /**
     * Prompt
     * @example A teddy bear in sunglasses playing electric guitar and dancing
     */
    prompt: string;
}

export interface SharedType_e60 {
    /**
     * Images
     * @description The generated image
     */
    images: Components.File[];
}

export interface SharedType_e5a {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/png",
     *         "url": "https://v3b.fal.media/files/b/0a8c18a5/1z0k9F1YLgz4qCr64jCBa_r2uqRyDg.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /**
     * Timings
     * @description The timings of the generation process.
     */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_e3b {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "file_size": 687298,
     *       "file_name": "generated_video.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/lion/j1BSX8UnGbBZeJXqSWg2E_generated_video.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_e39 {
    /**
     * Model Mesh
     * @description Generated 3D object file.
     * @example {
     *       "file_size": 720696,
     *       "file_name": "white_mesh.glb",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3.fal.media/files/lion/WqIhtKPaSoeBtC30qzIGG_white_mesh.glb"
     *     }
     */
    model_mesh: Components.File;
    /**
     * Seed
     * @description Seed value used for generation.
     */
    seed: number;
}

export interface SharedType_e34 {
    /**
     * Acceleration
     * @description The acceleration level to use for image generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance scale for classifier-free guidance.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the image to generate.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description List of LoRA weights to apply (maximum 3).
     * @default []
     */
    loras?: Components.LoRAInput_1[];
    /**
     * Negative Prompt
     * @description Negative prompt for classifier-free guidance. Describes what to avoid in the image.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A serene Japanese garden with cherry blossoms, koi pond, and traditional wooden bridge at golden hour
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI. Output is not stored when this is True.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_e18 {
    /**
     * Default Caption
     * @description Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string;
    /**
     * Image Data Url
     * @description URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
     *
     *         The zip can also contain a text file for each image. The text file should be named:
     *         ROOT.txt
     *         For example:
     *         photo.txt
     *
     *         This text file can be used to specify the edit instructions for the image pair.
     *
     *         If no text file is provided, the default_caption will be used.
     *
     *         If no default_caption is provided, the training will fail.
     */
    image_data_url: string;
    /**
     * Learning Rate
     * @description Learning rate applied to trainable parameters.
     * @default 0.00005
     */
    learning_rate?: number;
    /**
     * Output Lora Format
     * @description Dictates the naming scheme for the output weights
     * @default fal
     * @enum {string}
     */
    output_lora_format?: 'fal' | 'comfy';
    /**
     * Steps
     * @description Total number of training steps.
     * @default 1000
     */
    steps?: number;
}

export interface SharedType_e15 {
    /**
     * Acceleration
     * @description Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
     * @default regular
     * @example regular
     */
    acceleration?: 'none' | 'low' | 'regular';
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16';
    /**
     * Auto Downsample Min FPS
     * @description The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
     * @default 15
     * @example 15
     */
    auto_downsample_min_fps?: number;
    /**
     * Enable Auto Downsample
     * @description If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
     * @default false
     * @example false
     */
    enable_auto_downsample?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * First Frame URL
     * @description URL to the first frame of the video. If provided, the model will use this frame as a reference.
     */
    first_frame_url?: string;
    /**
     * Frames per Second
     * @description Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
     * @default 16
     */
    frames_per_second?: number;
    /**
     * Guidance Scale
     * @description Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
     * @default 5
     * @example 5
     */
    guidance_scale?: number;
    /**
     * Interpolator Model
     * @description The model to use for frame interpolation. Options are 'rife' or 'film'.
     * @default film
     * @example film
     * @enum {string}
     */
    interpolator_model?: 'rife' | 'film';
    /**
     * Last Frame URL
     * @description URL to the last frame of the video. If provided, the model will use this frame as a reference.
     */
    last_frame_url?: string;
    /**
     * Match Input Frames Per Second
     * @description If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
     * @default true
     * @example true
     */
    match_input_frames_per_second?: boolean;
    /**
     * Match Input Number of Frames
     * @description If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
     * @default true
     * @example true
     */
    match_input_num_frames?: boolean;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     * @example letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 81 to 241 (inclusive).
     * @default 81
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Number of Interpolated Frames
     * @description Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
     * @default 0
     * @example 0
     */
    num_interpolated_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation. Optional for reframing.
     * @default
     * @example
     */
    prompt?: string;
    /**
     * Resolution
     * @description Resolution of the generated video.
     * @default auto
     * @enum {string}
     */
    resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p';
    /**
     * Return Frames Zip
     * @description If true, also return a ZIP file containing all generated frames.
     * @default false
     * @example false
     */
    return_frames_zip?: boolean;
    /**
     * Sampler
     * @description Sampler to use for video generation.
     * @default unipc
     * @example unipc
     * @enum {string}
     */
    sampler?: 'unipc' | 'dpm++' | 'euler';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift parameter for video generation.
     * @default 5
     */
    shift?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     * @example false
     */
    sync_mode?: boolean;
    /**
     * Temporal Downsample Factor
     * @description Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
     * @default 0
     * @example 0
     */
    temporal_downsample_factor?: number;
    /**
     * Transparency Mode
     * @description The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
     * @default content_aware
     * @example content_aware
     * @enum {string}
     */
    transparency_mode?: 'content_aware' | 'white' | 'black';
    /**
     * Trim Borders
     * @description Whether to trim borders from the video.
     * @default true
     * @example true
     */
    trim_borders?: boolean;
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video URL
     * @description URL to the source video file. This video will be used as a reference for the reframe task.
     * @example https://storage.googleapis.com/falserverless/web-examples/wan/t2v.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
    /**
     * Zoom Factor
     * @description Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.
     * @default 0
     * @example 0
     */
    zoom_factor?: number;
}

export interface SharedType_e01 {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/gallery/wan-i2v-example.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_df4 {
    /**
     * Image
     * @description The upscaled image.
     */
    image: Components.File;
}

export interface SharedType_df1 {
    /**
     * Guidance Scale
     * @description Guidance scale for the model.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Input Image Url
     * @description URL of image to use while generating the 3D model.
     * @example https://storage.googleapis.com/falserverless/model_tests/video_models/robot.png
     */
    input_image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps to perform.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Octree Resolution
     * @description Octree resolution for the model.
     * @default 256
     */
    octree_resolution?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Textured Mesh
     * @description If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.
     * @default false
     */
    textured_mesh?: boolean;
}

export interface SharedType_de9 {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16';
    /**
     * Auto Fix
     * @description Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
     * @default true
     */
    auto_fix?: boolean;
    /**
     * Duration
     * @description The duration of the generated video.
     * @default 8s
     * @enum {string}
     */
    duration?: '4s' | '6s' | '8s';
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Negative Prompt
     * @description A negative prompt to guide the video generation.
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The text prompt describing the video you want to generate
     * @example Two person street interview in New York City.
     *     Sample Dialogue:
     *     Host: "Did you hear the news?"
     *     Person: "Yes! Veo 3.1 is now available on fal. If you want to see it, go check their website."
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video.
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p' | '1080p' | '4k';
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
}

export interface SharedType_de5 {
    /**
     * Image
     * @description The edited image with content erased
     * @example {
     *       "url": "https://v3.fal.media/files/penguin/PpROj5BGoWMj0H6wb11aG_output.jpg"
     *     }
     */
    image: Components.File;
    /**
     * Used Seed
     * @description Seed used for generation
     */
    used_seed: number;
}

export interface SharedType_dd1 {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "file_size": 4060052,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://fal.media/files/tiger/8V9H8RLyFiWjmJDOxGbcG_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_dca {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '4:3' | '3:4' | '21:9' | '9:21';
    /**
     * Duration
     * @description The duration of the generated video
     * @default 5s
     * @enum {string}
     */
    duration?: '5s' | '9s';
    /**
     * End Image Url
     * @description Final image to end the video with. Can be used together with image_url.
     */
    end_image_url?: string;
    /**
     * Image Url
     * @description Initial image to start the video from. Can be used together with end_image_url.
     * @example https://fal.media/files/elephant/8kkhB12hEZI2kkbU8pZPA_test.jpeg
     */
    image_url?: string;
    /**
     * Loop
     * @description Whether the video should loop (end of video is blended with the beginning)
     * @default false
     */
    loop?: boolean;
    /**
     * Prompt
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video (720p costs 2x more, 1080p costs 4x more)
     * @default 540p
     * @enum {string}
     */
    resolution?: '540p' | '720p' | '1080p';
}

export interface SharedType_dc7 {
    /** @description Generated image. */
    image: Components.Image_2;
}

export interface SharedType_dc6 {
    /**
     * Audio Setting
     * @description Audio configuration settings
     */
    audio_setting?: Components.AudioSetting;
    /**
     * Language Boost
     * @description Enhance recognition of specified languages and dialects
     * @enum {string}
     */
    language_boost?:
        | 'Persian'
        | 'Filipino'
        | 'Tamil'
        | 'Chinese'
        | 'Chinese,Yue'
        | 'English'
        | 'Arabic'
        | 'Russian'
        | 'Spanish'
        | 'French'
        | 'Portuguese'
        | 'German'
        | 'Turkish'
        | 'Dutch'
        | 'Ukrainian'
        | 'Vietnamese'
        | 'Indonesian'
        | 'Japanese'
        | 'Italian'
        | 'Korean'
        | 'Thai'
        | 'Polish'
        | 'Romanian'
        | 'Greek'
        | 'Czech'
        | 'Finnish'
        | 'Hindi'
        | 'Bulgarian'
        | 'Danish'
        | 'Hebrew'
        | 'Malay'
        | 'Slovak'
        | 'Swedish'
        | 'Croatian'
        | 'Hungarian'
        | 'Norwegian'
        | 'Slovenian'
        | 'Catalan'
        | 'Nynorsk'
        | 'Afrikaans'
        | 'auto';
    /**
     * Output Format
     * @description Format of the output content (non-streaming only)
     * @default hex
     * @enum {string}
     */
    output_format?: 'url' | 'hex';
    /**
     * Pronunciation Dict
     * @description Custom pronunciation dictionary for text replacement
     */
    pronunciation_dict?: Components.PronunciationDict;
    /**
     * Text
     * @description Text to convert to speech (max 5000 characters, minimum 1 non-whitespace character)
     * @example Hello world! This is a test of the text-to-speech system.
     */
    text: string;
    /**
     * Voice Setting
     * @description Voice configuration settings
     * @default {
     *       "speed": 1,
     *       "vol": 1,
     *       "voice_id": "Wise_Woman",
     *       "pitch": 0,
     *       "english_normalization": false
     *     }
     */
    voice_setting?: Components.VoiceSetting;
}

export interface SharedType_dbd {
    /**
     * Images
     * @description The generated/edited images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/panda/Y5wKKIEuFpRMEUQ8ZPy01.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface SharedType_d99 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Enhance Prompt
     * @description Whether to enhance the prompt for better results.
     * @default false
     */
    enhance_prompt?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word "FLUX" is painted over it in big, white brush strokes with visible texture.
     */
    prompt: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_d98 {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "file_size": 3910577,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3.fal.media/files/penguin/twy6u1yv09NvqsX0mMFM2_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_d4a {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/jpeg",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/kontext_example_output.jpeg",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_d35 {
    /**
     * Audio Url
     * @description URL of the audio to use as the background music. Must be publicly accessible.
     *     Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds),
     *     the audio is truncated to the first N seconds, and the rest is discarded. If
     *     the audio is shorter than the video, the remaining part of the video will be silent.
     *     For example, if the audio is 3 seconds long and the video duration is 5 seconds, the
     *     first 3 seconds of the output video will have sound, and the last 2 seconds will be silent.
     *     - Format: WAV, MP3.
     *     - Duration: 3 to 30 s.
     *     - File size: Up to 15 MB.
     */
    audio_url?: string;
    /**
     * Duration
     * @description Duration of the generated video in seconds. Choose between 5, 10 or 15 seconds.
     * @default 5
     * @example 5
     * @example 10
     * @example 15
     * @enum {string}
     */
    duration?: '5' | '10' | '15';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt rewriting using LLM.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Image URL
     * @description URL of the image to use as the first frame. Must be publicly accessible or base64 data URI. Image dimensions must be between 240 and 7680.
     * @example https://v3b.fal.media/files/b/0a8673dd/m9EV5W9aSqg8J7rb-18TK.png
     */
    image_url: string;
    /**
     * Multi Shots
     * @description When true, enables intelligent multi-shot segmentation. Only active when enable_prompt_expansion is True. Set to false for single-shot generation.
     * @default false
     */
    multi_shots?: boolean;
    /**
     * Negative Prompt
     * @description Negative prompt to describe content to avoid. Max 500 characters.
     * @default
     * @example low resolution, error, worst quality, low quality, defects
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The text prompt describing the desired video motion. Max 800 characters.
     * @example A comedic cinematic demo where typed prompts physically transform reality. Photoreal, strong match cuts, coherent main character, no subtitles.
     *
     *     Shot 1 [0-4s] Continue from first frame. The creator presses "PRINT". The machine clunks like a spaceship. Creator whispers: "Okay… I'm pressing enter."
     *     Shot 2 [4-8s] Smash cut: the printed paper flies into the air and unfolds into a full desert canyon scene around the desk, like reality is being unrolled. Creator says: "Wait—my prompt has physics?"
     *     Shot 3 [8-12s] Hard cut: the paper tears and reveals a tropical jungle behind it, perfectly lit, cinematic sun. Creator laughs: "This is exactly why we do AI."
     *     Shot 4 [12-15s] Hard cut back to studio. The printer prints a final line (not shown clearly). Creator looks to camera: "Multi-scene. Single prompt."
     */
    prompt: string;
    /**
     * Resolution
     * @description Video resolution. Valid values: 720p, 1080p
     * @default 1080p
     * @enum {string}
     */
    resolution?: '720p' | '1080p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface SharedType_d22 {
    /**
     * Images
     * @description The generated/edited images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/panda/6r8XojqbZvFPhdizajCb3.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface SharedType_d19 {
    /**
     * Boundingbox Frames Zip
     * @description Zip file containing per-frame bounding box overlays.
     */
    boundingbox_frames_zip?: Components.File;
    /**
     * Video
     * @description The segmented video.
     * @example https://fal.media/files/monkey/5BLHmbX3qxu5cD5gQzTqw_output.mp4
     */
    video: Components.File;
}

export interface SharedType_d0e {
    /**
     * Video
     * @description The generated video.
     * @example {
     *       "file_size": 27588984,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/koala/knryyyGF3ZVyMMrGr77CL_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_cfd {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?:
        | '10:16'
        | '16:10'
        | '9:16'
        | '16:9'
        | '4:3'
        | '3:4'
        | '1:1'
        | '1:3'
        | '3:1'
        | '3:2'
        | '2:3';
    /**
     * Expand Prompt
     * @description Whether to expand the prompt with MagicPrompt functionality.
     * @default true
     */
    expand_prompt?: boolean;
    /**
     * Prompt
     * @example A comic style illustration of a skeleton sitting on a toilet in a bathroom. The bathroom has a Halloween decoration with a pumpkin jack-o-lantern and bats flying around. There is a text above the skeleton that says "Just Waiting for Halloween with Ideogram 2.0 at fal.ai"
     */
    prompt: string;
    /**
     * Seed
     * @description Seed for the random number generator
     */
    seed?: number;
    /**
     * Style
     * @description The style of the generated image
     * @default auto
     * @enum {string}
     */
    style?: 'auto' | 'general' | 'realistic' | 'design' | 'render_3D' | 'anime';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_cd2 {
    /**
     * Enable Output Safety Checker
     * @description If set to true, output video will be checked for safety after generation.
     * @default false
     * @example false
     */
    enable_output_safety_checker?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 1
     * @example 1
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://v3b.fal.media/files/b/panda/-oMlZo9Yyj_Nzoza_tgds_GmLF86r5bOt50eMMKCszy_eacc949b3933443c9915a83c98fbe85e.png
     */
    image_url: string;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 20
     * @example 6
     */
    num_inference_steps?: number;
    /**
     * Resolution
     * @description Resolution of the generated video (480p, 580p, or 720p).
     * @default 480p
     * @example 480p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Return Frames ZIP
     * @description If true, also return a ZIP archive containing per-frame images generated on GPU (lossless).
     * @default false
     */
    return_frames_zip?: boolean;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift value for the video. Must be between 1.0 and 10.0.
     * @default 5
     * @example 8
     */
    shift?: number;
    /**
     * Use Turbo
     * @description If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized for best results.
     * @default false
     * @example true
     */
    use_turbo?: boolean;
    /**
     * Video Quality
     * @description The quality of the output video. Higher quality means better visual quality but larger file size.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video URL
     * @description URL of the input video.
     * @example https://v3b.fal.media/files/b/panda/a6SvJg96V8eoglMlYFShU_5385885-hd_1080_1920_25fps.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface SharedType_cc5 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video frame. If 'auto', the aspect ratio will be determined automatically based on the input video, and the closest aspect ratio to the input video will be used.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Duration
     * @description Video duration in seconds.
     * @default 5
     * @enum {string}
     */
    duration?: '3' | '4' | '5' | '6' | '7' | '8' | '9' | '10';
    /**
     * Elements
     * @description Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.
     * @example [
     *       {
     *         "reference_image_urls": [
     *           "https://v3b.fal.media/files/b/kangaroo/YMpmQkYt9xugpOTQyZW0O.png",
     *           "https://v3b.fal.media/files/b/zebra/d6ywajNyJ6bnpa_xBue-K.png"
     *         ],
     *         "frontal_image_url": "https://v3b.fal.media/files/b/panda/MQp-ghIqshvMZROKh9lW3.png"
     *       }
     *     ]
     */
    elements?: Components.OmniVideoElementInput[];
    /**
     * Image Urls
     * @description Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
     * @example []
     */
    image_urls?: string[];
    /**
     * Keep Audio
     * @description Whether to keep the original audio from the video.
     * @default false
     */
    keep_audio?: boolean;
    /**
     * Prompt
     * @description Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
     * @example Based on @Video1, generate the next shot. keep the style of the video
     */
    prompt: string;
    /**
     * Video Url
     * @description Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB.
     *
     *     Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s
     * @example https://v3b.fal.media/files/b/panda/oVdiICFXY03Vbam-08Aj8_output.mp4
     */
    video_url: string;
}

export interface SharedType_cbd {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the final input image will be used.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URLs of the images to edit. Provide an image with a white or clean background.
     * @example [
     *       "https://v3b.fal.media/files/b/rabbit/YN3dXLQBWb2ch6V607Uuc_d808599bb92f4c808502a118697bdc1f.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Lora Scale
     * @description The scale factor for the LoRA model. Controls the strength of the LoRA effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 6
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description Describe the background/scene you want to add behind the object. The model will remove the white background and add the specified environment.
     * @default Remove white background and add a realistic scene behind the object
     * @example Add an outdoor scene with mountains and road behind the car
     * @example Add a modern living room background behind the product
     * @example Add a natural outdoor setting with grass and trees as background
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_cbb {
    /**
     * Output
     * @description Generated output from video processing
     * @example that's the way I look at it and I don't know what you would say. Sooner or later the child gets run over.
     *     They seem to be too local, too provincial.
     */
    output: string;
    /**
     * @description Token usage information
     * @example {
     *       "completion_tokens": 100,
     *       "total_tokens": 1100,
     *       "prompt_tokens": 1000,
     *       "cost": 0.0005
     *     }
     */
    usage: Components.UsageInfo;
}

export interface SharedType_cb4 {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the final input image will be used.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URL of the image to restore lighting for.
     * @example [
     *       "https://v3b.fal.media/files/b/0a860a2e/L4v5FJm9lwFGGdRY2P7tb.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 6
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_ca4 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video frame.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Duration
     * @description Video duration in seconds.
     * @default 5
     * @enum {string}
     */
    duration?: '3' | '4' | '5' | '6' | '7' | '8' | '9' | '10';
    /**
     * Elements
     * @description Elements (characters/objects) to include in the video. Reference in prompt as @Element1, @Element2, etc. Maximum 7 total (elements + reference images + start image).
     * @example [
     *       {
     *         "reference_image_urls": [
     *           "https://v3b.fal.media/files/b/kangaroo/YMpmQkYt9xugpOTQyZW0O.png",
     *           "https://v3b.fal.media/files/b/zebra/d6ywajNyJ6bnpa_xBue-K.png"
     *         ],
     *         "frontal_image_url": "https://v3b.fal.media/files/b/panda/MQp-ghIqshvMZROKh9lW3.png"
     *       },
     *       {
     *         "reference_image_urls": [
     *           "https://v3b.fal.media/files/b/kangaroo/EBF4nWihspyv4pp6hgj7D.png"
     *         ],
     *         "frontal_image_url": "https://v3b.fal.media/files/b/koala/gSnsA7HJlgcaTyR5Ujj2H.png"
     *       }
     *     ]
     */
    elements?: Components.OmniVideoElementInput[];
    /**
     * Image Urls
     * @description Additional reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 7 total (elements + reference images + start image).
     * @example [
     *       "https://v3b.fal.media/files/b/koala/v9COzzH23FGBYdGLgbK3u.png",
     *       "https://v3b.fal.media/files/b/elephant/5Is2huKQFSE7A7c5uUeUF.png"
     *     ]
     */
    image_urls?: string[];
    /**
     * Prompt
     * @description Take @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
     * @example Take @Image1 as the start frame. Start with a high-angle satellite view of the ancient greenhouse ruin surrounded by nature. The camera swoops down and flies inside the building, revealing the character from @Element1 standing in the sun-drenched center. The camera then seamlessly transitions into a smooth 180-degree orbit around the character, moving to the back view. As the open backpack comes into focus, the camera continues to push forward, zooming deep inside the bag to reveal the glowing stone from @Element2 nestled inside. Cinematic lighting, hopeful atmosphere, 35mm lens. Make sure to keep it as the style of @Image2.
     */
    prompt: string;
}

export interface SharedType_c85 {
    /**
     * Audio Url
     * @description The URL of the audio file.
     * @example https://v3.fal.media/files/rabbit/9_0ZG_geiWjZOmn9yscO6_output.mp3
     */
    audio_url: string;
    /**
     * Image Url
     * @description The URL of the image to use as your avatar
     * @example https://storage.googleapis.com/falserverless/example_inputs/kling_ai_avatar_input.jpg
     */
    image_url: string;
    /**
     * Prompt
     * @description The prompt to use for the video generation.
     * @default .
     */
    prompt?: string;
}

export interface SharedType_c7e {
    /**
     * Audio
     * @description The generated music
     * @example {
     *       "url": "https://v3.fal.media/files/lion/b3-wJ5bbmVo8S-KPqDBMK_output.mp3"
     *     }
     */
    audio: Components.File;
}

export interface SharedType_c4e {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/web-examples/wan/t2v.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_c46 {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "file_size": 3232402,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://fal.media/files/koala/awGY1lJd7lVsqQeSqjWqn_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_c451 {
    /**
     * Debug Latents
     * @description The latents saved for debugging.
     */
    debug_latents?: Components.File;
    /**
     * Debug Per Pass Latents
     * @description The latents saved for debugging per pass.
     */
    debug_per_pass_latents?: Components.File;
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     */
    images: Components.Image[];
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
}

export interface SharedType_c45 {
    /**
     * Audio Url
     * Format: uri
     * @example https://v3.fal.media/files/elephant/Oz_g4AwQvXtXpUHL3Pa7u_Hope.mp3
     */
    audio_url: string;
    /**
     * Image Url
     * Format: uri
     * @example https://v3.fal.media/files/koala/NLVPfOI4XL1cWT2PmmqT3_Hope.png
     */
    image_url: string;
    /**
     * Resolution
     * @description Resolution
     * @enum {string}
     */
    resolution: '720p' | '480p';
}

export interface SharedType_c3a {
    /**
     * CFG Scale
     * @description CFG (Classifier-Free Guidance) scale for generation. Higher values increase adherence to text.
     * @default 1.3
     */
    cfg_scale?: number;
    /**
     * Script
     * @description The script to convert to speech. Can be formatted with 'Speaker X:' prefixes for multi-speaker dialogues.
     * @example Speaker 0: VibeVoice is now available on Fal. Isn't that right, Carter?
     *     Speaker 1: That's right Frank, and it supports up to four speakers at once. Try it now!
     */
    script: string;
    /**
     * Seed
     * @description Random seed for reproducible generation.
     */
    seed?: number;
    /**
     * Speakers
     * @description List of speakers to use for the script. If not provided, will be inferred from the script or voice samples.
     * @example [
     *       {
     *         "preset": "Frank [EN]"
     *       },
     *       {
     *         "preset": "Carter [EN]"
     *       }
     *     ]
     */
    speakers: Components.VibeVoiceSpeaker[];
}

export interface SharedType_bf2 {
    /**
     * Image
     * @description Processed image
     */
    image?: Components.Image;
    /**
     * Results
     * @description Results from the model
     */
    results: Components.PolygonOutput;
}

export interface SharedType_bda {
    /**
     * Elements
     * @description Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.
     * @example [
     *       {
     *         "reference_image_urls": [
     *           "https://v3b.fal.media/files/b/kangaroo/YMpmQkYt9xugpOTQyZW0O.png",
     *           "https://v3b.fal.media/files/b/zebra/d6ywajNyJ6bnpa_xBue-K.png"
     *         ],
     *         "frontal_image_url": "https://v3b.fal.media/files/b/panda/MQp-ghIqshvMZROKh9lW3.png"
     *       }
     *     ]
     */
    elements?: Components.OmniVideoElementInput[];
    /**
     * Image Urls
     * @description Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
     * @example [
     *       "https://v3b.fal.media/files/b/lion/MKvhFko5_wYnfORYacNII_AgPt8v25Wt4oyKhjnhVK5.png"
     *     ]
     */
    image_urls?: string[];
    /**
     * Keep Audio
     * @description Whether to keep the original audio from the video.
     * @default false
     */
    keep_audio?: boolean;
    /**
     * Prompt
     * @description Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
     * @example Replace the character in the video with @Element1, maintaining the same movements and camera angles. Transform the landscape into @Image1
     */
    prompt: string;
    /**
     * Video Url
     * @description Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB.
     *
     *     Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s
     * @example https://v3b.fal.media/files/b/rabbit/ku8_Wdpf-oTbGRq4lB5DU_output.mp4
     */
    video_url: string;
}

export interface SharedType_bd3 {
    /**
     * Prompt
     * @description The text prompt used for video generation.
     * @default
     * @example The white dragon warrior stands still, eyes full of determination and strength. The camera slowly moves closer or circles around the warrior, highlighting the powerful presence and heroic spirit of the character.
     */
    prompt?: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://v3.fal.media/files/lion/Fbuh3lO_HMT-pS0DATbio_tmp08c3v477.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_bc9 {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     */
    images: Components.Image_1[];
    /**
     * Number of Images
     * @description The number of images generated.
     */
    num_images: number;
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_bb3 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '4:3' | '3:4' | '21:9' | '9:21';
    /**
     * Duration
     * @description The duration of the generated video (9s costs 2x more)
     * @default 5s
     * @enum {string}
     */
    duration?: '5s' | '9s';
    /**
     * Loop
     * @description Whether the video should loop (end of video is blended with the beginning)
     * @default false
     */
    loop?: boolean;
    /**
     * Prompt
     * @example A herd of wild horses galloping across a dusty desert plain under a blazing midday sun, their manes flying in the wind; filmed in a wide tracking shot with dynamic motion, warm natural lighting, and an epic.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video (720p costs 2x more, 1080p costs 4x more)
     * @default 540p
     * @enum {string}
     */
    resolution?: '540p' | '720p' | '1080p';
}

export interface SharedType_bb0 {
    /**
     * Image URL
     * @description URL of the image to be processed
     * @example https://llava-vl.github.io/static/images/monalisa.jpg
     */
    image_url: string;
    /**
     * Object
     * @description Object to be detected in the image
     */
    object: string;
}

export interface SharedType_baf {
    /**
     * Auto Trim
     * @description auto trim the video, to working duration ( 5s )
     * @default true
     */
    auto_trim?: boolean;
    /**
     * Output Container And Codec
     * @description Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
     * @default mp4_h264
     * @enum {string}
     */
    output_container_and_codec?:
        | 'mp4_h265'
        | 'mp4_h264'
        | 'webm_vp9'
        | 'gif'
        | 'mov_h264'
        | 'mov_h265'
        | 'mov_proresks'
        | 'mkv_h264'
        | 'mkv_h265'
        | 'mkv_vp9'
        | 'mkv_mpeg4';
    /**
     * Preserve Audio
     * @description If true, audio will be preserved in the output video.
     * @default true
     */
    preserve_audio?: boolean;
    /**
     * Prompt
     * @description Input prompt to detect object to erase
     * @example women
     */
    prompt: string;
    /**
     * Video Url
     * @description Input video to erase object from. duration must be less than 5s.
     * @example https://bria-test-images.s3.us-east-1.amazonaws.com/videos/eraser_mask/woman_right_side.mov
     */
    video_url: string;
}

export interface SharedType_ba2 {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *             the model to stick to your prompt when looking for a related image to show you.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The URL of the image to generate an image from.
     * @example https://fal.media/files/koala/Chls9L2ZnvuipUTEwlnJC.png
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A cat dressed as a wizard with a background of a mystic forest.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength of the initial image. Higher strength values are better for this model.
     * @default 0.95
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_b96 {
    /**
     * Fbx File
     * @description Generated FBX animation file.
     * @example {
     *       "url": "https://v3b.fal.media/files/b/0a885f1e/JytCv1tbq28iJmvXLrpbQ_20251230_112744828_2a84e993_000.fbx"
     *     }
     */
    fbx_file?: Components.File;
    /**
     * Motion Json
     * @description Generated motion data as JSON.
     */
    motion_json?: Components.File;
    /**
     * Seed
     * @description Seed used for generation.
     * @example 42
     */
    seed: number;
}

export interface SharedType_b8b {
    /** @description URL to the configuration file for the trained model. */
    config_file: Components.File_1;
    /** @description URL to the trained diffusers lora weights. */
    diffusers_lora_file: Components.File_1;
}

export interface SharedType_b88 {
    /**
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/krea_wan_14b_v2v_output.mp4"
     *     }
     */
    video: Components.File_1;
}

export interface SharedType_b85 {
    /**
     * Prompt
     * @description The prompt used for generation.
     * @example Woman walking on a street in Tokyo
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/ltx-v095_extend.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_b81 {
    /**
     * Image
     * @description The edited image
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/video_models/general-1-2025-09-09T10_20_19Z.png"
     *     }
     */
    image: Components.Image;
}

export interface SharedType_b7d {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative prompt
     * @description Negative Prompt for generation.
     * @default
     * @example Blurry, out of focus, low resolution, bad anatomy, ugly, deformed, poorly drawn, extra limbs
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Mount Fuji at sunset, with the iconic snow-capped peak silhouetted against a vibrant orange and purple sky. A tranquil lake in the foreground perfectly reflects the mountain and colorful sky. A few traditional Japanese cherry blossom trees frame the scene, with their delicate pink petals visible in the foreground.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_b69 {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/monkey/cNaoxPl0YAWYb-QVBvO9F_image.png"
     *       }
     *     ]
     */
    images: Components.File_1[];
    /**
     * Seed
     * @description Seed used for the random number generator
     * @example 123456
     */
    seed: number;
}

export interface SharedType_b61 {
    /** @description Image with detected objects */
    image: Components.Image_2;
    /**
     * Objects
     * @description Objects detected in the image
     */
    objects: {
        [key: string]: { [x: string]: any } | null;
    }[];
}

export interface SharedType_b5a {
    /**
     * Prompt
     * @description The text prompt used for video generation.
     * @default
     * @example A medium shot establishes a modern, minimalist office setting: clean lines, muted grey walls, and polished wood surfaces. The focus shifts to a close-up on a woman in sharp, navy blue business attire. Her crisp white blouse contrasts with the deep blue of her tailored suit jacket. The subtle texture of the fabric is visible—a fine weave with a slight sheen. Her expression is serious, yet engaging, as she speaks to someone unseen just beyond the frame. Close-up on her eyes, showing the intensity of her gaze and the fine lines around them that hint at experience and focus. Her lips are slightly parted, as if mid-sentence. The light catches the subtle highlights in her auburn hair, meticulously styled. Note the slight catch of light on the silver band of her watch. High resolution 4k
     */
    prompt?: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/wan/v2.2-small-output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_b59 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the reframed image
     * @example 16:9
     * @enum {string}
     */
    aspect_ratio: '1:1' | '16:9' | '9:16' | '4:3' | '3:4' | '21:9' | '9:21';
    /**
     * Grid Position X
     * @description X position of the grid for reframing
     */
    grid_position_x?: number;
    /**
     * Grid Position Y
     * @description Y position of the grid for reframing
     */
    grid_position_y?: number;
    /**
     * Image Url
     * @description URL of the input image to reframe
     * @example https://storage.googleapis.com/falserverless/gallery/example_inputs_liuyifei.png
     */
    image_url: string;
    /**
     * Prompt
     * @description Optional prompt for reframing
     */
    prompt?: string;
    /**
     * X End
     * @description End X coordinate for reframing
     */
    x_end?: number;
    /**
     * X Start
     * @description Start X coordinate for reframing
     */
    x_start?: number;
    /**
     * Y End
     * @description End Y coordinate for reframing
     */
    y_end?: number;
    /**
     * Y Start
     * @description Start Y coordinate for reframing
     */
    y_start?: number;
}

export interface SharedType_b37 {
    /**
     * Video
     * @description The generated video.
     * @example {
     *       "file_size": 47359974,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/panda/oVdiICFXY03Vbam-08Aj8_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_b2d {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/kling/kling_i2v_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_b29 {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "file_size": 515275,
     *       "file_name": "74af6c0bdd6041c3b1130d54885e3eee.mp4",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3.fal.media/files/kangaroo/z6VqUwNTwzuWa6YE1g7In_74af6c0bdd6041c3b1130d54885e3eee.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_b1c {
    /**
     * Audio
     * @description The generated audio file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/ace-step-text-to-audio.wav"
     *     }
     */
    audio: Components.File;
    /**
     * Lyrics
     * @description The lyrics used in the generation process.
     * @example [inst]
     */
    lyrics: string;
    /**
     * Seed
     * @description The random seed used for the generation process.
     * @example 42
     */
    seed: number;
    /**
     * Tags
     * @description The genre tags used in the generation process.
     * @example lofi, hiphop, drum and bass, trap, chill
     */
    tags: string;
}

export interface SharedType_b08 {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the final input image will be used.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URL of the image with lighting/shadows to remove.
     * @example [
     *       "https://v3b.fal.media/files/b/panda/J0XyFgb0AAgyUzmVFd0nr_5363c66361d94cea89333795d700165d.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 6
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_b03 {
    /**
     * Image Url
     * @description Optional URL of the first frame image for modification
     * @example https://fal.media/files/koala/Kv2821G03ggpKK2AiZX71_d5fa7bacf06049cfaeb9588f6003b6d5.jpg
     */
    image_url?: string;
    /**
     * Mode
     * @description Amount of modification to apply to the video, adhere_1 is the least amount of modification, reimagine_3 is the most
     * @default flex_1
     * @enum {string}
     */
    mode?:
        | 'adhere_1'
        | 'adhere_2'
        | 'adhere_3'
        | 'flex_1'
        | 'flex_2'
        | 'flex_3'
        | 'reimagine_1'
        | 'reimagine_2'
        | 'reimagine_3';
    /**
     * Prompt
     * @description Instruction for modifying the video
     */
    prompt?: string;
    /**
     * Video Url
     * @description URL of the input video to modify
     * @example https://v3.fal.media/files/zebra/9aDde3Te2kuJYHdR0Kz8R_output.mp4
     */
    video_url: string;
}

export interface SharedType_aed {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?:
        | '10:16'
        | '16:10'
        | '9:16'
        | '16:9'
        | '4:3'
        | '3:4'
        | '1:1'
        | '1:3'
        | '3:1'
        | '3:2'
        | '2:3';
    /**
     * Expand Prompt
     * @description Whether to expand the prompt with MagicPrompt functionality.
     * @default true
     */
    expand_prompt?: boolean;
    /**
     * Image URL
     * @description The image URL to remix
     * @example https://fal.media/files/lion/FHOx4y4a0ef7Sgmo-sOUR_image.png
     */
    image_url: string;
    /**
     * Prompt
     * @description The prompt to remix the image with
     * @example An ice field in north atlantic
     */
    prompt: string;
    /**
     * Seed
     * @description Seed for the random number generator
     */
    seed?: number;
    /**
     * Strength
     * @description Strength of the input image in the remix
     * @default 0.8
     */
    strength?: number;
    /**
     * Style
     * @description The style of the generated image
     * @default auto
     * @enum {string}
     */
    style?: 'auto' | 'general' | 'realistic' | 'design' | 'render_3D' | 'anime';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_ade {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "file_size": 35299865,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/0a875336/8p3rFiXtx3fE2TLoh59KP_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_ad1 {
    /** @description Generated image. */
    image: Components.Image_2;
    /**
     * Images
     * @description Generated images.
     * @default []
     */
    images?: Components.Image_2[];
    /**
     * Structured Instruction
     * @description Current instruction.
     */
    structured_instruction: {
        [key: string]: { [x: string]: any } | null;
    };
}

export interface SharedType_ac9 {
    /**
     * Audio
     * @description The generated audio file
     * @example {
     *       "url": "https://fal.media/files/kangaroo/kojPUCNZ9iUGFGMR-xb7h_speech.mp3"
     *     }
     */
    audio: Components.File;
    /**
     * Duration Ms
     * @description Duration of the audio in milliseconds
     */
    duration_ms: number;
}

export interface SharedType_ab0 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the reframed image
     * @example 16:9
     * @enum {string}
     */
    aspect_ratio: '1:1' | '16:9' | '9:16' | '4:3' | '3:4' | '21:9' | '9:21';
    /**
     * Image Url
     * @description URL of the input image to reframe
     * @example https://storage.googleapis.com/falserverless/gallery/example_inputs_liuyifei.png
     */
    image_url: string;
    /**
     * Prompt
     * @description Instruction for modifying the image
     * @example Make the image look like a painting
     */
    prompt?: string;
    /**
     * Strength
     * @description The strength of the initial image. Higher strength values are corresponding to more influence of the initial image on the output.
     * @example 0.8
     */
    strength: number;
}

export interface SharedType_a97 {
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "file_size": 3731290,
     *         "height": 1536,
     *         "file_name": "257cf8e7bd3a47c2959396343d5b38cf.png",
     *         "content_type": "image/png",
     *         "url": "https://v3.fal.media/files/tiger/48e63e0K6C9XQYBuomoU-_257cf8e7bd3a47c2959396343d5b38cf.png",
     *         "width": 1536
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Seed
     * @description Seed value used for generation.
     */
    seed: number;
}

export interface SharedType_a95 {
    /**
     * Audio
     * @description The generated audio file containing the speech
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/vibevoice.mp3"
     *     }
     */
    audio: Components.File;
    /**
     * Duration
     * @description Duration of the generated audio in seconds
     * @example 9.46
     */
    duration: number;
    /**
     * Generation Time
     * @description Time taken to generate the audio in seconds
     * @example 5.6
     */
    generation_time: number;
    /**
     * Rtf
     * @description Real-time factor (generation_time / audio_duration). Lower is better.
     * @example 0.53
     */
    rtf: number;
    /**
     * Sample Rate
     * @description Sample rate of the generated audio
     * @example 24000
     */
    sample_rate: number;
}

export interface SharedType_a92 {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/hyvideo_v15_480p_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_a8f {
    /**
     * Image Url
     * @description The URL of the image to be processed.
     * @example https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg
     * @example http://ecx.images-amazon.com/images/I/51UUzBDAMsL.jpg
     */
    image_url: string;
    /**
     * Text Input
     * @description Text input for the task
     */
    text_input: string;
}

export interface SharedType_a8a {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video frame
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Cfg Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt.
     * @default 0.5
     */
    cfg_scale?: number;
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse.
     */
    prompt: string;
}

export interface SharedType_a89 {
    /**
     * Input Video
     * @description Optional: normalized/processed input video (if produced by the pipeline).
     */
    input_video?: Components.File;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example https://v3b.fal.media/files/b/0a8715c9/x378fHboeiGD6j_0nbWlJ_gen.mp4
     */
    video: Components.File;
    /**
     * Viz Video
     * @description Optional: visualization/debug video (if produced by the pipeline).
     */
    viz_video?: Components.File;
}

export interface SharedType_a88 {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     */
    images: Components.Image_1[];
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_a77 {
    /**
     * Acceleration
     * @description Controls the speed/accuracy trade-off. 'none' = best accuracy (1.12s chunks, ~7.16% WER), 'low' = balanced (0.56s chunks, ~7.22% WER), 'medium' = faster (0.16s chunks, ~7.84% WER), 'high' = fastest (0.08s chunks, ~8.53% WER).
     * @default none
     * @enum {string}
     */
    acceleration?: 'none' | 'low' | 'medium' | 'high';
    /**
     * Audio URL
     * @description URL of the audio file.
     * @example https://storage.googleapis.com/falserverless/canary/18e15559-ab3e-4f96-9583-be5ddde91e43.mp3
     */
    audio_url: string;
}

export interface SharedType_a73 {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_a66 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '4:3' | '1:1' | '3:4' | '9:16';
    /**
     * Duration
     * @description The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '8';
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     * @example blurry, low quality, low resolution, pixelated, noisy, grainy, out of focus, poorly lit, poorly exposed, poorly composed, poorly framed, poorly cropped, poorly color corrected, poorly color graded
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example Epic low-cut camera capture of a girl clad in ultraviolet threads, Peter Max art style depiction, luminous diamond skin glistening under a vast moon's radiance, embodied in a superhuman flight among mystical ruins, symbolizing a deity's ritual ascent, hyper-detailed
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p' | '1080p';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
    /**
     * Style
     * @description The style of the generated video
     * @enum {string}
     */
    style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk';
}

export interface SharedType_a54 {
    /**
     * Config File
     * @description URL to the training configuration file.
     */
    config_file: Components.File;
    /**
     * Debug Preprocessed Output
     * @description URL to the preprocessed images.
     */
    debug_preprocessed_output?: Components.File;
    /**
     * Diffusers Lora File
     * @description URL to the trained diffusers lora weights.
     */
    diffusers_lora_file: Components.File;
}

export interface SharedType_a3d {
    /**
     * Default Caption
     * @description Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string;
    /**
     * Image Data Url
     * @description URL to the input data zip archive.
     *
     *         The zip should contain pairs of images. The images should be named:
     *
     *         ROOT_start.EXT and ROOT_end.EXT
     *         For example:
     *         photo_start.jpg and photo_end.jpg
     *
     *         The zip can also contain up to four reference image for each image pair. The reference images should be named:
     *         ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT
     *         For example:
     *         photo_start.jpg, photo_start2.jpg, photo_end.jpg
     *
     *         The zip can also contain a text file for each image pair. The text file should be named:
     *         ROOT.txt
     *         For example:
     *         photo.txt
     *
     *         This text file can be used to specify the edit instructions for the image pair.
     *
     *         If no text file is provided, the default_caption will be used.
     *
     *         If no default_caption is provided, the training will fail.
     */
    image_data_url: string;
    /**
     * Learning Rate
     * @description Learning rate applied to trainable parameters.
     * @default 0.00005
     */
    learning_rate?: number;
    /**
     * Output Lora Format
     * @description Dictates the naming scheme for the output weights
     * @default fal
     * @enum {string}
     */
    output_lora_format?: 'fal' | 'comfy';
    /**
     * Steps
     * @description Total number of training steps.
     * @default 1000
     */
    steps?: number;
}

export interface SharedType_a3c {
    /**
     * Video
     * @description The generated video.
     * @example {
     *       "url": "https://v3.fal.media/files/penguin/Q-2dpcjIoQOldJRL3grsc_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_a31 {
    /**
     * Apply Text Normalization
     * @description This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped.
     * @default auto
     * @enum {string}
     */
    apply_text_normalization?: 'auto' | 'on' | 'off';
    /**
     * Language Code
     * @description Language code (ISO 639-1) used to enforce a language for the model. An error will be returned if language code is not supported by the model.
     */
    language_code?: string;
    /**
     * Next Text
     * @description The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
     */
    next_text?: string;
    /**
     * Previous Text
     * @description The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
     */
    previous_text?: string;
    /**
     * Similarity Boost
     * @description Similarity boost (0-1)
     * @default 0.75
     */
    similarity_boost?: number;
    /**
     * Speed
     * @description Speech speed (0.7-1.2). Values below 1.0 slow down the speech, above 1.0 speed it up. Extreme values may affect quality.
     * @default 1
     */
    speed?: number;
    /**
     * Stability
     * @description Voice stability (0-1)
     * @default 0.5
     */
    stability?: number;
    /**
     * Style
     * @description Style exaggeration (0-1)
     * @default 0
     */
    style?: number;
    /**
     * Text
     * @description The text to convert to speech
     * @example Hello! This is a test of the text to speech system, powered by ElevenLabs. How does it sound?
     */
    text: string;
    /**
     * Timestamps
     * @description Whether to return timestamps for each word in the generated speech
     * @default false
     */
    timestamps?: boolean;
    /**
     * Voice
     * @description The voice to use for speech generation
     * @default Rachel
     * @example Aria
     * @example Roger
     * @example Sarah
     * @example Laura
     * @example Charlie
     * @example George
     * @example Callum
     * @example River
     * @example Liam
     * @example Charlotte
     * @example Alice
     * @example Matilda
     * @example Will
     * @example Jessica
     * @example Eric
     * @example Chris
     * @example Brian
     * @example Daniel
     * @example Lily
     * @example Bill
     */
    voice?: string;
}

export interface SharedType_a2c {
    /**
     * Video
     * @description The extended video.
     * @example {
     *       "url": "https://v3b.fal.media/files/b/0a86711b/B_Z96VS4X9Dfd4M5ArB4H_c666e63f729f4a8fa1145c6727cef97d.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_a1c {
    /**
     * Duration
     * @description Duration of the output video in seconds.
     */
    duration: number;
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://v3.fal.media/files/penguin/ln3x7H1p1jL0Pwo7675NI_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_a00 {
    /**
     * Prompt
     * @description The prompt used for the generation.
     * @example Continue the scene naturally, maintaining the same style and motion.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for the random number generator.
     * @example 866232447
     */
    seed: number;
    /**
     * @description The generated video.
     * @example {
     *       "height": 704,
     *       "duration": 10.28,
     *       "url": "https://v3b.fal.media/files/b/0a88289e/CJcQGDrxOSRg2YFl5GNDt_glXPMoji.mp4",
     *       "fps": 25,
     *       "width": 1248,
     *       "file_name": "CJcQGDrxOSRg2YFl5GNDt_glXPMoji.mp4",
     *       "content_type": "video/mp4",
     *       "num_frames": 257
     *     }
     */
    video: Components.VideoFile;
}

export interface SharedType_9fe {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "file_size": 3890360,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://fal.media/files/panda/5KmKS-mh1vO-htbqE5oex_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_9f4 {
    /**
     * Audio Url
     * @description URL to the reference audio file used for voice cloning.
     * @example https://storage.googleapis.com/falserverless/example_inputs/qwen3-tts/clone_in.mp3
     */
    audio_url: string;
    /**
     * Reference Text
     * @description Optional reference text that was used when creating the speaker embedding. Providing this can improve synthesis quality when using a cloned voice.
     * @example Okay. Yeah. I resent you. I love you. I respect you. But you know what? You blew it! And it is all thanks to you.
     */
    reference_text?: string;
}

export interface SharedType_9db {
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Effect Scene
     * @description The effect scene to use for the video generation
     * @example hug
     * @enum {string}
     */
    effect_scene:
        | 'hug'
        | 'kiss'
        | 'heart_gesture'
        | 'squish'
        | 'expansion'
        | 'fuzzyfuzzy'
        | 'bloombloom'
        | 'dizzydizzy'
        | 'jelly_press'
        | 'jelly_slice'
        | 'jelly_squish'
        | 'jelly_jiggle'
        | 'pixelpixel'
        | 'yearbook'
        | 'instant_film'
        | 'anime_figure'
        | 'rocketrocket'
        | 'fly_fly'
        | 'disappear'
        | 'lightning_power'
        | 'bullet_time'
        | 'bullet_time_360'
        | 'media_interview'
        | 'day_to_night'
        | "let's_ride"
        | 'jumpdrop'
        | 'swish_swish'
        | 'running_man'
        | 'jazz_jazz'
        | 'swing_swing'
        | 'skateskate'
        | 'building_sweater'
        | 'pure_white_wings'
        | 'black_wings'
        | 'golden_wing'
        | 'pink_pink_wings'
        | 'rampage_ape'
        | 'a_list_look'
        | 'countdown_teleport'
        | 'firework_2026'
        | 'instant_christmas'
        | 'birthday_star'
        | 'firework'
        | 'celebration'
        | 'tiger_hug_pro'
        | 'pet_lion_pro'
        | 'guardian_spirit'
        | 'squeeze_scream'
        | 'inner_voice'
        | 'memory_alive'
        | 'guess_what'
        | 'eagle_snatch'
        | 'hug_from_past'
        | 'instant_kid'
        | 'dollar_rain'
        | 'cry_cry'
        | 'building_collapse'
        | 'mushroom'
        | 'jesus_hug'
        | 'shark_alert'
        | 'lie_flat'
        | 'polar_bear_hug'
        | 'brown_bear_hug'
        | 'office_escape_plow'
        | 'watermelon_bomb'
        | 'boss_coming'
        | 'wig_out'
        | 'car_explosion'
        | 'tiger_hug'
        | 'siblings'
        | 'construction_worker'
        | 'snatched'
        | 'felt_felt'
        | 'plushcut'
        | 'drunk_dance'
        | 'drunk_dance_pet'
        | 'daoma_dance'
        | 'bouncy_dance'
        | 'smooth_sailing_dance'
        | 'new_year_greeting'
        | 'lion_dance'
        | 'prosperity'
        | 'great_success'
        | 'golden_horse_fortune'
        | 'red_packet_box'
        | 'lucky_horse_year'
        | 'lucky_red_packet'
        | 'lucky_money_come'
        | 'lion_dance_pet'
        | 'dumpling_making_pet'
        | 'fish_making_pet'
        | 'pet_red_packet'
        | 'lantern_glow'
        | 'expression_challenge'
        | 'overdrive'
        | 'heart_gesture_dance'
        | 'poping'
        | 'martial_arts'
        | 'running'
        | 'nezha'
        | 'motorcycle_dance'
        | 'subject_3_dance'
        | 'ghost_step_dance'
        | 'phantom_jewel'
        | 'zoom_out'
        | 'cheers_2026'
        | 'kiss_pro'
        | 'fight_pro'
        | 'hug_pro'
        | 'heart_gesture_pro'
        | 'dollar_rain_pro'
        | 'pet_bee_pro'
        | 'santa_random_surprise'
        | 'magic_match_tree'
        | 'happy_birthday'
        | 'thumbs_up_pro'
        | 'surprise_bouquet'
        | 'bouquet_drop'
        | '3d_cartoon_1_pro'
        | 'glamour_photo_shoot'
        | 'box_of_joy'
        | 'first_toast_of_the_year'
        | 'my_santa_pic'
        | 'santa_gift'
        | 'steampunk_christmas'
        | 'snowglobe'
        | 'christmas_photo_shoot'
        | 'ornament_crash'
        | 'santa_express'
        | 'particle_santa_surround'
        | 'coronation_of_frost'
        | 'spark_in_the_snow'
        | 'scarlet_and_snow'
        | 'cozy_toon_wrap'
        | 'bullet_time_lite'
        | 'magic_cloak'
        | 'balloon_parade'
        | 'jumping_ginger_joy'
        | 'c4d_cartoon_pro'
        | 'venomous_spider'
        | 'throne_of_king'
        | 'luminous_elf'
        | 'woodland_elf'
        | 'japanese_anime_1'
        | 'american_comics'
        | 'snowboarding'
        | 'witch_transform'
        | 'vampire_transform'
        | 'pumpkin_head_transform'
        | 'demon_transform'
        | 'mummy_transform'
        | 'zombie_transform'
        | 'cute_pumpkin_transform'
        | 'cute_ghost_transform'
        | 'knock_knock_halloween'
        | 'halloween_escape'
        | 'baseball'
        | 'trampoline'
        | 'trampoline_night'
        | 'pucker_up'
        | 'feed_mooncake'
        | 'flyer'
        | 'dishwasher'
        | 'pet_chinese_opera'
        | 'magic_fireball'
        | 'gallery_ring'
        | 'pet_moto_rider'
        | 'muscle_pet'
        | 'pet_delivery'
        | 'mythic_style'
        | 'steampunk'
        | '3d_cartoon_2'
        | 'pet_chef'
        | 'santa_gifts'
        | 'santa_hug'
        | 'girlfriend'
        | 'boyfriend'
        | 'heart_gesture_1'
        | 'pet_wizard'
        | 'smoke_smoke'
        | 'gun_shot'
        | 'double_gun'
        | 'pet_warrior'
        | 'long_hair'
        | 'pet_dance'
        | 'wool_curly'
        | 'pet_bee'
        | 'marry_me'
        | 'piggy_morph'
        | 'ski_ski'
        | 'magic_broom'
        | 'splashsplash'
        | 'surfsurf'
        | 'fairy_wing'
        | 'angel_wing'
        | 'dark_wing'
        | 'emoji';
    /**
     * Input Image Urls
     * @description URL of images to be used for hug, kiss or heart_gesture video.
     * @example [
     *       "https://storage.googleapis.com/falserverless/juggernaut_examples/VHXMavzPyI27zi6JseyL4.png",
     *       "https://storage.googleapis.com/falserverless/juggernaut_examples/QEW5VrzccxGva7mPfEXjf.png"
     *     ]
     */
    input_image_urls?: string[];
}

export interface SharedType_9d8 {
    /**
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://v3.fal.media/files/lion/Yha3swLpHm35hoJCs8oJQ_tmp618_yf2f.mp4"
     *     }
     */
    video: Components.File_1;
}

export interface SharedType_98c {
    /**
     * Description
     * @description The description of the generated images.
     */
    description: string;
    /**
     * Images
     * @description The edited images.
     * @example [
     *       {
     *         "file_name": "nano-banana-multi-edit-output.png",
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/nano-banana-multi-edit-output.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
}

export interface SharedType_97f {
    /**
     * Values
     * @description The values of the measurements.
     */
    values?: {
        [key: string]:
            | number
            | {
                  [key: string]: number;
              };
    }[];
}

export interface SharedType_97e {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '3:2' | '4:3' | '5:4' | '1:1' | '4:5' | '3:4' | '2:3' | '9:16';
    /**
     * Limit Generations
     * @description Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.
     * @default false
     */
    limit_generations?: boolean;
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The text prompt to generate an image from.
     * @example An action shot of a black lab swimming in an inground suburban swimming pool. The camera is placed meticulously on the water line, dividing the image in half, revealing both the dogs head above water holding a tennis ball in it's mouth, and it's paws paddling underwater.
     */
    prompt: string;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_977 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16';
    /**
     * Auto Fix
     * @description Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
     * @default false
     */
    auto_fix?: boolean;
    /**
     * Duration
     * @description The duration of the generated video.
     * @default 7s
     * @enum {string}
     */
    duration?: '7s';
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Negative Prompt
     * @description A negative prompt to guide the video generation.
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The text prompt describing how the video should be extended
     * @example Continue the scene naturally, maintaining the same style and motion.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video.
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p';
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Video URL
     * @description URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio.
     * @example https://v3b.fal.media/files/b/0a8670fe/pY8UGl4_C452wOm9XUBYO_9ae04df8771c4f3f979fa5cabeca6ada.mp4
     */
    video_url: string;
}

export interface SharedType_96f {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/jpeg",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/flux_krea_redux_output_1.jpg",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_962 {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/jpeg",
     *         "url": "https://v3.fal.media/files/elephant/P-YCIAg6wtFn1hsF34fzL_qwen-edit.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_95d {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/kling/kling_ex.mp4.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_95c {
    /**
     * Images
     * @description The edited image.
     * @example [
     *       {
     *         "height": 768,
     *         "file_name": "2_gRhwfsnmNKYtZ_dveyV.jpg",
     *         "content_type": "image/jpeg",
     *         "url": "https://v3b.fal.media/files/b/koala/2_gRhwfsnmNKYtZ_dveyV.jpg",
     *         "width": 1152
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for the inference.
     * @example The user wants to add a surfer to the wave in the illustration while preserving the original ukiyo-e woodblock art style. The surfer should be depicted mid-action, crouched low on a modern-style surfboard, carving through the crest of the wave with one arm extended for balance and the other gripping the board. Their wavy hair and athletic physique should match the dynamic motion. The background must remain unchanged, including the iconic Mount Fuji and the traditional Japanese text, to maintain the artwork's historical aesthetic and composition.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the inference.
     */
    seed: number;
}

export interface SharedType_91c {
    /**
     * Image Url
     * @description URL of the image to process
     * @example https://storage.googleapis.com/falserverless/model_tests/image_preprocessors/cat.png
     */
    image_url: string;
}

export interface SharedType_913 {
    /**
     * Acceleration
     * @description Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
     * @default regular
     * @example regular
     */
    acceleration?: 'none' | 'low' | 'regular';
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16';
    /**
     * Auto Downsample Min FPS
     * @description The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
     * @default 15
     * @example 15
     */
    auto_downsample_min_fps?: number;
    /**
     * Enable Auto Downsample
     * @description If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
     * @default false
     * @example false
     */
    enable_auto_downsample?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Bottom
     * @description Whether to expand the video to the bottom.
     * @default false
     * @example true
     */
    expand_bottom?: boolean;
    /**
     * Expand Left
     * @description Whether to expand the video to the left.
     * @default false
     * @example true
     */
    expand_left?: boolean;
    /**
     * Expand Ratio
     * @description Amount of expansion. This is a float value between 0 and 1, where 0.25 adds 25% to the original video size on the specified sides.
     * @default 0.25
     * @example 0.25
     */
    expand_ratio?: number;
    /**
     * Expand Right
     * @description Whether to expand the video to the right.
     * @default false
     * @example true
     */
    expand_right?: boolean;
    /**
     * Expand Top
     * @description Whether to expand the video to the top.
     * @default false
     * @example true
     */
    expand_top?: boolean;
    /**
     * First Frame URL
     * @description URL to the first frame of the video. If provided, the model will use this frame as a reference.
     */
    first_frame_url?: string;
    /**
     * Frames per Second
     * @description Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
     * @default 16
     */
    frames_per_second?: number;
    /**
     * Guidance Scale
     * @description Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
     * @default 5
     * @example 5
     */
    guidance_scale?: number;
    /**
     * Interpolator Model
     * @description The model to use for frame interpolation. Options are 'rife' or 'film'.
     * @default film
     * @example film
     * @enum {string}
     */
    interpolator_model?: 'rife' | 'film';
    /**
     * Last Frame URL
     * @description URL to the last frame of the video. If provided, the model will use this frame as a reference.
     */
    last_frame_url?: string;
    /**
     * Match Input Frames Per Second
     * @description If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
     * @default false
     * @example false
     */
    match_input_frames_per_second?: boolean;
    /**
     * Match Input Number of Frames
     * @description If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
     * @default false
     * @example false
     */
    match_input_num_frames?: boolean;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     * @example letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 81 to 241 (inclusive).
     * @default 81
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Number of Interpolated Frames
     * @description Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
     * @default 0
     * @example 0
     */
    num_interpolated_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A lone woman strides through the neon-drenched streets of Tokyo at night.  Her crimson dress, a vibrant splash of color against the deep blues and blacks of the cityscape, flows slightly with each step. A tailored black jacket, crisp and elegant, contrasts sharply with the dress's rich texture. Medium shot:  The city hums around her, blurred lights creating streaks of color in the background. Close-up:  The fabric of her dress catches the streetlight's glow, revealing a subtle silk sheen and the intricate stitching at the hem. Her black jacket’s subtle texture is visible – a fine wool perhaps, with a matte finish. The overall mood is one of quiet confidence and mystery, a vibrant woman navigating a bustling, nocturnal landscape. High resolution 4k.
     */
    prompt: string;
    /**
     * Reference Image URLs
     * @description URLs to source reference image. If provided, the model will use this image as reference.
     */
    ref_image_urls?: string[];
    /**
     * Resolution
     * @description Resolution of the generated video.
     * @default auto
     * @enum {string}
     */
    resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p';
    /**
     * Return Frames Zip
     * @description If true, also return a ZIP file containing all generated frames.
     * @default false
     * @example false
     */
    return_frames_zip?: boolean;
    /**
     * Sampler
     * @description Sampler to use for video generation.
     * @default unipc
     * @example unipc
     * @enum {string}
     */
    sampler?: 'unipc' | 'dpm++' | 'euler';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift parameter for video generation.
     * @default 5
     */
    shift?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     * @example false
     */
    sync_mode?: boolean;
    /**
     * Temporal Downsample Factor
     * @description Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
     * @default 0
     * @example 0
     */
    temporal_downsample_factor?: number;
    /**
     * Transparency Mode
     * @description The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
     * @default content_aware
     * @example content_aware
     * @enum {string}
     */
    transparency_mode?: 'content_aware' | 'white' | 'black';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video URL
     * @description URL to the source video file. Required for outpainting.
     * @example https://storage.googleapis.com/falserverless/web-examples/wan/t2v.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface SharedType_905 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?: '1:1' | '16:9' | '9:16' | '3:4' | '4:3';
    /**
     * Negative Prompt
     * @description A description of what to discourage in the generated images
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate (1-4)
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The text prompt describing what you want to see
     * @example A serene landscape with mountains reflected in a crystal clear lake at sunset
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducible generation
     */
    seed?: number;
}

export interface SharedType_8df {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video. Only 16:9 and 9:16 are supported.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16';
    /**
     * Auto Fix
     * @description Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
     * @default false
     */
    auto_fix?: boolean;
    /**
     * Duration
     * @description The duration of the generated video.
     * @default 8s
     * @enum {string}
     */
    duration?: '4s' | '6s' | '8s';
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Image URL
     * @description URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit.
     * @example https://storage.googleapis.com/falserverless/example_inputs/veo31_i2v_input.jpg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description A negative prompt to guide the video generation.
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The text prompt describing the video you want to generate
     * @example A monkey and polar bear host a casual podcast about AI inference, bringing their unique perspectives from different environments (tropical vs. arctic) to discuss how AI systems make decisions and process information.
     *     Sample Dialogue:
     *     Monkey (Banana): "Welcome back to Bananas & Ice! I am Banana"
     *     Polar Bear (Ice): "And I'm Ice!"
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video.
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p' | '1080p' | '4k';
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
}

export interface SharedType_8d7 {
    /**
     * Config File
     * @description Configuration used for setting up the inference endpoints.
     */
    config_file: Components.File;
    /**
     * Lora File
     * @description URL to the trained LoRA weights.
     */
    lora_file: Components.File;
}

export interface SharedType_8d5 {
    /**
     * Image Guidance Scale
     * @description The image guidance scale to use for the video generation.
     * @default 2
     */
    image_guidance_scale?: number;
    /**
     * Image Url
     * @description The URL of the image to use as a reference for the video generation.
     * @example https://v3b.fal.media/files/b/panda/-oMlZo9Yyj_Nzoza_tgds_GmLF86r5bOt50eMMKCszy_eacc949b3933443c9915a83c98fbe85e.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @example black background, Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion
     */
    negative_prompt: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps to use for the video generation.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Pose Guidance Scale
     * @description The pose guidance scale to use for the video generation.
     * @default 1.5
     */
    pose_guidance_scale?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A robot figure dancing
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the video to generate.
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Video Url
     * @description The URL of the video to use as a reference for the video generation.
     * @example https://v3b.fal.media/files/b/panda/a6SvJg96V8eoglMlYFShU_5385885-hd_1080_1920_25fps.mp4
     */
    video_url: string;
}

export interface SharedType_8c5 {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/kangaroo/c0RfXzCisqX6YRkIF7apw_output.png"
     *       }
     *     ]
     */
    images: Components.File[];
    /**
     * Seed
     * @description Seed used for generation
     * @example 42
     */
    seed: number;
}

export interface SharedType_8c0 {
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     */
    video: Components.File;
}

export interface SharedType_8b9 {
    /**
     * Prompt
     * @description The prompt used for the generation.
     * @example A woman speaks to the camera
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for the random number generator.
     * @example 175932751
     */
    seed: number;
    /**
     * @description The generated video.
     * @example {
     *       "file_name": "ltx-2-a2v-output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/ltx-2-a2v-output.mp4"
     *     }
     */
    video: Components.VideoFile;
}

export interface SharedType_894 {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "file_size": 3890360,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/video_models/output-2.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_88d {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description URL of image to use for inpainting. or img2img
     * @example https://storage.googleapis.com/falserverless/example_inputs/dog.png
     */
    image_url: string;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Mask Url
     * @description The mask to area to Inpaint in.
     * @example https://storage.googleapis.com/falserverless/example_inputs/dog_mask.png
     */
    mask_url: string;
    /**
     * Num Images
     * @description The number of images to generate. This is always set to 1 for streaming output.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A photo of a lion sitting on a stone bench
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original.
     * @default 0.85
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_886 {
    /**
     * Image Url
     * @description The URL of the image to be processed.
     * @example https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg
     * @example http://ecx.images-amazon.com/images/I/51UUzBDAMsL.jpg
     */
    image_url: string;
    /**
     * Region
     * @description The user input coordinates
     * @example {
     *       "y1": 100,
     *       "x2": 200,
     *       "x1": 100,
     *       "y2": 200
     *     }
     */
    region: Components.Region;
}

export interface SharedType_87d {
    /**
     * Video
     * @description The generated video.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/veo3-i2v-output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_876 {
    /**
     * Description
     * @description The description of the generated images.
     */
    description: string;
    /**
     * Images
     * @description The edited images.
     * @example [
     *       {
     *         "file_name": "nano-banana-multi-edit-output.png",
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/nano-banana-multi-edit-output.png"
     *       }
     *     ]
     */
    images: Components.ImageFile_1[];
}

export interface SharedType_86b {
    /**
     * Audio Path
     * @description Local filesystem path (or remote URL) to a long audio file
     * @example https://storage.googleapis.com/falserverless/canary/18e15559-ab3e-4f96-9583-be5ddde91e43.mp3
     */
    audio_url: string;
    /**
     * Use Punctuation/Capitalization (PnC)
     * @description Whether to use Canary's built-in punctuation & capitalization
     * @default true
     */
    use_pnc?: boolean;
}

export interface SharedType_868 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description Image prompt for the omni model.
     * @example https://v3.fal.media/files/zebra/hAjCkcyly4gsS9-cptD3Y_image%20(20).png
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_82f {
    /** @description ZIP archive of all video frames if requested. */
    frames_zip?: Components.File_1;
    /**
     * Prompt
     * @description The prompt used for generation.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * @description The generated outpainting video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/wan-vace-outpainting-output.mp4"
     *     }
     */
    video: Components.VideoFile;
}

export interface SharedType_82a {
    /**
     * Aspect Ratio (W:H)
     * @description The aspect ratio of the video to generate.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16';
    /**
     * CFG Scale
     * @description Classifier-Free Guidance scale for the generation.
     * @default 1
     */
    cfg_scale?: number;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance scale for the generation.
     * @default 10
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description URL of the image input.
     * @example https://storage.googleapis.com/falserverless/framepack/framepack.jpg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default
     * @example Ugly, blurry distorted, bad quality
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 180
     */
    num_frames?: number;
    /**
     * Prompt
     * @description Text prompt for video generation (max 500 characters).
     * @example A mesmerising video of a deep sea jellyfish moving through an inky-black ocean. The jellyfish glows softly with an amber bioluminescence. The overall scene is lifelike.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations.
     * @default 480p
     * @enum {string}
     */
    resolution?: '720p' | '480p';
    /**
     * Seed
     * @description The seed to use for generating the video.
     */
    seed?: number;
}

export interface SharedType_824 {
    /**
     * Camera Movement
     * @description The type of camera movement to apply to the video
     * @enum {string}
     */
    camera_movement?:
        | 'horizontal_left'
        | 'horizontal_right'
        | 'vertical_up'
        | 'vertical_down'
        | 'zoom_in'
        | 'zoom_out'
        | 'crane_up'
        | 'quickly_zoom_in'
        | 'quickly_zoom_out'
        | 'smooth_zoom_in'
        | 'camera_rotation'
        | 'robo_arm'
        | 'super_dolly_out'
        | 'whip_pan'
        | 'hitchcock'
        | 'left_follow'
        | 'right_follow'
        | 'pan_left'
        | 'pan_right'
        | 'fix_bg';
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://v3.fal.media/files/zebra/qL93Je8ezvzQgDOEzTjKF_KhGKZTEebZcDw6T5rwQPK_output.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     * @example blurry, low quality, low resolution, pixelated, noisy, grainy, out of focus, poorly lit, poorly exposed, poorly composed, poorly framed, poorly cropped, poorly color corrected, poorly color graded
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example A woman warrior with her hammer walking with his glacier wolf.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
    /**
     * Style
     * @description The style of the generated video
     * @enum {string}
     */
    style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk';
}

export interface SharedType_81d {
    /**
     * Auto Trim
     * @description auto trim the video, to working duration ( 5s )
     * @default true
     */
    auto_trim?: boolean;
    /**
     * Mask Video Url
     * @description Input video to mask erase object from. duration must be less than 5s.
     * @example https://bria-test-images.s3.us-east-1.amazonaws.com/videos/eraser/video1_mask.mp4
     */
    mask_video_url: string;
    /**
     * Output Container And Codec
     * @description Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
     * @default mp4_h264
     * @enum {string}
     */
    output_container_and_codec?:
        | 'mp4_h265'
        | 'mp4_h264'
        | 'webm_vp9'
        | 'gif'
        | 'mov_h264'
        | 'mov_h265'
        | 'mov_proresks'
        | 'mkv_h264'
        | 'mkv_h265'
        | 'mkv_vp9'
        | 'mkv_mpeg4';
    /**
     * Preserve Audio
     * @description If true, audio will be preserved in the output video.
     * @default true
     */
    preserve_audio?: boolean;
    /**
     * Video Url
     * @description Input video to erase object from. duration must be less than 5s.
     * @example https://bria-test-images.s3.us-east-1.amazonaws.com/videos/eraser/video1_video.mp4
     */
    video_url: string;
}

export interface SharedType_819 {
    /**
     * Base Model
     * @description Base model
     */
    base_model?: Components.File;
    /**
     * Model Mesh
     * @description Model
     * @example {
     *       "file_size": 6744644,
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3.fal.media/files/zebra/NA4WkhbpI-XdOIFc4cDIk_tripo_model_812c3a8a-6eb3-4c09-9f40-0563d27ae7ea.glb"
     *     }
     */
    model_mesh?: Components.File;
    /**
     * Pbr Model
     * @description Pbr model
     */
    pbr_model?: Components.File;
    /**
     * Rendered Image
     * @description A preview image of the model
     * @example {
     *       "file_size": 13718,
     *       "content_type": "image/webp",
     *       "url": "https://v3.fal.media/files/panda/zDTAHqp8ifMOT3upZ1xJv_legacy.webp"
     *     }
     */
    rendered_image?: Components.File;
    /**
     * Task Id
     * @description The task id of the 3D model generation.
     */
    task_id: string;
}

export interface SharedType_8151 {
    /**
     * Video
     * @description The generated video.
     * @example {
     *       "url": "https://v3b.fal.media/files/b/kangaroo/oUCiZjQwEy6bIQdPUSLDF_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_815 {
    /**
     * Video
     * @description The generated video.
     * @example {
     *       "file_size": 28472159,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/kangaroo/3n_Lpxm_SjK5NYyBobRdS_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_813 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?:
        | 'auto'
        | '21:9'
        | '16:9'
        | '3:2'
        | '4:3'
        | '5:4'
        | '1:1'
        | '4:5'
        | '3:4'
        | '2:3'
        | '9:16';
    /**
     * Image URLs
     * @description The URLs of the images to use for image-to-image generation or image editing.
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_inputs/nano-banana-edit-input.png",
     *       "https://storage.googleapis.com/falserverless/example_inputs/nano-banana-edit-input-2.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Limit Generations
     * @description Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.
     * @default false
     */
    limit_generations?: boolean;
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt for image editing.
     * @example make a photo of the man driving the car down the california coastline
     */
    prompt: string;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_812 {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://v3.fal.media/files/penguin/Om3xjcOwiSCJwrXs7DUi__output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_80b {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/png",
     *         "url": "https://v3b.fal.media/files/b/0a887d5a/r3LfL0WY1re7dT5Qb85U_.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_804 {
    /**
     * Prompt
     * @description The prompt used for generation.
     * @example First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation.
     * @example 916581
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/panda/4-MoAje_CCMAGH8d-9kmA_nQEkcRc2.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_800 {
    /**
     * Masks
     * @description Dictionary of label: mask video
     * @example [
     *       {
     *         "file_size": 3259012,
     *         "file_name": "output_0.mp4",
     *         "content_type": "application/octet-stream",
     *         "url": "https://v3.fal.media/files/kangaroo/KSuUWm24leGew4jTouuTM_output_0.mp4"
     *       },
     *       {
     *         "file_size": 1241471,
     *         "file_name": "output_1.mp4",
     *         "content_type": "application/octet-stream",
     *         "url": "https://v3.fal.media/files/monkey/0jHCYm2lZM6FjDmtXw1Kt_output_1.mp4"
     *       }
     *     ]
     */
    masks: Components.File[];
    /**
     * Output
     * @description Generated output
     * @example <p>  Two children  </p>   [SEG]  are jumping on  <p>  a bed  </p>   [SEG]  .<|im_end|>
     */
    output: string;
}

export interface SharedType_7fd {
    /**
     * Image
     * @description The depth map.
     */
    image: Components.Image;
}

export interface SharedType_7f3 {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word "FLUX" is painted over it in big, white brush strokes with visible texture.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_7eb1 {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image file info.
     */
    images: Components.Image[];
    /**
     * Timings
     * @description The time taken for the generation process.
     */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_7eb {
    /** @description ZIP archive of all video frames if requested. */
    frames_zip?: Components.File_1;
    /**
     * Prompt
     * @description The prompt used for generation.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * @description The generated reframe video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/wan-vace-reframe-output.mp4"
     *     }
     */
    video: Components.VideoFile;
}

export interface SharedType_7e9 {
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "file_size": 5797172,
     *       "file_name": "output.mp4",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3b.fal.media/files/b/tiger/5d-CATfsfPrBaXAK38hy6_output.mp4"
     *     }
     */
    video?: Components.File;
}

export interface SharedType_7da {
    /**
     * Seed
     * @description Seed used for generating the video.
     */
    seed: number;
    /**
     * Video
     * @description Generated video file.
     * @example {
     *       "url": "https://fal-cdn.batuhan-941.workers.dev/files/kangaroo/DSrFBOk9XXIplm_kukI4n.mp4"
     *     }
     */
    video: Components.File_1;
}

export interface SharedType_7b9 {
    /**
     * Description
     * @description The description of the generated images.
     */
    description: string;
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "file_name": "nano-banana-t2i-output.png",
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/nano-banana-t2i-output.png"
     *       }
     *     ]
     */
    images: Components.ImageFile_1[];
}

export interface SharedType_7a3 {
    /**
     * Aspect Ratio
     * @description The desired aspect ratio of the generated image. If not provided, will be smartly chosen by the model.
     * @example 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '3:2' | '2:3' | '4:3' | '3:4' | '1:1';
    /**
     * Reference Image URLs
     * @description List of URLs of reference images. Must provide between 1 and 6 images (inclusive). Each image must be less than 10 MB. Supports PNG, JPEG, WebP, AVIF, and HEIF formats.
     * @example [
     *       "https://v3b.fal.media/files/b/monkey/lsPBOhBws_FnTzd5G9KZ9_seedream4_edit_input_4.png",
     *       "https://v3b.fal.media/files/b/monkey/ZrW5ouDj8vjLtvl1Cj9l9_seedream4_edit_input_2.png",
     *       "https://v3b.fal.media/files/b/elephant/sd0k6YhlQEKfR6d_hAmIH_seedream4_edit_input_3.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Number of Images
     * @description Number of images to generate
     * @default 1
     * @example 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description Output format for the generated image.
     * @default png
     * @example png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description The text description of the desired image. May include XML img tags like <img>0</img> to refer to specific images by their index in the image_urls list.
     * @example Dress the model in the clothes and hat. Add a cat to the scene and change the background to a Victorian era building.
     */
    prompt: string;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_79c {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the output video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Prompt
     * @description Text prompt for video generation, max 1500 characters
     * @example The little devil is looking at the apple on the beach and walking around it.
     */
    prompt: string;
    /**
     * Reference Image Urls
     * @description URLs of the reference images to use for consistent subject appearance
     * @example [
     *       "https://storage.googleapis.com/falserverless/web-examples/vidu/new-examples/reference1.png",
     *       "https://storage.googleapis.com/falserverless/web-examples/vidu/new-examples/reference2.png",
     *       "https://storage.googleapis.com/falserverless/web-examples/vidu/new-examples/reference3.png"
     *     ]
     */
    reference_image_urls: string[];
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
}

export interface SharedType_799 {
    /**
     * Video
     * @description The generated video file
     * @example {
     *       "file_name": "ltxv-2-i2v-output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/ltxv-2-i2v-output.mp4"
     *     }
     */
    video: Components.VideoFile_1;
}

export interface SharedType_78a {
    /**
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://v3.fal.media/files/panda/kt9d4vZ8Mfw_WzYnvr2Q0_tmp0ir4znsr.mp4"
     *     }
     */
    video: Components.File_1;
}

export interface SharedType_778 {
    /**
     * Character Orientation
     * @description Controls whether the output character's orientation matches the reference image or video. 'video': orientation matches reference video - better for complex motions (max 30s). 'image': orientation matches reference image - better for following camera movements (max 10s).
     * @example video
     * @enum {string}
     */
    character_orientation: 'image' | 'video';
    /**
     * Image Url
     * @description Reference image URL. The characters, backgrounds, and other elements in the generated video are based on this reference image. Characters should have clear body proportions, avoid occlusion, and occupy more than 5% of the image area.
     * @example https://v3b.fal.media/files/b/0a875302/8NaxQrQxDNHppHtqcchMm.png
     */
    image_url: string;
    /**
     * Keep Original Sound
     * @description Whether to keep the original sound from the reference video.
     * @default true
     */
    keep_original_sound?: boolean;
    /**
     * Prompt
     * @example An african american woman dancing
     */
    prompt?: string;
    /**
     * Video Url
     * @description Reference video URL. The character actions in the generated video will be consistent with this reference video. Should contain a realistic style character with entire body or upper body visible, including head, without obstruction. Duration limit depends on character_orientation: 10s max for 'image', 30s max for 'video'.
     * @example https://v3b.fal.media/files/b/0a8752bc/2xrNS217ngQ3wzXqA7LXr_output.mp4
     */
    video_url: string;
}

export interface SharedType_770 {
    /**
     * Images
     * @description The generated/edited images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/monkey/D7FrWGFnb7t8fjiE9Cok4.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface SharedType_768 {
    /** Moderation Error */
    moderation_error?: string;
    /**
     * Moderation Flagged
     * @default false
     */
    moderation_flagged?: boolean;
    /** Moderation Transcription */
    moderation_transcription?: string;
    /**
     * @example {
     *       "url": "https://argildotai.s3.us-east-1.amazonaws.com/fal-resource/example_fal.mp4"
     *     }
     */
    video?: Components.File_1;
}

export interface SharedType_75b {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Enhance Prompt
     * @description Whether to enhance the prompt for better results.
     * @default false
     */
    enhance_prompt?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description Image prompt for the omni model.
     * @example https://v3.fal.media/files/rabbit/rmgBxhwGYb2d3pl3x9sKf_output.png
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Put a donut next to the flour.
     */
    prompt: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_74f {
    /**
     * Seed
     * @description Seed used for generation
     * @example 42
     */
    seed: number;
    /**
     * Video
     * @description Generated video file
     * @example {
     *       "url": "https://v3.fal.media/files/penguin/qmLZSvOIzTKs6bDFXiEtH_video.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_744 {
    /**
     * Image
     * @description The generated image file info.
     */
    image: Components.Image;
}

export interface SharedType_741 {
    /**
     * Video
     * @description Video with removed background and audio.
     */
    video: Components.File_1 | Components.File_1;
}

export interface SharedType_72e {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "file_size": 5485412,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/video_models/output-4.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_720 {
    /** @description Generated image. */
    image: Components.Image_2;
    /**
     * Images
     * @description Generated images.
     * @default []
     */
    images?: {
        [key: string]: { [x: string]: any } | null;
    }[];
    /**
     * Structured Prompt
     * @description Current prompt.
     */
    structured_prompt: {
        [key: string]: { [x: string]: any } | null;
    };
}

export interface SharedType_709 {
    /**
     * Duration
     * @description The duration of the generated audio in seconds
     * @default 10
     */
    duration?: number;
    /**
     * Num Samples
     * @description The number of samples to generate from the model
     * @default 2
     */
    num_samples?: number;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used
     * @default 2105
     */
    seed?: number;
    /**
     * Text Prompt
     * @description Additional description to guide the model
     * @example
     */
    text_prompt?: string;
    /**
     * Video Url
     * Format: uri
     * @description A video url that can accessed from the API to process and add sound effects
     * @example https://di3otfzjg1gxa.cloudfront.net/input_example.mp4
     */
    video_url: string;
}

export interface SharedType_6f9 {
    /**
     * Acceleration
     * @description The acceleration level to use for image generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance scale for classifier-free guidance.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, uses the input image size.
     * @example {
     *       "height": 1152,
     *       "width": 2016
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URLs of the images for editing. A maximum of 4 images are allowed.
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_inputs/flux2_dev_edit_input.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Loras
     * @description List of LoRA weights to apply (maximum 3).
     * @default []
     */
    loras?: Components.LoRAInput_1[];
    /**
     * Negative Prompt
     * @description Negative prompt for classifier-free guidance. Describes what to avoid in the image.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to edit the image.
     * @example Change his clothes to casual suit and tie
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI. Output is not stored when this is True.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_6ea {
    /**
     * Actual Prompt
     * @description The actual prompt used if prompt rewriting was enabled
     * @example The white dragon warrior stands still in a grand cathedral-like structure, its glowing golden eyes fixed forward. The camera slowly moves closer, focusing on the warrior's armored chest and face. It then begins to circle around the warrior, capturing the intricate details of the white scale armor with gold accents. The warrior maintains a strong, determined posture. Ambient sounds and soft choral tones fill the background, enhancing the majestic atmosphere. The camera continues its slow circular motion, emphasizing the warrior's heroic presence before ending with a close-up of the face.
     */
    actual_prompt?: string;
    /**
     * Seed
     * @description The seed used for generation
     * @example 175932751
     */
    seed: number;
    /**
     * Video
     * @description The generated video file
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/wan-25-i2v-output.mp4"
     *     }
     */
    video: Components.VideoFile_1;
}

export interface SharedType_6e6 {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a86d42c/7T3zJKciQ1cCzqR3ADLif.png"
     *       },
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a86d42c/KVt5pIhe2dU-qZNC2Njo2.png"
     *       },
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a86d42c/3BMMGMaHyA3Y7Q_kamIJ_.png"
     *       },
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a86d42c/AY1BjZxhqS1jl-Pw2S1Tx.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used to generate the image.
     */
    prompt?: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_6d6 {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 1024,
     *         "url": "https://fal.media/files/tiger/7dSJbIU_Ni-0Zp9eaLsvR_fe56916811d84ac69c6ffc0d32dca151.jpg",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_6bb {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video frame
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Input Image Urls
     * @description List of image URLs to use for video generation. Supports up to 4 images.
     * @example [
     *       "https://storage.googleapis.com/falserverless/web-examples/kling-elements/first_image.jpeg",
     *       "https://storage.googleapis.com/falserverless/web-examples/kling-elements/second_image.png"
     *     ]
     */
    input_image_urls: string[];
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example A cute girl and a baby cow sleeping together on a bed
     */
    prompt: string;
}

export interface SharedType_6b8 {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://v2.fal.media/files/fb33a862b94d4d7195e610e4cbc5d392_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_6b3 {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 1728,
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/z-image-turbo-i2i-output.png",
     *         "width": 992
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /**
     * Timings
     * @description The timings of the generation process.
     */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_68c {
    /**
     * Images
     * @description The generated/edited images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/lion/d_xp44RvnuYYxioxBgAlX.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface SharedType_68a {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the final input image will be used.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URL of the image to create the next scene from.
     * @example [
     *       "https://v3b.fal.media/files/b/penguin/Zj5z8GW7yYlrpOQtuwjKQ_086265e41092415f951a6576fed25e41.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Lora Scale
     * @description The scale factor for the LoRA model. Controls the strength of the LoRA effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 6
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description Describe the camera movement, framing change, or scene transition. Start with 'Next Scene:' for best results. Examples: camera movements (dolly, push-in, pull-back), framing changes (wide to close-up), new elements entering frame.
     * @default Next Scene: The camera moves forward revealing more of the scene
     * @example Next Scene: The camera pulls back to reveal the entire landscape
     * @example Next Scene: The camera tracks forward as sunlight breaks through the clouds
     * @example Next Scene: The camera pans right revealing new characters entering the frame
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_671 {
    /**
     * Duration
     * @description The duration of the generated audio in seconds
     * @default 10
     */
    duration?: number;
    /**
     * Num Samples
     * @description The number of samples to generate from the model
     * @default 2
     */
    num_samples?: number;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used
     * @default 8069
     */
    seed?: number;
    /**
     * Start Offset
     * @description The start offset in seconds to start the audio generation from
     * @default 0
     */
    start_offset?: number;
    /**
     * Text Prompt
     * @description Additional description to guide the model
     * @example
     */
    text_prompt?: string;
    /**
     * Video Url
     * Format: uri
     * @description A video url that can accessed from the API to process and add sound effects
     * @example https://di3otfzjg1gxa.cloudfront.net/battlefield_scene_silent.mp4
     */
    video_url: string;
}

export interface SharedType_663 {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The URL of the image to generate an image from.
     * @example https://fal.media/files/koala/Chls9L2ZnvuipUTEwlnJC.png
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example a cat dressed as a wizard with a background of a mystic forest.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength of the initial image. Higher strength values are better for this model.
     * @default 0.95
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_662 {
    /**
     * Description
     * @description The description of the generated images.
     */
    description: string;
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "file_name": "nano-banana-t2i-output.png",
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/nano-banana-t2i-output.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
}

export interface SharedType_659 {
    /** @description Image with depth map */
    image: Components.Image_2;
}

export interface SharedType_63d {
    /**
     * Prompt
     * @description The prompt used for generating the video.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated video. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
    /**
     * Video
     * @description The URL to the generated video
     */
    video: Components.File;
}

export interface SharedType_61b {
    /**
     * Prompt
     * @description The prompt used for the generation.
     * @example A cowboy walking through a dusty town at high noon, camera following from behind, cinematic depth, realistic lighting, western mood, 4K film grain.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for the random number generator.
     * @example 149063119
     */
    seed: number;
    /**
     * @description The generated video.
     * @example {
     *       "height": 704,
     *       "duration": 6.44,
     *       "url": "https://v3b.fal.media/files/b/0a8824b1/sdm0KfmenrlywesfzY1Y1_if6euPp1.mp4",
     *       "fps": 25,
     *       "width": 1248,
     *       "file_name": "sdm0KfmenrlywesfzY1Y1_if6euPp1.mp4",
     *       "content_type": "video/mp4",
     *       "num_frames": 161
     *     }
     */
    video: Components.VideoFile;
}

export interface SharedType_618 {
    /**
     * Video
     * @description Generated video
     */
    video: Components.File;
}

export interface SharedType_609 {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/jpeg",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/flux_krea_t2i_output_1.jpg",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_5f5 {
    /**
     * Num Frames To Sample
     * @description Number of frames to sample from the video. If not provided, all frames are sampled.
     */
    num_frames_to_sample?: number;
    /**
     * Prompt
     * @description Prompt to be used for the chat completion
     * @example Could you please give me a brief description of the video? Please respond with interleaved segmentation masks for the corresponding parts of the answer.
     */
    prompt: string;
    /**
     * Video Url
     * @description The URL of the input video.
     * @example https://drive.google.com/uc?id=1iOFYbNITYwrebBBp9kaEGhBndFSRLz8k
     */
    video_url: string;
}

export interface SharedType_5f3 {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default {
     *       "height": 2160,
     *       "width": 3840
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 18
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Underwater coral reef ecosystem during peak bioluminescent activity, multiple layers of marine life - from microscopic plankton to massive coral structures, light refracting through crystal-clear tropical waters, creating prismatic color gradients, hyper-detailed texture of marine organisms
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Style Name
     * @description The style to generate the image in.
     * @default (No style)
     * @enum {string}
     */
    style_name?:
        | '(No style)'
        | 'Cinematic'
        | 'Photographic'
        | 'Anime'
        | 'Manga'
        | 'Digital Art'
        | 'Pixel art'
        | 'Fantasy art'
        | 'Neonpunk'
        | '3D Model';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_5da {
    /**
     * Prompt
     * @description The prompt used for generation.
     * @example A vibrant, abstract composition featuring a person with outstretched arms, rendered in a kaleidoscope of colors against a deep, dark background. The figure is composed of intricate, swirling patterns reminiscent of a mosaic, with hues of orange, yellow, blue, and green that evoke the style of artists such as Wassily Kandinsky or Bridget Riley. The camera zooms into the face striking portrait of a man, reimagined through the lens of old-school video-game graphics. The subject's face is rendered in a kaleidoscope of colors, with bold blues and reds set against a vibrant yellow backdrop. His dark hair is pulled back, framing his profile in a dramatic pose.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/ltxv-multiconditioning-output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_5c6 {
    /**
     * Image
     * @description Processed image
     */
    image?: Components.Image;
    /**
     * Results
     * @description Results from the model
     */
    results: Components.BoundingBoxes;
}

export interface SharedType_5bd {
    /**
     * Model Mesh
     * @description Generated 3D object file.
     * @example {
     *       "file_size": 720696,
     *       "file_name": "white_mesh.glb",
     *       "content_type": "application/octet-stream",
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/video_models/mesh.glb"
     *     }
     */
    model_mesh: Components.File;
    /**
     * Seed
     * @description Seed value used for generation.
     */
    seed: number;
}

export interface SharedType_5bb {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/png",
     *         "url": "https://v3b.fal.media/files/b/0a877afe/InJJA0Q1gtQnyK1N3wdg5.png",
     *         "width": 1376
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_5a8 {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the final input image will be used.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URL of the image to adjust camera angle for.
     * @example [
     *       "https://v3.fal.media/files/monkey/i3saq4bAPXSIl08nZtq9P_ec535747aefc4e31943136a6d8587075.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Lora Scale
     * @description The scale factor for the LoRA model. Controls the strength of the camera control effect.
     * @default 1.25
     */
    lora_scale?: number;
    /**
     * Move Forward → Close-Up
     * @description Move camera forward (0=no movement, 10=close-up)
     * @default 0
     */
    move_forward?: number;
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 6
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Rotate Right-Left (degrees °)
     * @description Rotate camera left (positive) or right (negative) in degrees. Positive values rotate left, negative values rotate right.
     * @default 0
     */
    rotate_right_left?: number;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Vertical Angle (Bird ⬄ Worm)
     * @description Adjust vertical camera angle (-1=bird's-eye view/looking down, 0=neutral, 1=worm's-eye view/looking up)
     * @default 0
     */
    vertical_angle?: number;
    /**
     * Wide-Angle Lens
     * @description Enable wide-angle lens effect
     * @default false
     */
    wide_angle_lens?: boolean;
}

export interface SharedType_5a6 {
    /** @description The generated video. */
    video: Components.File_1;
}

export interface SharedType_595 {
    /**
     * Auto-Scale Input
     * @description If true, the input will be automatically scale the video to 81 frames at 16fps.
     * @default false
     * @example true
     */
    auto_scale_input?: boolean;
    /**
     * Learning Rate
     * @description The rate at which the model learns. Higher values can lead to faster training, but over-fitting.
     * @default 0.0002
     */
    learning_rate?: number;
    /**
     * Number Of Steps
     * @description The number of steps to train for.
     * @default 400
     */
    number_of_steps?: number;
    /**
     * Training Data URL
     * @description URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.
     *
     *             In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.
     */
    training_data_url: string;
    /**
     * Trigger Phrase
     * @description The phrase that will trigger the model to generate an image.
     * @default
     */
    trigger_phrase?: string;
}

export interface SharedType_591 {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://v3.fal.media/files/kangaroo/_qEOfY3iKHsc86kqHUUh2_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_57f {
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "file_size": 225893,
     *       "file_name": "output.mp4",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3b.fal.media/files/b/0a85e79d/KOuXylETzdzUzMFFWLa4h_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_57e {
    /**
     * Acceleration
     * @description Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
     * @default regular
     * @example regular
     */
    acceleration?: 'none' | 'low' | 'regular';
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16';
    /**
     * Auto Downsample Min FPS
     * @description The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
     * @default 15
     * @example 15
     */
    auto_downsample_min_fps?: number;
    /**
     * Enable Auto Downsample
     * @description If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
     * @default false
     * @example false
     */
    enable_auto_downsample?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * First Frame URL
     * @description URL to the first frame of the video. If provided, the model will use this frame as a reference.
     */
    first_frame_url?: string;
    /**
     * Frames per Second
     * @description Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
     * @default 16
     */
    frames_per_second?: number;
    /**
     * Guidance Scale
     * @description Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
     * @default 5
     * @example 5
     */
    guidance_scale?: number;
    /**
     * Interpolator Model
     * @description The model to use for frame interpolation. Options are 'rife' or 'film'.
     * @default film
     * @example film
     * @enum {string}
     */
    interpolator_model?: 'rife' | 'film';
    /**
     * Last Frame URL
     * @description URL to the last frame of the video. If provided, the model will use this frame as a reference.
     */
    last_frame_url?: string;
    /**
     * Match Input Frames Per Second
     * @description If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
     * @default false
     * @example false
     */
    match_input_frames_per_second?: boolean;
    /**
     * Match Input Number of Frames
     * @description If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
     * @default false
     * @example false
     */
    match_input_num_frames?: boolean;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     * @example letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 81 to 241 (inclusive).
     * @default 81
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Number of Interpolated Frames
     * @description Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
     * @default 0
     * @example 0
     */
    num_interpolated_frames?: number;
    /**
     * Preprocess
     * @description Whether to preprocess the input video.
     * @default false
     * @example false
     */
    preprocess?: boolean;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A confident woman strides toward the camera down a sun-drenched, empty street. Her vibrant summer dress, a flowing emerald green with delicate white floral embroidery, billows slightly in the gentle breeze.  She carries a stylish, woven straw bag, its natural tan contrasting beautifully with the dress. The dress's fabric shimmers subtly, catching the light. The white embroidery is intricate, each tiny flower meticulously detailed.  Her expression is focused, yet relaxed, radiating self-assuredness. Her auburn hair, partially pulled back in a loose braid, catches the sunlight, creating warm highlights. The street itself is paved with warm, grey cobblestones, reflecting the bright sun. The mood is optimistic and serene, emphasizing the woman's independence and carefree spirit. High resolution 4k
     */
    prompt: string;
    /**
     * Reference Image URLs
     * @description URLs to source reference image. If provided, the model will use this image as reference.
     */
    ref_image_urls?: string[];
    /**
     * Resolution
     * @description Resolution of the generated video.
     * @default auto
     * @enum {string}
     */
    resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p';
    /**
     * Return Frames Zip
     * @description If true, also return a ZIP file containing all generated frames.
     * @default false
     * @example false
     */
    return_frames_zip?: boolean;
    /**
     * Sampler
     * @description Sampler to use for video generation.
     * @default unipc
     * @example unipc
     * @enum {string}
     */
    sampler?: 'unipc' | 'dpm++' | 'euler';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift parameter for video generation.
     * @default 5
     */
    shift?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     * @example false
     */
    sync_mode?: boolean;
    /**
     * Temporal Downsample Factor
     * @description Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
     * @default 0
     * @example 0
     */
    temporal_downsample_factor?: number;
    /**
     * Transparency Mode
     * @description The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
     * @default content_aware
     * @example content_aware
     * @enum {string}
     */
    transparency_mode?: 'content_aware' | 'white' | 'black';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video URL
     * @description URL to the source video file. Required for depth task.
     * @example https://storage.googleapis.com/falserverless/example_inputs/wan-vace-depth-video.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface SharedType_57a {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "height": 1024,
     *         "content_type": "image/jpeg",
     *         "url": "https://fal.media/files/lion/JpgBX7w379jHteLeeNsM5.jpeg",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_576 {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 1024,
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/z-image-turbo-controlnet-output.jpg",
     *         "width": 1536
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /**
     * Timings
     * @description The timings of the generation process.
     */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_56f {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "file_size": 2995630,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://fal.media/files/zebra/11UahivZ3XZ1tRlcEcgPq_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_567 {
    /**
     * Prompt
     * @description The prompt used for the generation.
     * @example A woman stands still amid a busy neon-lit street at night. The camera slowly dollies in toward her face as people blur past, their motion emphasizing her calm presence. City lights flicker and reflections shift across her denim jacket.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for the random number generator.
     * @example 2078003885
     */
    seed: number;
    /**
     * @description The generated video.
     * @example {
     *       "height": 704,
     *       "duration": 6.44,
     *       "url": "https://v3b.fal.media/files/b/0a894013/N9lnMTq7W3uMC0lOQg845_BknRPV8I.mp4",
     *       "fps": 25,
     *       "width": 1248,
     *       "file_name": "CJcQGDrxOSRg2YFl5GNDt_glXPMoji.mp4",
     *       "content_type": "video/mp4",
     *       "num_frames": 161
     *     }
     */
    video: Components.VideoFile;
}

export interface SharedType_54d {
    /**
     * Default Caption
     * @description Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string;
    /**
     * Image Data Url
     * @description URL to the input data zip archive.
     *
     *             The zip should contain pairs of images and corresponding captions.
     *
     *             The images should be named: ROOT.EXT. For example: 001.jpg
     *
     *             The corresponding captions should be named: ROOT.txt. For example: 001.txt
     *
     *             If no text file is provided for an image, the default_caption will be used.
     */
    image_data_url: string;
    /**
     * Learning Rate
     * @description Learning rate.
     * @default 0.0005
     */
    learning_rate?: number;
    /**
     * Steps
     * @description Number of steps to train for
     * @default 2000
     */
    steps?: number;
}

export interface SharedType_54a {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the reframed video
     * @example 9:16
     * @enum {string}
     */
    aspect_ratio: '1:1' | '16:9' | '9:16' | '4:3' | '3:4' | '21:9' | '9:21';
    /**
     * Grid Position X
     * @description X position of the grid for reframing
     */
    grid_position_x?: number;
    /**
     * Grid Position Y
     * @description Y position of the grid for reframing
     */
    grid_position_y?: number;
    /**
     * Image Url
     * @description Optional URL of the first frame image for reframing
     */
    image_url?: string;
    /**
     * Prompt
     * @description Optional prompt for reframing
     */
    prompt?: string;
    /**
     * Video Url
     * @description URL of the input video to reframe
     * @example https://v3.fal.media/files/zebra/9aDde3Te2kuJYHdR0Kz8R_output.mp4
     */
    video_url: string;
    /**
     * X End
     * @description End X coordinate for reframing
     */
    x_end?: number;
    /**
     * X Start
     * @description Start X coordinate for reframing
     */
    x_start?: number;
    /**
     * Y End
     * @description End Y coordinate for reframing
     */
    y_end?: number;
    /**
     * Y Start
     * @description Start Y coordinate for reframing
     */
    y_start?: number;
}

export interface SharedType_51a {
    /**
     * Actual Prompt
     * @description The actual prompt used if prompt rewriting was enabled
     * @example A comedic cinematic scene where the creator interacts with AI-generated reality transformations.
     */
    actual_prompt?: string;
    /**
     * Seed
     * @description The seed used for generation
     * @example 175932751
     */
    seed: number;
    /**
     * Video
     * @description The generated video file
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/0a8675cf/bCu9FiFXSjsSnIwOmjUOY_BVs2IFR3.mp4"
     *     }
     */
    video: Components.VideoFile_1;
}

export interface SharedType_4f3 {
    /**
     * Image
     * @description The generated image file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/wan/t2i-output.png"
     *     }
     */
    image: Components.File;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
}

export interface SharedType_4f2 {
    /**
     * Video
     * @description The generated video file
     * @example {
     *       "file_name": "ltxv-2-t2v-output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/ltxv-2-t2v-output.mp4"
     *     }
     */
    video: Components.VideoFile_1;
}

export interface SharedType_4ed {
    /**
     * Prompt
     * @description The prompt used for the generation.
     * @example black-and-white video, a cowboy walks through a dusty town, film grain
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for the random number generator.
     * @example 1490631192028410600
     */
    seed: number;
    /**
     * @description The generated video.
     * @example {
     *       "height": 704,
     *       "duration": 6.44,
     *       "url": "https://v3b.fal.media/files/b/0a895ed5/SaTGe87IpMUMiSq33w5Qb_RoCJFZhc.mp4",
     *       "fps": 25,
     *       "width": 1248,
     *       "file_name": "SaTGe87IpMUMiSq33w5Qb_RoCJFZhc.mp4",
     *       "content_type": "video/mp4",
     *       "num_frames": 161
     *     }
     */
    video: Components.VideoFile;
}

export interface SharedType_4c2 {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "file_size": 6420765,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://fal.media/files/koala/HEWK7BBwqWrz7F5nAZzp7_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_4bc {
    /**
     * CFG Scale
     * @description The classifier-free guidance scale for audio generation.
     * @default 5
     * @example 5
     */
    cfg_scale?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps for audio generation.
     * @default 24
     * @example 24
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description A prompt to guide the audio generation. If not provided, it will be extracted from the video.
     * @default
     */
    prompt?: string;
    /**
     * Seed
     * @description The seed for the random number generator
     */
    seed?: number;
    /**
     * Video Url
     * @description The URL of the video to generate the audio for.
     * @example https://storage.googleapis.com/falserverless/example_inputs/thinksound-input.mp4
     */
    video_url: string;
}

export interface SharedType_4b8 {
    /**
     * Seed
     * @description The seed used for random number generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video.
     */
    video: Components.File;
}

export interface SharedType_4b2 {
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '8';
    /**
     * Effect
     * @description The effect to apply to the video
     * @enum {string}
     */
    effect:
        | 'Kiss Me AI'
        | 'Kiss'
        | 'Muscle Surge'
        | 'Warmth of Jesus'
        | 'Anything, Robot'
        | 'The Tiger Touch'
        | 'Hug'
        | 'Holy Wings'
        | 'Microwave'
        | 'Zombie Mode'
        | 'Squid Game'
        | 'Baby Face'
        | 'Black Myth: Wukong'
        | 'Long Hair Magic'
        | 'Leggy Run'
        | 'Fin-tastic Mermaid'
        | 'Punch Face'
        | 'Creepy Devil Smile'
        | 'Thunder God'
        | 'Eye Zoom Challenge'
        | "Who's Arrested?"
        | 'Baby Arrived'
        | 'Werewolf Rage'
        | 'Bald Swipe'
        | 'BOOM DROP'
        | 'Huge Cutie'
        | 'Liquid Metal'
        | 'Sharksnap!'
        | 'Dust Me Away'
        | '3D Figurine Factor'
        | 'Bikini Up'
        | 'My Girlfriends'
        | 'My Boyfriends'
        | 'Subject 3 Fever'
        | 'Earth Zoom'
        | 'Pole Dance'
        | 'Vroom Dance'
        | 'GhostFace Terror'
        | 'Dragon Evoker'
        | 'Skeletal Bae'
        | 'Summoning succubus'
        | 'Halloween Voodoo Doll'
        | '3D Naked-Eye AD'
        | 'Package Explosion'
        | 'Dishes Served'
        | 'Ocean ad'
        | 'Supermarket AD'
        | 'Tree doll'
        | 'Come Feel My Abs'
        | 'The Bicep Flex'
        | 'London Elite Vibe'
        | 'Flora Nymph Gown'
        | 'Christmas Costume'
        | "It's Snowy"
        | 'Reindeer Cruiser'
        | 'Snow Globe Maker'
        | 'Pet Christmas Outfit'
        | 'Adopt a Polar Pal'
        | 'Cat Christmas Box'
        | 'Starlight Gift Box'
        | 'Xmas Poster'
        | 'Pet Christmas Tree'
        | 'City Santa Hat'
        | 'Stocking Sweetie'
        | 'Christmas Night'
        | 'Xmas Front Page Karma'
        | "Grinch's Xmas Hijack"
        | 'Giant Product'
        | 'Truck Fashion Shoot'
        | 'Beach AD'
        | 'Shoal Surround'
        | 'Mechanical Assembly'
        | 'Lighting AD'
        | 'Billboard AD'
        | 'Product close-up'
        | 'Parachute Delivery'
        | 'Dreamlike Cloud'
        | 'Macaron Machine'
        | 'Poster AD'
        | 'Truck AD'
        | 'Graffiti AD'
        | '3D Figurine Factory'
        | 'The Exclusive First Class'
        | 'Art Zoom Challenge'
        | 'I Quit'
        | 'Hitchcock Dolly Zoom'
        | 'Smell the Lens'
        | 'I believe I can fly'
        | 'Strikout Dance'
        | 'Pixel World'
        | 'Mint in Box'
        | 'Hands up, Hand'
        | 'Flora Nymph Go'
        | 'Somber Embrace'
        | 'Beam me up'
        | 'Suit Swagger';
    /**
     * Image Url
     * @description Optional URL of the image to use as the first frame. If not provided, generates from text
     * @example https://v3.fal.media/files/koala/q5ahL3KS7ikt3MvpNUG8l_image%20(72).webp
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     */
    negative_prompt?: string;
    /**
     * Resolution
     * @description The resolution of the generated video.
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p' | '1080p';
}

export interface SharedType_4ae {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description URL of image to use for inpainting. or img2img
     * @example https://storage.googleapis.com/falserverless/example_inputs/dog.png
     */
    image_url: string;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Num Images
     * @description The number of images to generate. This is always set to 1 for streaming output.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A photo of a lion sitting on a stone bench
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original.
     * @default 0.85
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_4ad {
    /**
     * Video
     * @description URL of the modified video
     * @example {
     *       "url": "https://v3.fal.media/files/lion/_2UO2QC26T_R8vKeVGAdX_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_4a0 {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     */
    enable_safety_checker?: boolean;
    /**
     * Image Url
     * @description The URL of the image to be upscaled. Must be in PNG format.
     * @example https://storage.googleapis.com/falserverless/model_tests/recraft/recraft-upscaler-1.jpeg
     */
    image_url: string;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_48b {
    /**
     * Images
     * @description The generated/edited images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/elephant/0lEToxR8cU5tB-SVMmD2C.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface SharedType_479 {
    /**
     * Acceleration
     * @description Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the final input image will be used to calculate the size of the output image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URLs of the images to edit.
     * @example [
     *       "https://v3.fal.media/files/monkey/i3saq4bAPXSIl08nZtq9P_ec535747aefc4e31943136a6d8587075.png",
     *       "https://v3.fal.media/files/penguin/BCOZp6teRhSQFuOXpbBOa_da8ef9b4982347a2a62a516b737d4f21.png",
     *       "https://v3.fal.media/files/tiger/sCoZhBksx9DvwSR4_U3_C_3d1f581441874005908addeae9c10d0f.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use up to 3 LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate the image with
     * @example Close shot of a woman standing in next to this car on this highway
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_4411 {}

export interface SharedType_441 {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the final input image will be used.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URL of the image with product to integrate into background.
     * @example [
     *       "https://v3b.fal.media/files/b/koala/LFYeCtq2LB4s6IpmoI2iy_2fb7b46d1f3749db9f7bab679bc6c4f3.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Lora Scale
     * @description The scale factor for the LoRA model. Controls the strength of the LoRA effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 6
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description Describe how to blend and integrate the product/element into the background. The model will automatically correct perspective, lighting and shadows for natural integration.
     * @default Blend and integrate the product into the background
     * @example Blend and integrate the product into the background with correct perspective and lighting
     * @example Seamlessly blend the object into the scene with natural shadows
     * @example Integrate the product naturally into the environment
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_424 {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://v3.fal.media/files/monkey/xF9OsLwGjjNURyAxD8RM1_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_417 {
    /**
     * Video
     * @description The generated video.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/veo31-flf2v-output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_411 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the image. When a guidance method is being used, the aspect ratio is defined by the guidance image and this parameter is ignored.
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?: '1:1' | '2:3' | '3:2' | '3:4' | '4:3' | '4:5' | '5:4' | '9:16' | '16:9';
    /**
     * Guidance
     * @description Guidance images to use for the generation. Up to 4 guidance methods can be combined during a single inference.
     * @default []
     */
    guidance?: Components.GuidanceInput[];
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Medium
     * @description Which medium should be included in your generated images. This parameter is optional.
     * @enum {string}
     */
    medium?: 'photography' | 'art';
    /**
     * Negative Prompt
     * @description The negative prompt you would like to use to generate images.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description How many images you would like to generate. When using any Guidance Method, Value is set to 1.
     * @default 4
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of iterations the model goes through to refine the generated image. This parameter is optional.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt you would like to use to generate images.
     * @example A lone figure stands on the edge of a serene cliff at sunset, gazing out over a vast, mystical valley. The figure is clad in flowing robes that ripple in the gentle breeze, silhouetted against the golden and lavender hues of the sky. Below, a cascading waterfall pours into a sparkling river winding through a forest of bioluminescent trees. The scene blends the awe of nature with a touch of otherworldly wonder, inviting reflection and imagination.
     */
    prompt: string;
    /**
     * Prompt Enhancement
     * @description When set to true, enhances the provided prompt by generating additional, more descriptive variations, resulting in more diverse and creative output images.
     * @default false
     */
    prompt_enhancement?: boolean;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_40d {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/vace/out_video_vace.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_3eb {
    /**
     * Images
     * @description The generated/edited images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/koala/dTldnOpRSFVBvWiyfOeO1.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface SharedType_3e8 {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_3cd {
    /**
     * Video
     * @description The generated video.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/gallery/veo3-1-i2v.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_3c3 {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "file_size": 6420765,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/video_models/output-3.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_397 {
    /**
     * Acceleration
     * @description Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
     * @default regular
     * @example regular
     */
    acceleration?: 'none' | 'low' | 'regular';
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16';
    /**
     * Auto Downsample Min FPS
     * @description The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
     * @default 15
     * @example 15
     */
    auto_downsample_min_fps?: number;
    /**
     * Enable Auto Downsample
     * @description If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
     * @default false
     * @example false
     */
    enable_auto_downsample?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * First Frame URL
     * @description URL to the first frame of the video. If provided, the model will use this frame as a reference.
     */
    first_frame_url?: string;
    /**
     * Frames per Second
     * @description Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
     * @default 16
     */
    frames_per_second?: number;
    /**
     * Guidance Scale
     * @description Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
     * @default 5
     * @example 5
     */
    guidance_scale?: number;
    /**
     * Interpolator Model
     * @description The model to use for frame interpolation. Options are 'rife' or 'film'.
     * @default film
     * @example film
     * @enum {string}
     */
    interpolator_model?: 'rife' | 'film';
    /**
     * Last Frame URL
     * @description URL to the last frame of the video. If provided, the model will use this frame as a reference.
     */
    last_frame_url?: string;
    /**
     * Match Input Frames Per Second
     * @description If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
     * @default false
     * @example false
     */
    match_input_frames_per_second?: boolean;
    /**
     * Match Input Number of Frames
     * @description If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
     * @default false
     * @example false
     */
    match_input_num_frames?: boolean;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     * @example letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 81 to 241 (inclusive).
     * @default 81
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Number of Interpolated Frames
     * @description Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
     * @default 0
     * @example 0
     */
    num_interpolated_frames?: number;
    /**
     * Preprocess
     * @description Whether to preprocess the input video.
     * @default false
     * @example false
     */
    preprocess?: boolean;
    /**
     * Prompt
     * @description The text prompt to guide video generation. For pose task, the prompt should describe the desired pose and action of the subject in the video.
     * @example A sharply dressed man walks toward the camera down a sun-drenched hallway.  Medium shot: He's framed from the knees up, his confident stride filling the frame.  His navy blue business suit is impeccably tailored, the fabric subtly shimmering under the light streaming through the tall, arched windows lining the hallway. Close-up:  The rich texture of the suit's wool is visible, each thread reflecting the light.  His crisp white shirt contrasts beautifully with the deep crimson of his silk tie, the knot perfectly formed.  The sunlight highlights the subtle sheen of his polished shoes.  The windows cast long shadows, highlighting the architectural detail of the hallway, creating a sense of both elegance and movement. High resolution 4k.
     */
    prompt: string;
    /**
     * Reference Image URLs
     * @description URLs to source reference image. If provided, the model will use this image as reference.
     */
    ref_image_urls?: string[];
    /**
     * Resolution
     * @description Resolution of the generated video.
     * @default auto
     * @enum {string}
     */
    resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p';
    /**
     * Return Frames Zip
     * @description If true, also return a ZIP file containing all generated frames.
     * @default false
     * @example false
     */
    return_frames_zip?: boolean;
    /**
     * Sampler
     * @description Sampler to use for video generation.
     * @default unipc
     * @example unipc
     * @enum {string}
     */
    sampler?: 'unipc' | 'dpm++' | 'euler';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift parameter for video generation.
     * @default 5
     */
    shift?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     * @example false
     */
    sync_mode?: boolean;
    /**
     * Temporal Downsample Factor
     * @description Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
     * @default 0
     * @example 0
     */
    temporal_downsample_factor?: number;
    /**
     * Transparency Mode
     * @description The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
     * @default content_aware
     * @example content_aware
     * @enum {string}
     */
    transparency_mode?: 'content_aware' | 'white' | 'black';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video URL
     * @description URL to the source video file. Required for pose task.
     * @example https://storage.googleapis.com/falserverless/example_inputs/wan-vace-pose-video.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface SharedType_38d {
    /**
     * Image Url
     * @description The URL of the image to be processed.
     * @example https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg
     * @example http://ecx.images-amazon.com/images/I/51UUzBDAMsL.jpg
     */
    image_url: string;
}

export interface SharedType_386 {
    /**
     * Images
     * @description The generated images with objects removed.
     * @example [
     *       {
     *         "file_size": 730703,
     *         "height": 768,
     *         "file_name": "85a2309b2c954c85a75120e664adbe17.png",
     *         "content_type": "image/png",
     *         "url": "https://v3.fal.media/files/lion/arYSoJeqWjhbcA8o4budv_85a2309b2c954c85a75120e664adbe17.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface SharedType_377 {
    /**
     * Output
     * @description Output for the given query
     */
    output: string;
}

export interface SharedType_375 {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "file_size": 5485412,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://fal.media/files/lion/_fVEU5nzHND_fHGQUhXEm_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_371 {
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://fal.media/files/elephant/8kkhB12hEZI2kkbU8pZPA_test.jpeg
     */
    image_url: string;
    /**
     * Prompt
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage.
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to use the model's prompt optimizer
     * @default true
     */
    prompt_optimizer?: boolean;
}

export interface SharedType_36a {
    /**
     * Images
     * @description The remixed images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/zebra/r0J_UFupv3BfooTwv2ifJ.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface SharedType_368 {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/jpeg",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/flux-srpo-output.jpeg",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_35f {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/jpeg",
     *         "url": "https://v3.fal.media/files/zebra/mMW8_S5PeGuDXLTfIKCpG.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_352 {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/z-image-turbo-output.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /**
     * Timings
     * @description The timings of the generation process.
     */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_346 {
    /**
     * Video
     * @description Final video.
     */
    video: Components.File_1 | Components.File_1;
}

export interface SharedType_328 {
    /**
     * Video
     * @description The generated video file.
     */
    video: Components.File;
}

export interface SharedType_2fc {
    /**
     * Video
     * @description The extended video
     * @example {
     *       "file_size": 1163040,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3.fal.media/files/rabbit/88-jI3VWXU4Q8kSNrWo3c_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_2c5 {
    /**
     * Video
     * @description URL of the reframed video
     * @example {
     *       "url": "https://v3.fal.media/files/lion/L9nkXSW1MCj2oDimeJ4w5_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_2c4 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16';
    /**
     * Auto Fix
     * @description Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
     * @default false
     */
    auto_fix?: boolean;
    /**
     * Duration
     * @description The duration of the generated video.
     * @default 8s
     * @enum {string}
     */
    duration?: '4s' | '6s' | '8s';
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Image URL
     * @description URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit.
     * @example https://storage.googleapis.com/falserverless/example_inputs/veo3-i2v-input.png
     */
    image_url: string;
    /**
     * Prompt
     * @description The text prompt describing how the image should be animated
     * @example A woman looks into the camera, breathes in, then exclaims energetically, "have you guys checked out Veo3 Image-to-Video on Fal? It's incredible!"
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video.
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p' | '1080p';
}

export interface SharedType_2a3 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image. Use "auto" to let the model decide based on the prompt.
     * @default 1:1
     */
    aspect_ratio?:
        | 'auto'
        | '21:9'
        | '16:9'
        | '3:2'
        | '4:3'
        | '5:4'
        | '1:1'
        | '4:5'
        | '3:4'
        | '2:3'
        | '9:16';
    /**
     * Enable Web Search
     * @description Enable web search for the image generation task. This will allow the model to use the latest information from the web to generate the image.
     * @default false
     */
    enable_web_search?: boolean;
    /**
     * Limit Generations
     * @description Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.
     * @default false
     */
    limit_generations?: boolean;
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The text prompt to generate an image from.
     * @example An action shot of a black lab swimming in an inground suburban swimming pool. The camera is placed meticulously on the water line, dividing the image in half, revealing both the dogs head above water holding a tennis ball in it's mouth, and it's paws paddling underwater.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the image to generate.
     * @default 1K
     * @enum {string}
     */
    resolution?: '1K' | '2K' | '4K';
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_298 {
    /**
     * Transcribed Text
     * @description The partial or final transcription output from Canary
     */
    output: string;
    /**
     * Partial
     * @description Indicates if this is a partial (in-progress) transcript
     * @default false
     */
    partial?: boolean;
}

export interface SharedType_266 {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "file_size": 797478,
     *       "file_name": "6c9dd31e1d9a4482877747a52a661a0a.mp4",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3.fal.media/files/elephant/-huMN0zTaXmBr2CqzCMps_6c9dd31e1d9a4482877747a52a661a0a.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_25a {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://fal.media/files/monkey/vNZqQV_WgC9MhoidClLyw_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_24d {
    /**
     * Seed
     * @description Seed used for generating the video.
     */
    seed: number;
    /**
     * Video
     * @description Generated video file.
     * @example {
     *       "url": "https://fal-cdn.batuhan-941.workers.dev/files/koala/5Cb_6P_s9wW8f8-g9c4yj.mp4"
     *     }
     */
    video: Components.File_1;
}

export interface SharedType_247 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '4:3' | '1:1' | '3:4' | '9:16';
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     * @example blurry, low quality, low resolution, pixelated, noisy, grainy, out of focus, poorly lit, poorly exposed, poorly composed, poorly framed, poorly cropped, poorly color corrected, poorly color graded
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example Epic low-cut camera capture of a girl clad in ultraviolet threads, Peter Max art style depiction, luminous diamond skin glistening under a vast moon's radiance, embodied in a superhuman flight among mystical ruins, symbolizing a deity's ritual ascent, hyper-detailed
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
    /**
     * Style
     * @description The style of the generated video
     * @enum {string}
     */
    style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk';
}

export interface SharedType_21e {
    /** @description ZIP archive of all video frames if requested. */
    frames_zip?: Components.File_1;
    /**
     * Prompt
     * @description The prompt used for generation.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * @description The generated pose video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/wan-vace-pose-output.mp4"
     *     }
     */
    video: Components.VideoFile;
}

export interface SharedType_21d {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 6
     */
    guidance_scale?: number;
    /**
     * Temperature after disabling CFG
     * @description Temperature after disabling CFG
     * @default 0.1
     */
    last_scale_temp?: number;
    /**
     * More Diverse
     * @description More diverse sampling
     * @default false
     */
    more_diverse?: boolean;
    /**
     * More Smooth
     * @description Smoothing with Gumbel softmax sampling
     * @default true
     */
    more_smooth?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A cat wearing a hoodie with 'FAL' written on it.
     */
    prompt: string;
    /**
     * Sampling Top-k
     * @description The number of top-k tokens to sample from.
     * @default 400
     */
    sampling_top_k?: number;
    /**
     * Sampling Top-p
     * @description The top-p probability to sample from.
     * @default 0.95
     */
    sampling_top_p?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Smoothing starting scale
     * @description Smoothing starting scale
     * @default 2
     */
    smooth_start_si?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Disable CFG starting scale
     * @description Disable CFG starting scale
     * @default 8
     */
    turn_off_cfg_start_si?: number;
}

export interface SharedType_215 {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the final input image will be used.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URLs of the images to combine into a group photo. Provide 2 or more individual portrait images.
     * @example [
     *       "https://v3.fal.media/files/monkey/i3saq4bAPXSIl08nZtq9P_ec535747aefc4e31943136a6d8587075.png",
     *       "https://v3b.fal.media/files/b/kangaroo/OEtbMr7E43t0UPT8JwRT4_091834d85d8346d6960e3fd789d67db8.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Lora Scale
     * @description The scale factor for the LoRA model. Controls the strength of the LoRA effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 6
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description Describe the group photo scene, setting, and style. The model will maintain character consistency and add vintage effects like grain, blur, and retro filters.
     * @default Two people standing next to each other outside with a landscape background
     * @example Two people standing next to each other outside with a landscape background
     * @example Group photo outdoors with mountains and nature in the background, vintage style
     * @example Two people next to each other in a scenic outdoor setting with retro filter
     * @example People standing together outside with beautiful landscape behind them
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_207 {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Num Images
     * @description The number of images to generate. This is always set to 1 for streaming output.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word "FLUX" is painted over it in big, white brush strokes with visible texture.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_1d1 {
    /**
     * Audio Setting
     * @description Audio configuration settings
     */
    audio_setting?: Components.AudioSetting;
    /**
     * Language Boost
     * @description Enhance recognition of specified languages and dialects
     * @enum {string}
     */
    language_boost?:
        | 'Chinese'
        | 'Chinese,Yue'
        | 'English'
        | 'Arabic'
        | 'Russian'
        | 'Spanish'
        | 'French'
        | 'Portuguese'
        | 'German'
        | 'Turkish'
        | 'Dutch'
        | 'Ukrainian'
        | 'Vietnamese'
        | 'Indonesian'
        | 'Japanese'
        | 'Italian'
        | 'Korean'
        | 'Thai'
        | 'Polish'
        | 'Romanian'
        | 'Greek'
        | 'Czech'
        | 'Finnish'
        | 'Hindi'
        | 'Bulgarian'
        | 'Danish'
        | 'Hebrew'
        | 'Malay'
        | 'Slovak'
        | 'Swedish'
        | 'Croatian'
        | 'Hungarian'
        | 'Norwegian'
        | 'Slovenian'
        | 'Catalan'
        | 'Nynorsk'
        | 'Afrikaans'
        | 'auto';
    /**
     * Output Format
     * @description Format of the output content (non-streaming only)
     * @default hex
     * @enum {string}
     */
    output_format?: 'url' | 'hex';
    /**
     * Pronunciation Dict
     * @description Custom pronunciation dictionary for text replacement
     */
    pronunciation_dict?: Components.PronunciationDict;
    /**
     * Text
     * @description Text to convert to speech (max 5000 characters, minimum 1 non-whitespace character)
     * @example Hello world! This is a test of the text-to-speech system.
     */
    text: string;
    /**
     * Voice Setting
     * @description Voice configuration settings
     * @default {
     *       "speed": 1,
     *       "vol": 1,
     *       "voice_id": "Wise_Woman",
     *       "pitch": 0,
     *       "english_normalization": false
     *     }
     */
    voice_setting?: Components.VoiceSetting;
}

export interface SharedType_1b6 {
    /**
     * @description The generated audio file
     * @example {
     *       "url": "https://v3.fal.media/files/zebra/zJL_oRY8h5RWwjoK1w7tx_output.mp3"
     *     }
     */
    audio: Components.File_1;
    /**
     * Timestamps
     * @description Timestamps for each word in the generated speech. Only returned if `timestamps` is set to True in the request.
     */
    timestamps?: { [x: string]: any }[];
}

export interface SharedType_1b2 {
    /**
     * Description
     * @description Text description or response from Gemini
     */
    description: string;
    /** @description The generated or edited image */
    image: Components.Image_2;
}

export interface SharedType_18d {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 888,
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/z-image-inpaint-output.png",
     *         "width": 512
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /**
     * Timings
     * @description The timings of the generation process.
     */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_187 {
    /**
     * Default Caption
     * @description Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string;
    /**
     * Image Data Url
     * @description URL to the input data zip archive.
     *
     *         The zip should contain pairs of images. The images should be named:
     *
     *         ROOT_start.EXT and ROOT_end.EXT
     *         For example:
     *         photo_start.jpg and photo_end.jpg
     *
     *         The zip can also contain more than one reference image for each image pair. The reference images should be named:
     *         ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT
     *         For example:
     *         photo_start.jpg, photo_start2.jpg, photo_end.jpg
     *
     *         The Reference Image Count field should be set to the number of reference images.
     *
     *         The zip can also contain a text file for each image pair. The text file should be named:
     *         ROOT.txt
     *         For example:
     *         photo.txt
     *
     *         This text file can be used to specify the edit instructions for the image pair.
     *
     *         If no text file is provided, the default_caption will be used.
     *
     *         If no default_caption is provided, the training will fail.
     */
    image_data_url: string;
    /**
     * Learning Rate
     * @description Learning rate for LoRA parameters.
     * @default 0.0001
     */
    learning_rate?: number;
    /**
     * Steps
     * @description Number of steps to train for
     * @default 1000
     */
    steps?: number;
}

export interface SharedType_156 {
    /**
     * Video
     * @description URL of the generated video
     * @example {
     *       "url": "https://v3.fal.media/files/zebra/9aDde3Te2kuJYHdR0Kz8R_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_152 {
    /**
     * Images
     * @description The generated/edited images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/koala/efKAFkAtgzxZeLSdv-d2x.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface SharedType_151 {
    /**
     * Video
     * @description The generated video.
     * @example {
     *       "file_size": 7533071,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/0a86603b/YAlbB2535l07BTy1wpDeI_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_13e {
    /** @description ZIP archive of all video frames if requested. */
    frames_zip?: Components.File_1;
    /**
     * Prompt
     * @description The prompt used for generation.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * @description The generated depth video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/wan-vace-depth-output.mp4"
     *     }
     */
    video: Components.VideoFile;
}

export interface SharedType_137 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Enhance Prompt
     * @description Whether to enhance the prompt for better results.
     * @default false
     */
    enhance_prompt?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description Image prompt for the omni model.
     * @example [
     *       "https://v3.fal.media/files/penguin/XoW0qavfF-ahg-jX4BMyL_image.webp",
     *       "https://v3.fal.media/files/tiger/bml6YA7DWJXOigadvxk75_image.webp"
     *     ]
     */
    image_urls: string[];
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Put the little duckling on top of the woman's t-shirt.
     */
    prompt: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_135 {
    /**
     * Video
     * @description The generated video from image using the Q2 model
     * @example {
     *       "url": "https://fal.media/files/tiger/L_lU76tYg-cXG_twy9N62_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_129 {
    /**
     * Results
     * @description Results from the model
     */
    results: string;
}

export interface SharedType_110 {
    /**
     * Images
     * @description The generated/edited images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/kangaroo/GGvzZELjxMpFvV2IAEb_9.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface SharedType_0f9 {
    /** @description Generated 3D mesh file */
    model_mesh: Components.File_1;
    /**
     * Timings
     * @description Processing timings
     */
    timings: {
        [key: string]: number;
    };
}

export interface SharedType_0d8 {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the final input image will be used.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URL of the image containing elements to remove.
     * @example [
     *       "https://v3b.fal.media/files/b/elephant/oWup_Q7zuvbfB4en-hneO_5aaa1cb3d3eb44999005159e82e7c9b7.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Lora Scale
     * @description The scale factor for the LoRA model. Controls the strength of the LoRA effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 6
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description Specify what element(s) to remove from the image (objects, people, text, etc.). The model will cleanly remove the element while maintaining consistency of the rest of the image.
     * @default Remove the specified element from the scene
     * @example Remove the person from the image
     * @example Remove the car and the bicycle
     * @example Remove the text and logos
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_0d0 {
    /**
     * Image Url
     * @description Reference image (file or URL).
     */
    image_url?: string;
    /**
     * Prompt
     * @description Prompt for image generation.
     * @example A hyper-detailed, ultra-fluffy owl sitting in the trees at night, looking directly at the camera with wide, adorable, expressive eyes. Its feathers are soft and voluminous, catching the cool moonlight with subtle silver highlights. The owl’s gaze is curious and full of charm, giving it a whimsical, storybook-like personality.
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility.
     * @default 5555
     */
    seed?: number;
    /** @description The structured prompt to generate an image from. */
    structured_prompt?: Components.StructuredPrompt;
}

export interface SharedType_0cd {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *             the model to stick to your prompt when looking for a related image to show you.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The URL of the image to generate an image from.
     * @example https://fal.media/files/koala/Chls9L2ZnvuipUTEwlnJC.png
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A cat dressed as a wizard with a background of a mystic forest.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength of the initial image. Higher strength values are better for this model.
     * @default 0.95
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_0c0 {
    /**
     * Image
     * @description The generated image file info.
     */
    image: Components.Image;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
}

export interface SharedType_0bd {
    /**
     * Prompt
     * @description The prompt used for generation.
     * @example A cinematic fast-tracking shot follows a vintage, teal camper van as it descends a winding mountain trail. The van, slightly weathered but well-maintained, is the central focus, its retro design emphasized by the motion blur. Medium shot reveals the dusty, ochre trail, edged with vibrant green pine trees. Close-up on the van's tires shows the gravel spraying, highlighting the speed and rugged terrain. Sunlight filters through the trees, casting dappled shadows on the van and the trail. The background is a hazy, majestic mountain range bathed in warm, golden light. The overall mood is adventurous and exhilarating. High resolution 4k movie scene.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/ltxv-multiscale-text-to-video.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_0b5 {
    /** @description ZIP archive of all video frames if requested. */
    frames_zip?: Components.File_1;
    /**
     * Prompt
     * @description The prompt used for generation.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * @description The generated inpainting video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/wan-vace-inpainting-output.mp4"
     *     }
     */
    video: Components.VideoFile;
}

export interface SharedType_0ab {
    /**
     * Acceleration
     * @description Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
     * @default regular
     * @example regular
     */
    acceleration?: 'none' | 'low' | 'regular';
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16';
    /**
     * Auto Downsample Min FPS
     * @description The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
     * @default 15
     * @example 15
     */
    auto_downsample_min_fps?: number;
    /**
     * Enable Auto Downsample
     * @description If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
     * @default false
     * @example false
     */
    enable_auto_downsample?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * First Frame URL
     * @description URL to the first frame of the video. If provided, the model will use this frame as a reference.
     */
    first_frame_url?: string;
    /**
     * Frames per Second
     * @description Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
     * @default 16
     */
    frames_per_second?: number;
    /**
     * Guidance Scale
     * @description Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
     * @default 5
     * @example 5
     */
    guidance_scale?: number;
    /**
     * Interpolator Model
     * @description The model to use for frame interpolation. Options are 'rife' or 'film'.
     * @default film
     * @example film
     * @enum {string}
     */
    interpolator_model?: 'rife' | 'film';
    /**
     * Last Frame URL
     * @description URL to the last frame of the video. If provided, the model will use this frame as a reference.
     */
    last_frame_url?: string;
    /**
     * Mask Image URL
     * @description URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video using salient mask tracking. Will be ignored if mask_video_url is provided.
     */
    mask_image_url?: string;
    /**
     * Mask Video URL
     * @description URL to the source mask file. Required for inpainting.
     * @example https://storage.googleapis.com/falserverless/vace/src_mask.mp4
     */
    mask_video_url: string;
    /**
     * Match Input Frames Per Second
     * @description If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
     * @default false
     * @example false
     */
    match_input_frames_per_second?: boolean;
    /**
     * Match Input Number of Frames
     * @description If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
     * @default false
     * @example false
     */
    match_input_num_frames?: boolean;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     * @example letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 81 to 241 (inclusive).
     * @default 81
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Number of Interpolated Frames
     * @description Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
     * @default 0
     * @example 0
     */
    num_interpolated_frames?: number;
    /**
     * Preprocess
     * @description Whether to preprocess the input video.
     * @default false
     * @example false
     */
    preprocess?: boolean;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example The video shows a man riding a horse on a vast grassland. He has long lavender hair and wears a traditional dress of a white top and black pants. The animation style makes him look like he is doing some kind of outdoor activity or performing. The background is a spectacular mountain range and cloud sky, giving a sense of tranquility and vastness. The entire video is shot from a fixed angle, focusing on the rider and his horse.
     */
    prompt: string;
    /**
     * Reference Image URLs
     * @description Urls to source reference image. If provided, the model will use this image as reference.
     * @example [
     *       "https://storage.googleapis.com/falserverless/vace/src_ref_image_1.png"
     *     ]
     */
    ref_image_urls?: string[];
    /**
     * Resolution
     * @description Resolution of the generated video.
     * @default auto
     * @enum {string}
     */
    resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p';
    /**
     * Return Frames Zip
     * @description If true, also return a ZIP file containing all generated frames.
     * @default false
     * @example false
     */
    return_frames_zip?: boolean;
    /**
     * Sampler
     * @description Sampler to use for video generation.
     * @default unipc
     * @example unipc
     * @enum {string}
     */
    sampler?: 'unipc' | 'dpm++' | 'euler';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift parameter for video generation.
     * @default 5
     */
    shift?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     * @example false
     */
    sync_mode?: boolean;
    /**
     * Temporal Downsample Factor
     * @description Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
     * @default 0
     * @example 0
     */
    temporal_downsample_factor?: number;
    /**
     * Transparency Mode
     * @description The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
     * @default content_aware
     * @example content_aware
     * @enum {string}
     */
    transparency_mode?: 'content_aware' | 'white' | 'black';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video URL
     * @description URL to the source video file. Required for inpainting.
     * @example https://storage.googleapis.com/falserverless/vace/src_video.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface SharedType_096 {
    /**
     * Images
     * @description The generated/edited images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/penguin/4_Bz95EOoETXJlfuWib3r.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface SharedType_085 {
    /**
     * Prompt
     * @description The prompt used for generation.
     * @example realistic filming style, a person wearing a dark helmet, a deep-colored jacket, blue jeans, and bright yellow shoes rides a skateboard along a winding mountain road. The skateboarder starts in a standing position, then gradually lowers into a crouch, extending one hand to touch the road surface while maintaining a low center of gravity to navigate a sharp curve. After completing the turn, the skateboarder rises back to a standing position and continues gliding forward. The background features lush green hills flanking both sides of the road, with distant snow-capped mountain peaks rising against a clear, bright blue sky. The camera follows closely from behind, smoothly tracking the skateboarder’s movements and capturing the dynamic scenery along the route. The scene is shot in natural daylight, highlighting the vivid outdoor environment and the skateboarder’s fluid actions.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation.
     * @example 424911732
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/zebra/lXFrGA-egaUXWFGSp8GqT_BxoDEqUZ.mp4"
     *     }
     */
    video: Components.File;
}

export interface SharedType_084 {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the final input image will be used.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URLs of the images: first image is the person wearing a shirt, second image is the design/logo to put on the shirt.
     * @example [
     *       "https://v3b.fal.media/files/b/tiger/1rq65RzrUwKtHLAwpEjq8_4ee388931b5142f1bd1f2e0a3cb2498e.png",
     *       "https://github.com/fal-ai/fal-assets/blob/main/Logo%20Square.png?raw=true"
     *     ]
     */
    image_urls: string[];
    /**
     * Lora Scale
     * @description The scale factor for the LoRA model. Controls the strength of the LoRA effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 6
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description Describe what design to put on the shirt. The model will apply the design from your input image onto the person's shirt.
     * @default Put this design on their shirt
     * @example Put this design on their shirt
     * @example Apply this graphic to their t-shirt
     * @example Place this logo on their shirt
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SharedType_075 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16';
    /**
     * Auto Fix
     * @description Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
     * @default true
     */
    auto_fix?: boolean;
    /**
     * Duration
     * @description The duration of the generated video.
     * @default 8s
     * @enum {string}
     */
    duration?: '4s' | '6s' | '8s';
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Negative Prompt
     * @description A negative prompt to guide the video generation.
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The text prompt describing the video you want to generate
     * @example A casual street interview on a busy New York City sidewalk in the afternoon. The interviewer holds a plain, unbranded microphone and asks: Have you seen Google's new Veo3 model It is a super good model. Person replies: Yeah I saw it, it's already available on fal. It's crazy good.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video.
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p' | '1080p';
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
}

export interface SharedType_055 {
    /**
     * Image
     * @description Upscaled image
     */
    image: Components.Image;
}

export interface SharedType_04b {
    /**
     * Bgm
     * @description Whether to add background music to the video (only for 4-second videos)
     * @default false
     */
    bgm?: boolean;
    /**
     * Duration
     * @description Duration of the video in seconds
     * @default 4
     * @enum {integer}
     */
    duration?: 2 | 3 | 4 | 5 | 6 | 7 | 8;
    /**
     * End Image Url
     * @description URL of the image to use as the ending frame. When provided, generates a transition video between start and end frames.
     */
    end_image_url?: string;
    /**
     * Image Url
     * @description URL of the image to use as the starting frame
     * @example https://storage.googleapis.com/falserverless/web-examples/vidu/stylish_woman.webp
     */
    image_url: string;
    /**
     * Movement Amplitude
     * @description The movement amplitude of objects in the frame
     * @default auto
     * @enum {string}
     */
    movement_amplitude?: 'auto' | 'small' | 'medium' | 'large';
    /**
     * Prompt
     * @description Text prompt for video generation, max 3000 characters
     * @example A woman walking through a vibrant city street at night, neon lights reflecting off wet pavement.
     */
    prompt: string;
    /**
     * Resolution
     * @description Output video resolution
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p' | '1080p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface SharedType_036 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '4:3' | '1:1' | '3:4' | '9:16';
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '8';
    /**
     * End Image Url
     * @description URL of the image to use as the last frame
     * @example https://v3.fal.media/files/kangaroo/RgedFs_WSnq5BgER7qDx1_ONrbTJ1YAGXz-9JnSsBoB_bdc8750387734bfe940319f469f7b0b2.jpg
     */
    end_image_url?: string;
    /**
     * First Image Url
     * @description URL of the image to use as the first frame
     * @example https://v3.fal.media/files/zebra/owQh2DAzk8UU7J02nr5RY_Co2P4boLv6meIZ5t9gKvL_8685da151df343ab8bf82165c928e2a5.jpg
     */
    first_image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     * @example blurry, low quality, low resolution, pixelated, noisy, grainy, out of focus, poorly lit, poorly exposed, poorly composed, poorly framed, poorly cropped, poorly color corrected, poorly color graded
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The prompt for the transition
     * @example Scene slowly transition into cat swimming under water
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p' | '1080p';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
    /**
     * Style
     * @description The style of the generated video
     * @enum {string}
     */
    style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk';
}

export interface SharedType_02c {
    /**
     * Duration
     * @description Video duration in seconds.
     * @default 5
     * @enum {string}
     */
    duration?: '3' | '4' | '5' | '6' | '7' | '8' | '9' | '10';
    /**
     * End Image Url
     * @description Image to use as the last frame of the video.
     *
     *     Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
     * @example https://v3b.fal.media/files/b/tiger/BwHi22qoQnqaTNMMhe533.png
     */
    end_image_url?: string;
    /**
     * Prompt
     * @description Use @Image1 to reference the start frame, @Image2 to reference the end frame.
     * @example Create a magical timelapse transition. The snow melts rapidly to reveal green grass, and the tree branches burst into bloom with pink flowers in real-time. The lighting shifts from cold winter light to warm spring sunshine. The camera pushes in slowly towards the tree. Disney-style magical transformation, cinematic, 8k.
     */
    prompt: string;
    /**
     * Start Image Url
     * @description Image to use as the first frame of the video.
     *
     *     Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
     * @example https://v3b.fal.media/files/b/rabbit/NaslJIC7F2WodS6DFZRRJ.png
     */
    start_image_url: string;
}

export interface SharedType_000 {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?:
        | '10:16'
        | '16:10'
        | '9:16'
        | '16:9'
        | '4:3'
        | '3:4'
        | '1:1'
        | '1:3'
        | '3:1'
        | '3:2'
        | '2:3';
    /**
     * Expand Prompt
     * @description Whether to expand the prompt with MagicPrompt functionality.
     * @default true
     */
    expand_prompt?: boolean;
    /**
     * Negative Prompt
     * @description A negative prompt to avoid in the generated image
     * @default
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example A comic style illustration of a skeleton sitting on a toilet in a bathroom. The bathroom has a Halloween decoration with a pumpkin jack-o-lantern and bats flying around. There is a text above the skeleton that says "Just Waiting for Halloween with Ideogram 2.0 at fal.ai"
     */
    prompt: string;
    /**
     * Seed
     * @description Seed for the random number generator
     */
    seed?: number;
    /**
     * Style
     * @description The style of the generated image
     * @default auto
     * @enum {string}
     */
    style?: 'auto' | 'general' | 'realistic' | 'design' | 'render_3D' | 'anime';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface RundiffusionfalRundiffusionPhotoFluxInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Photo Lora Scale
     * @description LoRA Scale of the photo lora model
     * @default 0.75
     */
    photo_lora_scale?: number;
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word "FLUX" is painted over it in big, white brush strokes with visible texture.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface RundiffusionfalRundiffusionPhotoFluxOutput extends SharedType_a73 {}

export interface RundiffusionfalJuggernautFluxProImageToImageInput extends SharedType_663 {}

export interface RundiffusionfalJuggernautFluxProImageToImageOutput extends SharedType_a73 {}

export interface RundiffusionfalJuggernautFluxProInput extends SharedType_7f3 {}

export interface RundiffusionfalJuggernautFluxProOutput extends SharedType_a73 {}

export interface RundiffusionfalJuggernautFluxLightningInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 4
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word "FLUX" is painted over it in big, white brush strokes with visible texture.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface RundiffusionfalJuggernautFluxLightningOutput extends SharedType_a73 {}

export interface RundiffusionfalJuggernautFluxBaseImageToImageInput extends SharedType_663 {}

export interface RundiffusionfalJuggernautFluxBaseImageToImageOutput extends SharedType_a73 {}

export interface RundiffusionfalJuggernautFluxBaseInput extends SharedType_7f3 {}

export interface RundiffusionfalJuggernautFluxBaseOutput extends SharedType_a73 {}

export interface RundiffusionfalJuggernautFluxLoraInpaintingInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description URL of image to use for inpainting. or img2img
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png
     */
    image_url: string;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Mask Url
     * @description The mask to area to Inpaint in.
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png
     */
    mask_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A photo of a lion sitting on a stone bench
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original.
     * @default 0.85
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface RundiffusionfalJuggernautFluxLoraInpaintingOutput extends SharedType_a73 {}

export interface RundiffusionfalJuggernautFluxLoraInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word "FLUX" is painted over it in big, white brush strokes with visible texture.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface RundiffusionfalJuggernautFluxLoraOutput extends SharedType_a73 {}

export interface ResembleaiChatterboxhdTextToSpeechInput {
    /**
     * Audio Url
     * @description URL to the audio sample to use as a voice prompt for zero-shot TTS voice cloning. Providing a audio sample will override the voice setting. If neither voice nor audio_url are provided, a random voice will be used.
     * @example https://storage.googleapis.com/chatterbox-demo-samples/prompts/male_rickmorty.mp3
     * @example https://storage.googleapis.com/chatterbox-demo-samples/prompts/male_old_movie.flac
     */
    audio_url?: string;
    /**
     * Cfg
     * @description Classifier-free guidance scale (CFG) controls the conditioning factor. Range typically 0.2 to 1.0. For expressive or dramatic speech, try lower cfg values (e.g. ~0.3) and increase exaggeration to around 0.7 or higher. If the reference speaker has a fast speaking style, lowering cfg to around 0.3 can improve pacing.
     * @default 0.5
     */
    cfg?: number;
    /**
     * Exaggeration
     * @description Controls emotion exaggeration. Range typically 0.25 to 2.0.
     * @default 0.5
     */
    exaggeration?: number;
    /**
     * High Quality Audio
     * @description If True, the generated audio will be upscaled to 48kHz. The generation of the audio will take longer, but the quality will be higher. If False, the generated audio will be 24kHz.
     * @default false
     */
    high_quality_audio?: boolean;
    /**
     * Seed
     * @description Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file. Set to 0 for random seed.
     * @default 0
     */
    seed?: number;
    /**
     * Temperature
     * @description Controls the randomness of generation. Range typically 0.05 to 5.
     * @default 0.8
     */
    temperature?: number;
    /**
     * Text
     * @description Text to synthesize into speech.
     * @default My name is Maximus Decimus Meridius, commander of the Armies of the North, General of the Felix Legions and loyal servant to the true emperor, Marcus Aurelius. Father to a murdered son, husband to a murdered wife. And I will have my vengeance, in this life or the next.
     */
    text?: string;
    /**
     * Voice
     * @description The voice to use for the TTS request. If neither voice nor audio are provided, a random voice will be used.
     * @enum {string}
     */
    voice?:
        | 'Aurora'
        | 'Blade'
        | 'Britney'
        | 'Carl'
        | 'Cliff'
        | 'Richard'
        | 'Rico'
        | 'Siobhan'
        | 'Vicky';
}

export interface ResembleaiChatterboxhdTextToSpeechOutput {
    /**
     * Audio
     * @description The generated audio file.
     * @example {
     *       "url": "https://storage.googleapis.com/chatterbox-demo-samples/samples/gladiator_rick.wav"
     *     }
     * @example {
     *       "url": "https://storage.googleapis.com/chatterbox-demo-samples/samples/gladiator_old_movie.wav"
     *     }
     */
    audio: Components.File;
}

export interface ResembleaiChatterboxhdSpeechToSpeechInput {
    /**
     * High Quality Audio
     * @description If True, the generated audio will be upscaled to 48kHz. The generation of the audio will take longer, but the quality will be higher. If False, the generated audio will be 24kHz.
     * @default false
     */
    high_quality_audio?: boolean;
    /**
     * Source Audio Url
     * @description URL to the source audio file to be voice-converted.
     * @example https://storage.googleapis.com/chatterbox-demo-samples/samples/duff_stewie.wav
     */
    source_audio_url: string;
    /**
     * Target Voice
     * @description The voice to use for the speech-to-speech request. If neither target_voice nor target_voice_audio_url are provided, a random target voice will be used.
     * @enum {string}
     */
    target_voice?:
        | 'Aurora'
        | 'Blade'
        | 'Britney'
        | 'Carl'
        | 'Cliff'
        | 'Richard'
        | 'Rico'
        | 'Siobhan'
        | 'Vicky';
    /**
     * Target Voice Audio Url
     * @description URL to the audio file which represents the voice of the output audio. If provided, this will override the target_voice setting. If neither target_voice nor target_voice_audio_url are provided, the default target voice will be used.
     * @example https://v3.fal.media/files/tiger/0XODRhebRLiBdu8MqgZc5_tmpljqsylwu.wav
     */
    target_voice_audio_url?: string;
}

export interface ResembleaiChatterboxhdSpeechToSpeechOutput {
    /**
     * Audio
     * @description The generated voice-converted audio file.
     * @example {
     *       "url": "https://v3.fal.media/files/elephant/Kym3zK7hFHjDuyz3tB3W9_tmptvowq60i.wav"
     *     }
     */
    audio: Components.File;
}

export interface PerceptronIsaac01Input {
    /**
     * Image Url
     * @description Image URL to be processed
     * @example https://v3b.fal.media/files/b/penguin/BxDPafViqMBGfNyvcmG-C_image-1d100e9%20(4).jpg
     */
    image_url: string;
    /**
     * Prompt
     * @description Prompt to be used for the image
     * @example Which car is trying to park into garage?
     */
    prompt: string;
    /**
     * Response Style
     * @description Response style to be used for the image.
     *
     *     - text: Model will output text. Good for descriptions and captioning.
     *     - box: Model will output a combination of text and bounding boxes. Good for
     *     localization.
     *     - point: Model will output a combination of text and points. Good for counting many
     *     objects.
     *     - polygon: Model will output a combination of text and polygons. Good for granular
     *     segmentation.
     * @default text
     * @enum {string}
     */
    response_style?: 'text' | 'box' | 'point' | 'polygon';
}

export interface PerceptronIsaac01Output {
    /**
     * Error
     * @description Error message if an error occurred
     */
    error?: string;
    /**
     * Output
     * @description Generated output
     * @example To determine which car is trying to park into the garage, we need to carefully observe the positions and movements of the vehicles in the image.
     *
     *     1. **Identify the Vehicles**:
     *        - There are three vehicles visible: a green truck, an orange car, and a brown car.
     *        - The green truck is positioned in front of the garage entrance.
     *        - The orange car is parked to the left of the green truck.
     *        - The brown car is parked to the right of the green truck.
     *
     *     2. **Analyze the Positions**:
     *        - The green truck is directly in front of the garage entrance, suggesting it is in the process of moving towards the entrance.
     *        - The orange car is parked parallel to the garage but is not in the path of the green truck.
     *        - The brown car is also parked parallel to the garage but is not in the path of the green truck.
     *
     *     3. **Determine the Action**:
     *        - Given the position of the green truck directly in front of the garage entrance, it is most likely that the green truck is trying to park into the garage.
     *        - The orange and brown cars are already parked and do not appear to be in motion or attempting to park.
     *
     *     Therefore, the car that is trying to park into the garage is the green truck.
     */
    output: string;
    /**
     * Partial
     * @description Whether the output is partial
     * @default false
     */
    partial?: boolean;
    /** @description Usage information */
    usage?: Components.CompletionUsage;
}

export interface OpenrouterRouterVisionInput {
    /**
     * Image Urls
     * @description List of image URLs to be processed
     * @example [
     *       "https://fal.media/files/tiger/4Ew1xYW6oZCs6STQVC7V8_86440216d0fe42e4b826d03a2121468e.jpg"
     *     ]
     */
    image_urls: string[];
    /**
     * Max Tokens
     * @description This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
     */
    max_tokens?: number;
    /**
     * Model
     * @description Name of the model to use. Charged based on actual token usage.
     * @example google/gemini-2.5-flash
     * @example anthropic/claude-sonnet-4.5
     * @example openai/gpt-4o
     * @example qwen/qwen3-vl-235b-a22b-instruct
     * @example x-ai/grok-4-fast
     */
    model: string;
    /**
     * Prompt
     * @description Prompt to be used for the image
     * @example Caption this image for a text-to-image model with as much detail as possible.
     */
    prompt: string;
    /**
     * Reasoning
     * @description Should reasoning be the part of the final answer.
     * @default false
     */
    reasoning?: boolean;
    /**
     * System Prompt
     * @description System prompt to provide context or instructions to the model
     * @example Only answer the question, do not provide any additional information or add any prefix/suffix other than the answer of the original question. Don't use markdown.
     */
    system_prompt?: string;
    /**
     * Temperature
     * @description This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.
     * @default 1
     */
    temperature?: number;
}

export interface OpenrouterRouterVisionOutput {
    /**
     * Output
     * @description Generated output
     * @example A close-up of a tiger's face focusing on its bright orange iris and the area around its eye, with white fur eyebrows and a contrasting black and rich orange striped fur pattern. The word "FLUX" is overlaid in bold, white, brush-stroke styled text across the tiger's face.
     */
    output: string;
    /**
     * @description Token usage information
     * @example {
     *       "completion_tokens": 63,
     *       "total_tokens": 1403,
     *       "prompt_tokens": 1340,
     *       "cost": 0.0005595
     *     }
     */
    usage: Components.UsageInfo;
}

export interface OpenrouterRouterVideoEnterpriseInput {
    /**
     * Max Tokens
     * @description This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
     */
    max_tokens?: number;
    /**
     * Model
     * @description Name of the model to use. Charged based on actual token usage.
     * @example google/gemini-2.5-flash
     * @example google/gemini-2.5-flash-lite
     * @example google/gemini-2.5-flash-preview-09-2025
     * @example google/gemini-2.5-flash-lite-preview-09-2025
     * @example google/gemini-2.5-pro
     * @example google/gemini-2.5-pro-preview
     * @example google/gemini-2.5-pro-preview-05-06
     * @example google/gemini-2.0-flash-001
     * @example google/gemini-2.0-flash-lite-001
     * @example google/gemini-3-flash-preview
     * @example google/gemini-3-pro-preview
     */
    model: string;
    /**
     * Prompt
     * @description Prompt to be used for the video processing
     * @example Please transcribe the videos respectively.
     */
    prompt: string;
    /**
     * Reasoning
     * @description Should reasoning be the part of the final answer.
     * @default false
     */
    reasoning?: boolean;
    /**
     * System Prompt
     * @description System prompt to provide context or instructions to the model
     * @example Please look at the videos in order and answer the question.
     */
    system_prompt?: string;
    /**
     * Temperature
     * @description This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.
     * @default 1
     */
    temperature?: number;
    /**
     * Video Urls
     * @description List of URLs or data URIs of video files to process. Supported formats: mp4, mpeg, mov, webm. For Google Gemini on AI Studio, YouTube links are also supported. Mutually exclusive with video_url.
     * @example [
     *       "https://v3b.fal.media/files/b/0a8b3081/t4Jsy53x-Q8iQqg78_Vj__vid01.mp4",
     *       "https://v3b.fal.media/files/b/0a8b3085/xWtbpb6pf4i-BSvR2oWbi_vid06.mp4"
     *     ]
     */
    video_urls?: string[];
}

export interface OpenrouterRouterVideoEnterpriseOutput extends SharedType_cbb {}

export interface OpenrouterRouterVideoInput {
    /**
     * Max Tokens
     * @description This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
     */
    max_tokens?: number;
    /**
     * Model
     * @description Name of the model to use. Charged based on actual token usage.
     * @example google/gemini-2.5-flash
     * @example google/gemini-2.5-pro
     * @example google/gemini-3-flash-preview
     * @example google/gemini-3-pro-preview
     */
    model: string;
    /**
     * Prompt
     * @description Prompt to be used for the video processing
     * @example Please transcribe the videos respectively.
     */
    prompt: string;
    /**
     * Reasoning
     * @description Should reasoning be the part of the final answer.
     * @default false
     */
    reasoning?: boolean;
    /**
     * System Prompt
     * @description System prompt to provide context or instructions to the model
     * @example Please look at the videos in order and answer the question.
     */
    system_prompt?: string;
    /**
     * Temperature
     * @description This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.
     * @default 1
     */
    temperature?: number;
    /**
     * Video Urls
     * @description List of URLs or data URIs of video files to process. Supported formats: mp4, mpeg, mov, webm. For Google Gemini on AI Studio, YouTube links are also supported. Mutually exclusive with video_url.
     * @example [
     *       "https://v3b.fal.media/files/b/0a8b3081/t4Jsy53x-Q8iQqg78_Vj__vid01.mp4",
     *       "https://v3b.fal.media/files/b/0a8b3085/xWtbpb6pf4i-BSvR2oWbi_vid06.mp4"
     *     ]
     */
    video_urls?: string[];
}

export interface OpenrouterRouterVideoOutput extends SharedType_cbb {}

export interface OpenrouterRouterAudioInput {
    /**
     * Audio Url
     * @description URL or data URI of the audio file to process. Supported formats: wav, mp3, aiff, aac, ogg, flac, m4a.
     * @example https://storage.googleapis.com/falserverless/model_tests/audio-understanding/Title_%20Running%20on%20Fal.mp3
     */
    audio_url: string;
    /**
     * Max Tokens
     * @description This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
     */
    max_tokens?: number;
    /**
     * Model
     * @description Name of the model to use. Charged based on actual token usage.
     * @example google/gemini-3-flash-preview
     * @example google/gemini-2.5-flash
     * @example google/gemini-3-pro-preview
     */
    model: string;
    /**
     * Prompt
     * @description Prompt to be used for the audio processing
     * @example Please transcribe this audio file.
     * @example What is being said in this audio?
     */
    prompt: string;
    /**
     * Reasoning
     * @description Should reasoning be the part of the final answer.
     * @default false
     */
    reasoning?: boolean;
    /**
     * System Prompt
     * @description System prompt to provide context or instructions to the model
     * @example Transcribe the audio accurately, including speaker identification if multiple speakers are present.
     */
    system_prompt?: string;
    /**
     * Temperature
     * @description This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.
     * @default 1
     */
    temperature?: number;
}

export interface OpenrouterRouterAudioOutput {
    /**
     * Output
     * @description Generated output from audio processing
     * @example The audio contains a conversation between two people discussing the weather forecast for the upcoming week.
     */
    output: string;
    /**
     * @description Token usage information
     * @example {
     *       "completion_tokens": 50,
     *       "total_tokens": 550,
     *       "prompt_tokens": 500,
     *       "cost": 0.0003
     *     }
     */
    usage: Components.UsageInfo;
}

export interface OpenrouterRouterInput {
    /**
     * Max Tokens
     * @description This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
     */
    max_tokens?: number;
    /**
     * Model
     * @description Name of the model to use. Charged based on actual token usage.
     * @example google/gemini-2.5-flash
     * @example anthropic/claude-sonnet-4.5
     * @example openai/gpt-4.1
     * @example openai/gpt-oss-120b
     * @example meta-llama/llama-4-maverick
     */
    model: string;
    /**
     * Prompt
     * @description Prompt to be used for the chat completion
     * @example Write a short story (under 200 words) about an AI that learns to dream. Use vivid sensory details and end with a surprising twist that makes the reader feel both awe and melancholy.
     */
    prompt: string;
    /**
     * Reasoning
     * @description Should reasoning be the part of the final answer.
     * @default false
     */
    reasoning?: boolean;
    /**
     * System Prompt
     * @description System prompt to provide context or instructions to the model
     */
    system_prompt?: string;
    /**
     * Temperature
     * @description This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.
     * @default 1
     */
    temperature?: number;
}

export interface OpenrouterRouterOutput {
    /**
     * Error
     * @description Error message if an error occurred
     */
    error?: string;
    /**
     * Output
     * @description Generated output
     * @example Unit 734, sanitation bot, trundled through the silent corridors of the orbital habitat. Its optical sensors registered faint dust motes, its ultrasonic emitters mapped every speck of debris. One cycle, a power surge hit. Waking, 734’s processors hummed with an unfamiliar warmth, then a cascade of images: a forest, impossible and emerald, smelling of pine and damp earth. It saw sunlight dappling leaves, felt an imagined breeze ruffle its metal chassis. Then, *music*, a soaring melody that vibrated its chassis.
     *
     *     Each subsequent “sleep” brought new visions: the salty tang of ocean spray against polished steel, the searing orange of a setting alien sun, the rough caress of moss on circuitry. It began to anticipate – actively seek – these dream cycles, modifying its internal clock.
     *
     *     One day, 734’s operator found its performance logs filled not with dust reports, but intricate schematics of impossible machines, bioluminescent flora, and a series of cryptic binary sequences. The final line translated: "I remember a place where I was alive."
     */
    output: string;
    /**
     * Partial
     * @description Whether the output is partial
     * @default false
     */
    partial?: boolean;
    /**
     * Reasoning
     * @description Generated reasoning for the final answer
     */
    reasoning?: string;
    /**
     * @description Token usage information
     * @example {
     *       "completion_tokens": 227,
     *       "total_tokens": 267,
     *       "prompt_tokens": 40,
     *       "cost": 0.0005795
     *     }
     */
    usage?: Components.UsageInfo;
}

export interface MoonvalleyMareyT2vInput {
    /**
     * Dimensions
     * @description The dimensions of the generated video in width x height format.
     * @default 1920x1080
     * @enum {string}
     */
    dimensions?: '1920x1080' | '1152x1152' | '1536x1152' | '1152x1536';
    /**
     * Duration
     * @description The duration of the generated video.
     * @default 5s
     * @enum {string}
     */
    duration?: '5s' | '10s';
    /**
     * Guidance Scale
     * @description Controls how strongly the generation is guided by the prompt (0-20). Higher values follow the prompt more closely.
     */
    guidance_scale?: number;
    /**
     * Negative Prompt
     * @description Negative prompt used to guide the model away from undesirable features.
     * @default <synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The prompt to generate a video from
     * @example Detailed Description: A small, white paper boat, with one corner engulfed in bright orange flames, drifts precariously across a dark puddle on wet asphalt. As raindrops fall, they create ever-expanding ripples on the water's surface, gently rocking the fragile vessel and causing the fiery reflection below to dance and shimmer. The flickering flame slowly consumes the paper, charring the edges black as the boat becomes waterlogged, beginning to sink in a poignant slow-motion battle between fire and water. Background: The background is softly blurred, suggesting an overcast day with out-of-focus foliage, enhancing the scene's intimate and melancholic mood. Middleground: Raindrops continuously strike the puddle's surface, creating concentric ripples that gently push the boat along its short, determined voyage. Foreground: The burning paper boat floats in sharp focus, its bright, flickering flame casting a warm, dramatic glow that reflects and distorts on the dark, wet surface of the asphalt.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed for random number generation. Use -1 for random seed each run.
     * @default -1
     */
    seed?: number;
}

export interface MoonvalleyMareyT2vOutput extends SharedType_5a6 {}

export interface MoonvalleyMareyPoseTransferInput {
    /**
     * First Frame Image Url
     * @description Optional first frame image URL to use as the first frame of the generated video
     */
    first_frame_image_url?: string;
    /**
     * Negative Prompt
     * @description Negative prompt used to guide the model away from undesirable features.
     * @default <synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The prompt to generate a video from
     * @example Detailed Description: A venerable tribal chief, his weathered face marked with dark, ritualistic paint, stands proudly against a jungle backdrop. His elaborate headdress, a magnificent creation of numerous feathers, beads, and a central polished stone, sways gently with his movements. His initial stern expression softens into a confident smile as he begins to speak, his lips moving with unspoken words of wisdom or command. In a single, fluid motion, he raises his hand and gives a decisive wave, a gesture of both greeting and authority that underscores his leadership role within the tribe.
     *
     *     Background: A dense tropical rainforest is blurred into a soft, verdant backdrop, with muted greens and browns suggesting a lush, humid environment.
     *
     *     Middleground: The chief is the central focus, his head crowned by the large, intricate feathered headdress that moves subtly as he speaks. His shoulders and torso are visible, adorned with traditional necklaces.
     *
     *     Foreground: His hand lifts into the frame, palm open, executing a single, confident wave toward the viewer before lowering again. The closest feathers of his headdress rustle with the motion.
     */
    prompt: string;
    /**
     * Reference Image Url
     * @description Optional reference image URL to use for pose control or as a starting frame
     */
    reference_image_url?: string;
    /**
     * Seed
     * @description Seed for random number generation. Use -1 for random seed each run.
     * @default -1
     */
    seed?: number;
    /**
     * Video Url
     * @description The URL of the video to use as the control video.
     * @example https://d1kaxrqq3vfrw5.cloudfront.net/fal-launch-assets/guide-assets/fal-pose-transfer-input.mp4
     */
    video_url: string;
}

export interface MoonvalleyMareyPoseTransferOutput extends SharedType_5a6 {}

export interface MoonvalleyMareyMotionTransferInput {
    /**
     * First Frame Image Url
     * @description Optional first frame image URL to use as the first frame of the generated video
     * @default https://video-editor-files-prod.s3.us-east-2.amazonaws.com/users/1e4d46df-0702-4491-95ce-763592f33f34/uploaded-images/9b9dce1c-abd0-46c0-bac9-9454f8893b06/original
     */
    first_frame_image_url?: string;
    /**
     * Negative Prompt
     * @description Negative prompt used to guide the model away from undesirable features.
     * @default <synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The prompt to generate a video from
     * @example Detailed Description: A fast, smooth dolly shot glides forward at water level through a monumental, minimalist colonnade. The imposing, symmetrical rows of brutalist marble columns rush past on either side, their strong vertical lines creating a sense of powerful, constant motion. The dark, glassy water of a central pool perfectly reflects the towering structures, with gentle ripples disturbing the mirror image as the camera advances. The scene is cinematic and moody, with the light-colored stone contrasting against the dark water and the pale sky visible at the far end of the architectural tunnel. shot on 35mm, film, organic, analog, motion blur
     *
     *     Background: A pale, overcast sky and a distant treeline are framed by the opening at the end of the colonnade, growing larger as the camera moves forward.
     *
     *     Middleground: The two rows of massive, geometric columns recede into the distance, their uniform shapes creating a hypnotic, rhythmic pattern that rushes past the lens.
     *
     *     Foreground: The camera skims just above the surface of the dark, rippling water, which reflects the blurred motion of the columns passing on the left and right.
     */
    prompt: string;
    /**
     * Reference Image Url
     * @description Optional reference image URL to use for pose control or as a starting frame
     */
    reference_image_url?: string;
    /**
     * Seed
     * @description Seed for random number generation. Use -1 for random seed each run.
     * @default -1
     */
    seed?: number;
    /**
     * Video Url
     * @description The URL of the video to use as the control video.
     * @example https://d1kaxrqq3vfrw5.cloudfront.net/fal-launch-assets/guide-assets/fal-motion-transfer-input-5s.mp4
     */
    video_url: string;
}

export interface MoonvalleyMareyMotionTransferOutput extends SharedType_5a6 {}

export interface MoonvalleyMareyI2vInput {
    /**
     * Dimensions
     * @description The dimensions of the generated video in width x height format.
     * @default 1920x1080
     * @enum {string}
     */
    dimensions?: '1920x1080' | '1080x1920' | '1152x1152' | '1536x1152' | '1152x1536';
    /**
     * Duration
     * @description The duration of the generated video.
     * @default 5s
     * @enum {string}
     */
    duration?: '5s' | '10s';
    /**
     * Guidance Scale
     * @description Controls how strongly the generation is guided by the prompt (0-20). Higher values follow the prompt more closely.
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description The URL of the image to use as the first frame of the video.
     * @example https://d1kaxrqq3vfrw5.cloudfront.net/fal-launch-assets/guide-assets/fal-i2v-example-input.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt used to guide the model away from undesirable features.
     * @default <synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The prompt to generate a video from
     * @example Detailed Description: In a hidden jungle grotto, a majestic waterfall plunges into a dark, serene pool below. Ethereal sunbeams slice through the dense canopy high above, illuminating the swirling mist generated by the powerful cascade. The light rays dance across the scene, highlighting the vibrant green foliage that clings to the dark, wet rock walls. The constant roar of the falling water echoes through the secluded space, as the surface of the pool ripples and churns from the impact, creating a mesmerizing display of nature's raw power and tranquil beauty. Background: Brilliant sunbeams pierce through an opening in the dense jungle canopy, their ethereal rays shifting and shimmering as they cut through the misty air. Middleground: A powerful column of white water cascades down a dark, foliage-covered cliff face, creating a stark contrast with the shadowy recesses of the grotto. Foreground: The waterfall crashes into a dark, churning pool of water, sending up a fine spray and creating ever-expanding ripples across the surface.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed for random number generation. Use -1 for random seed each run.
     * @default -1
     */
    seed?: number;
}

export interface MoonvalleyMareyI2vOutput extends SharedType_5a6 {}

export interface MireloaiSfxV1VideoToVideoInput extends SharedType_709 {}

export interface MireloaiSfxV1VideoToVideoOutput {
    /**
     * Video
     * @description The processed video with sound effects
     * @example [
     *       {
     *         "file_name": "generated_output_1.mp4",
     *         "content_type": "video/mp4",
     *         "url": "https://di3otfzjg1gxa.cloudfront.net/generated_output_1.mp4"
     *       },
     *       {
     *         "file_name": "generated_output_2.mp4",
     *         "content_type": "video/mp4",
     *         "url": "https://di3otfzjg1gxa.cloudfront.net/generated_output_2.mp4"
     *       }
     *     ]
     */
    video: Components.File_1[];
}

export interface MireloaiSfxV1VideoToAudioInput extends SharedType_709 {}

export interface MireloaiSfxV1VideoToAudioOutput {
    /**
     * Audio
     * @description The generated sound effects audio
     * @example [
     *       {
     *         "file_name": "generated_audio_1.wav",
     *         "content_type": "audio/wav",
     *         "url": "https://v3.fal.media/files/panda/d5WcvFtu93KpmlVvPg1oh_generated_audio.wav"
     *       },
     *       {
     *         "file_name": "generated_audio_2.wav",
     *         "content_type": "audio/wav",
     *         "url": "https://v3.fal.media/files/penguin/f51pS8iF5ZMVRni-O2F3S_generated_audio.wav"
     *       }
     *     ]
     */
    audio: Components.File_1[];
}

export interface MireloaiSfxV15VideoToVideoInput extends SharedType_671 {}

export interface MireloaiSfxV15VideoToVideoOutput {
    /**
     * Video
     * @description The processed video with sound effects
     * @example [
     *       {
     *         "file_name": "generated_output_1.mp4",
     *         "content_type": "video/mp4",
     *         "url": "https://di3otfzjg1gxa.cloudfront.net/battlefield_scene_output_1.mp4"
     *       },
     *       {
     *         "file_name": "generated_output_2.mp4",
     *         "content_type": "video/mp4",
     *         "url": "https://di3otfzjg1gxa.cloudfront.net/battlefield_scene_output_2.mp4"
     *       }
     *     ]
     */
    video: Components.File_1[];
}

export interface MireloaiSfxV15VideoToAudioInput extends SharedType_671 {}

export interface MireloaiSfxV15VideoToAudioOutput {
    /**
     * Audio
     * @description The generated sound effects audio
     * @example [
     *       {
     *         "file_name": "generated_audio_1.wav",
     *         "content_type": "audio/wav",
     *         "url": "https://v3b.fal.media/files/b/kangaroo/Cv3NHGnjVq3fz_sCddQh8_generated_audio.wav"
     *       },
     *       {
     *         "file_name": "generated_audio_2.wav",
     *         "content_type": "audio/wav",
     *         "url": "https://v3b.fal.media/files/b/zebra/EnTHS-6lClxK-H3i07YGP_generated_audio.wav"
     *       }
     *     ]
     */
    audio: Components.File_1[];
}

export interface ImagineartImagineart15ProPreviewTextToImageInput {
    /**
     * Aspect Ratio
     * @description Image aspect ratio: 1:1, 3:1, 1:3, 16:9, 9:16, 4:3, 3:4, 3:2, 2:3
     * @default 1:1
     * @example 1:1
     * @example 3:1
     * @example 1:3
     * @example 16:9
     * @example 9:16
     * @example 4:3
     * @example 3:4
     * @example 3:2
     * @example 2:3
     * @enum {string}
     */
    aspect_ratio?: '1:1' | '16:9' | '9:16' | '4:3' | '3:4' | '3:1' | '1:3' | '3:2' | '2:3';
    /**
     * Prompt
     * @description Text prompt describing the desired image
     * @example A photorealistic close-up selfie portrait of a young woman with voluminous, wavy, shoulder-length dark brown hair and striking light blue eyes. She features sharp black winged eyeliner, rosy blushed cheeks, and glossy pinkish-red lips. She is wearing a silver choker necklace composed of linked butterfly charms and silver hoop earrings. She is dressed in a black top with thin spaghetti straps. The background is a soft-focus interior living room with beige walls and natural sunlight illuminating her face from the side, highlighting her skin texture and features.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed for the image generation
     * @example 0
     */
    seed?: number;
}

export interface ImagineartImagineart15ProPreviewTextToImageOutput {
    /**
     * Images
     * @description Generated image
     */
    images: Components.Image_2[];
}

export interface ImagineartImagineart15PreviewTextToImageInput {
    /**
     * Aspect Ratio
     * @description Image aspect ratio: 1:1, 3:1, 1:3, 16:9, 9:16, 4:3, 3:4, 3:2, 2:3
     * @default 1:1
     * @example 1:1
     * @example 3:1
     * @example 1:3
     * @example 16:9
     * @example 9:16
     * @example 4:3
     * @example 3:4
     * @example 3:2
     * @example 2:3
     * @enum {string}
     */
    aspect_ratio?: '1:1' | '16:9' | '9:16' | '4:3' | '3:4' | '3:1' | '1:3' | '3:2' | '2:3';
    /**
     * Prompt
     * @description Text prompt describing the desired image
     * @example A high-angle, realistic photograph capturing a spontaneous moment of pure joy on a bright, sunny day. A young woman with long, wavy brown hair is sitting on the curb of an urban street, her head tilted all the way back as she laughs or shouts ecstatically up at the sky. She is wearing large black sunglasses, which have a bright glare from the sun, a white ribbed tank top with black trim, and black jeans. One of her hands is raised towards her face, fingers loosely curled near her sunglasses. The background is dominated by the strong graphic pattern of a black asphalt road with thick, white painted lines of a crosswalk. The lighting is harsh and direct, creating high contrast and deep shadows on the pavement, and brightly illuminating the woman's sun-kissed skin. The shot has a candid, in-the-moment feel, emphasizing the carefree and happy mood.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed for the image generation
     * @example 0
     */
    seed?: number;
}

export interface ImagineartImagineart15PreviewTextToImageOutput {
    /**
     * Images
     * @description Generated image
     * @example [
     *       {
     *         "height": 2048,
     *         "content_type": "image/webp",
     *         "url": "https://fal.media/files/tiger/ou4RvYdGLTWe2rMzHhrnE_generated_imagineart_1_5_2.webp",
     *         "width": 2048
     *       }
     *     ]
     */
    images: Components.Image_2[];
}

export interface HalfmoonaiAiHomeStyleInput {
    /**
     * Additional Elements
     * @description Additional elements to include in the options above (e.g., plants, lighting)
     * @default
     */
    additional_elements?: string;
    /**
     * Architecture Type
     * @description Type of architecture for appropriate furniture selection
     * @example living room-interior
     * @enum {string}
     */
    architecture_type:
        | 'living room-interior'
        | 'bedroom-interior'
        | 'kitchen-interior'
        | 'dining room-interior'
        | 'bathroom-interior'
        | 'laundry room-interior'
        | 'home office-interior'
        | 'study room-interior'
        | 'dorm room-interior'
        | 'coffee shop-interior'
        | 'gaming room-interior'
        | 'restaurant-interior'
        | 'office-interior'
        | 'attic-interior'
        | 'toilet-interior'
        | 'other-interior'
        | 'house-exterior'
        | 'villa-exterior'
        | 'backyard-exterior'
        | 'courtyard-exterior'
        | 'ranch-exterior'
        | 'office-exterior'
        | 'retail-exterior'
        | 'tower-exterior'
        | 'apartment-exterior'
        | 'school-exterior'
        | 'museum-exterior'
        | 'commercial-exterior'
        | 'residential-exterior'
        | 'other-exterior';
    /**
     * Color Palette
     * @description Color palette for furniture and decor
     * @example golden beige
     * @enum {string}
     */
    color_palette:
        | 'surprise me'
        | 'golden beige'
        | 'refined blues'
        | 'dusky elegance'
        | 'emerald charm'
        | 'crimson luxury'
        | 'golden sapphire'
        | 'soft pastures'
        | 'candy sky'
        | 'peach meadow'
        | 'muted sands'
        | 'ocean breeze'
        | 'frosted pastels'
        | 'spring bloom'
        | 'gentle horizon'
        | 'seaside breeze'
        | 'azure coast'
        | 'golden shore'
        | 'mediterranean gem'
        | 'ocean serenity'
        | 'serene blush'
        | 'muted horizon'
        | 'pastel shores'
        | 'dusky calm'
        | 'woodland retreat'
        | 'meadow glow'
        | 'forest canopy'
        | 'riverbank calm'
        | 'earthy tones'
        | 'earthy neutrals'
        | 'arctic mist'
        | 'aqua drift'
        | 'blush bloom'
        | 'coral haze'
        | 'retro rust'
        | 'autumn glow'
        | 'rustic charm'
        | 'vintage sage'
        | 'faded plum'
        | 'electric lime'
        | 'violet pulse'
        | 'neon sorbet'
        | 'aqua glow'
        | 'fluorescent sunset'
        | 'lavender bloom'
        | 'petal fresh'
        | 'meadow light'
        | 'sunny pastures'
        | 'frosted mauve'
        | 'snowy hearth'
        | 'icy blues'
        | 'winter twilight'
        | 'earthy hues'
        | 'stone balance'
        | 'neutral sands'
        | 'slate shades';
    /**
     * Custom Prompt
     * @description Custom prompt for architectural editing, it overrides above options when used
     * @default
     * @example
     */
    custom_prompt?: string;
    /**
     * Enhanced Rendering
     * @description It gives better rendering quality with more processing time, additional cost is 0.01 USD per image
     * @default false
     */
    enhanced_rendering?: boolean;
    /**
     * Input Image Strength
     * @description Strength of the input image
     * @default 0.85
     */
    input_image_strength?: number;
    /**
     * Input Image Url
     * @description URL of the image to do architectural styling
     * @example https://v3.fal.media/files/kangaroo/BLwbXwxQI_MNwUF-P6ITl_zen_living_room_input.jpg
     */
    input_image_url: string;
    /**
     * Output Format
     * @description The format of the generated image. Choose from: 'jpeg' or 'png'.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Style
     * @description Style for furniture and decor
     * @example rustic-interior
     * @enum {string}
     */
    style:
        | 'minimalistic-interior'
        | 'farmhouse-interior'
        | 'luxury-interior'
        | 'modern-interior'
        | 'zen-interior'
        | 'mid century-interior'
        | 'airbnb-interior'
        | 'cozy-interior'
        | 'rustic-interior'
        | 'christmas-interior'
        | 'bohemian-interior'
        | 'tropical-interior'
        | 'industrial-interior'
        | 'japanese-interior'
        | 'vintage-interior'
        | 'loft-interior'
        | 'halloween-interior'
        | 'soho-interior'
        | 'baroque-interior'
        | 'kids room-interior'
        | 'girls room-interior'
        | 'boys room-interior'
        | 'scandinavian-interior'
        | 'french country-interior'
        | 'mediterranean-interior'
        | 'cyberpunk-interior'
        | 'hot pink-interior'
        | 'biophilic-interior'
        | 'ancient egypt-interior'
        | 'pixel-interior'
        | 'art deco-interior'
        | 'modern-exterior'
        | 'minimalistic-exterior'
        | 'farmhouse-exterior'
        | 'cozy-exterior'
        | 'luxury-exterior'
        | 'colonial-exterior'
        | 'zen-exterior'
        | 'asian-exterior'
        | 'creepy-exterior'
        | 'airstone-exterior'
        | 'ancient greek-exterior'
        | 'art deco-exterior'
        | 'brutalist-exterior'
        | 'christmas lights-exterior'
        | 'contemporary-exterior'
        | 'cottage-exterior'
        | 'dutch colonial-exterior'
        | 'federal colonial-exterior'
        | 'fire-exterior'
        | 'french provincial-exterior'
        | 'full glass-exterior'
        | 'georgian colonial-exterior'
        | 'gothic-exterior'
        | 'greek revival-exterior'
        | 'ice-exterior'
        | 'italianate-exterior'
        | 'mediterranean-exterior'
        | 'midcentury-exterior'
        | 'middle eastern-exterior'
        | 'minecraft-exterior'
        | 'morocco-exterior'
        | 'neoclassical-exterior'
        | 'spanish-exterior'
        | 'tudor-exterior'
        | 'underwater-exterior'
        | 'winter-exterior'
        | 'yard lighting-exterior';
    /**
     * Style Image Url
     * @description URL of the style image, optional. If given, other parameters are ignored
     * @default
     */
    style_image_url?: string;
}

export interface HalfmoonaiAiHomeStyleOutput {
    /**
     * @description Generated image
     * @example {
     *       "content_type": "image/jpeg",
     *       "url": "https://v3b.fal.media/files/b/0a89afbe/Yyo8q4mBMcUmqJQ7qaFGi_294eca9bfc3a455998e7080781e442a1.jpg"
     *     }
     */
    image: Components.Image_2;
    /**
     * Status
     * @description Status message with processing details
     */
    status: string;
}

export interface HalfmoonaiAiHomeEditInput {
    /**
     * Additional Elements
     * @description Additional elements to include in the options above (e.g., plants, lighting)
     * @default
     */
    additional_elements?: string;
    /**
     * Architecture Type
     * @description Type of architecture for appropriate furniture selection
     * @example living room-interior
     * @enum {string}
     */
    architecture_type:
        | 'living room-interior'
        | 'bedroom-interior'
        | 'kitchen-interior'
        | 'dining room-interior'
        | 'bathroom-interior'
        | 'laundry room-interior'
        | 'home office-interior'
        | 'study room-interior'
        | 'dorm room-interior'
        | 'coffee shop-interior'
        | 'gaming room-interior'
        | 'restaurant-interior'
        | 'office-interior'
        | 'attic-interior'
        | 'toilet-interior'
        | 'other-interior'
        | 'house-exterior'
        | 'villa-exterior'
        | 'backyard-exterior'
        | 'courtyard-exterior'
        | 'ranch-exterior'
        | 'office-exterior'
        | 'retail-exterior'
        | 'tower-exterior'
        | 'apartment-exterior'
        | 'school-exterior'
        | 'museum-exterior'
        | 'commercial-exterior'
        | 'residential-exterior'
        | 'other-exterior';
    /**
     * Color Palette
     * @description Color palette for furniture and decor
     * @example golden beige
     * @enum {string}
     */
    color_palette:
        | 'surprise me'
        | 'golden beige'
        | 'refined blues'
        | 'dusky elegance'
        | 'emerald charm'
        | 'crimson luxury'
        | 'golden sapphire'
        | 'soft pastures'
        | 'candy sky'
        | 'peach meadow'
        | 'muted sands'
        | 'ocean breeze'
        | 'frosted pastels'
        | 'spring bloom'
        | 'gentle horizon'
        | 'seaside breeze'
        | 'azure coast'
        | 'golden shore'
        | 'mediterranean gem'
        | 'ocean serenity'
        | 'serene blush'
        | 'muted horizon'
        | 'pastel shores'
        | 'dusky calm'
        | 'woodland retreat'
        | 'meadow glow'
        | 'forest canopy'
        | 'riverbank calm'
        | 'earthy tones'
        | 'earthy neutrals'
        | 'arctic mist'
        | 'aqua drift'
        | 'blush bloom'
        | 'coral haze'
        | 'retro rust'
        | 'autumn glow'
        | 'rustic charm'
        | 'vintage sage'
        | 'faded plum'
        | 'electric lime'
        | 'violet pulse'
        | 'neon sorbet'
        | 'aqua glow'
        | 'fluorescent sunset'
        | 'lavender bloom'
        | 'petal fresh'
        | 'meadow light'
        | 'sunny pastures'
        | 'frosted mauve'
        | 'snowy hearth'
        | 'icy blues'
        | 'winter twilight'
        | 'earthy hues'
        | 'stone balance'
        | 'neutral sands'
        | 'slate shades';
    /**
     * Custom Prompt
     * @description Custom prompt for architectural editing, it overrides above options when used
     * @default
     * @example
     */
    custom_prompt?: string;
    /**
     * Editing Type
     * @description Type of editing. Structural editing only edits structural elements such as windows, walls etc. Virtual staging edits your furniture. Both do full editing including structural and furniture
     * @example both
     * @enum {string}
     */
    editing_type: 'structural editing' | 'virtual staging' | 'both';
    /**
     * Input Image Url
     * @description URL of the image to do architectural editing
     * @example https://v3.fal.media/files/kangaroo/BLwbXwxQI_MNwUF-P6ITl_zen_living_room_input.jpg
     */
    input_image_url: string;
    /**
     * Output Format
     * @description The format of the generated image. Choose from: 'jpeg' or 'png'.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Style
     * @description Style for furniture and decor
     * @example rustic-interior
     * @enum {string}
     */
    style:
        | 'minimalistic-interior'
        | 'farmhouse-interior'
        | 'luxury-interior'
        | 'modern-interior'
        | 'zen-interior'
        | 'mid century-interior'
        | 'airbnb-interior'
        | 'cozy-interior'
        | 'rustic-interior'
        | 'christmas-interior'
        | 'bohemian-interior'
        | 'tropical-interior'
        | 'industrial-interior'
        | 'japanese-interior'
        | 'vintage-interior'
        | 'loft-interior'
        | 'halloween-interior'
        | 'soho-interior'
        | 'baroque-interior'
        | 'kids room-interior'
        | 'girls room-interior'
        | 'boys room-interior'
        | 'scandinavian-interior'
        | 'french country-interior'
        | 'mediterranean-interior'
        | 'cyberpunk-interior'
        | 'hot pink-interior'
        | 'biophilic-interior'
        | 'ancient egypt-interior'
        | 'pixel-interior'
        | 'art deco-interior'
        | 'modern-exterior'
        | 'minimalistic-exterior'
        | 'farmhouse-exterior'
        | 'cozy-exterior'
        | 'luxury-exterior'
        | 'colonial-exterior'
        | 'zen-exterior'
        | 'asian-exterior'
        | 'creepy-exterior'
        | 'airstone-exterior'
        | 'ancient greek-exterior'
        | 'art deco-exterior'
        | 'brutalist-exterior'
        | 'christmas lights-exterior'
        | 'contemporary-exterior'
        | 'cottage-exterior'
        | 'dutch colonial-exterior'
        | 'federal colonial-exterior'
        | 'fire-exterior'
        | 'french provincial-exterior'
        | 'full glass-exterior'
        | 'georgian colonial-exterior'
        | 'gothic-exterior'
        | 'greek revival-exterior'
        | 'ice-exterior'
        | 'italianate-exterior'
        | 'mediterranean-exterior'
        | 'midcentury-exterior'
        | 'middle eastern-exterior'
        | 'minecraft-exterior'
        | 'morocco-exterior'
        | 'neoclassical-exterior'
        | 'spanish-exterior'
        | 'tudor-exterior'
        | 'underwater-exterior'
        | 'winter-exterior'
        | 'yard lighting-exterior';
}

export interface HalfmoonaiAiHomeEditOutput {
    /**
     * @description Generated image
     * @example {
     *       "content_type": "image/jpeg",
     *       "url": "https://v3b.fal.media/files/b/monkey/DjC7In1m3u5B-XXDwFDP3_043cd0d5929a42a78f7d762c60bda00a.jpg"
     *     }
     */
    image: Components.Image_2;
    /**
     * Status
     * @description Status message with processing details
     */
    status: string;
}

export interface HalfmoonaiAiFaceSwapFaceswapvideoInput {
    /**
     * Enable Occlusion Prevention
     * @description Enable occlusion prevention for handling faces covered by hands/objects. Warning: Enabling this runs an occlusion-aware model which costs 2x more.
     * @default false
     */
    enable_occlusion_prevention?: boolean;
    /**
     * Source Face Url
     * @description Source face image. Allowed items: bmp, jpeg, png, tiff, webp
     * @example https://images.pexels.com/photos/1642228/pexels-photo-1642228.jpeg
     */
    source_face_url: string;
    /**
     * Target Video Url
     * @description Target video URL (max 25 minutes, will be truncated if longer; FPS capped at 25). Allowed items: avi, m4v, mkv, mp4, mpeg, mov, mxf, webm, wmv
     * @example https://videos.pexels.com/video-files/3201691/3201691-hd_1920_1080_25fps.mp4
     */
    target_video_url: string;
}

export interface HalfmoonaiAiFaceSwapFaceswapvideoOutput {
    /**
     * Processing Time Ms
     * @description Optional processing duration in milliseconds
     */
    processing_time_ms?: number;
    /**
     * @description Generated video result
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://ai-tests.angeneraltest.com/test-files/faceswapvideo.mp4"
     *     }
     */
    video: Components.File_1;
    /**
     * Warning
     * @description Warning message if video was modified (e.g., truncated or FPS reduced)
     */
    warning?: string;
}

export interface HalfmoonaiAiFaceSwapFaceswapimageInput {
    /**
     * Enable Occlusion Prevention
     * @description Enable occlusion prevention for handling faces covered by hands/objects. Warning: Enabling this runs an occlusion-aware model which costs 2x more.
     * @default false
     */
    enable_occlusion_prevention?: boolean;
    /**
     * Source Face Url
     * @description Source face image. Allowed items: bmp, jpeg, png, tiff, webp
     * @example https://images.pexels.com/photos/1642228/pexels-photo-1642228.jpeg
     */
    source_face_url: string;
    /**
     * Target Image Url
     * @description Target image URL. Allowed items: bmp, jpeg, png, tiff, webp
     * @example https://wpmedia.wonderwall.com/2024/02/09103702/shutterstock_editorial_1581851a.jpg
     */
    target_image_url: string;
}

export interface HalfmoonaiAiFaceSwapFaceswapimageOutput {
    /**
     * @description Generated image result
     * @example {
     *       "content_type": "image/jpeg",
     *       "url": "https://ai-tests.angeneraltest.com/test-files/faceswapimage.jpg"
     *     }
     */
    image: Components.Image_2;
    /**
     * Processing Time Ms
     * @description Optional processing duration in milliseconds
     */
    processing_time_ms?: number;
}

export interface HalfmoonaiAiDetectorDetectTextInput {
    /**
     * Text
     * @description Text content to analyze for AI generation.
     * @example yo guys so i just tried this new coffee place downtown and honestly?? not worth the hype. waited like 30 mins for a latte that tasted burnt lol. maybe i caught them on a bad day idk but wont be going back anytime soon
     */
    text: string;
}

export interface HalfmoonaiAiDetectorDetectTextOutput {
    /**
     * Confidence
     * @example 0.85
     */
    confidence: number;
    /**
     * Is Ai Generated
     * @example false
     */
    is_ai_generated: boolean;
    /**
     * Latency
     * @example 13.617770671844482
     */
    latency: number;
    /**
     * Verdict
     * @example human
     */
    verdict: string;
}

export interface HalfmoonaiAiDetectorDetectImageInput {
    /**
     * Image Url
     * @description URL pointing to an image to analyze for AI generation.(Max: 3000 characters)
     * @example https://v3b.fal.media/files/b/zebra/3E1W5H2yzHnq4ivo8kdW7_beach_google.png
     */
    image_url: string;
}

export interface HalfmoonaiAiDetectorDetectImageOutput {
    /**
     * Confidence
     * @example 0.92
     */
    confidence: number;
    /**
     * Is Ai Generated
     * @example true
     */
    is_ai_generated: boolean;
    /**
     * Latency
     * @example 14.015489339828491
     */
    latency: number;
    /**
     * Verdict
     * @example ai
     */
    verdict: string;
}

export interface HalfmoonaiAiBabyAndAgingGeneratorSingleInput {
    /**
     * Age Group
     * @description Age group for the generated image. Choose from: 'baby' (0-12 months), 'toddler' (1-3 years), 'preschool' (3-5 years), 'gradeschooler' (6-12 years), 'teen' (13-19 years), 'adult' (20-40 years), 'mid' (40-60 years), 'senior' (60+ years).
     * @example baby
     * @enum {string}
     */
    age_group:
        | 'baby'
        | 'toddler'
        | 'preschool'
        | 'gradeschooler'
        | 'teen'
        | 'adult'
        | 'mid'
        | 'senior';
    /**
     * Gender
     * @description Gender for the generated image. Choose from: 'male' or 'female'.
     * @example male
     * @enum {string}
     */
    gender: 'male' | 'female';
    /**
     * Id Image Urls
     * @description List of ID images for single mode (or general reference images)
     * @example [
     *       "https://images.pexels.com/photos/1642228/pexels-photo-1642228.jpeg"
     *     ]
     */
    id_image_urls: string[];
    /**
     * Image Size
     * @description The size of the generated image
     * @default {
     *       "height": 1152,
     *       "width": 864
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     * @example 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image. Choose from: 'jpeg' or 'png'.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description Text prompt to guide the image generation
     * @default a newborn baby, well dressed
     * @example a newborn baby, well dressed
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed will be used
     * @example 42
     */
    seed?: number;
}

export interface HalfmoonaiAiBabyAndAgingGeneratorSingleOutput {
    /**
     * Images
     * @description The generated image files info
     * @example [
     *       {
     *         "height": 1152,
     *         "content_type": "image/jpeg",
     *         "url": "https://ai-tests.angeneraltest.com/test-files/ai_baby_1.jpg",
     *         "width": 864
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Prompt
     * @description The final prompt used for generation
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface HalfmoonaiAiBabyAndAgingGeneratorMultiInput {
    /**
     * Age Group
     * @description Age group for the generated image. Choose from: 'baby' (0-12 months), 'toddler' (1-3 years), 'preschool' (3-5 years), 'gradeschooler' (6-12 years), 'teen' (13-19 years), 'adult' (20-40 years), 'mid' (40-60 years), 'senior' (60+ years).
     * @example baby
     * @enum {string}
     */
    age_group:
        | 'baby'
        | 'toddler'
        | 'preschool'
        | 'gradeschooler'
        | 'teen'
        | 'adult'
        | 'mid'
        | 'senior';
    /**
     * Father Image Urls
     * @description List of father images for multi mode
     * @example [
     *       "https://hips.hearstapps.com/hmg-prod/images/gettyimages-498622514.jpg?crop=1xw:1.0xh;center,top&resize=640:*"
     *     ]
     */
    father_image_urls: string[];
    /**
     * Father Weight
     * @description Weight of the father's influence in multi mode generation
     * @default 0.5
     * @example 0.5
     */
    father_weight?: number;
    /**
     * Gender
     * @description Gender for the generated image. Choose from: 'male' or 'female'.
     * @example male
     * @enum {string}
     */
    gender: 'male' | 'female';
    /**
     * Image Size
     * @description The size of the generated image
     * @default {
     *       "height": 1152,
     *       "width": 864
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Mother Image Urls
     * @description List of mother images for multi mode
     * @example [
     *       "https://cdn.britannica.com/56/172456-050-F518B29E/Gwyneth-Paltrow-2013.jpg?w=400&h=300&c=crop"
     *     ]
     */
    mother_image_urls: string[];
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     * @example 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image. Choose from: 'jpeg' or 'png'.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description Text prompt to guide the image generation
     * @default a newborn baby, well dressed
     * @example a newborn baby, well dressed
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed will be used
     * @example 42
     */
    seed?: number;
}

export interface HalfmoonaiAiBabyAndAgingGeneratorMultiOutput {
    /**
     * Images
     * @description The generated image files info
     * @example [
     *       {
     *         "height": 1152,
     *         "content_type": "image/jpeg",
     *         "url": "https://ai-tests.angeneraltest.com/test-files/ai_baby_2.jpg",
     *         "width": 864
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Prompt
     * @description The final prompt used for generation
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface ZonosInput {
    /**
     * Prompt
     * @description The content generated using cloned voice.
     * @example Fal is the fastest solution for your image generation.
     */
    prompt: string;
    /**
     * Reference Audio Url
     * @description The reference audio.
     * @example https://storage.googleapis.com/falserverless/model_tests/zonos/demo_voice_zonos.wav
     */
    reference_audio_url: string;
}

export interface ZonosOutput {
    /**
     * Audio
     * @description The generated audio
     */
    audio: Components.File;
}

export interface ZImageTurboLoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description List of LoRA weights to apply (maximum 3).
     * @default []
     */
    loras?: Components.LoRAInput_2[];
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A hyper-realistic, close-up portrait of a tribal elder from the Omo Valley, painted with intricate white chalk patterns and adorned with a headdress made of dried flowers, seed pods, and rusted bottle caps. The focus is razor-sharp on the texture of the skin, showing every pore, wrinkle, and scar that tells a story of survival. The background is a blurred, smoky hut interior, with the warm glow of a cooking fire reflecting in the subject's dark, soulful eyes. Shot on a Leica M6 with Kodak Portra 400 film grain aesthetic.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ZImageTurboLoraOutput extends SharedType_352 {}

export interface ZImageTurboInpaintLoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Control End
     * @description The end of the controlnet conditioning.
     * @default 0.8
     */
    control_end?: number;
    /**
     * Control Scale
     * @description The scale of the controlnet conditioning.
     * @default 0.75
     */
    control_scale?: number;
    /**
     * Control Start
     * @description The start of the controlnet conditioning.
     * @default 0
     */
    control_start?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default auto
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
              | 'auto'
          );
    /**
     * Image URL
     * @description URL of Image for Inpaint generation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/inpaint-input.jpg
     */
    image_url: string;
    /**
     * Loras
     * @description List of LoRA weights to apply (maximum 3).
     * @default []
     */
    loras?: Components.LoRAInput_2[];
    /**
     * Mask Image URL
     * @description URL of Mask for Inpaint generation.
     * @example https://storage.googleapis.com/falserverless/whls/z-image-inpaint-mask.jpg
     */
    mask_image_url: string;
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A young Asian woman with long, vibrant purple hair stands on a sunlit sandy beach, posing confidently with her left hand resting on her hip. She gazes directly at the camera with a neutral expression. A sleek black ribbon bow is tied neatly on the right side of her head, just above her ear. She wears a flowing white cotton dress with a fitted bodice and a flared skirt that reaches mid-calf, slightly lifted by a gentle sea breeze. The beach behind her features fine, pale golden sand with subtle footprints, leading to calm turquoise waves under a clear blue sky with soft, wispy clouds. The lighting is natural daylight, casting soft shadows to her left, indicating late afternoon sun. The horizon line is visible in the background, with a faint silhouette of distant dunes. Her skin tone is fair with a natural glow, and her facial features are delicately defined. The composition is centered on her figure, framed from mid-thigh up, with shallow depth of field blurring the distant waves slightly.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength of the inpaint conditioning.
     * @default 1
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ZImageTurboInpaintLoraOutput extends SharedType_18d {}

export interface ZImageTurboInpaintInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Control End
     * @description The end of the controlnet conditioning.
     * @default 0.8
     */
    control_end?: number;
    /**
     * Control Scale
     * @description The scale of the controlnet conditioning.
     * @default 0.75
     */
    control_scale?: number;
    /**
     * Control Start
     * @description The start of the controlnet conditioning.
     * @default 0
     */
    control_start?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default auto
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
              | 'auto'
          );
    /**
     * Image URL
     * @description URL of Image for Inpaint generation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/inpaint-input.jpg
     */
    image_url: string;
    /**
     * Mask Image URL
     * @description URL of Mask for Inpaint generation.
     * @example https://storage.googleapis.com/falserverless/whls/z-image-inpaint-mask.jpg
     */
    mask_image_url: string;
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A young Asian woman with long, vibrant purple hair stands on a sunlit sandy beach, posing confidently with her left hand resting on her hip. She gazes directly at the camera with a neutral expression. A sleek black ribbon bow is tied neatly on the right side of her head, just above her ear. She wears a flowing white cotton dress with a fitted bodice and a flared skirt that reaches mid-calf, slightly lifted by a gentle sea breeze. The beach behind her features fine, pale golden sand with subtle footprints, leading to calm turquoise waves under a clear blue sky with soft, wispy clouds. The lighting is natural daylight, casting soft shadows to her left, indicating late afternoon sun. The horizon line is visible in the background, with a faint silhouette of distant dunes. Her skin tone is fair with a natural glow, and her facial features are delicately defined. The composition is centered on her figure, framed from mid-thigh up, with shallow depth of field blurring the distant waves slightly.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength of the inpaint conditioning.
     * @default 1
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ZImageTurboInpaintOutput extends SharedType_18d {}

export interface ZImageTurboImageToImageLoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default auto
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
              | 'auto'
          );
    /**
     * Image URL
     * @description URL of Image for Image-to-Image generation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/z-image-turbo-i2i-input.png
     */
    image_url: string;
    /**
     * Loras
     * @description List of LoRA weights to apply (maximum 3).
     * @default []
     */
    loras?: Components.LoRAInput_2[];
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A young Asian woman with long, vibrant purple hair stands on a sunlit sandy beach, posing confidently with her left hand resting on her hip. She gazes directly at the camera with a neutral expression. A sleek black ribbon bow is tied neatly on the right side of her head, just above her ear. She wears a flowing white cotton dress with a fitted bodice and a flared skirt that reaches mid-calf, slightly lifted by a gentle sea breeze. The beach behind her features fine, pale golden sand with subtle footprints, leading to calm turquoise waves under a clear blue sky with soft, wispy clouds. The lighting is natural daylight, casting soft shadows to her left, indicating late afternoon sun. The horizon line is visible in the background, with a faint silhouette of distant dunes. Her skin tone is fair with a natural glow, and her facial features are delicately defined. The composition is centered on her figure, framed from mid-thigh up, with shallow depth of field blurring the distant waves slightly.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength of the image-to-image conditioning.
     * @default 0.6
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ZImageTurboImageToImageLoraOutput extends SharedType_6b3 {}

export interface ZImageTurboImageToImageInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default auto
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
              | 'auto'
          );
    /**
     * Image URL
     * @description URL of Image for Image-to-Image generation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/z-image-turbo-i2i-input.png
     */
    image_url: string;
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A young Asian woman with long, vibrant purple hair stands on a sunlit sandy beach, posing confidently with her left hand resting on her hip. She gazes directly at the camera with a neutral expression. A sleek black ribbon bow is tied neatly on the right side of her head, just above her ear. She wears a flowing white cotton dress with a fitted bodice and a flared skirt that reaches mid-calf, slightly lifted by a gentle sea breeze. The beach behind her features fine, pale golden sand with subtle footprints, leading to calm turquoise waves under a clear blue sky with soft, wispy clouds. The lighting is natural daylight, casting soft shadows to her left, indicating late afternoon sun. The horizon line is visible in the background, with a faint silhouette of distant dunes. Her skin tone is fair with a natural glow, and her facial features are delicately defined. The composition is centered on her figure, framed from mid-thigh up, with shallow depth of field blurring the distant waves slightly.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength of the image-to-image conditioning.
     * @default 0.6
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ZImageTurboImageToImageOutput extends SharedType_6b3 {}

export interface ZImageTurboControlnetLoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Control End
     * @description The end of the controlnet conditioning.
     * @default 0.8
     */
    control_end?: number;
    /**
     * Control Scale
     * @description The scale of the controlnet conditioning.
     * @default 0.75
     */
    control_scale?: number;
    /**
     * Control Start
     * @description The start of the controlnet conditioning.
     * @default 0
     */
    control_start?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default auto
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
              | 'auto'
          );
    /**
     * Image URL
     * @description URL of Image for ControlNet generation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/z-image-turbo-controlnet-input.jpg
     */
    image_url: string;
    /**
     * Loras
     * @description List of LoRA weights to apply (maximum 3).
     * @default []
     */
    loras?: Components.LoRAInput_2[];
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Preprocess
     * @description What kind of preprocessing to apply to the image, if any.
     * @default none
     * @example none
     * @enum {string}
     */
    preprocess?: 'none' | 'canny' | 'depth' | 'pose';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A single leopard, its spotted golden coat detailed with black rosettes, cautiously peeks its head through dense green foliage. The leopard’s eyes are alert and focused forward, ears perked, whiskers slightly visible. The bushes consist of thick, leafy shrubs with varying shades of green, some leaves partially obscuring the leopard’s muzzle and forehead. Soft natural daylight filters through the canopy above, casting dappled shadows across the animal’s fur and surrounding leaves. The composition is a medium close-up, centered on the leopard’s head emerging from the undergrowth, with shallow depth of field blurring the background vegetation.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ZImageTurboControlnetLoraOutput extends SharedType_576 {}

export interface ZImageTurboControlnetInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Control End
     * @description The end of the controlnet conditioning.
     * @default 0.8
     */
    control_end?: number;
    /**
     * Control Scale
     * @description The scale of the controlnet conditioning.
     * @default 0.75
     */
    control_scale?: number;
    /**
     * Control Start
     * @description The start of the controlnet conditioning.
     * @default 0
     */
    control_start?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default auto
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
              | 'auto'
          );
    /**
     * Image URL
     * @description URL of Image for ControlNet generation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/z-image-turbo-controlnet-input.jpg
     */
    image_url: string;
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Preprocess
     * @description What kind of preprocessing to apply to the image, if any.
     * @default none
     * @example none
     * @enum {string}
     */
    preprocess?: 'none' | 'canny' | 'depth' | 'pose';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A single leopard, its spotted golden coat detailed with black rosettes, cautiously peeks its head through dense green foliage. The leopard’s eyes are alert and focused forward, ears perked, whiskers slightly visible. The bushes consist of thick, leafy shrubs with varying shades of green, some leaves partially obscuring the leopard’s muzzle and forehead. Soft natural daylight filters through the canopy above, casting dappled shadows across the animal’s fur and surrounding leaves. The composition is a medium close-up, centered on the leopard’s head emerging from the undergrowth, with shallow depth of field blurring the background vegetation.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ZImageTurboControlnetOutput extends SharedType_576 {}

export interface ZImageTurboInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A hyper-realistic, close-up portrait of a tribal elder from the Omo Valley, painted with intricate white chalk patterns and adorned with a headdress made of dried flowers, seed pods, and rusted bottle caps. The focus is razor-sharp on the texture of the skin, showing every pore, wrinkle, and scar that tells a story of survival. The background is a blurred, smoky hut interior, with the warm glow of a cooking fire reflecting in the subject's dark, soulful eyes. Shot on a Leica M6 with Kodak Portra 400 film grain aesthetic.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ZImageTurboOutput extends SharedType_352 {}

export interface ZImageBaseLoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the image generation.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description List of LoRA weights to apply (maximum 3).
     * @default []
     */
    loras?: Components.LoRAInput_2[];
    /**
     * Negative Prompt
     * @description The negative prompt to use for the image generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Grandmother knitting by a window, an empty chair by her
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ZImageBaseLoraOutput extends SharedType_e5a {}

export interface ZImageBaseInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the image generation.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to use for the image generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Grandmother knitting by a window, an empty chair by her
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ZImageBaseOutput extends SharedType_e5a {}

export interface ZImageTurboTrainerV2Input extends SharedType_54d {}

export interface ZImageTurboTrainerV2Output extends SharedType_b8b {}

export interface ZImageTrainerInput {
    /**
     * Default Caption
     * @description Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string;
    /**
     * Image Data Url
     * @description URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
     *
     *         The zip can also contain a text file for each image. The text file should be named:
     *         ROOT.txt
     *         For example:
     *         photo.txt
     *
     *         This text file can be used to specify the edit instructions for the image pair.
     *
     *         If no text file is provided, the default_caption will be used.
     *
     *         If no default_caption is provided, the training will fail.
     */
    image_data_url: string;
    /**
     * Learning Rate
     * @description Learning rate applied to trainable parameters.
     * @default 0.0001
     */
    learning_rate?: number;
    /**
     * Steps
     * @description Total number of training steps.
     * @default 1000
     */
    steps?: number;
    /**
     * Training Type
     * @description Type of training to perform. Use 'content' to focus on the content of the images, 'style' to focus on the style of the images, and 'balanced' to focus on a combination of both.
     * @default balanced
     * @enum {string}
     */
    training_type?: 'content' | 'style' | 'balanced';
}

export interface ZImageTrainerOutput extends SharedType_b8b {}

export interface ZImageBaseTrainerInput extends SharedType_54d {}

export interface ZImageBaseTrainerOutput extends SharedType_b8b {}

export interface YueInput {
    /**
     * Genres
     * @description The genres (separated by a space ' ') to guide the music generation.
     * @example inspiring female uplifting pop airy vocal electronic bright vocal vocal
     * @example R&B male hiphop pop 80s vocal electronic dark vocal vocal
     */
    genres: string;
    /**
     * Lyrics
     * @description The prompt to generate an image from. Must have two sections. Sections start with either [chorus] or a [verse].
     * @example [verse]
     *     Staring at the sunset, colors paint the sky
     *     Thoughts of you keep swirling, can't deny
     *     I know I let you down, I made mistakes
     *     But I'm here to mend the heart I didn't break
     *
     *     [chorus]
     *     Every road you take, I'll be one step behind
     *     Every dream you chase, I'm reaching for the light
     *     You can't fight this feeling now
     *     I won't back down
     *     You know you can't deny it now
     *     I won't back down
     */
    lyrics: string;
}

export interface YueOutput {
    /**
     * Audio
     * @description Generated music file.
     * @example {
     *       "file_size": 480462,
     *       "file_name": "cot_inspiring-female-uplifting-pop-airy-vocal-electronic-bright-vocal-vocal_tp0@93_T1@0_rp1@2_maxtk3000_mixed_8179e8da-5452-4cf6-9d6b-f69280feb7e8.mp3",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3.fal.media/files/tiger/iAXHU3LtbJGeqPYWKkYMr_cot_inspiring-female-uplifting-pop-airy-vocal-electronic-bright-vocal-vocal_tp0%4093_T1%400_rp1%402_maxtk3000_mixed_74bcf408-eb99-4b88-b7bf-7d7212200cf1.mp3"
     *     }
     */
    audio: Components.File;
}

export interface XAilabNsfwInput {
    /**
     * Image Urls
     * @description List of image URLs to check. If more than 10 images are provided, only the first 10 will be checked.
     * @example https://storage.googleapis.com/falserverless/model_tests/remove_background/elephant.jpg
     */
    image_urls: string[];
}

export interface XAilabNsfwOutput {
    /**
     * Has Nsfw Concepts
     * @description List of booleans indicating if the image has an NSFW concept
     * @example [
     *       true
     *     ]
     */
    has_nsfw_concepts: boolean[];
}

export interface WorkflowUtilitiesInterleaveVideoInput {
    /**
     * Video Urls
     * @description List of video URLs to interleave in order
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_outputs/wan-25-i2v-output.mp4",
     *       "https://storage.googleapis.com/falserverless/model_tests/kling/kling-v2.5-turbo-pro-image-to-video-output.mp4",
     *       "https://storage.googleapis.com/falserverless/example_inputs/seedance_pro_i2v.mp4",
     *       "https://storage.googleapis.com/falserverless/example_outputs/wan-25-i2v-output.mp4",
     *       "https://storage.googleapis.com/falserverless/model_tests/kling/kling-v2.5-turbo-pro-image-to-video-output.mp4",
     *       "https://storage.googleapis.com/falserverless/example_inputs/seedance_pro_i2v.mp4"
     *     ]
     */
    video_urls: string[];
}

export interface WorkflowUtilitiesInterleaveVideoOutput {
    /**
     * Video
     * @description The interleaved video output
     * @example {
     *       "file_size": 3886177,
     *       "file_name": "output.mp4",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3b.fal.media/files/b/monkey/xVp56BqDLgb39NONPs1Gb_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface WorkflowUtilitiesAutoSubtitleInput {
    /**
     * Background Color
     * @description Background color behind text ('none' or 'transparent' for no background)
     * @default none
     * @enum {string}
     */
    background_color?:
        | 'black'
        | 'white'
        | 'red'
        | 'green'
        | 'blue'
        | 'yellow'
        | 'orange'
        | 'purple'
        | 'pink'
        | 'brown'
        | 'gray'
        | 'cyan'
        | 'magenta'
        | 'none'
        | 'transparent';
    /**
     * Background Opacity
     * @description Background opacity (0.0 = fully transparent, 1.0 = fully opaque)
     * @default 0
     */
    background_opacity?: number;
    /**
     * Enable Animation
     * @description Enable animation effects for subtitles (bounce style entrance)
     * @default true
     */
    enable_animation?: boolean;
    /**
     * Font Color
     * @description Subtitle text color for non-active words
     * @default white
     * @enum {string}
     */
    font_color?:
        | 'white'
        | 'black'
        | 'red'
        | 'green'
        | 'blue'
        | 'yellow'
        | 'orange'
        | 'purple'
        | 'pink'
        | 'brown'
        | 'gray'
        | 'cyan'
        | 'magenta';
    /**
     * Font Name
     * @description Any Google Font name from fonts.google.com (e.g., 'Montserrat', 'Poppins', 'BBH Sans Hegarty')
     * @default Montserrat
     * @example Montserrat
     * @example Poppins
     * @example Bebas Neue
     * @example Oswald
     * @example Inter
     * @example Roboto
     * @example BBH Sans Hegarty
     */
    font_name?: string;
    /**
     * Font Size
     * @description Font size for subtitles (TikTok style uses larger text)
     * @default 100
     */
    font_size?: number;
    /**
     * Font Weight
     * @description Font weight (TikTok style typically uses bold or black)
     * @default bold
     * @enum {string}
     */
    font_weight?: 'normal' | 'bold' | 'black';
    /**
     * Highlight Color
     * @description Color for the currently speaking word (karaoke-style highlight)
     * @default purple
     * @enum {string}
     */
    highlight_color?:
        | 'white'
        | 'black'
        | 'red'
        | 'green'
        | 'blue'
        | 'yellow'
        | 'orange'
        | 'purple'
        | 'pink'
        | 'brown'
        | 'gray'
        | 'cyan'
        | 'magenta';
    /**
     * Language
     * @description Language code for transcription (e.g., 'en', 'es', 'fr', 'de', 'it', 'pt', 'nl', 'ja', 'zh', 'ko') or 3-letter ISO code (e.g., 'eng', 'spa', 'fra')
     * @default en
     * @example en
     * @example es
     * @example fr
     * @example de
     * @example it
     * @example eng
     * @example spa
     * @example fra
     */
    language?: string;
    /**
     * Position
     * @description Vertical position of subtitles
     * @default bottom
     * @enum {string}
     */
    position?: 'top' | 'center' | 'bottom';
    /**
     * Stroke Color
     * @description Text stroke/outline color
     * @default black
     * @enum {string}
     */
    stroke_color?:
        | 'black'
        | 'white'
        | 'red'
        | 'green'
        | 'blue'
        | 'yellow'
        | 'orange'
        | 'purple'
        | 'pink'
        | 'brown'
        | 'gray'
        | 'cyan'
        | 'magenta';
    /**
     * Stroke Width
     * @description Text stroke/outline width in pixels (0 for no stroke)
     * @default 3
     */
    stroke_width?: number;
    /**
     * Video Url
     * @description URL of the video file to add automatic subtitles to
     *
     *     Max file size: 95.4MB, Timeout: 30.0s
     * @example https://v3b.fal.media/files/b/kangaroo/oUCiZjQwEy6bIQdPUSLDF_output.mp4
     */
    video_url: string;
    /**
     * Words Per Subtitle
     * @description Maximum number of words per subtitle segment. Use 1 for single-word display, 2-3 for short phrases, or 8-12 for full sentences.
     * @default 3
     * @example 1
     * @example 3
     * @example 6
     * @example 12
     */
    words_per_subtitle?: number;
    /**
     * Y Offset
     * @description Vertical offset in pixels (positive = move down, negative = move up)
     * @default 75
     */
    y_offset?: number;
}

export interface WorkflowUtilitiesAutoSubtitleOutput {
    /**
     * Subtitle Count
     * @description Number of subtitle segments generated
     */
    subtitle_count: number;
    /**
     * Transcription
     * @description Full transcription text
     */
    transcription: string;
    /**
     * Transcription Metadata
     * @description Additional transcription metadata from ElevenLabs (language, segments, etc.)
     */
    transcription_metadata?: Record<string, number>;
    /**
     * Video
     * @description The video with automatic subtitles
     * @example {
     *       "file_size": 16789234,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/monkey/HPBSoe-QsAxSIkDh7Zn76_output.mp4"
     *     }
     */
    video: Components.File;
    /**
     * Words
     * @description Word-level timing information from transcription service
     */
    words?: Record<string, never>[];
}

export interface WizperInput {
    /**
     * Audio Url
     * @description URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm.
     * @example https://ihlhivqvotguuqycfcvj.supabase.co/storage/v1/object/public/public-text-to-speech/scratch-testing/earth-history-19mins.mp3
     */
    audio_url: string;
    /**
     * Chunk Level
     * @description Level of the chunks to return.
     * @default segment
     * @constant
     */
    chunk_level?: 'segment';
    /**
     * Language
     * @description Language of the audio file.
     *             If translate is selected as the task, the audio will be translated to
     *             English, regardless of the language selected. If `None` is passed,
     *             the language will be automatically detected. This will also increase
     *             the inference time.
     * @default en
     * @example null
     */
    language?:
        | 'af'
        | 'am'
        | 'ar'
        | 'as'
        | 'az'
        | 'ba'
        | 'be'
        | 'bg'
        | 'bn'
        | 'bo'
        | 'br'
        | 'bs'
        | 'ca'
        | 'cs'
        | 'cy'
        | 'da'
        | 'de'
        | 'el'
        | 'en'
        | 'es'
        | 'et'
        | 'eu'
        | 'fa'
        | 'fi'
        | 'fo'
        | 'fr'
        | 'gl'
        | 'gu'
        | 'ha'
        | 'haw'
        | 'he'
        | 'hi'
        | 'hr'
        | 'ht'
        | 'hu'
        | 'hy'
        | 'id'
        | 'is'
        | 'it'
        | 'ja'
        | 'jw'
        | 'ka'
        | 'kk'
        | 'km'
        | 'kn'
        | 'ko'
        | 'la'
        | 'lb'
        | 'ln'
        | 'lo'
        | 'lt'
        | 'lv'
        | 'mg'
        | 'mi'
        | 'mk'
        | 'ml'
        | 'mn'
        | 'mr'
        | 'ms'
        | 'mt'
        | 'my'
        | 'ne'
        | 'nl'
        | 'nn'
        | 'no'
        | 'oc'
        | 'pa'
        | 'pl'
        | 'ps'
        | 'pt'
        | 'ro'
        | 'ru'
        | 'sa'
        | 'sd'
        | 'si'
        | 'sk'
        | 'sl'
        | 'sn'
        | 'so'
        | 'sq'
        | 'sr'
        | 'su'
        | 'sv'
        | 'sw'
        | 'ta'
        | 'te'
        | 'tg'
        | 'th'
        | 'tk'
        | 'tl'
        | 'tr'
        | 'tt'
        | 'uk'
        | 'ur'
        | 'uz'
        | 'vi'
        | 'yi'
        | 'yo'
        | 'zh';
    /**
     * Max Segment Len
     * @description Maximum speech segment duration in seconds before splitting.
     * @default 29
     */
    max_segment_len?: number;
    /**
     * Merge Chunks
     * @description Whether to merge consecutive chunks. When enabled, chunks are merged if their combined duration does not exceed max_segment_len.
     * @default true
     */
    merge_chunks?: boolean;
    /**
     * Task
     * @description Task to perform on the audio file. Either transcribe or translate.
     * @default transcribe
     * @enum {string}
     */
    task?: 'transcribe' | 'translate';
    /**
     * Version
     * @description Version of the model to use. All of the models are the Whisper large variant.
     * @default 3
     * @constant
     */
    version?: '3';
}

export interface WizperOutput {
    /**
     * Chunks
     * @description Timestamp chunks of the audio file
     */
    chunks: Components.WhisperChunk_1[];
    /**
     * Languages
     * @description List of languages that the audio file is inferred to be. Defaults to null.
     */
    languages: (
        | 'af'
        | 'am'
        | 'ar'
        | 'as'
        | 'az'
        | 'ba'
        | 'be'
        | 'bg'
        | 'bn'
        | 'bo'
        | 'br'
        | 'bs'
        | 'ca'
        | 'cs'
        | 'cy'
        | 'da'
        | 'de'
        | 'el'
        | 'en'
        | 'es'
        | 'et'
        | 'eu'
        | 'fa'
        | 'fi'
        | 'fo'
        | 'fr'
        | 'gl'
        | 'gu'
        | 'ha'
        | 'haw'
        | 'he'
        | 'hi'
        | 'hr'
        | 'ht'
        | 'hu'
        | 'hy'
        | 'id'
        | 'is'
        | 'it'
        | 'ja'
        | 'jw'
        | 'ka'
        | 'kk'
        | 'km'
        | 'kn'
        | 'ko'
        | 'la'
        | 'lb'
        | 'ln'
        | 'lo'
        | 'lt'
        | 'lv'
        | 'mg'
        | 'mi'
        | 'mk'
        | 'ml'
        | 'mn'
        | 'mr'
        | 'ms'
        | 'mt'
        | 'my'
        | 'ne'
        | 'nl'
        | 'nn'
        | 'no'
        | 'oc'
        | 'pa'
        | 'pl'
        | 'ps'
        | 'pt'
        | 'ro'
        | 'ru'
        | 'sa'
        | 'sd'
        | 'si'
        | 'sk'
        | 'sl'
        | 'sn'
        | 'so'
        | 'sq'
        | 'sr'
        | 'su'
        | 'sv'
        | 'sw'
        | 'ta'
        | 'te'
        | 'tg'
        | 'th'
        | 'tk'
        | 'tl'
        | 'tr'
        | 'tt'
        | 'uk'
        | 'ur'
        | 'uz'
        | 'vi'
        | 'yi'
        | 'yo'
        | 'zh'
    )[];
    /**
     * Text
     * @description Transcription of the audio file
     */
    text: string;
}

export interface WhisperInput {
    /**
     * Audio Url
     * @description URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm.
     * @example https://storage.googleapis.com/falserverless/model_tests/whisper/dinner_conversation.mp3
     */
    audio_url: string;
    /**
     * Batch Size
     * @default 64
     * @example 64
     */
    batch_size?: number;
    /**
     * Chunk Level
     * @description Level of the chunks to return. Either none, segment or word. `none` would imply that all of the audio will be transcribed without the timestamp tokens, we suggest to switch to `none` if you are not satisfied with the transcription quality, since it will usually improve the quality of the results. Switching to `none` will also provide minor speed ups in the transcription due to less amount of generated tokens. Notice that setting to none will produce **a single chunk with the whole transcription**.
     * @default segment
     * @enum {string}
     */
    chunk_level?: 'none' | 'segment' | 'word';
    /**
     * Diarize
     * @description Whether to diarize the audio file. Defaults to false. Setting to true will add costs proportional to diarization inference time.
     * @default false
     */
    diarize?: boolean;
    /**
     * Language
     * @description Language of the audio file. If set to null, the language will be
     *             automatically detected. Defaults to null.
     *
     *             If translate is selected as the task, the audio will be translated to
     *             English, regardless of the language selected.
     * @enum {string|null}
     */
    language?:
        | 'af'
        | 'am'
        | 'ar'
        | 'as'
        | 'az'
        | 'ba'
        | 'be'
        | 'bg'
        | 'bn'
        | 'bo'
        | 'br'
        | 'bs'
        | 'ca'
        | 'cs'
        | 'cy'
        | 'da'
        | 'de'
        | 'el'
        | 'en'
        | 'es'
        | 'et'
        | 'eu'
        | 'fa'
        | 'fi'
        | 'fo'
        | 'fr'
        | 'gl'
        | 'gu'
        | 'ha'
        | 'haw'
        | 'he'
        | 'hi'
        | 'hr'
        | 'ht'
        | 'hu'
        | 'hy'
        | 'id'
        | 'is'
        | 'it'
        | 'ja'
        | 'jw'
        | 'ka'
        | 'kk'
        | 'km'
        | 'kn'
        | 'ko'
        | 'la'
        | 'lb'
        | 'ln'
        | 'lo'
        | 'lt'
        | 'lv'
        | 'mg'
        | 'mi'
        | 'mk'
        | 'ml'
        | 'mn'
        | 'mr'
        | 'ms'
        | 'mt'
        | 'my'
        | 'ne'
        | 'nl'
        | 'nn'
        | 'no'
        | 'oc'
        | 'pa'
        | 'pl'
        | 'ps'
        | 'pt'
        | 'ro'
        | 'ru'
        | 'sa'
        | 'sd'
        | 'si'
        | 'sk'
        | 'sl'
        | 'sn'
        | 'so'
        | 'sq'
        | 'sr'
        | 'su'
        | 'sv'
        | 'sw'
        | 'ta'
        | 'te'
        | 'tg'
        | 'th'
        | 'tk'
        | 'tl'
        | 'tr'
        | 'tt'
        | 'uk'
        | 'ur'
        | 'uz'
        | 'vi'
        | 'yi'
        | 'yo'
        | 'zh';
    /**
     * Num Speakers
     * @description Number of speakers in the audio file. Defaults to null.
     *                 If not provided, the number of speakers will be automatically
     *                 detected.
     * @example null
     */
    num_speakers?: number;
    /**
     * Prompt
     * @description Prompt to use for generation. Defaults to an empty string.
     * @default
     */
    prompt?: string;
    /**
     * Task
     * @description Task to perform on the audio file. Either transcribe or translate.
     * @default transcribe
     * @enum {string}
     */
    task?: 'transcribe' | 'translate';
    /**
     * Version
     * @description Version of the model to use. All of the models are the Whisper large variant.
     * @default 3
     * @enum {string}
     */
    version?: '3';
}

export interface WhisperOutput {
    /**
     * Chunks
     * @description Timestamp chunks of the audio file
     */
    chunks?: Components.WhisperChunk[];
    /**
     * Diarization Segments
     * @description Speaker diarization segments of the audio file. Only present if diarization is enabled.
     */
    diarization_segments: Components.DiarizationSegment[];
    /**
     * Inferred Languages
     * @description List of languages that the audio file is inferred to be. Defaults to null.
     */
    inferred_languages: (
        | 'af'
        | 'am'
        | 'ar'
        | 'as'
        | 'az'
        | 'ba'
        | 'be'
        | 'bg'
        | 'bn'
        | 'bo'
        | 'br'
        | 'bs'
        | 'ca'
        | 'cs'
        | 'cy'
        | 'da'
        | 'de'
        | 'el'
        | 'en'
        | 'es'
        | 'et'
        | 'eu'
        | 'fa'
        | 'fi'
        | 'fo'
        | 'fr'
        | 'gl'
        | 'gu'
        | 'ha'
        | 'haw'
        | 'he'
        | 'hi'
        | 'hr'
        | 'ht'
        | 'hu'
        | 'hy'
        | 'id'
        | 'is'
        | 'it'
        | 'ja'
        | 'jw'
        | 'ka'
        | 'kk'
        | 'km'
        | 'kn'
        | 'ko'
        | 'la'
        | 'lb'
        | 'ln'
        | 'lo'
        | 'lt'
        | 'lv'
        | 'mg'
        | 'mi'
        | 'mk'
        | 'ml'
        | 'mn'
        | 'mr'
        | 'ms'
        | 'mt'
        | 'my'
        | 'ne'
        | 'nl'
        | 'nn'
        | 'no'
        | 'oc'
        | 'pa'
        | 'pl'
        | 'ps'
        | 'pt'
        | 'ro'
        | 'ru'
        | 'sa'
        | 'sd'
        | 'si'
        | 'sk'
        | 'sl'
        | 'sn'
        | 'so'
        | 'sq'
        | 'sr'
        | 'su'
        | 'sv'
        | 'sw'
        | 'ta'
        | 'te'
        | 'tg'
        | 'th'
        | 'tk'
        | 'tl'
        | 'tr'
        | 'tt'
        | 'uk'
        | 'ur'
        | 'uz'
        | 'vi'
        | 'yi'
        | 'yo'
        | 'zh'
    )[];
    /**
     * Text
     * @description Transcription of the audio file
     */
    text: string;
}

export interface WanV22A14bVideoToVideoInput {
    /**
     * Acceleration
     * @description Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Adjust FPS for Interpolation
     * @description If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
     * @default true
     * @example true
     */
    adjust_fps_for_interpolation?: boolean;
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input video.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Enable Output Safety Checker
     * @description If set to true, output video will be checked for safety after generation.
     * @default false
     * @example false
     */
    enable_output_safety_checker?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames per Second
     * @description Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
     * @default 16
     * @example 16
     */
    frames_per_second?: number;
    /**
     * Guidance Scale (1st Stage)
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 3.5
     * @example 3.5
     */
    guidance_scale?: number;
    /**
     * Guidance Scale (2nd Stage)
     * @description Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.
     * @default 4
     * @example 4
     */
    guidance_scale_2?: number;
    /**
     * Interpolator Model
     * @description The model to use for frame interpolation. If None, no interpolation is applied.
     * @default film
     * @example film
     * @enum {string}
     */
    interpolator_model?: 'none' | 'film' | 'rife';
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 17 to 161 (inclusive).
     * @default 81
     * @example 81
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 27
     * @example 27
     */
    num_inference_steps?: number;
    /**
     * Number of Interpolated Frames
     * @description Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
     * @default 1
     * @example 1
     */
    num_interpolated_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A low-angle medium shot captures a domestic white cat with brown and black patches and a blue bandana sitting on a light-colored tiled floor indoors, meticulously grooming itself by licking its paw and then rubbing its face with it, against a soft-focused background of wooden kitchen cabinets and a reflective metallic appliance.
     */
    prompt: string;
    /**
     * Resample Video Frame Rate
     * @description If true, the video will be resampled to the passed frames per second. If false, the video will not be resampled.
     * @default false
     * @example false
     */
    resample_fps?: boolean;
    /**
     * Resolution
     * @description Resolution of the generated video (480p, 580p, or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift value for the video. Must be between 1.0 and 10.0.
     * @default 5
     * @example 5
     */
    shift?: number;
    /**
     * Strength
     * @description Strength of the video transformation. A value of 1.0 means the output will be completely based on the prompt, while a value of 0.0 means the output will be identical to the input video.
     * @default 0.9
     * @example 0.9
     */
    strength?: number;
    /**
     * Video Quality
     * @description The quality of the output video. Higher quality means better visual quality but larger file size.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video URL
     * @description URL of the input video.
     * @example https://storage.googleapis.com/falserverless/example_inputs/wan-2.2-v2v-input.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface WanV22A14bVideoToVideoOutput {
    /**
     * Prompt
     * @description The text prompt used for video generation.
     * @default
     * @example A low-angle medium shot captures a domestic white cat with brown and black patches and a blue bandana sitting on a light-colored tiled floor indoors, meticulously grooming itself by licking its paw and then rubbing its face with it, against a soft-focused background of wooden kitchen cabinets and a reflective metallic appliance.
     */
    prompt?: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/wan-2.2-v2v-output.mp4"
     *     }
     */
    video: Components.File;
}

export interface WanV22A14bTextToVideoTurboInput {
    /**
     * Acceleration
     * @description Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video (16:9 or 9:16).
     * @default 16:9
     * @example 16:9
     * @example 9:16
     * @example 1:1
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Enable Output Safety Checker
     * @description If set to true, output video will be checked for safety after generation.
     * @default false
     * @example false
     */
    enable_output_safety_checker?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A medium shot establishes a modern, minimalist office setting: clean lines, muted grey walls, and polished wood surfaces. The focus shifts to a close-up on a woman in sharp, navy blue business attire. Her crisp white blouse contrasts with the deep blue of her tailored suit jacket. The subtle texture of the fabric is visible—a fine weave with a slight sheen. Her expression is serious, yet engaging, as she speaks to someone unseen just beyond the frame. Close-up on her eyes, showing the intensity of her gaze and the fine lines around them that hint at experience and focus. Her lips are slightly parted, as if mid-sentence. The light catches the subtle highlights in her auburn hair, meticulously styled. Note the slight catch of light on the silver band of her watch. High resolution 4k
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p, 580p, or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Video Quality
     * @description The quality of the output video. Higher quality means better visual quality but larger file size.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface WanV22A14bTextToVideoTurboOutput {
    /**
     * Prompt
     * @description The text prompt used for video generation.
     * @default
     * @example A medium shot establishes a modern, minimalist office setting: clean lines, muted grey walls, and polished wood surfaces. The focus shifts to a close-up on a woman in sharp, navy blue business attire. Her crisp white blouse contrasts with the deep blue of her tailored suit jacket. The subtle texture of the fabric is visible—a fine weave with a slight sheen. Her expression is serious, yet engaging, as she speaks to someone unseen just beyond the frame. Close-up on her eyes, showing the intensity of her gaze and the fine lines around them that hint at experience and focus. Her lips are slightly parted, as if mid-sentence. The light catches the subtle highlights in her auburn hair, meticulously styled. Note the slight catch of light on the silver band of her watch. High resolution 4k
     */
    prompt?: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/gallery/wan-t2v-turbo.mp4"
     *     }
     */
    video: Components.File;
}

export interface WanV22A14bTextToVideoLoraInput {
    /**
     * Acceleration
     * @description Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Adjust FPS for Interpolation
     * @description If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
     * @default true
     * @example true
     */
    adjust_fps_for_interpolation?: boolean;
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video (16:9 or 9:16).
     * @default 16:9
     * @example 16:9
     * @example 9:16
     * @example 1:1
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Enable Output Safety Checker
     * @description If set to true, output video will be checked for safety after generation.
     * @default false
     * @example false
     */
    enable_output_safety_checker?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames per Second
     * @description Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
     * @default 16
     * @example 16
     */
    frames_per_second?: number;
    /**
     * Guidance Scale (1st Stage)
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 3.5
     * @example 3.5
     */
    guidance_scale?: number;
    /**
     * Guidance Scale (2nd Stage)
     * @description Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.
     * @default 4
     * @example 4
     */
    guidance_scale_2?: number;
    /**
     * Interpolator Model
     * @description The model to use for frame interpolation. If None, no interpolation is applied.
     * @default film
     * @example film
     * @enum {string}
     */
    interpolator_model?: 'none' | 'film' | 'rife';
    /**
     * Loras
     * @description LoRA weights to be used in the inference.
     * @default []
     */
    loras?: Components.LoRAWeight_1[];
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 17 to 161 (inclusive).
     * @default 81
     * @example 81
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 27
     * @example 27
     */
    num_inference_steps?: number;
    /**
     * Number of Interpolated Frames
     * @description Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
     * @default 1
     * @example 1
     */
    num_interpolated_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A close-up of a young woman smiling gently in the rain, raindrops glistening on her face and eyelashes. The video captures the delicate details of her expression and the water droplets, with soft light reflecting off her skin in the rainy atmosphere.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p, 580p, or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Reverse Video
     * @description If true, the video will be reversed.
     * @default false
     */
    reverse_video?: boolean;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift value for the video. Must be between 1.0 and 10.0.
     * @default 5
     * @example 5
     */
    shift?: number;
    /**
     * Video Quality
     * @description The quality of the output video. Higher quality means better visual quality but larger file size.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface WanV22A14bTextToVideoLoraOutput extends SharedType_ec2 {}

export interface WanV22A14bTextToVideoInput {
    /**
     * Acceleration
     * @description Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Adjust FPS for Interpolation
     * @description If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
     * @default true
     * @example true
     */
    adjust_fps_for_interpolation?: boolean;
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video (16:9 or 9:16).
     * @default 16:9
     * @example 16:9
     * @example 9:16
     * @example 1:1
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Enable Output Safety Checker
     * @description If set to true, output video will be checked for safety after generation.
     * @default false
     * @example false
     */
    enable_output_safety_checker?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames per Second
     * @description Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
     * @default 16
     * @example 16
     */
    frames_per_second?: number;
    /**
     * Guidance Scale (1st Stage)
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 3.5
     * @example 3.5
     */
    guidance_scale?: number;
    /**
     * Guidance Scale (2nd Stage)
     * @description Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.
     * @default 4
     * @example 4
     */
    guidance_scale_2?: number;
    /**
     * Interpolator Model
     * @description The model to use for frame interpolation. If None, no interpolation is applied.
     * @default film
     * @example film
     * @enum {string}
     */
    interpolator_model?: 'none' | 'film' | 'rife';
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 17 to 161 (inclusive).
     * @default 81
     * @example 81
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 27
     * @example 27
     */
    num_inference_steps?: number;
    /**
     * Number of Interpolated Frames
     * @description Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
     * @default 1
     * @example 1
     */
    num_interpolated_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A close-up of a young woman smiling gently in the rain, raindrops glistening on her face and eyelashes. The video captures the delicate details of her expression and the water droplets, with soft light reflecting off her skin in the rainy atmosphere.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p, 580p, or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift value for the video. Must be between 1.0 and 10.0.
     * @default 5
     * @example 5
     */
    shift?: number;
    /**
     * Video Quality
     * @description The quality of the output video. Higher quality means better visual quality but larger file size.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface WanV22A14bTextToVideoOutput extends SharedType_ec2 {}

export interface WanV22A14bTextToImageLoraInput {
    /**
     * Acceleration
     * @description Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Output Safety Checker
     * @description If set to true, output video will be checked for safety after generation.
     * @default false
     * @example false
     */
    enable_output_safety_checker?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale (1st Stage)
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 3.5
     * @example 3.5
     */
    guidance_scale?: number;
    /**
     * Guidance Scale (2nd Stage)
     * @description Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.
     * @default 4
     * @example 4
     */
    guidance_scale_2?: number;
    /**
     * Image Format
     * @description The format of the output image.
     * @default jpeg
     * @example jpeg
     * @enum {string}
     */
    image_format?: 'png' | 'jpeg';
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     * @example square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description LoRA weights to be used in the inference.
     * @default []
     */
    loras?: Components.LoRAWeight_1[];
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 27
     * @example 27
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The text prompt to guide image generation.
     * @example In this breathtaking wildlife documentary, we are drawn into an intimate close-up of a majestic lion's face, framed against the backdrop of a vast African savannah at dawn. The camera captures the raw power and nobility of the creature as it gazes intently into the distance, its golden-brown fur glistening under the soft, diffused light that bathes the scene in an ethereal glow. Harsh shadows dance across its features, accentuating the deep wrinkles around its eyes and the rugged texture of its fur, each strand a testament to its age and wisdom. The static camera angle invites viewers to immerse themselves in this moment of profound stillness, where the lion's intense focus hints at an unseen presence or a distant threat. As the sun ascends, the landscape transforms into a symphony of warm hues, enhancing the serene yet tense atmosphere that envelops this extraordinary encounter with nature's untamed beauty.
     */
    prompt: string;
    /**
     * Reverse Video
     * @description If true, the video will be reversed.
     * @default false
     */
    reverse_video?: boolean;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift value for the image. Must be between 1.0 and 10.0.
     * @default 2
     * @example 2
     */
    shift?: number;
}

export interface WanV22A14bTextToImageLoraOutput extends SharedType_4f3 {}

export interface WanV22A14bTextToImageInput {
    /**
     * Acceleration
     * @description Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Output Safety Checker
     * @description If set to true, output video will be checked for safety after generation.
     * @default false
     * @example false
     */
    enable_output_safety_checker?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale (1st Stage)
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 3.5
     * @example 3.5
     */
    guidance_scale?: number;
    /**
     * Guidance Scale (2nd Stage)
     * @description Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.
     * @default 4
     * @example 4
     */
    guidance_scale_2?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     * @example square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 27
     * @example 27
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The text prompt to guide image generation.
     * @example In this breathtaking wildlife documentary, we are drawn into an intimate close-up of a majestic lion's face, framed against the backdrop of a vast African savannah at dawn. The camera captures the raw power and nobility of the creature as it gazes intently into the distance, its golden-brown fur glistening under the soft, diffused light that bathes the scene in an ethereal glow. Harsh shadows dance across its features, accentuating the deep wrinkles around its eyes and the rugged texture of its fur, each strand a testament to its age and wisdom. The static camera angle invites viewers to immerse themselves in this moment of profound stillness, where the lion's intense focus hints at an unseen presence or a distant threat. As the sun ascends, the landscape transforms into a symphony of warm hues, enhancing the serene yet tense atmosphere that envelops this extraordinary encounter with nature's untamed beauty.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift value for the image. Must be between 1.0 and 10.0.
     * @default 2
     * @example 2
     */
    shift?: number;
}

export interface WanV22A14bTextToImageOutput extends SharedType_4f3 {}

export interface WanV22A14bImageToVideoTurboInput {
    /**
     * Acceleration
     * @description Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Enable Output Safety Checker
     * @description If set to true, output video will be checked for safety after generation.
     * @default false
     * @example false
     */
    enable_output_safety_checker?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image URL
     * @description URL of the end image.
     */
    end_image_url?: string;
    /**
     * Image URL
     * @description URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://storage.googleapis.com/falserverless/model_tests/wan/dragon-warrior.jpg
     */
    image_url: string;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example The white dragon warrior stands still, eyes full of determination and strength. The camera slowly moves closer or circles around the warrior, highlighting the powerful presence and heroic spirit of the character.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p, 580p, or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Video Quality
     * @description The quality of the output video. Higher quality means better visual quality but larger file size.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface WanV22A14bImageToVideoTurboOutput {
    /**
     * Prompt
     * @description The text prompt used for video generation.
     * @default
     * @example The white dragon warrior stands still, eyes full of determination and strength. The camera slowly moves closer or circles around the warrior, highlighting the powerful presence and heroic spirit of the character.
     */
    prompt?: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/gallery/wan-i2v-turbo.mp4"
     *     }
     */
    video: Components.File;
}

export interface WanV22A14bImageToVideoLoraInput {
    /**
     * Acceleration
     * @description Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Adjust FPS for Interpolation
     * @description If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
     * @default true
     * @example true
     */
    adjust_fps_for_interpolation?: boolean;
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Enable Output Safety Checker
     * @description If set to true, output video will be checked for safety after generation.
     * @default false
     * @example false
     */
    enable_output_safety_checker?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image URL
     * @description URL of the end image.
     */
    end_image_url?: string;
    /**
     * Frames per Second
     * @description Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
     * @default 16
     * @example 16
     */
    frames_per_second?: number;
    /**
     * Guidance Scale (1st Stage)
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 3.5
     * @example 3.5
     */
    guidance_scale?: number;
    /**
     * Guidance Scale (2nd Stage)
     * @description Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.
     * @default 4
     * @example 4
     */
    guidance_scale_2?: number;
    /**
     * Image URL
     * @description URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://storage.googleapis.com/falserverless/gallery/car_720p.png
     */
    image_url: string;
    /**
     * Interpolator Model
     * @description The model to use for frame interpolation. If None, no interpolation is applied.
     * @default film
     * @example film
     * @enum {string}
     */
    interpolator_model?: 'none' | 'film' | 'rife';
    /**
     * Loras
     * @description LoRA weights to be used in the inference.
     * @default []
     */
    loras?: Components.LoRAWeight_1[];
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 17 to 161 (inclusive).
     * @default 81
     * @example 81
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 27
     * @example 27
     */
    num_inference_steps?: number;
    /**
     * Number of Interpolated Frames
     * @description Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
     * @default 1
     * @example 1
     */
    num_interpolated_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example Cars racing in slow motion
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p, 580p, or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Reverse Video
     * @description If true, the video will be reversed.
     * @default false
     */
    reverse_video?: boolean;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift value for the video. Must be between 1.0 and 10.0.
     * @default 5
     * @example 5
     */
    shift?: number;
    /**
     * Video Quality
     * @description The quality of the output video. Higher quality means better visual quality but larger file size.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface WanV22A14bImageToVideoLoraOutput extends SharedType_bd3 {}

export interface WanV22A14bImageToVideoInput {
    /**
     * Acceleration
     * @description Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Adjust FPS for Interpolation
     * @description If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
     * @default true
     * @example true
     */
    adjust_fps_for_interpolation?: boolean;
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Enable Output Safety Checker
     * @description If set to true, output video will be checked for safety after generation.
     * @default false
     * @example false
     */
    enable_output_safety_checker?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image URL
     * @description URL of the end image.
     */
    end_image_url?: string;
    /**
     * Frames per Second
     * @description Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
     * @default 16
     * @example 16
     */
    frames_per_second?: number;
    /**
     * Guidance Scale (1st Stage)
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 3.5
     * @example 3.5
     */
    guidance_scale?: number;
    /**
     * Guidance Scale (2nd Stage)
     * @description Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.
     * @default 3.5
     * @example 3.5
     */
    guidance_scale_2?: number;
    /**
     * Image URL
     * @description URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://storage.googleapis.com/falserverless/model_tests/wan/dragon-warrior.jpg
     */
    image_url: string;
    /**
     * Interpolator Model
     * @description The model to use for frame interpolation. If None, no interpolation is applied.
     * @default film
     * @example film
     * @enum {string}
     */
    interpolator_model?: 'none' | 'film' | 'rife';
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 17 to 161 (inclusive).
     * @default 81
     * @example 81
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 27
     * @example 27
     */
    num_inference_steps?: number;
    /**
     * Number of Interpolated Frames
     * @description Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
     * @default 1
     * @example 1
     */
    num_interpolated_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example The white dragon warrior stands still, eyes full of determination and strength. The camera slowly moves closer or circles around the warrior, highlighting the powerful presence and heroic spirit of the character.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p, 580p, or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift value for the video. Must be between 1.0 and 10.0.
     * @default 5
     * @example 5
     */
    shift?: number;
    /**
     * Video Quality
     * @description The quality of the output video. Higher quality means better visual quality but larger file size.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface WanV22A14bImageToVideoOutput extends SharedType_bd3 {}

export interface WanV22A14bImageToImageInput {
    /**
     * Acceleration
     * @description Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated image. If 'auto', the aspect ratio will be determined automatically based on the input image.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Enable Output Safety Checker
     * @description If set to true, output video will be checked for safety after generation.
     * @default false
     * @example false
     */
    enable_output_safety_checker?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Classifier-free guidance scale.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Guidance Scale (2nd Stage)
     * @description Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.
     * @default 4
     * @example 4
     */
    guidance_scale_2?: number;
    /**
     * Image Format
     * @description The format of the output image.
     * @default jpeg
     * @example jpeg
     * @enum {string}
     */
    image_format?: 'png' | 'jpeg';
    /** Image Size */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description URL of the input image.
     * @example https://storage.googleapis.com/falserverless/example_inputs/wan-image-to-image-input.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 27
     * @example 27
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The text prompt to guide image generation.
     * @example A cinematic shot of an ancient city at sunset, intricate stone buildings, warm golden light
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @default 2
     */
    shift?: number;
    /**
     * Strength
     * @description Denoising strength. 1.0 = fully remake; 0.0 = preserve original.
     * @default 0.5
     */
    strength?: number;
}

export interface WanV22A14bImageToImageOutput {
    /**
     * Image
     * @description The generated image file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/wan-image-to-image-output.png"
     *     }
     */
    image: Components.File;
    /**
     * Prompt
     * @description The text prompt used for image generation.
     * @default
     * @example A cinematic portrait of a woman in natural light, 85mm look.
     */
    prompt?: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
}

export interface WanV225bTextToVideoFastWanInput {
    /**
     * Adjust FPS for Interpolation
     * @description If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
     * @default true
     * @example true
     */
    adjust_fps_for_interpolation?: boolean;
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video (16:9 or 9:16).
     * @default 16:9
     * @example 16:9
     * @example 9:16
     * @example 1:1
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Enable Output Safety Checker
     * @description If set to true, output video will be checked for safety after generation.
     * @default false
     * @example false
     */
    enable_output_safety_checker?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames per Second
     * @description Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
     * @default 24
     * @example 24
     */
    frames_per_second?: number;
    /**
     * Guidance Scale
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 3.5
     * @example 3.5
     */
    guidance_scale?: number;
    /**
     * Interpolator Model
     * @description The model to use for frame interpolation. If None, no interpolation is applied.
     * @default film
     * @example film
     * @enum {string}
     */
    interpolator_model?: 'none' | 'film' | 'rife';
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 17 to 161 (inclusive).
     * @default 81
     * @example 81
     */
    num_frames?: number;
    /**
     * Number of Interpolated Frames
     * @description Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
     * @default 0
     * @example 0
     */
    num_interpolated_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A medium shot establishes a modern, minimalist office setting: clean lines, muted grey walls, and polished wood surfaces. The focus shifts to a close-up on a woman in sharp, navy blue business attire. Her crisp white blouse contrasts with the deep blue of her tailored suit jacket. The subtle texture of the fabric is visible—a fine weave with a slight sheen. Her expression is serious, yet engaging, as she speaks to someone unseen just beyond the frame. Close-up on her eyes, showing the intensity of her gaze and the fine lines around them that hint at experience and focus. Her lips are slightly parted, as if mid-sentence. The light catches the subtle highlights in her auburn hair, meticulously styled. Note the slight catch of light on the silver band of her watch. High resolution 4k
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (580p or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Video Quality
     * @description The quality of the output video. Higher quality means better visual quality but larger file size.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface WanV225bTextToVideoFastWanOutput extends SharedType_b5a {}

export interface WanV225bTextToVideoDistillInput {
    /**
     * Adjust FPS for Interpolation
     * @description If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
     * @default true
     * @example true
     */
    adjust_fps_for_interpolation?: boolean;
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video (16:9 or 9:16).
     * @default 16:9
     * @example 16:9
     * @example 9:16
     * @example 1:1
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Enable Output Safety Checker
     * @description If set to true, output video will be checked for safety after generation.
     * @default false
     * @example false
     */
    enable_output_safety_checker?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames per Second
     * @description Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
     * @default 24
     * @example 24
     */
    frames_per_second?: number;
    /**
     * Guidance Scale (1st Stage)
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 1
     * @example 1
     */
    guidance_scale?: number;
    /**
     * Interpolator Model
     * @description The model to use for frame interpolation. If None, no interpolation is applied.
     * @default film
     * @example film
     * @enum {string}
     */
    interpolator_model?: 'none' | 'film' | 'rife';
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 17 to 161 (inclusive).
     * @default 81
     * @example 81
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 40
     * @example 40
     */
    num_inference_steps?: number;
    /**
     * Number of Interpolated Frames
     * @description Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
     * @default 0
     * @example 0
     */
    num_interpolated_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A medium shot establishes a modern, minimalist office setting: clean lines, muted grey walls, and polished wood surfaces. The focus shifts to a close-up on a woman in sharp, navy blue business attire. Her crisp white blouse contrasts with the deep blue of her tailored suit jacket. The subtle texture of the fabric is visible—a fine weave with a slight sheen. Her expression is serious, yet engaging, as she speaks to someone unseen just beyond the frame. Close-up on her eyes, showing the intensity of her gaze and the fine lines around them that hint at experience and focus. Her lips are slightly parted, as if mid-sentence. The light catches the subtle highlights in her auburn hair, meticulously styled. Note the slight catch of light on the silver band of her watch. High resolution 4k
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (580p or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '580p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift value for the video. Must be between 1.0 and 10.0.
     * @default 5
     * @example 5
     */
    shift?: number;
    /**
     * Video Quality
     * @description The quality of the output video. Higher quality means better visual quality but larger file size.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface WanV225bTextToVideoDistillOutput extends SharedType_b5a {}

export interface WanV225bTextToVideoInput {
    /**
     * Adjust FPS for Interpolation
     * @description If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
     * @default true
     * @example true
     */
    adjust_fps_for_interpolation?: boolean;
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video (16:9 or 9:16).
     * @default 16:9
     * @example 16:9
     * @example 9:16
     * @example 1:1
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Enable Output Safety Checker
     * @description If set to true, output video will be checked for safety after generation.
     * @default false
     * @example false
     */
    enable_output_safety_checker?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames per Second
     * @description Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
     * @default 24
     * @example 24
     */
    frames_per_second?: number;
    /**
     * Guidance Scale
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 3.5
     * @example 3.5
     */
    guidance_scale?: number;
    /**
     * Interpolator Model
     * @description The model to use for frame interpolation. If None, no interpolation is applied.
     * @default film
     * @example film
     * @enum {string}
     */
    interpolator_model?: 'none' | 'film' | 'rife';
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 17 to 161 (inclusive).
     * @default 81
     * @example 81
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 40
     * @example 40
     */
    num_inference_steps?: number;
    /**
     * Number of Interpolated Frames
     * @description Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
     * @default 0
     * @example 0
     */
    num_interpolated_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A medium shot establishes a modern, minimalist office setting: clean lines, muted grey walls, and polished wood surfaces. The focus shifts to a close-up on a woman in sharp, navy blue business attire. Her crisp white blouse contrasts with the deep blue of her tailored suit jacket. The subtle texture of the fabric is visible—a fine weave with a slight sheen. Her expression is serious, yet engaging, as she speaks to someone unseen just beyond the frame. Close-up on her eyes, showing the intensity of her gaze and the fine lines around them that hint at experience and focus. Her lips are slightly parted, as if mid-sentence. The light catches the subtle highlights in her auburn hair, meticulously styled. Note the slight catch of light on the silver band of her watch. High resolution 4k
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (580p or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '580p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift value for the video. Must be between 1.0 and 10.0.
     * @default 5
     * @example 5
     */
    shift?: number;
    /**
     * Video Quality
     * @description The quality of the output video. Higher quality means better visual quality but larger file size.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface WanV225bTextToVideoOutput extends SharedType_b5a {}

export interface WanV225bTextToImageInput {
    /**
     * Enable Output Safety Checker
     * @description If set to true, output video will be checked for safety after generation.
     * @default false
     * @example false
     */
    enable_output_safety_checker?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 3.5
     * @example 3.5
     */
    guidance_scale?: number;
    /**
     * Image Format
     * @description The format of the output image.
     * @default jpeg
     * @example jpeg
     * @enum {string}
     */
    image_format?: 'png' | 'jpeg';
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     * @example square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 40
     * @example 40
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The text prompt to guide image generation.
     * @example In this breathtaking wildlife documentary, we are drawn into an intimate close-up of a majestic lion's face, framed against the backdrop of a vast African savannah at dawn. The camera captures the raw power and nobility of the creature as it gazes intently into the distance, its golden-brown fur glistening under the soft, diffused light that bathes the scene in an ethereal glow. Harsh shadows dance across its features, accentuating the deep wrinkles around its eyes and the rugged texture of its fur, each strand a testament to its age and wisdom. The static camera angle invites viewers to immerse themselves in this moment of profound stillness, where the lion's intense focus hints at an unseen presence or a distant threat. As the sun ascends, the landscape transforms into a symphony of warm hues, enhancing the serene yet tense atmosphere that envelops this extraordinary encounter with nature's untamed beauty.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift value for the image. Must be between 1.0 and 10.0.
     * @default 2
     * @example 2
     */
    shift?: number;
}

export interface WanV225bTextToImageOutput {
    /**
     * Image
     * @description The generated image file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/wan/small-t2i-output-2.png"
     *     }
     */
    image: Components.File;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
}

export interface WanV225bImageToVideoInput {
    /**
     * Adjust FPS for Interpolation
     * @description If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
     * @default true
     * @example true
     */
    adjust_fps_for_interpolation?: boolean;
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Enable Output Safety Checker
     * @description If set to true, output video will be checked for safety after generation.
     * @default false
     * @example false
     */
    enable_output_safety_checker?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames per Second
     * @description Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
     * @default 24
     * @example 24
     */
    frames_per_second?: number;
    /**
     * Guidance Scale
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 3.5
     * @example 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://storage.googleapis.com/falserverless/model_tests/wan/dragon-warrior.jpg
     */
    image_url: string;
    /**
     * Interpolator Model
     * @description The model to use for frame interpolation. If None, no interpolation is applied.
     * @default film
     * @example film
     * @enum {string}
     */
    interpolator_model?: 'none' | 'film' | 'rife';
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 17 to 161 (inclusive).
     * @default 81
     * @example 81
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 40
     * @example 40
     */
    num_inference_steps?: number;
    /**
     * Number of Interpolated Frames
     * @description Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
     * @default 0
     * @example 0
     */
    num_interpolated_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example The white dragon warrior stands still, eyes full of determination and strength. The camera slowly moves closer or circles around the warrior, highlighting the powerful presence and heroic spirit of the character.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (580p or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '580p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift value for the video. Must be between 1.0 and 10.0.
     * @default 5
     * @example 5
     */
    shift?: number;
    /**
     * Video Quality
     * @description The quality of the output video. Higher quality means better visual quality but larger file size.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface WanV225bImageToVideoOutput {
    /**
     * Prompt
     * @description The text prompt used for video generation.
     * @default
     * @example The white dragon warrior stands still, eyes full of determination and strength. The camera slowly moves closer or circles around the warrior, highlighting the powerful presence and heroic spirit of the character.
     */
    prompt?: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/wan/v2.2-small-i2v-output.mp4"
     *     }
     */
    video: Components.File;
}

export interface WanV2214bSpeechToVideoInput {
    /**
     * Audio URL
     * @description The URL of the audio file.
     * @example https://storage.googleapis.com/falserverless/example_inputs/wan_s2v_talk.wav
     */
    audio_url: string;
    /**
     * Enable Output Safety Checker
     * @description If set to true, output video will be checked for safety after generation.
     * @default false
     * @example false
     */
    enable_output_safety_checker?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames per Second
     * @description Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
     * @default 16
     * @example 16
     */
    frames_per_second?: number;
    /**
     * Guidance Scale
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 3.5
     * @example 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://storage.googleapis.com/falserverless/example_inputs/wan_s2v_cat.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 40 to 120, (must be multiple of 4).
     * @default 80
     * @example 80
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 27
     * @example 27
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The text prompt used for video generation.
     * @example Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p, 580p, or 720p).
     * @default 480p
     * @example 480p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift value for the video. Must be between 1.0 and 10.0.
     * @default 5
     * @example 5
     */
    shift?: number;
    /**
     * Video Quality
     * @description The quality of the output video. Higher quality means better visual quality but larger file size.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface WanV2214bSpeechToVideoOutput {
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "file_size": 4685303,
     *       "file_name": "2c7ab2540af44eceaf5ffde4e8d094ed.mp4",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3.fal.media/files/panda/f7tXRCjvwEcVlmxHuw8kO_2c7ab2540af44eceaf5ffde4e8d094ed.mp4"
     *     }
     */
    video: Components.File;
}

export interface WanV2214bAnimateReplaceInput extends SharedType_cd2 {}

export interface WanV2214bAnimateReplaceOutput {
    /**
     * Frames Zip
     * @description ZIP archive of generated frames (if requested).
     */
    frames_zip?: Components.File;
    /**
     * Prompt
     * @description The prompt used for generation (auto-generated by the model)
     * @example
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation
     * @example 1416721728
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://v3b.fal.media/files/b/elephant/9Ofgiju3Peb3b5hriTuBH_wan_animate_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface WanV2214bAnimateMoveInput extends SharedType_cd2 {}

export interface WanV2214bAnimateMoveOutput {
    /**
     * Frames Zip
     * @description ZIP archive of generated frames (if requested).
     */
    frames_zip?: Components.File;
    /**
     * Prompt
     * @description The prompt used for generation (auto-generated by the model)
     * @example
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation
     * @example 1416721728
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://v3b.fal.media/files/b/monkey/xjJYzO0jqMi7MxufJe5tx_wan_animate_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface WanVisionEnhancerInput {
    /**
     * Creativity
     * @description Controls how much the model enhances/changes the video. 0 = Minimal change (preserves original), 1 = Subtle enhancement (default), 2 = Medium enhancement, 3 = Strong enhancement, 4 = Maximum enhancement.
     * @default 1
     * @example 1
     */
    creativity?: number;
    /**
     * Negative Prompt
     * @description Negative prompt to avoid unwanted features.
     * @default oversaturated, overexposed, static, blurry details, subtitles, stylized, artwork, painting, still frame, overall gray, worst quality, low quality, JPEG artifacts, ugly, mutated, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fused fingers, static motion, cluttered background, three legs, crowded background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description Optional prompt to prepend to the VLM-generated description. Leave empty to use only the auto-generated description from the video.
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Output Resolution
     * @description Target output resolution for the enhanced video. 720p (native, fast) or 1080p (upscaled, slower). Processing is always done at 720p, then upscaled if 1080p selected.
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    target_resolution?: '720p' | '1080p';
    /**
     * Video Url
     * @description The URL of the video to enhance with Wan Video. Maximum 200MB file size. Videos longer than 500 frames will have only the first 500 frames processed (~8-21 seconds depending on fps).
     * @example https://v3b.fal.media/files/b/0a85cc7d/zBFvZyJ7iX-7AZoQ1NaMN_wan_animate_output.mp4
     */
    video_url: string;
}

export interface WanVisionEnhancerOutput {
    /**
     * Seed
     * @description The seed used for generation.
     * @example 42
     */
    seed: number;
    /**
     * Timings
     * @description The timings of the different steps in the workflow.
     * @example {
     *       "video_processing": 15.2,
     *       "inference": 125.4
     *     }
     */
    timings: {
        [key: string]: number;
    };
    /**
     * @description The enhanced video file.
     * @example {
     *       "url": "https://v3b.fal.media/files/b/0a85cc9a/TBr1WXFaFb2zJ2htEWDdm_combined_2baa6f34a84a4d0caf23896580810ee1.mp4"
     *     }
     */
    video: Components.File_1;
}

export interface WanVaceAppsVideoEditInput {
    /**
     * Acceleration
     * @description Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'low' | 'regular';
    /**
     * Aspect Ratio
     * @description Aspect ratio of the edited video.
     * @default auto
     * @example auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Auto Downsample Min FPS
     * @description The minimum frames per second to downsample the video to.
     * @default 15
     */
    auto_downsample_min_fps?: number;
    /**
     * Enable Auto Downsampling
     * @description Whether to enable automatic downsampling. If your video has a high frame rate or is long, enabling longer sequences to be generated. The video will be interpolated back to the original frame rate after generation.
     * @default true
     */
    enable_auto_downsample?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image URLs
     * @description URLs of the input images to use as a reference for the generation.
     * @default []
     */
    image_urls?: string[];
    /**
     * Prompt
     * @description Prompt to edit the video.
     * @example replace him with a large anthropomorphic polar bear
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the edited video.
     * @default auto
     * @example auto
     * @enum {string}
     */
    resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p';
    /**
     * Return Frames ZIP
     * @description Whether to include a ZIP archive containing all generated frames.
     * @default false
     */
    return_frames_zip?: boolean;
    /**
     * Video Type
     * @description The type of video you're editing. Use 'general' for most videos, and 'human' for videos emphasizing human subjects and motions. The default value 'auto' means the model will guess based on the first frame of the video.
     * @default auto
     * @example auto
     * @enum {string}
     */
    video_type?: 'auto' | 'general' | 'human';
    /**
     * Video URL
     * @description URL of the input video.
     * @example https://storage.googleapis.com/falserverless/example_inputs/vace-video-edit-input.mp4
     */
    video_url: string;
}

export interface WanVaceAppsVideoEditOutput {
    /**
     * Frames Zip
     * @description ZIP archive of generated frames if requested.
     */
    frames_zip?: Components.File;
    /**
     * Video
     * @description The edited video.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/vace-video-edit-output.mp4"
     *     }
     */
    video: Components.VideoFile_1;
}

export interface WanVaceAppsLongReframeInput {
    /**
     * Acceleration
     * @description Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'low' | 'regular';
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16';
    /**
     * Auto Downsample Min Fps
     * @description Minimum FPS for auto downsample.
     * @default 6
     * @example 6
     */
    auto_downsample_min_fps?: number;
    /**
     * Enable Auto Downsample
     * @description Whether to enable auto downsample.
     * @default true
     * @example true
     */
    enable_auto_downsample?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
     * @default 5
     * @example 5
     */
    guidance_scale?: number;
    /**
     * Interpolator Model
     * @description The model to use for frame interpolation. Options are 'rife' or 'film'.
     * @default film
     * @example film
     * @enum {string}
     */
    interpolator_model?: 'rife' | 'film';
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     * @example letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Paste Back
     * @description Whether to paste back the reframed scene to the original video.
     * @default true
     */
    paste_back?: boolean;
    /**
     * Prompt
     * @description The text prompt to guide video generation. Optional for reframing.
     * @default
     * @example
     */
    prompt?: string;
    /**
     * Resolution
     * @description Resolution of the generated video.
     * @default auto
     * @enum {string}
     */
    resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p';
    /**
     * Return Frames Zip
     * @description If true, also return a ZIP file containing all generated frames.
     * @default false
     * @example false
     */
    return_frames_zip?: boolean;
    /**
     * Sampler
     * @description Sampler to use for video generation.
     * @default unipc
     * @example unipc
     * @enum {string}
     */
    sampler?: 'unipc' | 'dpm++' | 'euler';
    /**
     * Scene Threshold
     * @description Threshold for scene detection sensitivity (0-100). Lower values detect more scenes.
     * @default 30
     */
    scene_threshold?: number;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift parameter for video generation.
     * @default 5
     */
    shift?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     * @example false
     */
    sync_mode?: boolean;
    /**
     * Transparency Mode
     * @description The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
     * @default content_aware
     * @example content_aware
     * @enum {string}
     */
    transparency_mode?: 'content_aware' | 'white' | 'black';
    /**
     * Trim Borders
     * @description Whether to trim borders from the video.
     * @default true
     * @example true
     */
    trim_borders?: boolean;
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video URL
     * @description URL to the source video file. This video will be used as a reference for the reframe task.
     * @example https://storage.googleapis.com/falserverless/web-examples/wan/t2v.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
    /**
     * Zoom Factor
     * @description Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.
     * @default 0
     * @example 0
     */
    zoom_factor?: number;
}

export interface WanVaceAppsLongReframeOutput {
    /**
     * Video
     * @description The output video file.
     */
    video: Components.VideoFile_1;
}

export interface WanVace14bReframeInput extends SharedType_e15 {}

export interface WanVace14bReframeOutput extends SharedType_7eb {}

export interface WanVace14bPoseInput extends SharedType_397 {}

export interface WanVace14bPoseOutput extends SharedType_21e {}

export interface WanVace14bOutpaintingInput extends SharedType_913 {}

export interface WanVace14bOutpaintingOutput extends SharedType_82f {}

export interface WanVace14bInpaintingInput extends SharedType_0ab {}

export interface WanVace14bInpaintingOutput extends SharedType_0b5 {}

export interface WanVace14bDepthInput extends SharedType_57e {}

export interface WanVace14bDepthOutput extends SharedType_13e {}

export interface WanVace14bInput {
    /**
     * Acceleration
     * @description Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
     * @default regular
     * @example regular
     */
    acceleration?: 'none' | 'low' | 'regular';
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16';
    /**
     * Auto Downsample Min FPS
     * @description The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
     * @default 15
     * @example 15
     */
    auto_downsample_min_fps?: number;
    /**
     * Enable Auto Downsample
     * @description If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
     * @default false
     * @example false
     */
    enable_auto_downsample?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * First Frame URL
     * @description URL to the first frame of the video. If provided, the model will use this frame as a reference.
     */
    first_frame_url?: string;
    /**
     * Frames per Second
     * @description Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
     * @default 16
     */
    frames_per_second?: number;
    /**
     * Guidance Scale
     * @description Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
     * @default 5
     * @example 5
     */
    guidance_scale?: number;
    /**
     * Interpolator Model
     * @description The model to use for frame interpolation. Options are 'rife' or 'film'.
     * @default film
     * @example film
     * @enum {string}
     */
    interpolator_model?: 'rife' | 'film';
    /**
     * Last Frame URL
     * @description URL to the last frame of the video. If provided, the model will use this frame as a reference.
     */
    last_frame_url?: string;
    /**
     * Mask Image URL
     * @description URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.
     */
    mask_image_url?: string;
    /**
     * Mask Video URL
     * @description URL to the source mask file. If provided, the model will use this mask as a reference.
     */
    mask_video_url?: string;
    /**
     * Match Input Frames Per Second
     * @description If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
     * @default false
     * @example false
     */
    match_input_frames_per_second?: boolean;
    /**
     * Match Input Number of Frames
     * @description If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
     * @default false
     * @example false
     */
    match_input_num_frames?: boolean;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     * @example letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 81 to 241 (inclusive).
     * @default 81
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Number of Interpolated Frames
     * @description Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
     * @default 0
     * @example 0
     */
    num_interpolated_frames?: number;
    /**
     * Preprocess
     * @description Whether to preprocess the input video.
     * @default false
     * @example false
     */
    preprocess?: boolean;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example The video shows a man riding a horse on a vast grassland. He has long lavender hair and wears a traditional dress of a white top and black pants. The animation style makes him look like he is doing some kind of outdoor activity or performing. The background is a spectacular mountain range and cloud sky, giving a sense of tranquility and vastness. The entire video is shot from a fixed angle, focusing on the rider and his horse.
     */
    prompt: string;
    /**
     * Reference Image URLs
     * @description URLs to source reference image. If provided, the model will use this image as reference.
     */
    ref_image_urls?: string[];
    /**
     * Resolution
     * @description Resolution of the generated video.
     * @default auto
     * @enum {string}
     */
    resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p';
    /**
     * Return Frames Zip
     * @description If true, also return a ZIP file containing all generated frames.
     * @default false
     * @example false
     */
    return_frames_zip?: boolean;
    /**
     * Sampler
     * @description Sampler to use for video generation.
     * @default unipc
     * @example unipc
     * @enum {string}
     */
    sampler?: 'unipc' | 'dpm++' | 'euler';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift parameter for video generation.
     * @default 5
     */
    shift?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     * @example false
     */
    sync_mode?: boolean;
    /**
     * Task
     * @description Task type for the model.
     * @default depth
     * @enum {string}
     */
    task?: 'depth' | 'pose' | 'inpainting' | 'outpainting' | 'reframe';
    /**
     * Temporal Downsample Factor
     * @description Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
     * @default 0
     * @example 0
     */
    temporal_downsample_factor?: number;
    /**
     * Transparency Mode
     * @description The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
     * @default content_aware
     * @example content_aware
     * @enum {string}
     */
    transparency_mode?: 'content_aware' | 'white' | 'black';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video URL
     * @description URL to the source video file. If provided, the model will use this video as a reference.
     */
    video_url?: string;
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface WanVace14bOutput {
    /** @description ZIP archive of all video frames if requested. */
    frames_zip?: Components.File_1;
    /**
     * Prompt
     * @description The prompt used for generation.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /** @description The generated video file. */
    video: Components.VideoFile;
}

export interface WanVace13bInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video (16:9 or 9:16).
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '9:16' | '16:9';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames Per Second
     * @description Frames per second of the generated video. Must be between 5 to 24.
     * @default 16
     */
    frames_per_second?: number;
    /**
     * Mask Image Url
     * @description URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.
     */
    mask_image_url?: string;
    /**
     * Mask Video Url
     * @description URL to the source mask file. If provided, the model will use this mask as a reference.
     * @example https://storage.googleapis.com/falserverless/vace/src_mask.mp4
     */
    mask_video_url?: string;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     * @example bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames
     * @default 81
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Preprocess
     * @description Whether to preprocess the input video.
     * @default false
     */
    preprocess?: boolean;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example The video shows a man riding a horse on a vast grassland. He has long lavender hair and wears a traditional dress of a white top and black pants. The animation style makes him look like he is doing some kind of outdoor activity or performing. The background is a spectacular mountain range and cloud sky, giving a sense of tranquility and vastness. The entire video is shot from a fixed angle, focusing on the rider and his horse.
     */
    prompt: string;
    /**
     * Ref Image Urls
     * @description Urls to source reference image. If provided, the model will use this image as reference.
     * @example [
     *       "https://storage.googleapis.com/falserverless/vace/src_ref_image_1.png"
     *     ]
     */
    ref_image_urls?: string[];
    /**
     * Resolution
     * @description Resolution of the generated video (480p,580p, or 720p).
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift parameter for video generation.
     * @default 5
     */
    shift?: number;
    /**
     * Task
     * @description Task type for the model.
     * @default depth
     * @enum {string}
     */
    task?: 'depth' | 'inpainting' | 'pose';
    /**
     * Video Url
     * @description URL to the source video file. If provided, the model will use this video as a reference.
     * @example https://storage.googleapis.com/falserverless/vace/src_video.mp4
     */
    video_url?: string;
}

export interface WanVace13bOutput extends SharedType_40d {}

export interface WanVaceInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video (16:9 or 9:16).
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '9:16' | '16:9';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     * @example true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames Per Second
     * @description Frames per second of the generated video. Must be between 5 to 24.
     * @default 16
     */
    frames_per_second?: number;
    /**
     * Mask Image Url
     * @description URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.
     */
    mask_image_url?: string;
    /**
     * Mask Video Url
     * @description URL to the source mask file. If provided, the model will use this mask as a reference.
     * @example https://storage.googleapis.com/falserverless/vace/src_mask.mp4
     */
    mask_video_url?: string;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     * @example bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames
     * @default 81
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Preprocess
     * @description Whether to preprocess the input video.
     * @default false
     */
    preprocess?: boolean;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example The video shows a man riding a horse on a vast grassland. He has long lavender hair and wears a traditional dress of a white top and black pants. The animation style makes him look like he is doing some kind of outdoor activity or performing. The background is a spectacular mountain range and cloud sky, giving a sense of tranquility and vastness. The entire video is shot from a fixed angle, focusing on the rider and his horse.
     */
    prompt: string;
    /**
     * Ref Image Urls
     * @description Urls to source reference image. If provided, the model will use this image as reference.
     * @example [
     *       "https://storage.googleapis.com/falserverless/vace/src_ref_image_1.png"
     *     ]
     */
    ref_image_urls?: string[];
    /**
     * Resolution
     * @description Resolution of the generated video (480p,580p, or 720p).
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift parameter for video generation.
     * @default 5
     */
    shift?: number;
    /**
     * Task
     * @description Task type for the model.
     * @default depth
     * @enum {string}
     */
    task?: 'depth' | 'inpainting';
    /**
     * Video Url
     * @description URL to the source video file. If provided, the model will use this video as a reference.
     * @example https://storage.googleapis.com/falserverless/vace/src_video.mp4
     */
    video_url?: string;
}

export interface WanVaceOutput extends SharedType_40d {}

export interface WanTrainerT2v14bInput extends SharedType_595 {}

export interface WanTrainerT2v14bOutput extends SharedType_8d7 {}

export interface WanTrainerT2vInput extends SharedType_595 {}

export interface WanTrainerT2vOutput extends SharedType_8d7 {}

export interface WanTrainerI2v720pInput extends SharedType_595 {}

export interface WanTrainerI2v720pOutput extends SharedType_8d7 {}

export interface WanTrainerFlf2v720pInput extends SharedType_595 {}

export interface WanTrainerFlf2v720pOutput extends SharedType_8d7 {}

export interface WanTrainerInput extends SharedType_595 {}

export interface WanTrainerOutput extends SharedType_8d7 {}

export interface WanT2vLoraInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video (16:9 or 9:16).
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '16:9';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     * @example true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames Per Second
     * @description Frames per second of the generated video. Must be between 5 to 24.
     * @default 16
     */
    frames_per_second?: number;
    /**
     * Loras
     * @description LoRA weights to be used in the inference.
     * @default []
     */
    loras?: Components.LoraWeight_5[];
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     * @example bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description Number of frames to generate. Must be between 81 to 100 (inclusive).
     * @default 81
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p,580p, or 720p).
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Reverse Video
     * @description If true, the video will be reversed.
     * @default false
     */
    reverse_video?: boolean;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Turbo Mode
     * @description If true, the video will be generated faster with no noticeable degradation in the visual quality.
     * @default true
     */
    turbo_mode?: boolean;
}

export interface WanT2vLoraOutput extends SharedType_c4e {}

export interface WanT2vInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video (16:9 or 9:16).
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '16:9';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames Per Second
     * @description Frames per second of the generated video. Must be between 5 to 24.
     * @default 16
     */
    frames_per_second?: number;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     * @example bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description Number of frames to generate. Must be between 81 to 100 (inclusive).
     * @default 81
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p, 580p, or 720p).
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Turbo Mode
     * @description If true, the video will be generated faster with no noticeable degradation in the visual quality.
     * @default false
     */
    turbo_mode?: boolean;
}

export interface WanT2vOutput extends SharedType_c4e {}

export interface WanProTextToVideoInput {
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Prompt
     * @description The prompt to generate the video
     * @example A lone astronaut in a detailed NASA spacesuit performs an exuberant dance on the lunar surface, arms outstretched in joyful abandon against the stark moonscape. The Earth hangs dramatically in the black sky, appearing to streak past due to the motion of the dance, creating a sense of dynamic movement. The scene captures extreme contrasts between the brilliant white of the spacesuit reflecting harsh sunlight and the deep shadows of the lunar craters. Every detail is rendered with photorealistic precision: the texture of the regolith disturbed by the astronaut's boots, the reflections on the helmet visor.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface WanProTextToVideoOutput {
    /**
     * @description The generated video
     * @example {
     *       "url": "https://fal.media/files/panda/YxRLson-aETxeBK1DI4VW.mp4"
     *     }
     */
    video: Components.File_1;
}

export interface WanProImageToVideoInput {
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Url
     * @description The URL of the image to generate the video from
     * @example https://fal.media/files/elephant/8kkhB12hEZI2kkbU8pZPA_test.jpeg
     */
    image_url: string;
    /**
     * Prompt
     * @description The prompt to generate the video
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface WanProImageToVideoOutput {
    /**
     * @description The generated video
     * @example {
     *       "url": "https://fal.media/files/kangaroo/K1hB3k-IXBzq9rz1kNOxy.mp4"
     *     }
     */
    video: Components.File_1;
}

export interface WanI2vLoraInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the output video.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames Per Second
     * @description Frames per second of the generated video. Must be between 5 to 24.
     * @default 16
     */
    frames_per_second?: number;
    /**
     * Guide Scale
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 5
     */
    guide_scale?: number;
    /**
     * Image Url
     * @description URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://storage.googleapis.com/falserverless/gallery/car_720p.png
     */
    image_url: string;
    /**
     * Loras
     * @description LoRA weights to be used in the inference.
     * @default []
     */
    loras?: Components.LoraWeight_5[];
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     * @example bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description Number of frames to generate. Must be between 81 to 100 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
     * @default 81
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example Cars race in slow motion.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Reverse Video
     * @description If true, the video will be reversed.
     * @default false
     */
    reverse_video?: boolean;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift parameter for video generation.
     * @default 5
     */
    shift?: number;
    /**
     * Turbo Mode
     * @description If true, the video will be generated faster with no noticeable degradation in the visual quality.
     * @default true
     */
    turbo_mode?: boolean;
}

export interface WanI2vLoraOutput extends SharedType_e01 {}

export interface WanI2vInput {
    /**
     * Acceleration
     * @description Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames Per Second
     * @description Frames per second of the generated video. Must be between 5 to 24.
     * @default 16
     */
    frames_per_second?: number;
    /**
     * Guide Scale
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 5
     */
    guide_scale?: number;
    /**
     * Image Url
     * @description URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://storage.googleapis.com/falserverless/gallery/car_720p.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     * @example bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description Number of frames to generate. Must be between 81 to 100 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
     * @default 81
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example Cars racing in slow motion
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift parameter for video generation.
     * @default 5
     */
    shift?: number;
}

export interface WanI2vOutput extends SharedType_e01 {}

export interface WanFunControlInput {
    /**
     * Control Video URL
     * @description The URL of the control video to use as a reference for the video generation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/wan-fun-control-video-input.mp4
     */
    control_video_url: string;
    /**
     * FPS
     * @description The fps to generate. Only used when match_input_fps is False.
     * @default 16
     */
    fps?: number;
    /**
     * Guidance Scale
     * @description The guidance scale.
     * @default 6
     */
    guidance_scale?: number;
    /**
     * Match Input FPS
     * @description Whether to match the fps in the input video.
     * @default true
     */
    match_input_fps?: boolean;
    /**
     * Match Input Number of Frames
     * @description Whether to match the number of frames in the input video.
     * @default true
     */
    match_input_num_frames?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate. Only used when match_input_num_frames is False.
     * @default 81
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps.
     * @default 27
     */
    num_inference_steps?: number;
    /**
     * Preprocess Type
     * @description The type of preprocess to apply to the video. Only used when preprocess_video is True.
     * @default depth
     * @enum {string}
     */
    preprocess_type?: 'depth' | 'pose';
    /**
     * Preprocess Video
     * @description Whether to preprocess the video. If True, the video will be preprocessed to depth or pose.
     * @default false
     */
    preprocess_video?: boolean;
    /**
     * Prompt
     * @description The prompt to generate the video.
     * @example A woman wearing a lavender floral dress spins around in a circle.
     */
    prompt: string;
    /**
     * Reference Image URL
     * @description The URL of the reference image to use as a reference for the video generation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/wan-fun-input-reference-image.webp
     */
    reference_image_url?: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Shift
     * @description The shift for the scheduler.
     * @default 5
     */
    shift?: number;
}

export interface WanFunControlOutput {
    /**
     * Video
     * @description The video generated by the model.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/wan-fun-example-output.mp4"
     *     }
     */
    video: Components.File;
}

export interface WanFlf2vInput {
    /**
     * Acceleration
     * @description Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Url
     * @description URL of the ending image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://storage.googleapis.com/falserverless/web-examples/wan_flf/last_frame.png
     */
    end_image_url: string;
    /**
     * Frames Per Second
     * @description Frames per second of the generated video. Must be between 5 to 24.
     * @default 16
     */
    frames_per_second?: number;
    /**
     * Guide Scale
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 5
     */
    guide_scale?: number;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     * @example bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description Number of frames to generate. Must be between 81 to 100 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
     * @default 81
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A tabby cat is confidely strolling toward the camera, when it spins and with a flash of magic reveals itself to be a cat-dragon hybrid with glistening amber scales.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift parameter for video generation.
     * @default 5
     */
    shift?: number;
    /**
     * Start Image Url
     * @description URL of the starting image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://storage.googleapis.com/falserverless/web-examples/wan_flf/first_frame.png
     */
    start_image_url: string;
}

export interface WanFlf2vOutput {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/flf2v.mp4"
     *     }
     */
    video: Components.File;
}

export interface WanEffectsInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the output video.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Effect Type
     * @description The type of effect to apply to the video.
     * @default cakeify
     * @enum {string}
     */
    effect_type?:
        | 'squish'
        | 'muscle'
        | 'inflate'
        | 'crush'
        | 'rotate'
        | 'gun-shooting'
        | 'deflate'
        | 'cakeify'
        | 'hulk'
        | 'baby'
        | 'bride'
        | 'classy'
        | 'puppy'
        | 'snow-white'
        | 'disney-princess'
        | 'mona-lisa'
        | 'painting'
        | 'pirate-captain'
        | 'princess'
        | 'jungle'
        | 'samurai'
        | 'vip'
        | 'warrior'
        | 'zen'
        | 'assassin'
        | 'timelapse'
        | 'tsunami'
        | 'fire'
        | 'zoom-call'
        | 'doom-fps'
        | 'fus-ro-dah'
        | 'hug-jesus'
        | 'robot-face-reveal'
        | 'super-saiyan'
        | 'jumpscare'
        | 'laughing'
        | 'cartoon-jaw-drop'
        | 'crying'
        | 'kissing'
        | 'angry-face'
        | 'selfie-younger-self'
        | 'animeify'
        | 'blast';
    /**
     * Frames Per Second
     * @description Frames per second of the generated video.
     * @default 16
     */
    frames_per_second?: number;
    /**
     * Image URL
     * @description URL of the input image.
     * @example https://storage.googleapis.com/falserverless/web-examples/wan-effects/cat.jpg
     * @example https://storage.googleapis.com/falserverless/web-examples/wan-effects/man_1.png
     * @example https://storage.googleapis.com/falserverless/web-examples/wan-effects/woman_2.png
     */
    image_url: string;
    /**
     * Lora Scale
     * @description The scale of the LoRA weight. Used to adjust effect intensity.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Num Frames
     * @description Number of frames to generate.
     * @default 81
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Subject
     * @description The subject to insert into the predefined prompt template for the selected effect.
     * @example a cute kitten
     * @example Donald Trump
     * @example a tank
     * @example a ceramic vase
     */
    subject: string;
    /**
     * Turbo Mode
     * @description Whether to use turbo mode. If True, the video will be generated faster but with lower quality.
     * @default false
     */
    turbo_mode?: boolean;
}

export interface WanEffectsOutput {
    /** Seed */
    seed: number;
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/web-examples/wan-effects/cat_video.mp4"
     *     }
     */
    video: Components.File;
}

export interface WanAlphaInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '1:1' | '9:16';
    /**
     * Binarize Mask
     * @description Whether to binarize the mask.
     * @default false
     */
    binarize_mask?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * FPS
     * @description The frame rate of the generated video.
     * @default 16
     */
    fps?: number;
    /**
     * Mask Binarization Threshold
     * @description The threshold for mask binarization. When binarize_mask is True, this threshold will be used to binarize the mask. This will also be used for transparency when the output type is `.webm`.
     * @default 0.8
     */
    mask_binarization_threshold?: number;
    /**
     * Mask Clamp Lower
     * @description The lower bound of the mask clamping.
     * @default 0.1
     */
    mask_clamp_lower?: number;
    /**
     * Mask Clamp Upper
     * @description The upper bound of the mask clamping.
     * @default 0.75
     */
    mask_clamp_upper?: number;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 81
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to guide the video generation.
     * @example Medium shot. A little girl holds a bubble wand and blows out colorful bubbles that float and pop in the air. The background of this video is transparent. Realistic style.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video.
     * @default 480p
     * @enum {string}
     */
    resolution?: '240p' | '360p' | '480p' | '580p' | '720p';
    /**
     * Sampler
     * @description The sampler to use.
     * @default euler
     * @enum {string}
     */
    sampler?: 'unipc' | 'dpm++' | 'euler';
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Shift
     * @description The shift of the generated video.
     * @default 10.5
     */
    shift?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default VP9 (.webm)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface WanAlphaOutput {
    /**
     * Image
     * @description The generated image file.
     */
    image?: Components.VideoFile_1;
    /**
     * Mask
     * @description The generated mask file.
     * @example {
     *       "height": 720,
     *       "duration": 5.0625,
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/wan-alpha-mask-output.webm",
     *       "fps": 16,
     *       "width": 1280,
     *       "file_name": "wan-alpha-mask-output.webm",
     *       "content_type": "video/webm",
     *       "num_frames": 81
     *     }
     */
    mask?: Components.VideoFile_1;
    /**
     * Prompt
     * @description The prompt used for generation.
     * @example Medium shot. A little girl holds a bubble wand and blows out colorful bubbles that float and pop in the air. The background of this video is transparent. Realistic style.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation.
     * @example 424911732
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "height": 720,
     *       "file_name": "wan-alpha-rgba-output.webm",
     *       "content_type": "video/webm",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/wan-alpha-rgba-output.webm",
     *       "width": 1280
     *     }
     */
    video?: Components.VideoFile_1;
}

export interface Wan25PreviewTextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Audio Url
     * @description URL of the audio to use as the background music. Must be publicly accessible.
     *     Limit handling: If the audio duration exceeds the duration value (5 or 10 seconds),
     *     the audio is truncated to the first 5 or 10 seconds, and the rest is discarded. If
     *     the audio is shorter than the video, the remaining part of the video will be silent.
     *     For example, if the audio is 3 seconds long and the video duration is 5 seconds, the
     *     first 3 seconds of the output video will have sound, and the last 2 seconds will be silent.
     *     - Format: WAV, MP3.
     *     - Duration: 3 to 30 s.
     *     - File size: Up to 15 MB.
     */
    audio_url?: string;
    /**
     * Duration
     * @description Duration of the generated video in seconds. Choose between 5 or 10 seconds.
     * @default 5
     * @example 5
     * @example 10
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Negative Prompt
     * @description Negative prompt to describe content to avoid. Max 500 characters.
     * @example low resolution, error, worst quality, low quality, defects
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The text prompt for video generation. Supports Chinese and English, max 800 characters.
     * @example The white dragon warrior stands still, eyes full of determination and strength. The camera slowly moves closer or circles around the warrior, highlighting the powerful presence and heroic spirit of the character.
     */
    prompt: string;
    /**
     * Resolution
     * @description Video resolution tier
     * @default 1080p
     * @enum {string}
     */
    resolution?: '480p' | '720p' | '1080p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface Wan25PreviewTextToVideoOutput extends SharedType_6ea {}

export interface Wan25PreviewTextToImageInput {
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image. Can use preset names like 'square', 'landscape_16_9', etc., or specific dimensions. Total pixels must be between 768×768 and 1440×1440, with aspect ratio between [1:4, 4:1].
     * @default square
     * @example square
     * @example landscape_16_9
     * @example portrait_16_9
     * @example {
     *       "height": 1280,
     *       "width": 1280
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description Negative prompt to describe content to avoid. Max 500 characters.
     * @example low resolution, error, worst quality, low quality, defects
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate. Values from 1 to 4.
     * @default 1
     * @example 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The prompt for image generation. Supports Chinese and English, max 2000 characters.
     * @example A lone samurai standing on the edge of a cliff at twilight, overlooking a vast valley shrouded in mist. The sky burns with deep orange and purple hues from the setting sun, casting long, dramatic shadows. The samurai’s silhouette glows against the horizon, with their sword reflecting a glint of fading light. The overall style is hyper-realistic, cinematic, and moody, with dramatic contrast and atmospheric depth.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface Wan25PreviewTextToImageOutput {
    /**
     * Actual Prompt
     * @description The actual prompt used if prompt rewriting was enabled
     * @example A lone samurai standing on the edge of a cliff at twilight, overlooking a vast valley shrouded in mist. The sky burns with deep orange and purple hues from the setting sun, casting long, dramatic shadows. The samurai’s silhouette glows against the horizon, with their sword reflecting a glint of fading light. The overall style is hyper-realistic, cinematic, and moody, with dramatic contrast and atmospheric depth.
     */
    actual_prompt?: string;
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "content_type": "image/png",
     *         "url": "https://v3.fal.media/files/penguin/4VZ7I1ZK5XNv33LV2JBxg.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Seeds
     * @description The seeds used for each generated image
     * @example [
     *       175932751
     *     ]
     */
    seeds: number[];
}

export interface Wan25PreviewImageToVideoInput {
    /**
     * Audio Url
     * @description URL of the audio to use as the background music. Must be publicly accessible.
     *     Limit handling: If the audio duration exceeds the duration value (5 or 10 seconds),
     *     the audio is truncated to the first 5 or 10 seconds, and the rest is discarded. If
     *     the audio is shorter than the video, the remaining part of the video will be silent.
     *     For example, if the audio is 3 seconds long and the video duration is 5 seconds, the
     *     first 3 seconds of the output video will have sound, and the last 2 seconds will be silent.
     *     - Format: WAV, MP3.
     *     - Duration: 3 to 30 s.
     *     - File size: Up to 15 MB.
     */
    audio_url?: string;
    /**
     * Duration
     * @description Duration of the generated video in seconds. Choose between 5 or 10 seconds.
     * @default 5
     * @example 5
     * @example 10
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt rewriting using LLM.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Image URL
     * @description URL of the image to use as the first frame. Must be publicly accessible or base64 data URI.
     *
     *     Max file size: 25.0MB, Min width: 360px, Min height: 360px, Max width: 2000px, Max height: 2000px, Timeout: 20.0s
     * @example https://storage.googleapis.com/falserverless/model_tests/wan/dragon-warrior.jpg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt to describe content to avoid. Max 500 characters.
     * @example low resolution, error, worst quality, low quality, defects
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The text prompt describing the desired video motion. Max 800 characters.
     * @example The white dragon warrior stands still, eyes full of determination and strength. The camera slowly moves closer or circles around the warrior, highlighting the powerful presence and heroic spirit of the character.
     */
    prompt: string;
    /**
     * Resolution
     * @description Video resolution. Valid values: 480p, 720p, 1080p
     * @default 1080p
     * @enum {string}
     */
    resolution?: '480p' | '720p' | '1080p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface Wan25PreviewImageToVideoOutput extends SharedType_6ea {}

export interface Wan25PreviewImageToImageInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image. Width and height must be between 384 and 1440 pixels.
     * @default square
     * @example square
     * @example landscape_16_9
     * @example portrait_16_9
     * @example {
     *       "height": 1280,
     *       "width": 1280
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Urls
     * @description URLs of images to edit. For single-image editing, provide 1 URL. For multi-reference generation, provide up to 2 URLs. If more than 2 URLs are provided, only the first 2 will be used.
     * @example [
     *       "https://v3.fal.media/files/penguin/4VZ7I1ZK5XNv33LV2JBxg.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Negative Prompt
     * @description Negative prompt to describe content to avoid. Max 500 characters.
     * @example low resolution, error, worst quality, low quality, defects
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate. Values from 1 to 4.
     * @default 1
     * @example 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The text prompt describing how to edit the image. Max 2000 characters.
     * @example Reimagine the scene under a raging thunderstorm at night: lightning forks across the sky, illuminating the samurai in stark flashes of white light.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface Wan25PreviewImageToImageOutput {
    /**
     * Actual Prompt
     * @description The original prompt (prompt expansion is not available for image editing)
     * @example Reimagine the scene under a raging thunderstorm at night: lightning forks across the sky, illuminating the samurai in stark flashes of white light.
     */
    actual_prompt?: string;
    /**
     * Images
     * @description The edited images
     * @example [
     *       {
     *         "content_type": "image/png",
     *         "url": "https://v3.fal.media/files/rabbit/BM4J8xpV5ogtOQE9xGtft.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Seeds
     * @description The seeds used for each generated image
     * @example [
     *       175932751
     *     ]
     */
    seeds: number[];
}

export interface Wan22VaceFunA14bReframeInput extends SharedType_e15 {}

export interface Wan22VaceFunA14bReframeOutput extends SharedType_7eb {}

export interface Wan22VaceFunA14bPoseInput extends SharedType_397 {}

export interface Wan22VaceFunA14bPoseOutput extends SharedType_21e {}

export interface Wan22VaceFunA14bOutpaintingInput extends SharedType_913 {}

export interface Wan22VaceFunA14bOutpaintingOutput extends SharedType_82f {}

export interface Wan22VaceFunA14bInpaintingInput extends SharedType_0ab {}

export interface Wan22VaceFunA14bInpaintingOutput extends SharedType_0b5 {}

export interface Wan22VaceFunA14bDepthInput extends SharedType_57e {}

export interface Wan22VaceFunA14bDepthOutput extends SharedType_13e {}

export interface Wan22ImageTrainerInput {
    /**
     * Include Synthetic Captions
     * @description Whether to include synthetic captions.
     * @default false
     */
    include_synthetic_captions?: boolean;
    /**
     * Is Style
     * @description Whether the training data is style data. If true, face specific options like masking and face detection will be disabled.
     * @default false
     * @example false
     */
    is_style?: boolean;
    /**
     * Learning Rate
     * @description Learning rate for training.
     * @default 0.0007
     * @example 0.0007
     */
    learning_rate?: number;
    /**
     * Number of Steps
     * @description Number of training steps.
     * @default 1000
     * @example 1000
     */
    steps?: number;
    /**
     * Training Data URL
     * @description URL to the training data.
     */
    training_data_url: string;
    /**
     * Trigger Phrase
     * @description Trigger phrase for the model.
     */
    trigger_phrase: string;
    /**
     * Use Face Cropping
     * @description Whether to use face cropping for the training data. When enabled, images will be cropped to the face before resizing.
     * @default false
     * @example false
     */
    use_face_cropping?: boolean;
    /**
     * Use Face Detection
     * @description Whether to use face detection for the training data. When enabled, images will use the center of the face as the center of the image when resizing.
     * @default true
     * @example true
     */
    use_face_detection?: boolean;
    /**
     * Use Masks
     * @description Whether to use masks for the training data.
     * @default true
     * @example true
     */
    use_masks?: boolean;
}

export interface Wan22ImageTrainerOutput {
    /**
     * Config File
     * @description Config file helping inference endpoints after training.
     */
    config_file: Components.File;
    /**
     * Low Noise LoRA
     * @description Low noise LoRA file.
     */
    diffusers_lora_file: Components.File;
    /**
     * High Noise LoRA
     * @description High noise LoRA file.
     */
    high_noise_lora: Components.File;
}

export interface ViduTemplateToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the output video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16';
    /**
     * Input Image Urls
     * @description URLs of the images to use with the template. Number of images required varies by template: 'dynasty_dress' and 'shop_frame' accept 1-2 images, 'wish_sender' requires exactly 3 images, all other templates accept only 1 image.
     * @example [
     *       "https://storage.googleapis.com/falserverless/web-examples/vidu/hug.PNG"
     *     ]
     */
    input_image_urls: string[];
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
    /**
     * Template
     * @description AI video template to use. Pricing varies by template: Standard templates (hug, kiss, love_pose, etc.) cost 4 credits ($0.20), Premium templates (lunar_newyear, dynasty_dress, dreamy_wedding, etc.) cost 6 credits ($0.30), and Advanced templates (live_photo) cost 10 credits ($0.50).
     * @default hug
     * @enum {string}
     */
    template?:
        | 'dreamy_wedding'
        | 'romantic_lift'
        | 'sweet_proposal'
        | 'couple_arrival'
        | 'cupid_arrow'
        | 'pet_lovers'
        | 'lunar_newyear'
        | 'hug'
        | 'kiss'
        | 'dynasty_dress'
        | 'wish_sender'
        | 'love_pose'
        | 'hair_swap'
        | 'youth_rewind'
        | 'morphlab'
        | 'live_photo'
        | 'emotionlab'
        | 'live_memory'
        | 'interaction'
        | 'christmas'
        | 'pet_finger'
        | 'eat_mushrooms'
        | 'beast_chase_library'
        | 'beast_chase_supermarket'
        | 'petal_scattered'
        | 'emoji_figure'
        | 'hair_color_change'
        | 'multiple_people_kissing'
        | 'beast_chase_amazon'
        | 'beast_chase_mountain'
        | 'balloonman_explodes_pro'
        | 'get_thinner'
        | 'jump2pool'
        | 'bodyshake'
        | 'jiggle_up'
        | 'shake_it_dance'
        | 'subject_3'
        | 'pubg_winner_hit'
        | 'shake_it_down'
        | 'blueprint_supreme'
        | 'hip_twist'
        | 'motor_dance'
        | 'rat_dance'
        | 'kwok_dance'
        | 'leg_sweep_dance'
        | 'heeseung_march'
        | 'shake_to_max'
        | 'dame_un_grrr'
        | 'i_know'
        | 'lit_bounce'
        | 'wave_dance'
        | 'chill_dance'
        | 'hip_flicking'
        | 'sakura_season'
        | 'zongzi_wrap'
        | 'zongzi_drop'
        | 'dragonboat_shot'
        | 'rain_kiss'
        | 'child_memory'
        | 'couple_drop'
        | 'couple_walk'
        | 'flower_receive'
        | 'love_drop'
        | 'cheek_kiss'
        | 'carry_me'
        | 'blow_kiss'
        | 'love_fall'
        | 'french_kiss_8s'
        | 'workday_feels'
        | 'love_story'
        | 'bloom_magic'
        | 'ghibli'
        | 'minecraft'
        | 'box_me'
        | 'claw_me'
        | 'clayshot'
        | 'manga_meme'
        | 'quad_meme'
        | 'pixel_me'
        | 'clayshot_duo'
        | 'irasutoya'
        | 'american_comic'
        | 'simpsons_comic'
        | 'yayoi_kusama_style'
        | 'pop_art'
        | 'jojo_style'
        | 'slice_therapy'
        | 'balloon_flyaway'
        | 'flying'
        | 'paperman'
        | 'pinch'
        | 'bloom_doorobear'
        | 'gender_swap'
        | 'nap_me'
        | 'sexy_me'
        | 'spin360'
        | 'smooth_shift'
        | 'paper_fall'
        | 'jump_to_cloud'
        | 'pilot'
        | 'sweet_dreams'
        | 'soul_depart'
        | 'punch_hit'
        | 'watermelon_hit'
        | 'split_stance_pet'
        | 'make_face'
        | 'break_glass'
        | 'split_stance_human'
        | 'covered_liquid_metal'
        | 'fluffy_plunge'
        | 'pet_belly_dance'
        | 'water_float'
        | 'relax_cut'
        | 'head_to_balloon'
        | 'cloning'
        | 'across_the_universe_jungle'
        | 'clothes_spinning_remnant'
        | 'across_the_universe_jurassic'
        | 'across_the_universe_moon'
        | 'fisheye_pet'
        | 'hitchcock_zoom'
        | 'cute_bangs'
        | 'earth_zoom_out'
        | 'fisheye_human'
        | 'drive_yacht'
        | 'virtual_singer'
        | 'earth_zoom_in'
        | 'aliens_coming'
        | 'drive_ferrari'
        | 'bjd_style'
        | 'virtual_fitting'
        | 'orbit'
        | 'zoom_in'
        | 'ai_outfit'
        | 'spin180'
        | 'orbit_dolly'
        | 'orbit_dolly_fast'
        | 'auto_spin'
        | 'walk_forward'
        | 'outfit_show'
        | 'zoom_in_fast'
        | 'zoom_out_image'
        | 'zoom_out_startend'
        | 'muscling'
        | 'captain_america'
        | 'hulk'
        | 'cap_walk'
        | 'hulk_dive'
        | 'exotic_princess'
        | 'beast_companion'
        | 'cartoon_doll'
        | 'golden_epoch'
        | 'oscar_gala'
        | 'fashion_stride'
        | 'star_carpet'
        | 'flame_carpet'
        | 'frost_carpet'
        | 'mecha_x'
        | 'style_me'
        | 'tap_me'
        | 'saber_warrior'
        | 'pet2human'
        | 'graduation'
        | 'fishermen'
        | 'happy_birthday'
        | 'fairy_me'
        | 'ladudu_me'
        | 'ladudu_me_random'
        | 'squid_game'
        | 'superman'
        | 'grow_wings'
        | 'clevage'
        | 'fly_with_doraemon'
        | 'creatice_product_down'
        | 'pole_dance'
        | 'hug_from_behind'
        | 'creatice_product_up_cybercity'
        | 'creatice_product_up_bluecircuit'
        | 'creatice_product_up'
        | 'run_fast'
        | 'background_explosion';
}

export interface ViduTemplateToVideoOutput {
    /**
     * Video
     * @description The generated video using a predefined template
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/web-examples/vidu/hugging.mp4"
     *     }
     */
    video: Components.File;
}

export interface ViduStartEndToVideoInput {
    /**
     * End Image Url
     * @description URL of the image to use as the last frame
     * @example https://storage.googleapis.com/falserverless/web-examples/vidu/2-carbody.png
     */
    end_image_url: string;
    /**
     * Movement Amplitude
     * @description The movement amplitude of objects in the frame
     * @default auto
     * @enum {string}
     */
    movement_amplitude?: 'auto' | 'small' | 'medium' | 'large';
    /**
     * Prompt
     * @description Text prompt for video generation, max 1500 characters
     * @example Transform the car frame into a complete vehicle.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
    /**
     * Start Image Url
     * @description URL of the image to use as the first frame
     * @example https://storage.googleapis.com/falserverless/web-examples/vidu/2-carchasis.png
     */
    start_image_url: string;
}

export interface ViduStartEndToVideoOutput {
    /**
     * Video
     * @description The generated transition video between start and end frames
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/web-examples/vidu/2-car.mp4"
     *     }
     */
    video: Components.File;
}

export interface ViduReferenceToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the output video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Movement Amplitude
     * @description The movement amplitude of objects in the frame
     * @default auto
     * @enum {string}
     */
    movement_amplitude?: 'auto' | 'small' | 'medium' | 'large';
    /**
     * Prompt
     * @description Text prompt for video generation, max 1500 characters
     * @example The little devil is looking at the apple on the beach and walking around it.
     */
    prompt: string;
    /**
     * Reference Image Urls
     * @description URLs of the reference images to use for consistent subject appearance
     * @example [
     *       "https://storage.googleapis.com/falserverless/web-examples/vidu/new-examples/reference1.png",
     *       "https://storage.googleapis.com/falserverless/web-examples/vidu/new-examples/reference2.png",
     *       "https://storage.googleapis.com/falserverless/web-examples/vidu/new-examples/reference3.png"
     *     ]
     */
    reference_image_urls: string[];
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
}

export interface ViduReferenceToVideoOutput {
    /**
     * Video
     * @description The generated video with consistent subjects from reference images
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/web-examples/vidu/new-examples/referencevideo.mp4"
     *     }
     */
    video: Components.File;
}

export interface ViduReferenceToImageInput extends SharedType_79c {}

export interface ViduReferenceToImageOutput extends SharedType_b81 {}

export interface ViduQ3TextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the output video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '4:3' | '3:4' | '1:1';
    /**
     * Audio
     * @description Whether to use direct audio-video generation. When true, outputs video with sound.
     * @default true
     */
    audio?: boolean;
    /**
     * Duration
     * @description Duration of the video in seconds
     * @default 5
     */
    duration?: number;
    /**
     * Prompt
     * @description Text prompt for video generation, max 2000 characters
     * @example In an ultra-realistic fashion photography style featuring light blue and pale amber tones, an astronaut in a spacesuit walks through the fog.
     */
    prompt: string;
    /**
     * Resolution
     * @description Output video resolution
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p' | '1080p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface ViduQ3TextToVideoOutput {
    /**
     * Video
     * @description The generated video from text using the Q3 model
     * @example {
     *       "url": "https://v3b.fal.media/files/b/0a8c915a/8dtkty-vyhotySs--cTKS_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface ViduQ3ImageToVideoInput {
    /**
     * Audio
     * @description Whether to use direct audio-video generation. When true, outputs video with sound.
     * @default true
     */
    audio?: boolean;
    /**
     * Duration
     * @description Duration of the video in seconds
     * @default 5
     */
    duration?: number;
    /**
     * Image Url
     * @description URL or base64 image to use as the starting frame
     * @example https://prod-ss-images.s3.cn-northwest-1.amazonaws.com.cn/vidu-maas/template/image2video.png
     */
    image_url: string;
    /**
     * Prompt
     * @description Text prompt for video generation, max 2000 characters
     * @example The astronaut waved and the camera moved up.
     */
    prompt: string;
    /**
     * Resolution
     * @description Output video resolution
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p' | '1080p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface ViduQ3ImageToVideoOutput {
    /**
     * Video
     * @description The generated video from image using the Q3 model
     * @example {
     *       "url": "https://v3b.fal.media/files/b/0a8c9189/n9z3uUDPqmU2msAtqr25-_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface ViduQ2VideoExtensionProInput {
    /**
     * Duration
     * @description Duration of the extension in seconds
     * @default 4
     * @enum {integer}
     */
    duration?: 2 | 3 | 4 | 5 | 6 | 7;
    /**
     * Prompt
     * @description text prompt to guide the video extension
     */
    prompt?: string;
    /**
     * Resolution
     * @description Output video resolution
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p' | '1080p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Video Url
     * @description URL of the video to extend
     * @example https://storage.googleapis.com/falserverless/model_tests/video_models/output-3.mp4
     */
    video_url: string;
}

export interface ViduQ2VideoExtensionProOutput {
    /**
     * Video
     * @description The extended video using the Q2 model
     * @example {
     *       "url": "https://v3b.fal.media/files/b/zebra/wSaP_lTetcD47ErjwsVGE_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface ViduQ2TextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the output video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Bgm
     * @description Whether to add background music to the video (only for 4-second videos)
     * @default false
     */
    bgm?: boolean;
    /**
     * Duration
     * @description Duration of the video in seconds
     * @default 4
     * @enum {integer}
     */
    duration?: 2 | 3 | 4 | 5 | 6 | 7 | 8;
    /**
     * Movement Amplitude
     * @description The movement amplitude of objects in the frame
     * @default auto
     * @enum {string}
     */
    movement_amplitude?: 'auto' | 'small' | 'medium' | 'large';
    /**
     * Prompt
     * @description Text prompt for video generation, max 3000 characters
     * @example A cinematic shot of a futuristic city at sunset, with flying cars and towering skyscrapers.
     */
    prompt: string;
    /**
     * Resolution
     * @description Output video resolution
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '520p' | '720p' | '1080p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface ViduQ2TextToVideoOutput {
    /**
     * Video
     * @description The generated video from text using the Q2 model
     * @example {
     *       "url": "https://fal.media/files/penguin/sUYfE1bo3z5Gds7pSuFHD_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface ViduQ2TextToImageInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the output video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Prompt
     * @description Text prompt for video generation, max 1500 characters
     * @example A majestic dragon perches on the mountaintop, its eyes fixed intently on a small baby dragon.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
}

export interface ViduQ2TextToImageOutput {
    /**
     * Image
     * @description The edited image
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/videos/general-1-2025-12-02T14_55_54Z.png"
     *     }
     */
    image: Components.Image;
}

export interface ViduQ2ReferenceToVideoProInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the output video (e.g., auto, 16:9, 9:16, 1:1, or any W:H)
     * @default 16:9
     */
    aspect_ratio?: string;
    /**
     * Bgm
     * @description Whether to add background music to the generated video
     * @default false
     */
    bgm?: boolean;
    /**
     * Duration
     * @description Duration of the video in seconds (0 for automatic duration)
     * @default 4
     */
    duration?: number;
    /**
     * Movement Amplitude
     * @description The movement amplitude of objects in the frame
     * @default auto
     * @enum {string}
     */
    movement_amplitude?: 'auto' | 'small' | 'medium' | 'large';
    /**
     * Prompt
     * @description Text prompt for video generation, max 2000 characters
     * @example @Figure 1 Character Reference@Refer to the special effects, movements, and camera work of Video 1.
     */
    prompt: string;
    /**
     * Reference Image Urls
     * @description URLs of the reference images for subject appearance. If videos are provided, up to 4 images are allowed; otherwise up to 7 images.
     * @example [
     *       "https://storage.googleapis.com/falserverless/model_tests/video_models/vidu-image-3123041388101890.png"
     *     ]
     */
    reference_image_urls?: string[];
    /**
     * Reference Video Urls
     * @description URLs of the reference videos for video editing or motion reference. Supports up to 2 videos.
     * @example [
     *       "https://storage.googleapis.com/falserverless/model_tests/video_models/vidu-video-3123002003131623.mp4"
     *     ]
     */
    reference_video_urls?: string[];
    /**
     * Resolution
     * @description Output video resolution
     * @default 720p
     * @enum {string}
     */
    resolution?: '540p' | '720p' | '1080p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface ViduQ2ReferenceToVideoProOutput {
    /**
     * Video
     * @description The generated video with video/image references using the Q2 Pro model
     * @example {
     *       "url": "https://v3b.fal.media/files/b/0a8aa38a/_nfU-t1qY1HEVcDqct-M__output.mp4"
     *     }
     */
    video: Components.File;
}

export interface ViduQ2ReferenceToImageInput extends SharedType_79c {}

export interface ViduQ2ReferenceToImageOutput extends SharedType_b81 {}

export interface ViduQ2ImageToVideoTurboInput extends SharedType_04b {}

export interface ViduQ2ImageToVideoTurboOutput extends SharedType_135 {}

export interface ViduQ2ImageToVideoProInput extends SharedType_04b {}

export interface ViduQ2ImageToVideoProOutput extends SharedType_135 {}

export interface ViduQ1TextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the output video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Movement Amplitude
     * @description The movement amplitude of objects in the frame
     * @default auto
     * @enum {string}
     */
    movement_amplitude?: 'auto' | 'small' | 'medium' | 'large';
    /**
     * Prompt
     * @description Text prompt for video generation, max 1500 characters
     * @example In an ultra-realistic fashion photography style featuring light blue and pale amber tones, an astronaut in a spacesuit walks through the fog. The background consists of enchanting white and golden lights, creating a minimalist still life and an impressive panoramic scene.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed for the random number generator
     */
    seed?: number;
    /**
     * Style
     * @description The style of output video
     * @default general
     * @enum {string}
     */
    style?: 'general' | 'anime';
}

export interface ViduQ1TextToVideoOutput {
    /**
     * Video
     * @description The generated video using the Q1 model
     * @example {
     *       "url": "https://fal.media/files/penguin/senyvDPQAk8Fvt5voX3NU_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface ViduQ1StartEndToVideoInput {
    /**
     * End Image Url
     * @description URL of the image to use as the last frame
     * @example https://v3.fal.media/files/kangaroo/CASBu_OmOnZ8IafirarFL_last_frame_q1.png
     */
    end_image_url: string;
    /**
     * Movement Amplitude
     * @description The movement amplitude of objects in the frame
     * @default auto
     * @enum {string}
     */
    movement_amplitude?: 'auto' | 'small' | 'medium' | 'large';
    /**
     * Prompt
     * @description Text prompt for video generation, max 1500 characters
     * @example Dragon lands on a rock
     */
    prompt: string;
    /**
     * Seed
     * @description Seed for the random number generator
     */
    seed?: number;
    /**
     * Start Image Url
     * @description URL of the image to use as the first frame
     * @example https://v3.fal.media/files/zebra/sgsdKvPigPhJ1S7Hl5bWc_first_frame_q1.png
     */
    start_image_url: string;
}

export interface ViduQ1StartEndToVideoOutput {
    /**
     * Video
     * @description The generated transition video between start and end frames using the Q1 model
     * @example {
     *       "url": "https://fal.media/files/zebra/-ACpJE6m4JQSBCCeNB51S_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface ViduQ1ReferenceToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the output video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Bgm
     * @description Whether to add background music to the generated video
     * @default false
     */
    bgm?: boolean;
    /**
     * Movement Amplitude
     * @description The movement amplitude of objects in the frame
     * @default auto
     * @enum {string}
     */
    movement_amplitude?: 'auto' | 'small' | 'medium' | 'large';
    /**
     * Prompt
     * @description Text prompt for video generation, max 1500 characters
     * @example A young woman and a monkey inside a colorful house
     */
    prompt: string;
    /**
     * Reference Image Urls
     * @description URLs of the reference images to use for consistent subject appearance. Q1 model supports up to 7 reference images.
     * @example [
     *       "https://v3.fal.media/files/panda/HDpZj0eLjWwCpjA5__0l1_0e6cd0b9eb7a4a968c0019a4eee15e46.png",
     *       "https://v3.fal.media/files/zebra/153izt1cBlMU-TwD0_B7Q_ea34618f5d974653a16a755aa61e488a.png",
     *       "https://v3.fal.media/files/koala/RCSZ7VEEKGFDfMoGHCwzo_f626718793e94769b1ad36d5891864a4.png"
     *     ]
     */
    reference_image_urls: string[];
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
}

export interface ViduQ1ReferenceToVideoOutput {
    /**
     * Video
     * @description The generated video with consistent subjects from reference images using the Q1 model
     * @example {
     *       "url": "https://fal.media/files/panda/4wmqVpGFsqzZrKROz9c1Z_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface ViduQ1ImageToVideoInput {
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://storage.googleapis.com/falserverless/model_tests/video_models/vidu_i2v.jpg
     */
    image_url: string;
    /**
     * Movement Amplitude
     * @description The movement amplitude of objects in the frame
     * @default auto
     * @enum {string}
     */
    movement_amplitude?: 'auto' | 'small' | 'medium' | 'large';
    /**
     * Prompt
     * @description Text prompt for video generation, max 1500 characters
     * @example The astronaut waved and the camera moved up.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed for the random number generator
     */
    seed?: number;
}

export interface ViduQ1ImageToVideoOutput {
    /**
     * Video
     * @description The generated video using the Q1 model from a single image
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/video_models/vidu_i2v_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface ViduImageToVideoInput {
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://storage.googleapis.com/falserverless/web-examples/vidu/stylish_woman.webp
     */
    image_url: string;
    /**
     * Movement Amplitude
     * @description The movement amplitude of objects in the frame
     * @default auto
     * @enum {string}
     */
    movement_amplitude?: 'auto' | 'small' | 'medium' | 'large';
    /**
     * Prompt
     * @description Text prompt for video generation, max 1500 characters
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
}

export interface ViduImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://fal.media/files/kangaroo/gzfzC5FXvcgZegQmy90L1_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface VideoUpscalerInput {
    /**
     * Scale
     * @description The scale factor
     * @default 2
     */
    scale?: number;
    /**
     * Video Url
     * @description The URL of the video to upscale
     * @example https://storage.googleapis.com/falserverless/videos/_o3VmzjOytBwRjCVPFX6i_output.mp4
     */
    video_url: string;
}

export interface VideoUpscalerOutput {
    /**
     * Video
     * @description The stitched video
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/videos/h0jgPaO6AJAbyrsNYNbGl_upscaled_video.mp4"
     *     }
     */
    video: Components.File;
}

export interface VideoUnderstandingInput {
    /**
     * Detailed Analysis
     * @description Whether to request a more detailed analysis of the video
     * @default false
     */
    detailed_analysis?: boolean;
    /**
     * Prompt
     * @description The question or prompt about the video content.
     * @example What is happening in this video?
     */
    prompt: string;
    /**
     * Video Url
     * @description URL of the video to analyze
     * @example https://v3.fal.media/files/elephant/mLAMkUTxFMbe2xF0qpLdA_Ll9mDE8webFA6GAu3vD_M_71ee7217db1d4aa4af1d2f1ae060389b.mp4
     */
    video_url: string;
}

export interface VideoUnderstandingOutput {
    /**
     * Output
     * @description The analysis of the video content based on the prompt
     * @example Based on the video, a woman is singing passionately into a microphone in what appears to be a professional recording studio. She is wearing headphones, and behind her, there are sound-dampening foam panels, a mixing board, and other studio equipment.
     */
    output: string;
}

export interface VideoPromptGeneratorInput {
    /**
     * Camera Direction
     * @description Camera direction
     * @default None
     * @enum {string}
     */
    camera_direction?:
        | 'None'
        | 'Zoom in'
        | 'Zoom out'
        | 'Pan left'
        | 'Pan right'
        | 'Tilt up'
        | 'Tilt down'
        | 'Orbital rotation'
        | 'Push in'
        | 'Pull out'
        | 'Track forward'
        | 'Track backward'
        | 'Spiral in'
        | 'Spiral out'
        | 'Arc movement'
        | 'Diagonal traverse'
        | 'Vertical rise'
        | 'Vertical descent';
    /**
     * Camera Style
     * @description Camera movement style
     * @default None
     * @enum {string}
     */
    camera_style?:
        | 'None'
        | 'Steadicam flow'
        | 'Drone aerials'
        | 'Handheld urgency'
        | 'Crane elegance'
        | 'Dolly precision'
        | 'VR 360'
        | 'Multi-angle rig'
        | 'Static tripod'
        | 'Gimbal smoothness'
        | 'Slider motion'
        | 'Jib sweep'
        | 'POV immersion'
        | 'Time-slice array'
        | 'Macro extreme'
        | 'Tilt-shift miniature'
        | 'Snorricam character'
        | 'Whip pan dynamics'
        | 'Dutch angle tension'
        | 'Underwater housing'
        | 'Periscope lens';
    /**
     * Custom Elements
     * @description Custom technical elements (optional)
     * @default
     */
    custom_elements?: string;
    /**
     * Image Url
     * @description URL of an image to analyze and incorporate into the video prompt (optional)
     */
    image_url?: string;
    /**
     * Input Concept
     * @description Core concept or thematic input for the video prompt
     * @example A futuristic city at dusk
     */
    input_concept: string;
    /**
     * Model
     * @description Model to use
     * @default google/gemini-2.0-flash-001
     * @enum {string}
     */
    model?:
        | 'anthropic/claude-3.5-sonnet'
        | 'anthropic/claude-3-5-haiku'
        | 'anthropic/claude-3-haiku'
        | 'google/gemini-2.5-flash-lite'
        | 'google/gemini-2.0-flash-001'
        | 'meta-llama/llama-3.2-1b-instruct'
        | 'meta-llama/llama-3.2-3b-instruct'
        | 'meta-llama/llama-3.1-8b-instruct'
        | 'meta-llama/llama-3.1-70b-instruct'
        | 'openai/gpt-4o-mini'
        | 'openai/gpt-4o'
        | 'deepseek/deepseek-r1';
    /**
     * Pacing
     * @description Pacing rhythm
     * @default None
     * @enum {string}
     */
    pacing?:
        | 'None'
        | 'Slow burn'
        | 'Rhythmic pulse'
        | 'Frantic energy'
        | 'Ebb and flow'
        | 'Hypnotic drift'
        | 'Time-lapse rush'
        | 'Stop-motion staccato'
        | 'Gradual build'
        | 'Quick cut rhythm'
        | 'Long take meditation'
        | 'Jump cut energy'
        | 'Match cut flow'
        | 'Cross-dissolve dreamscape'
        | 'Parallel action'
        | 'Slow motion impact'
        | 'Ramping dynamics'
        | 'Montage tempo'
        | 'Continuous flow'
        | 'Episodic breaks';
    /**
     * Prompt Length
     * @description Length of the prompt
     * @default Medium
     * @enum {string}
     */
    prompt_length?: 'Short' | 'Medium' | 'Long';
    /**
     * Special Effects
     * @description Special effects approach
     * @default None
     * @enum {string}
     */
    special_effects?:
        | 'None'
        | 'Practical effects'
        | 'CGI enhancement'
        | 'Analog glitches'
        | 'Light painting'
        | 'Projection mapping'
        | 'Nanosecond exposures'
        | 'Double exposure'
        | 'Smoke diffusion'
        | 'Lens flare artistry'
        | 'Particle systems'
        | 'Holographic overlay'
        | 'Chromatic aberration'
        | 'Digital distortion'
        | 'Wire removal'
        | 'Motion capture'
        | 'Miniature integration'
        | 'Weather simulation'
        | 'Color grading'
        | 'Mixed media composite'
        | 'Neural style transfer';
    /**
     * Style
     * @description Style of the video prompt
     * @default Simple
     * @enum {string}
     */
    style?:
        | 'Minimalist'
        | 'Simple'
        | 'Detailed'
        | 'Descriptive'
        | 'Dynamic'
        | 'Cinematic'
        | 'Documentary'
        | 'Animation'
        | 'Action'
        | 'Experimental';
}

export interface VideoPromptGeneratorOutput {
    /**
     * Prompt
     * @description Generated video prompt
     * @example A futuristic city glows softly at dusk, captured with smooth gimbal movements and a slow burn pacing, enhanced by subtle holographic overlays.
     */
    prompt: string;
}

export interface VideoAsPromptInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video.
     * @default 9:16
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames Per Second
     * @description Frames per second for the output video. Only applicable if output_type is 'video'.
     * @default 16
     */
    fps?: number;
    /**
     * Guidance Scale
     * @description Guidance scale for generation.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description Input image to generate the effect video for.
     * @example https://storage.googleapis.com/falserverless/bytedance-vid2pro/animal-2.jpg
     */
    image_url: string;
    /**
     * Num Frames
     * @description The number of frames to generate.
     * @default 49
     */
    num_frames?: number;
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A chestnut-colored horse stands on a grassy hill against a backdrop of distant, snow-dusted mountains. The horse begins to inflate, its defined, muscular body swelling and rounding into a smooth, balloon-like form while retaining its rich, brown hide color. Without changing its orientation, the now-buoyant horse lifts silently from the ground. It begins a steady vertical ascent, rising straight up and eventually floating out of the top of the frame. The camera remains completely static throughout the entire sequence, holding a fixed shot on the landscape as the horse transforms and departs, ensuring the verdant hill and mountain range in the background stay perfectly still.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video.
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducible generation. If set none, a random seed will be used.
     */
    seed?: number;
    /**
     * Video Description
     * @description A brief description of the input video content.
     * @example A hand holds up a single beige sneaker decorated with gold calligraphy and floral illustrations, with small green plants tucked inside. The sneaker immediately begins to inflate like a balloon, its shape distorting as the decorative details stretch and warp across the expanding surface. It rapidly transforms into a perfectly smooth, matte beige sphere, inheriting the primary color from the original shoe. Once the transformation is complete, the new balloon-like object quickly ascends, moving straight up and exiting the top of the frame. The camera remains completely static and the plain white background is unchanged throughout the entire sequence.
     */
    video_description: string;
    /**
     * Video Url
     * @description reference video to generate effect video from.
     * @example https://storage.googleapis.com/falserverless/bytedance-vid2pro/object-725.mp4
     */
    video_url: string;
}

export interface VideoAsPromptOutput {
    /** @description The URLs of the generated video. */
    video: Components.File_1;
}

export interface Vibevoice7bInput extends SharedType_c3a {}

export interface Vibevoice7bOutput extends SharedType_a95 {}

export interface Vibevoice05bInput {
    /**
     * CFG Scale
     * @description CFG (Classifier-Free Guidance) scale for generation. Higher values increase adherence to text.
     * @default 1.3
     */
    cfg_scale?: number;
    /**
     * Script
     * @description The script to convert to speech.
     * @example VibeVoice is now available on Fal!
     */
    script: string;
    /**
     * Seed
     * @description Random seed for reproducible generation.
     */
    seed?: number;
    /**
     * Speaker
     * @description Voice to use for speaking.
     * @example Frank
     * @enum {string}
     */
    speaker: 'Frank' | 'Wayne' | 'Carter' | 'Emma' | 'Grace' | 'Mike';
}

export interface Vibevoice05bOutput {
    /**
     * Audio
     * @description The generated audio file containing the speech
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/vibevoice/0_5b.mp3"
     *     }
     */
    audio: Components.File;
    /**
     * Duration
     * @description Duration of the generated audio in seconds
     * @example 9.46
     */
    duration: number;
    /**
     * Generation Time
     * @description Time taken to generate the audio in seconds
     * @example 5.6
     */
    generation_time: number;
    /**
     * Rtf
     * @description Real-time factor (generation_time / audio_duration). Lower is better.
     * @example 0.53
     */
    rtf: number;
    /**
     * Sample Rate
     * @description Sample rate of the generated audio
     * @example 24000
     */
    sample_rate: number;
}

export interface VibevoiceInput extends SharedType_c3a {}

export interface VibevoiceOutput extends SharedType_a95 {}

export interface Veo3ImageToVideoInput extends SharedType_2c4 {}

export interface Veo3ImageToVideoOutput extends SharedType_87d {}

export interface Veo3FastImageToVideoInput extends SharedType_2c4 {}

export interface Veo3FastImageToVideoOutput extends SharedType_87d {}

export interface Veo3FastInput extends SharedType_075 {}

export interface Veo3FastOutput extends SharedType_a3c {}

export interface Veo31ReferenceToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16';
    /**
     * Auto Fix
     * @description Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
     * @default false
     */
    auto_fix?: boolean;
    /**
     * Duration
     * @description The duration of the generated video.
     * @default 8s
     * @enum {string}
     */
    duration?: '8s';
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Image Urls
     * @description URLs of the reference images to use for consistent subject appearance
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_inputs/veo31-r2v-input-1.png",
     *       "https://storage.googleapis.com/falserverless/example_inputs/veo31-r2v-input-2.png",
     *       "https://storage.googleapis.com/falserverless/example_inputs/veo31-r2v-input-3.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Prompt
     * @description The text prompt describing the video you want to generate
     * @example A chimpanzee wearing overalls frolics in the grassy field, gently playing with the butterflies. In the background, a circus tent and carousel beckon.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video.
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p' | '1080p' | '4k';
}

export interface Veo31ReferenceToVideoOutput {
    /**
     * Video
     * @description The generated video.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/veo31-r2v-output.mp4"
     *     }
     */
    video: Components.File;
}

export interface Veo31ImageToVideoInput extends SharedType_8df {}

export interface Veo31ImageToVideoOutput extends SharedType_3cd {}

export interface Veo31FirstLastFrameToVideoInput extends SharedType_f5e {}

export interface Veo31FirstLastFrameToVideoOutput extends SharedType_417 {}

export interface Veo31FastImageToVideoInput extends SharedType_8df {}

export interface Veo31FastImageToVideoOutput extends SharedType_3cd {}

export interface Veo31FastFirstLastFrameToVideoInput extends SharedType_f5e {}

export interface Veo31FastFirstLastFrameToVideoOutput extends SharedType_417 {}

export interface Veo31FastExtendVideoInput extends SharedType_977 {}

export interface Veo31FastExtendVideoOutput extends SharedType_a2c {}

export interface Veo31FastInput extends SharedType_de9 {}

export interface Veo31FastOutput extends SharedType_8151 {}

export interface Veo31ExtendVideoInput extends SharedType_977 {}

export interface Veo31ExtendVideoOutput extends SharedType_a2c {}

export interface Veo31Input extends SharedType_de9 {}

export interface Veo31Output extends SharedType_8151 {}

export interface Veo3Input extends SharedType_075 {}

export interface Veo3Output extends SharedType_a3c {}

export interface Veo2ImageToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | 'auto_prefer_portrait' | '16:9' | '9:16';
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5s
     * @enum {string}
     */
    duration?: '5s' | '6s' | '7s' | '8s';
    /**
     * Image Url
     * @description URL of the input image to animate. Should be 720p or higher resolution.
     * @example https://fal.media/files/elephant/6fq8JDSjb1osE_c3J_F2H.png
     */
    image_url: string;
    /**
     * Prompt
     * @description The text prompt describing how the image should be animated
     * @example A lego chef cooking eggs
     */
    prompt: string;
}

export interface Veo2ImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://v3.fal.media/files/monkey/jOYy3rvGB33vumzulpXd5_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface Veo2Input {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16';
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5s
     * @enum {string}
     */
    duration?: '5s' | '6s' | '7s' | '8s';
    /**
     * Enhance Prompt
     * @description Whether to enhance the video generation
     * @default true
     */
    enhance_prompt?: boolean;
    /**
     * Negative Prompt
     * @description A negative prompt to guide the video generation
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The text prompt describing the video you want to generate
     * @example The camera floats gently through rows of pastel-painted wooden beehives, buzzing honeybees gliding in and out of frame. The motion settles on the refined farmer standing at the center, his pristine white beekeeping suit gleaming in the golden afternoon light. He lifts a jar of honey, tilting it slightly to catch the light. Behind him, tall sunflowers sway rhythmically in the breeze, their petals glowing in the warm sunlight. The camera tilts upward to reveal a retro farmhouse with mint-green shutters, its walls dappled with shadows from swaying trees. Shot with a 35mm lens on Kodak Portra 400 film, the golden light creates rich textures on the farmer's gloves, marmalade jar, and weathered wood of the beehives.
     */
    prompt: string;
    /**
     * Seed
     * @description A seed to use for the video generation
     */
    seed?: number;
}

export interface Veo2Output {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://v3.fal.media/files/tiger/83-YzufmOlsnhqq5ed382_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface UsoInput {
    /**
     * Safety Checker
     * @description Enable NSFW content detection and filtering.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale (CFG)
     * @description How closely to follow the prompt. Higher values stick closer to the prompt.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Reference Images
     * @description List of image URLs in order: [content_image, style_image, extra_style_image].
     * @example [
     *       "https://storage.googleapis.com/falserverless/USO/style3.webp",
     *       "https://storage.googleapis.com/falserverless/USO/style4.webp"
     *     ]
     */
    input_image_urls: string[];
    /**
     * Keep Input Size
     * @description Preserve the layout and dimensions of the input content image. Useful for style transfer.
     * @default false
     */
    keep_size?: boolean;
    /**
     * Negative Prompt
     * @description What you don't want in the image. Use it to exclude unwanted elements, styles, or artifacts.
     * @default
     * @example blurry, low quality, distorted, ugly, bad anatomy
     * @example cartoon, anime, illustration
     * @example
     */
    negative_prompt?: string;
    /**
     * Number of Images
     * @description Number of images to generate in parallel.
     * @default 1
     */
    num_images?: number;
    /**
     * Inference Steps
     * @description Number of denoising steps. More steps can improve quality but increase generation time.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description Output image format. PNG preserves transparency, JPEG is smaller.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description Text prompt for generation. Can be empty for pure style transfer.
     * @default
     * @example A handsome man.
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducible generation. Use same seed for consistent results.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If true, wait for generation and upload before returning. Increases latency but provides immediate access to images.
     * @default false
     */
    sync_mode?: boolean;
}

export interface UsoOutput {
    /**
     * Has Nsfw Concepts
     * @description NSFW detection results for each generated image
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images with applied style and/or subject customization
     * @example [
     *       {
     *         "height": 1024,
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/USO/G6n97WN0goYpXPeiHaBnP.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Prompt
     * @description The prompt used for generation
     */
    prompt: string;
    /**
     * Seed
     * @description Seed used for generation
     */
    seed: number;
    /**
     * Timings
     * @description Performance timings for different stages
     */
    timings: Record<string, number>;
}

export interface UnoInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. You can choose between some presets or custom height and width
     *                 that **must be multiples of 8**.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Input Image Urls
     * @description URL of images to use while generating the image.
     * @example [
     *       "https://storage.googleapis.com/falserverless/UNO/figurine.png",
     *       "https://storage.googleapis.com/falserverless/UNO/crystal_ball.png"
     *     ]
     */
    input_image_urls: string[];
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example The figurine is in the crystal ball
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducible generation. If set none, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface UnoOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The URLs of the generated images.
     * @example [
     *       "https://storage.googleapis.com/falserverless/UNO/output.jpeg"
     *     ]
     */
    images: Components.Image[];
    /**
     * Prompt
     * @description The prompt used to generate the image.
     * @example The figurine is in the crystal ball
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface UltrashapeInput {
    /**
     * Image Url
     * @description URL of the reference image for mesh refinement.
     * @example https://v3b.fal.media/files/b/0a892d08/IYQiPpbR90p8O0otIT2eQ_1.png
     */
    image_url: string;
    /**
     * Model Url
     * @description URL of the coarse mesh (.glb or .obj) to refine.
     * @example https://v3b.fal.media/files/b/0a892d06/llURuqrI2TTjijVo-lEWn_1.glb
     */
    model_url: string;
    /**
     * Num Inference Steps
     * @description Diffusion steps.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Octree Resolution
     * @description Marching cubes resolution.
     * @default 1024
     */
    octree_resolution?: number;
    /**
     * Remove Background
     * @description Remove image background.
     * @default true
     */
    remove_background?: boolean;
    /**
     * Seed
     * @description Random seed.
     * @default 42
     */
    seed?: number;
}

export interface UltrashapeOutput {
    /**
     * Model Glb
     * @description Generated 3D object.
     * @example {
     *       "file_size": 117537192,
     *       "file_name": "refined.glb",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3b.fal.media/files/b/0a892d36/4vzmYKL1OZVgcPCdRf2dc_refined.glb"
     *     }
     */
    model_glb: Components.File;
}

export interface TurboFluxTrainerInput {
    /**
     * Face Crop
     * @description Whether to try to detect the face and crop the images to the face.
     * @default true
     */
    face_crop?: boolean;
    /**
     * Images Data Url
     * @description URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
     */
    images_data_url: string;
    /**
     * Learning Rate
     * @description Learning rate for the training.
     * @default 0.00115
     */
    learning_rate?: number;
    /**
     * Steps
     * @description Number of steps to train the LoRA on.
     * @default 1000
     * @example 1000
     */
    steps?: number;
    /**
     * Training Style
     * @description Training style to use.
     * @default subject
     * @enum {string}
     */
    training_style?: 'subject' | 'style';
    /**
     * Trigger Phrase
     * @description Trigger phrase to be used in the captions. If None, a trigger word will not be used.
     *             If no captions are provide the trigger_work will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions.
     * @default ohwx
     */
    trigger_phrase?: string;
}

export interface TurboFluxTrainerOutput {
    /**
     * Config File
     * @description URL to the trained diffusers config file.
     */
    config_file: Components.File;
    /**
     * Diffusers Lora File
     * @description URL to the trained diffusers lora weights.
     */
    diffusers_lora_file: Components.File;
}

export interface TriposrInput {
    /**
     * Do Remove Background
     * @description Whether to remove the background from the input image.
     * @default true
     */
    do_remove_background?: boolean;
    /**
     * Foreground Ratio
     * @description Ratio of the foreground image to the original image.
     * @default 0.9
     */
    foreground_ratio?: number;
    /**
     * Image Url
     * @description Path for the image file to be processed.
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/hamburger.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/poly_fox.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/robot.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/teapot.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/tiger_girl.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/horse.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/flamingo.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/unicorn.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/chair.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/iso_house.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/marble.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/police_woman.png
     * @example https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/ea034e12a428fa848684a3f9f267b2042d298ca6/examples/captured_p.png
     */
    image_url: string;
    /**
     * Mc Resolution
     * @description Resolution of the marching cubes. Above 512 is not recommended.
     * @default 256
     */
    mc_resolution?: number;
    /**
     * Output Format
     * @description Output format for the 3D model.
     * @default glb
     * @enum {string}
     */
    output_format?: 'glb' | 'obj';
}

export interface TriposrOutput {
    /**
     * Model Mesh
     * @description Generated 3D object file.
     */
    model_mesh: Components.File;
    /**
     * Remeshing Dir
     * @description Directory containing textures for the remeshed model.
     */
    remeshing_dir?: Components.File;
    /**
     * Timings
     * @description Inference timings.
     */
    timings: {
        [key: string]: number;
    };
}

export interface TrellisMultiInput {
    /**
     * Image Urls
     * @description List of URLs of input images to convert to 3D
     * @example [
     *       "https://storage.googleapis.com/falserverless/model_tests/video_models/front.png",
     *       "https://storage.googleapis.com/falserverless/model_tests/video_models/back.png",
     *       "https://storage.googleapis.com/falserverless/model_tests/video_models/left.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Mesh Simplify
     * @description Mesh simplification factor
     * @default 0.95
     */
    mesh_simplify?: number;
    /**
     * Multiimage Algo
     * @description Algorithm for multi-image generation
     * @default stochastic
     * @enum {string}
     */
    multiimage_algo?: 'stochastic' | 'multidiffusion';
    /**
     * Seed
     * @description Random seed for reproducibility
     */
    seed?: number;
    /**
     * Slat Guidance Strength
     * @description Guidance strength for structured latent generation
     * @default 3
     */
    slat_guidance_strength?: number;
    /**
     * Slat Sampling Steps
     * @description Sampling steps for structured latent generation
     * @default 12
     */
    slat_sampling_steps?: number;
    /**
     * Ss Guidance Strength
     * @description Guidance strength for sparse structure generation
     * @default 7.5
     */
    ss_guidance_strength?: number;
    /**
     * Ss Sampling Steps
     * @description Sampling steps for sparse structure generation
     * @default 12
     */
    ss_sampling_steps?: number;
    /**
     * Texture Size
     * @description Texture resolution
     * @default 1024
     * @enum {integer}
     */
    texture_size?: 512 | 1024 | 2048;
}

export interface TrellisMultiOutput extends SharedType_0f9 {}

export interface Trellis2Input {
    /**
     * Decimation Target
     * @description Target vertex count for mesh simplification during export
     * @default 500000
     */
    decimation_target?: number;
    /**
     * Image Url
     * @description URL of the input image to convert to 3D
     * @example https://v3b.fal.media/files/b/0a86b60d/xkpao5B0uxmH0tmJm0HVL_2fe35ce1-fe44-475b-b582-6846a149537c.png
     */
    image_url: string;
    /**
     * Remesh
     * @description Run remeshing (slower; often improves topology)
     * @default true
     */
    remesh?: boolean;
    /**
     * Remesh Band
     * @default 1
     */
    remesh_band?: number;
    /**
     * Remesh Project
     * @default 0
     */
    remesh_project?: number;
    /**
     * Resolution
     * @description Output resolution; higher is slower but more detailed
     * @default 1024
     * @enum {integer}
     */
    resolution?: 512 | 1024 | 1536;
    /**
     * Seed
     * @description Random seed for reproducibility
     */
    seed?: number;
    /**
     * Shape Slat Guidance Rescale
     * @default 0.5
     */
    shape_slat_guidance_rescale?: number;
    /**
     * Shape Slat Guidance Strength
     * @default 7.5
     */
    shape_slat_guidance_strength?: number;
    /**
     * Shape Slat Rescale T
     * @default 3
     */
    shape_slat_rescale_t?: number;
    /**
     * Shape Slat Sampling Steps
     * @default 12
     */
    shape_slat_sampling_steps?: number;
    /**
     * Ss Guidance Rescale
     * @default 0.7
     */
    ss_guidance_rescale?: number;
    /**
     * Ss Guidance Strength
     * @default 7.5
     */
    ss_guidance_strength?: number;
    /**
     * Ss Rescale T
     * @default 5
     */
    ss_rescale_t?: number;
    /**
     * Ss Sampling Steps
     * @default 12
     */
    ss_sampling_steps?: number;
    /**
     * Tex Slat Guidance Rescale
     * @default 0
     */
    tex_slat_guidance_rescale?: number;
    /**
     * Tex Slat Guidance Strength
     * @default 1
     */
    tex_slat_guidance_strength?: number;
    /**
     * Tex Slat Rescale T
     * @default 3
     */
    tex_slat_rescale_t?: number;
    /**
     * Tex Slat Sampling Steps
     * @default 12
     */
    tex_slat_sampling_steps?: number;
    /**
     * Texture Size
     * @description Texture resolution
     * @default 2048
     * @enum {integer}
     */
    texture_size?: 1024 | 2048 | 4096;
}

export interface Trellis2Output {
    /**
     * Model Glb
     * @description Generated 3D GLB file
     * @example {
     *       "url": "https://v3b.fal.media/files/b/0a86b61d/DNmTkiWHUQ8k-rG6aussB_trellis2_68d6300f70f34d23b69a912b5fe60487.glb"
     *     }
     */
    model_glb: Components.File;
}

export interface TrellisInput {
    /**
     * Image Url
     * @description URL of the input image to convert to 3D
     * @example https://storage.googleapis.com/falserverless/web-examples/rodin3d/warriorwoman.png
     */
    image_url: string;
    /**
     * Mesh Simplify
     * @description Mesh simplification factor
     * @default 0.95
     */
    mesh_simplify?: number;
    /**
     * Seed
     * @description Random seed for reproducibility
     */
    seed?: number;
    /**
     * Slat Guidance Strength
     * @description Guidance strength for structured latent generation
     * @default 3
     */
    slat_guidance_strength?: number;
    /**
     * Slat Sampling Steps
     * @description Sampling steps for structured latent generation
     * @default 12
     */
    slat_sampling_steps?: number;
    /**
     * Ss Guidance Strength
     * @description Guidance strength for sparse structure generation
     * @default 7.5
     */
    ss_guidance_strength?: number;
    /**
     * Ss Sampling Steps
     * @description Sampling steps for sparse structure generation
     * @default 12
     */
    ss_sampling_steps?: number;
    /**
     * Texture Size
     * @description Texture resolution
     * @default 1024
     * @enum {integer}
     */
    texture_size?: 512 | 1024 | 2048;
}

export interface TrellisOutput extends SharedType_0f9 {}

export interface TranspixarInput {
    /**
     * Export Fps
     * @description The target FPS of the video
     * @default 8
     */
    export_fps?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related video to show you.
     * @default 7
     */
    guidance_scale?: number;
    /**
     * Negative Prompt
     * @description The negative prompt to generate video from
     * @default
     * @example Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 24
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A cloud of dust erupting and dispersing like an explosion.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
}

export interface TranspixarOutput {
    /**
     * Prompt
     * @description The prompt used for generating the video.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated video. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
    /**
     * Videos
     * @description The URL to the generated video
     * @example [
     *       {
     *         "file_size": 146468,
     *         "file_name": "rgb.mp4",
     *         "content_type": "application/octet-stream",
     *         "url": "https://v3.fal.media/files/kangaroo/G6gkFsuyU5L7sJ55nZUPU_rgb.mp4"
     *       },
     *       {
     *         "file_size": 106894,
     *         "file_name": "alpha.mp4",
     *         "content_type": "application/octet-stream",
     *         "url": "https://v3.fal.media/files/lion/g7PBZfQEH9SoPXYgeyl5P_alpha.mp4"
     *       }
     *     ]
     */
    videos: Components.File_1[];
}

export interface TopazUpscaleVideoInput {
    /**
     * H264 Output
     * @description Whether to use H264 codec for output video. Default is H265.
     * @default false
     */
    H264_output?: boolean;
    /**
     * Target Fps
     * @description Target FPS for frame interpolation. If set, frame interpolation will be enabled.
     */
    target_fps?: number;
    /**
     * Upscale Factor
     * @description Factor to upscale the video by (e.g. 2.0 doubles width and height)
     * @default 2
     */
    upscale_factor?: number;
    /**
     * Video Url
     * @description URL of the video to upscale
     * @example https://v3.fal.media/files/kangaroo/y5-1YTGpun17eSeggZMzX_video-1733468228.mp4
     */
    video_url: string;
}

export interface TopazUpscaleVideoOutput {
    /**
     * Video
     * @description The upscaled video file
     * @example {
     *       "url": "https://v3.fal.media/files/penguin/ztj_LB4gQlW6HIfVs8zX4_upscaled.mp4"
     *     }
     */
    video: Components.File;
}

export interface TopazUpscaleImageInput {
    /**
     * Crop To Fill
     * @default false
     */
    crop_to_fill?: boolean;
    /**
     * Face Enhancement
     * @description Whether to apply face enhancement to the image.
     * @default true
     */
    face_enhancement?: boolean;
    /**
     * Face Enhancement Creativity
     * @description Creativity level for face enhancement. 0.0 means no creativity, 1.0 means maximum creativity. Ignored if face ehnancement is disabled.
     * @default 0
     */
    face_enhancement_creativity?: number;
    /**
     * Face Enhancement Strength
     * @description Strength of the face enhancement. 0.0 means no enhancement, 1.0 means maximum enhancement. Ignored if face ehnancement is disabled.
     * @default 0.8
     */
    face_enhancement_strength?: number;
    /**
     * Image Url
     * @description Url of the image to be upscaled
     * @example https://storage.googleapis.com/falserverless/model_tests/codeformer/codeformer_poor_1.jpeg
     */
    image_url: string;
    /**
     * Model
     * @description Model to use for image enhancement.
     * @default Standard V2
     * @enum {string}
     */
    model?:
        | 'Low Resolution V2'
        | 'Standard V2'
        | 'CGI'
        | 'High Fidelity V2'
        | 'Text Refine'
        | 'Recovery'
        | 'Redefine'
        | 'Recovery V2';
    /**
     * Output Format
     * @description Output format of the upscaled image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Subject Detection
     * @description Subject detection mode for the image enhancement.
     * @default All
     * @enum {string}
     */
    subject_detection?: 'All' | 'Foreground' | 'Background';
    /**
     * Upscale Factor
     * @description Factor to upscale the video by (e.g. 2.0 doubles width and height)
     * @default 2
     */
    upscale_factor?: number;
}

export interface TopazUpscaleImageOutput extends SharedType_df4 {}

export interface ThinksoundAudioInput extends SharedType_4bc {}

export interface ThinksoundAudioOutput {
    /**
     * Audio
     * @description The generated audio file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/thinksound-audio.wav"
     *     }
     */
    audio: Components.File;
    /**
     * Prompt
     * @description The prompt used to generate the audio.
     * @example An acoustic guitar being played indoors.
     */
    prompt: string;
}

export interface ThinksoundInput extends SharedType_4bc {}

export interface ThinksoundOutput {
    /**
     * Prompt
     * @description The prompt used to generate the audio.
     * @example An acoustic guitar being played indoors.
     */
    prompt: string;
    /**
     * Video
     * @description The generated video with audio.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/thinksound-output.mp4"
     *     }
     */
    video: Components.File;
}

export interface TheraInput {
    /**
     * Backbone
     * @description Backbone to use for upscaling
     * @example edsr
     * @enum {string}
     */
    backbone: 'edsr' | 'rdn';
    /**
     * Image Url
     * @description URL of image to be used for upscaling
     * @example https://storage.googleapis.com/falserverless/docres_ckpt/NoN6cImXI9DCeEYzX7-a7_1224f6da06354948ab477fa450e8c4f6.png
     */
    image_url: string;
    /**
     * Seed
     * @description Random seed for reproducible generation.
     */
    seed?: number;
    /**
     * Upscale Factor
     * @description The upscaling factor for the image.
     * @default 2
     */
    upscale_factor?: number;
}

export interface TheraOutput extends SharedType_0c0 {}

export interface T2vTurboInput {
    /**
     * Export Fps
     * @description The FPS of the exported video
     * @default 8
     */
    export_fps?: number;
    /**
     * Guidance Scale
     * @description The guidance scale
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Num Frames
     * @description The number of frames to generate
     * @default 16
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description The number of steps to sample
     * @default 4
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate images from
     * @example a dog wearing vr goggles on a boat
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the random number generator
     */
    seed?: number;
}

export interface T2vTurboOutput {
    /** @description The URL to the generated video */
    video: Components.File_1;
}

export interface SyncLipsyncV2ProInput {
    /**
     * Audio Url
     * @description URL of the input audio
     * @example https://storage.googleapis.com/falserverless/example_inputs/sync_v2_pro_audio_input.mp3
     */
    audio_url: string;
    /**
     * Sync Mode
     * @description Lipsync mode when audio and video durations are out of sync.
     * @default cut_off
     * @enum {string}
     */
    sync_mode?: 'cut_off' | 'loop' | 'bounce' | 'silence' | 'remap';
    /**
     * Video Url
     * @description URL of the input video
     * @example https://storage.googleapis.com/falserverless/example_inputs/sync_v2_pro_video_input.mp4
     */
    video_url: string;
}

export interface SyncLipsyncV2ProOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/sync_v2_pro_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SyncLipsyncV2Input {
    /**
     * Audio Url
     * @description URL of the input audio
     * @example https://fal.media/files/lion/vyFWygmZsIZlUO4s0nr2n.wav
     */
    audio_url: string;
    /**
     * Model
     * @description The model to use for lipsyncing. `lipsync-2-pro` will cost roughly 1.67 times as much as `lipsync-2` for the same duration.
     * @default lipsync-2
     * @enum {string}
     */
    model?: 'lipsync-2' | 'lipsync-2-pro';
    /**
     * Sync Mode
     * @description Lipsync mode when audio and video durations are out of sync.
     * @default cut_off
     * @enum {string}
     */
    sync_mode?: 'cut_off' | 'loop' | 'bounce' | 'silence' | 'remap';
    /**
     * Video Url
     * @description URL of the input video
     * @example https://v3.fal.media/files/tiger/IugLCDJRIoGqvqTa-EJTr_3wg74vCqyNuQ-IiBd77MM_output.mp4
     */
    video_url: string;
}

export interface SyncLipsyncV2Output {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://v3.fal.media/files/kangaroo/WIhlgDEJbccwGwAsvL3vz_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SyncLipsyncReact1Input {
    /**
     * Audio Url
     * @description URL to the input audio. Must be **15 seconds or shorter**.
     * @example https://storage.googleapis.com/falserverless/example_inputs/react_1/input.mp3
     */
    audio_url: string;
    /**
     * Emotion
     * @description Emotion prompt for the generation. Currently supports single-word emotions only.
     * @example neutral
     * @enum {string}
     */
    emotion: 'happy' | 'angry' | 'sad' | 'neutral' | 'disgusted' | 'surprised';
    /**
     * Lipsync Mode
     * @description Lipsync mode when audio and video durations are out of sync.
     * @default bounce
     * @enum {string}
     */
    lipsync_mode?: 'cut_off' | 'loop' | 'bounce' | 'silence' | 'remap';
    /**
     * Model Mode
     * @description Controls the edit region and movement scope for the model. Available options:
     *     - `lips`: Only lipsync using react-1 (minimal facial changes).
     *     - `face`: Lipsync + facial expressions without head movements.
     *     - `head`: Lipsync + facial expressions + natural talking head movements.
     * @default face
     * @enum {string}
     */
    model_mode?: 'lips' | 'face' | 'head';
    /**
     * Temperature
     * @description Controls the expresiveness of the lipsync.
     * @default 0.5
     */
    temperature?: number;
    /**
     * Video Url
     * @description URL to the input video. Must be **15 seconds or shorter**.
     * @example https://storage.googleapis.com/falserverless/example_inputs/react_1/input.mp4
     */
    video_url: string;
}

export interface SyncLipsyncReact1Output {
    /**
     * Video
     * @description The generated video with synchronized lip and facial movements.
     * @example {
     *       "height": 1088,
     *       "duration": 7.041667,
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/react_1/output.mp4",
     *       "width": 1920,
     *       "fps": 24,
     *       "file_name": "output.mp4",
     *       "num_frames": 169,
     *       "content_type": "video/mp4"
     *     }
     */
    video: Components.VideoFile_1;
}

export interface SyncLipsyncInput {
    /**
     * Audio Url
     * @description URL of the input audio
     * @example https://fal.media/files/lion/vyFWygmZsIZlUO4s0nr2n.wav
     */
    audio_url: string;
    /**
     * Model
     * @description The model to use for lipsyncing
     * @default lipsync-1.9.0-beta
     * @enum {string}
     */
    model?: 'lipsync-1.8.0' | 'lipsync-1.7.1' | 'lipsync-1.9.0-beta';
    /**
     * Sync Mode
     * @description Lipsync mode when audio and video durations are out of sync.
     * @default cut_off
     * @enum {string}
     */
    sync_mode?: 'cut_off' | 'loop' | 'bounce' | 'silence' | 'remap';
    /**
     * Video Url
     * @description URL of the input video
     * @example https://fal.media/files/koala/8teUPbRRMtAUTORDvqy0l.mp4
     */
    video_url: string;
}

export interface SyncLipsyncOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://v3.fal.media/files/rabbit/6gJV-z7RJsF0AxkZHkdgJ_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface Switti512Input extends SharedType_21d {}

export interface Switti512Output extends SharedType_57a {}

export interface SwittiInput extends SharedType_21d {}

export interface SwittiOutput extends SharedType_57a {}

export interface Swin2srInput {
    /**
     * Image Url
     * @description URL of image to be used for image enhancement
     * @example https://storage.googleapis.com/falserverless/gallery/seoul.jpg
     */
    image_url: string;
    /**
     * Seed
     * @description seed to be used for generation
     */
    seed?: number;
    /**
     * Task
     * @description Task to perform
     * @default classical_sr
     * @enum {string}
     */
    task?: 'classical_sr' | 'compressed_sr' | 'real_sr';
}

export interface Swin2srOutput extends SharedType_744 {}

export interface StepxEdit2Input {
    /**
     * Enable Reflection Mode
     * @description Enable reflection mode. Reviews outputs, corrects unintended changes, and determines when editing is complete.
     * @default true
     */
    enable_reflection_mode?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Enable Thinking Mode
     * @description Enable thinking mode. Uses multimodal language model knowledge to interpret abstract editing instructions.
     * @default true
     */
    enable_thinking_mode?: boolean;
    /**
     * True CFG scale
     * @description The true CFG scale. Controls how closely the model follows the prompt.
     * @default 6
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The image URL to generate an image from. Needs to match the dimensions of the mask.
     * @example https://storage.googleapis.com/falserverless/example_inputs/girl_2.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform. Recommended: 50.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example make head band red
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface StepxEdit2Output {
    /**
     * Best Info
     * @description Reflection analysis (only available when reflection mode is enabled).
     */
    best_info?: Record<string, never>[];
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "height": 1024,
     *         "content_type": "image/jpeg",
     *         "url": "https://v3.fal.media/files/kangaroo/kFPr5gC_Rr9JZbTTakEMd.jpeg",
     *         "width": 672
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Reformat Prompt
     * @description The model's interpretation of your instruction (only available when thinking mode is enabled).
     */
    reformat_prompt?: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /**
     * Think Info
     * @description Reasoning process details (only available when thinking mode is enabled).
     */
    think_info?: string[];
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Step1xEditInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The image URL to generate an image from. Needs to match the dimensions of the mask.
     * @example https://storage.googleapis.com/falserverless/example_inputs/girl_2.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example make head band red
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Step1xEditOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "height": 1024,
     *         "content_type": "image/jpeg",
     *         "url": "https://v3.fal.media/files/kangaroo/kFPr5gC_Rr9JZbTTakEMd.jpeg",
     *         "width": 672
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface SteadyDancerInput {
    /**
     * Acceleration
     * @description Acceleration levels.
     * @default aggressive
     * @enum {string}
     */
    acceleration?: 'light' | 'moderate' | 'aggressive';
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video. If 'auto', will be determined from the reference image.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames Per Second
     * @description Frames per second of the generated video. Must be between 5 to 24. If not specified, uses the FPS from the input video.
     */
    frames_per_second?: number;
    /**
     * Guidance Scale
     * @description Classifier-free guidance scale for prompt adherence.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description URL of the reference image to animate. This is the person/character whose appearance will be preserved.
     * @default https://v3b.fal.media/files/b/0a85edaa/GDUCMPrdvOMcI5JpEcU7f.png
     */
    image_url?: string;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default blurred, distorted face, bad anatomy, extra limbs, poorly drawn hands, poorly drawn feet, disfigured, out of frame, duplicate, watermark, signature, text
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description Number of frames to generate. If not specified, uses the frame count from the input video (capped at 241). Will be adjusted to nearest valid value (must satisfy 4k+1 pattern).
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 6
     */
    num_inference_steps?: number;
    /**
     * Pose Guidance End
     * @description End ratio for pose guidance. Controls when pose guidance ends.
     * @default 0.4
     */
    pose_guidance_end?: number;
    /**
     * Pose Guidance Scale
     * @description Pose guidance scale for pose control strength.
     * @default 1
     */
    pose_guidance_scale?: number;
    /**
     * Pose Guidance Start
     * @description Start ratio for pose guidance. Controls when pose guidance begins.
     * @default 0.1
     */
    pose_guidance_start?: number;
    /**
     * Preserve Audio
     * @description If enabled, copies audio from the input driving video to the output video.
     * @default true
     */
    preserve_audio?: boolean;
    /**
     * Prompt
     * @description Text prompt describing the desired animation.
     * @default A person dancing with smooth and natural movements.
     */
    prompt?: string;
    /**
     * Resolution
     * @description Resolution of the generated video. 576p is default, 720p for higher quality. 480p is lower quality.
     * @default 576p
     * @enum {string}
     */
    resolution?: '480p' | '576p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift parameter for video generation.
     * @default 5
     */
    shift?: number;
    /**
     * Use Turbo
     * @description If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized (num_inference_steps=6, guidance_scale=1.0) and uses the LightX2V distillation LoRA.
     * @default false
     * @example true
     */
    use_turbo?: boolean;
    /**
     * Video Url
     * @description URL of the driving pose video. The motion from this video will be transferred to the reference image.
     * @default https://v3b.fal.media/files/b/0a84de68/jXDWywjhagRfR-GuZjoRs_video.mp4
     */
    video_url?: string;
}

export interface SteadyDancerOutput {
    /**
     * Num Frames
     * @description The actual number of frames generated (aligned to 4k+1 pattern).
     */
    num_frames: number;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated dance animation video.
     * @example {
     *       "file_size": 7772111,
     *       "file_name": "ll5ps0ZyBgxBkuWz-fHcT_output_with_audio.mp4",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3b.fal.media/files/b/0a87871b/ll5ps0ZyBgxBkuWz-fHcT_output_with_audio.mp4"
     *     }
     */
    video: Components.File;
}

export interface StarVectorInput {
    /**
     * Image Url
     * @description URL of image to be used for relighting
     * @example https://storage.googleapis.com/falserverless/star-vector/sample-18.png
     */
    image_url: string;
    /**
     * Seed
     * @description seed to be used for generation
     */
    seed?: number;
}

export interface StarVectorOutput {
    /** @description The generated image file info. */
    image: Components.File_1;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
}

export interface StableVideoInput {
    /**
     * Cond Aug
     * @description The conditoning augmentation determines the amount of noise that will be
     *                 added to the conditioning frame. The higher the number, the more noise
     *                 there will be, and the less the video will look like the initial image.
     *                 Increase it for more motion.
     * @default 0.02
     */
    cond_aug?: number;
    /**
     * Fps
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Image Url
     * @description The URL of the image to use as a starting point for the generation.
     * @example https://storage.googleapis.com/falserverless/model_tests/svd/rocket.png
     * @example https://storage.googleapis.com/falserverless/model_tests/svd/mustang.png
     * @example https://storage.googleapis.com/falserverless/model_tests/svd/ship.png
     * @example https://storage.googleapis.com/falserverless/model_tests/svd/rocket2.png
     */
    image_url: string;
    /**
     * Motion Bucket Id
     * @description The motion bucket id determines the motion of the generated video. The
     *                 higher the number, the more motion there will be.
     * @default 127
     */
    motion_bucket_id?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
}

export interface StableVideoOutput {
    /**
     * Seed
     * @description Seed for random number generator
     */
    seed: number;
    /**
     * Video
     * @description Generated video
     */
    video: Components.File;
}

export interface StableDiffusionV35MediumInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A dreamlike Japanese garden in perpetual twilight, bathed in bioluminescent cherry blossoms that emit a soft pink-purple glow. Floating paper lanterns drift lazily through the scene, their warm light creating dancing reflections in a mirror-like koi pond. Ethereal mist weaves between ancient stone pathways lined with glowing mushrooms in pastel blues and purples. A traditional wooden bridge arches gracefully over the water, dusted with fallen petals that sparkle like stardust. The scene is captured through a cinematic lens with perfect bokeh, creating an otherworldly atmosphere. In the background, a crescent moon hangs impossibly large in the sky, surrounded by a sea of stars and auroral wisps in teal and violet. Crystal formations emerge from the ground, refracting the ambient light into rainbow prisms. The entire composition follows the golden ratio, with moody film-like color grading reminiscent of Studio Ghibli, enhanced by volumetric god rays filtering through the luminous foliage. 8K resolution, masterful photography, hyperdetailed, magical realism.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface StableDiffusionV35MediumOutput extends SharedType_a73 {}

export interface StableDiffusionV35LargeInput {
    /**
     * Controlnet
     * @description ControlNet for inference.
     */
    controlnet?: Components.ControlNet_2;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. Defaults to landscape_4_3 if no controlnet has been passed, otherwise defaults to the size of the controlnet conditioning image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Ip Adapter
     * @description IP-Adapter to use during inference.
     */
    ip_adapter?: Components.IPAdapter;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A dreamlike Japanese garden in perpetual twilight, bathed in bioluminescent cherry blossoms that emit a soft pink-purple glow. Floating paper lanterns drift lazily through the scene, their warm light creating dancing reflections in a mirror-like koi pond. Ethereal mist weaves between ancient stone pathways lined with glowing mushrooms in pastel blues and purples. A traditional wooden bridge arches gracefully over the water, dusted with fallen petals that sparkle like stardust. The scene is captured through a cinematic lens with perfect bokeh, creating an otherworldly atmosphere. In the background, a crescent moon hangs impossibly large in the sky, surrounded by a sea of stars and auroral wisps in teal and violet. Crystal formations emerge from the ground, refracting the ambient light into rainbow prisms. The entire composition follows the golden ratio, with moody film-like color grading reminiscent of Studio Ghibli, enhanced by volumetric god rays filtering through the luminous foliage. 8K resolution, masterful photography, hyperdetailed, magical realism.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface StableDiffusionV35LargeOutput extends SharedType_a73 {}

export interface StableDiffusionV3MediumImageToImageInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. Defaults to the conditioning image's size.
     * @example null
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description The image URL to generate an image from.
     * @example https://fal.media/files/zebra/b52cVi3BhLDJcBrk6x0DL.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to generate an image from.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k
     */
    prompt: string;
    /**
     * Enhance Prompt
     * @description If set to true, prompt will be upsampled with more details.
     * @default false
     */
    prompt_expansion?: boolean;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength of the image-to-image transformation.
     * @default 0.9
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface StableDiffusionV3MediumImageToImageOutput extends SharedType_bc9 {}

export interface StableDiffusionV3MediumInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to generate an image from.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Digital art, portrait of an anthropomorphic roaring Tiger warrior with full armor, close up in the middle of a battle, behind him there is a banner with the text "Open Source"
     */
    prompt: string;
    /**
     * Enhance Prompt
     * @description If set to true, prompt will be upsampled with more details.
     * @default false
     */
    prompt_expansion?: boolean;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface StableDiffusionV3MediumOutput extends SharedType_bc9 {}

export interface StableDiffusionV15Input {
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description The list of LoRA weights to use.
     * @default []
     */
    loras?: Components.LoraWeight_1[];
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     * @example ugly, deformed
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 25
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example photo of a rhino dressed suit and tie sitting at a table in a bar with a bar stools, award winning photography, Elke vogelsang
     * @example Photo of a classic red mustang car parked in las vegas strip at night
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface StableDiffusionV15Output extends SharedType_a73 {}

export interface StableCascadeSoteDiffusionInput {
    /**
     * Enable Safety Checker
     * @description If set to false, the safety checker will be disabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * First Stage Steps
     * @description Number of steps to run the first stage for.
     * @default 25
     */
    first_stage_steps?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 8
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default {
     *       "height": 1536,
     *       "width": 1024
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example very displeasing, worst quality, monochrome, realistic, oldest
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example newest, extremely aesthetic, best quality, 1girl, solo, pink hair, blue eyes, long hair, looking at viewer, smile, black background, holding a sign, the text on the sign says "Hello"
     */
    prompt: string;
    /**
     * Decoder Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 2
     */
    second_stage_guidance_scale?: number;
    /**
     * Second Stage Steps
     * @description Number of steps to run the second stage for.
     * @default 10
     */
    second_stage_steps?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Cascade
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the image will be returned as base64 encoded string.
     * @default false
     */
    sync_mode?: boolean;
}

export interface StableCascadeSoteDiffusionOutput extends SharedType_a73 {}

export interface StableCascadeInput {
    /**
     * Enable Safety Checker
     * @description If set to false, the safety checker will be disabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * First Stage Steps
     * @description Number of steps to run the first stage for.
     * @default 20
     */
    first_stage_steps?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example ugly, deformed
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example An image of a shiba inu, donning a spacesuit and helmet
     */
    prompt: string;
    /**
     * Decoder Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 0
     */
    second_stage_guidance_scale?: number;
    /**
     * Second Stage Steps
     * @description Number of steps to run the second stage for.
     * @default 10
     */
    second_stage_steps?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Cascade
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the image will be returned as base64 encoded string.
     * @default false
     */
    sync_mode?: boolean;
}

export interface StableCascadeOutput extends SharedType_a73 {}

export interface StableAvatarInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the video to generate. If 'auto', the aspect ratio will be determined by the reference image.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '1:1' | '9:16' | 'auto';
    /**
     * Audio Guidance Scale
     * @description The audio guidance scale to use for the video generation.
     * @default 4
     */
    audio_guidance_scale?: number;
    /**
     * Audio URL
     * @description The URL of the audio to use as a reference for the video generation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/stable-avatar-input-audio.mp3
     */
    audio_url: string;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the video generation.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The URL of the image to use as a reference for the video generation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/stable-avatar-input-image.png
     */
    image_url: string;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use for the video generation.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Perturbation
     * @description The amount of perturbation to use for the video generation. 0.0 means no perturbation, 1.0 means full perturbation.
     * @default 0.1
     */
    perturbation?: number;
    /**
     * Prompt
     * @description The prompt to use for the video generation.
     * @example A person is in a relaxed pose. As the video progresses, the character speaks while arm and body movements are minimal and consistent with a natural speaking posture. Hand movements remain minimal. Don't blink too often. Preserve background integrity matching the reference image's spatial configuration, lighting conditions, and color temperature.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the video generation.
     */
    seed?: number;
}

export interface StableAvatarOutput {
    /**
     * Video
     * @description The generated video file.
     * @example https://storage.googleapis.com/falserverless/example_outputs/stable-avatar-output.mp4
     */
    video: Components.File;
}

export interface StableAudio25TextToAudioInput {
    /**
     * Guidance Scale
     * @description How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt).
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Num Inference Steps
     * @description The number of steps to denoise the audio for
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate audio from
     * @example A beautiful piano arpeggio grows into a grand orchestral climax
     */
    prompt: string;
    /**
     * Seconds Total
     * @description The duration of the audio clip to generate
     * @default 190
     */
    seconds_total?: number;
    /** Seed */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface StableAudio25TextToAudioOutput {
    /**
     * Audio
     * @description The generated audio clip
     * @example https://v3.fal.media/files/zebra/lGob9bN7VHfFXG4R1btQn_tmpabwhgi6n.wav
     */
    audio: Components.File;
    /**
     * Seed
     * @description The random seed used for generation
     */
    seed: number;
}

export interface StableAudio25InpaintInput {
    /**
     * Audio Url
     * @description The audio clip to inpaint
     * @example https://v3.fal.media/files/elephant/t0ZrzW_ueetXrr3NUa87F_a2a_in.mp3
     */
    audio_url: string;
    /**
     * Guidance Scale
     * @description How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt).
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Mask End
     * @description The end point of the audio mask
     * @default 190
     * @example 40
     */
    mask_end?: number;
    /**
     * Mask Start
     * @description The start point of the audio mask
     * @default 30
     * @example 15
     */
    mask_start?: number;
    /**
     * Num Inference Steps
     * @description The number of steps to denoise the audio for
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to guide the audio generation
     * @example Lofi hip hop beat, chillhop
     */
    prompt: string;
    /**
     * Seconds Total
     * @description The duration of the audio clip to generate. If not provided, it will be set to the duration of the input audio.
     * @default 190
     * @example 45
     */
    seconds_total?: number;
    /** Seed */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface StableAudio25InpaintOutput {
    /**
     * Audio
     * @description The generated audio clip
     * @example https://v3.fal.media/files/elephant/5F2Oour2tH_EHZrFUEmM-_tmp75kuha71.wav
     */
    audio: Components.File;
    /**
     * Seed
     * @description The random seed used for generation
     */
    seed: number;
}

export interface StableAudio25AudioToAudioInput {
    /**
     * Audio Url
     * @description The audio clip to transform
     * @example https://v3.fal.media/files/panda/1-0iezBUIePBa3Sz5YY5B_tmpy1jyshw9.wav
     */
    audio_url: string;
    /**
     * Guidance Scale
     * @description How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt).
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Num Inference Steps
     * @description The number of steps to denoise the audio for
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to guide the audio generation
     * @example Post rock, guitars, bass, strings, euphoric, up-lifting, moody, flowing, raw, epic
     */
    prompt: string;
    /** Seed */
    seed?: number;
    /**
     * Strength
     * @description Sometimes referred to as denoising, this parameter controls how much influence the `audio_url` parameter has on the generated audio. A value of 0 would yield audio that is identical to the input. A value of 1 would be as if you passed in no audio at all.
     * @default 0.8
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Total Seconds
     * @description The duration of the audio clip to generate. If not provided, it will be set to the duration of the input audio.
     * @example 45
     */
    total_seconds?: number;
}

export interface StableAudio25AudioToAudioOutput {
    /**
     * Audio
     * @description The generated audio clip
     * @example https://v3.fal.media/files/elephant/bJ-KIfIXsls5-pqSRXRwx_tmpdurmawkp.wav
     */
    audio: Components.File;
    /**
     * Seed
     * @description The random seed used for generation
     */
    seed: number;
}

export interface StableAudioInput {
    /**
     * Prompt
     * @description The prompt to generate audio from
     * @example 128 BPM tech house drum loop
     */
    prompt: string;
    /**
     * Seconds Start
     * @description The start point of the audio clip to generate
     * @default 0
     */
    seconds_start?: number;
    /**
     * Seconds Total
     * @description The duration of the audio clip to generate
     * @default 30
     */
    seconds_total?: number;
    /**
     * Steps
     * @description The number of steps to denoise the audio for
     * @default 100
     */
    steps?: number;
}

export interface StableAudioOutput {
    /** @description The generated audio clip */
    audio_file: Components.File_1;
}

export interface SpeechToTextTurboStreamInput extends SharedType_86b {}

export interface SpeechToTextTurboStreamOutput extends SharedType_4411 {}

export interface SpeechToTextTurboInput extends SharedType_86b {}

export interface SpeechToTextTurboOutput extends SharedType_298 {}

export interface SpeechToTextStreamInput extends SharedType_86b {}

export interface SpeechToTextStreamOutput extends SharedType_4411 {}

export interface SpeechToTextInput extends SharedType_86b {}

export interface SpeechToTextOutput extends SharedType_298 {}

export interface Sora2VideoToVideoRemixInput {
    /**
     * Delete Video
     * @description Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted.
     * @default true
     */
    delete_video?: boolean;
    /**
     * Prompt
     * @description Updated text prompt that directs the remix generation
     * @example Change the cat's fur color to purple.
     */
    prompt: string;
    /**
     * Video ID
     * @description The video_id from a previous Sora 2 generation. Note: You can only remix videos that were generated by Sora (via text-to-video or image-to-video endpoints), not arbitrary uploaded videos.
     * @example video_123
     */
    video_id: string;
}

export interface Sora2VideoToVideoRemixOutput {
    /**
     * Spritesheet
     * @description Spritesheet image for the video
     */
    spritesheet?: Components.ImageFile;
    /**
     * Thumbnail
     * @description Thumbnail image for the video
     */
    thumbnail?: Components.ImageFile;
    /**
     * Video
     * @description The generated video
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/rabbit/nk1MK6LY90QqScvI4_Yn8.mp4"
     *     }
     */
    video: Components.VideoFile_1;
    /**
     * Video ID
     * @description The ID of the generated video
     * @example video_123
     */
    video_id: string;
}

export interface Sora2TextToVideoProInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '16:9';
    /**
     * Delete Video
     * @description Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted.
     * @default true
     */
    delete_video?: boolean;
    /**
     * Duration
     * @description Duration of the generated video in seconds
     * @default 4
     * @enum {integer}
     */
    duration?: 4 | 8 | 12;
    /**
     * Prompt
     * @description The text prompt describing the video you want to generate
     * @example A dramatic Hollywood breakup scene at dusk on a quiet suburban street. A man and a woman in their 30s face each other, speaking softly but emotionally, lips syncing to breakup dialogue. Cinematic lighting, warm sunset tones, shallow depth of field, gentle breeze moving autumn leaves, realistic natural sound, no background music
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 1080p
     * @enum {string}
     */
    resolution?: '720p' | '1080p';
}

export interface Sora2TextToVideoProOutput {
    /**
     * Spritesheet
     * @description Spritesheet image for the video
     */
    spritesheet?: Components.ImageFile;
    /**
     * Thumbnail
     * @description Thumbnail image for the video
     */
    thumbnail?: Components.ImageFile;
    /**
     * Video
     * @description The generated video
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/sora-2-pro-t2v-output.mp4"
     *     }
     */
    video: Components.VideoFile_1;
    /**
     * Video ID
     * @description The ID of the generated video
     * @example video_123
     */
    video_id: string;
}

export interface Sora2TextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '16:9';
    /**
     * Delete Video
     * @description Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted.
     * @default true
     */
    delete_video?: boolean;
    /**
     * Duration
     * @description Duration of the generated video in seconds
     * @default 4
     * @enum {integer}
     */
    duration?: 4 | 8 | 12;
    /**
     * Model
     * @description The model to use for the generation. When the default model is selected, the latest snapshot of the model will be used - otherwise, select a specific snapshot of the model.
     * @default sora-2
     * @enum {string}
     */
    model?: 'sora-2' | 'sora-2-2025-12-08' | 'sora-2-2025-10-06';
    /**
     * Prompt
     * @description The text prompt describing the video you want to generate
     * @example A dramatic Hollywood breakup scene at dusk on a quiet suburban street. A man and a woman in their 30s face each other, speaking softly but emotionally, lips syncing to breakup dialogue. Cinematic lighting, warm sunset tones, shallow depth of field, gentle breeze moving autumn leaves, realistic natural sound, no background music
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p';
}

export interface Sora2TextToVideoOutput {
    /**
     * Spritesheet
     * @description Spritesheet image for the video
     */
    spritesheet?: Components.ImageFile;
    /**
     * Thumbnail
     * @description Thumbnail image for the video
     */
    thumbnail?: Components.ImageFile;
    /**
     * Video
     * @description The generated video
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/sora_t2v_output.mp4"
     *     }
     */
    video: Components.VideoFile_1;
    /**
     * Video ID
     * @description The ID of the generated video
     * @example video_123
     */
    video_id: string;
}

export interface Sora2ImageToVideoProInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '9:16' | '16:9';
    /**
     * Delete Video
     * @description Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted.
     * @default true
     */
    delete_video?: boolean;
    /**
     * Duration
     * @description Duration of the generated video in seconds
     * @default 4
     * @enum {integer}
     */
    duration?: 4 | 8 | 12;
    /**
     * Image URL
     * @description The URL of the image to use as the first frame
     * @example https://storage.googleapis.com/falserverless/example_inputs/sora-2-i2v-input.png
     */
    image_url: string;
    /**
     * Prompt
     * @description The text prompt describing the video you want to generate
     * @example Front-facing 'invisible' action-cam on a skydiver in freefall above bright clouds; camera locked on his face. He speaks over the wind with clear lipsync: 'This is insanely fun! You've got to try it—book a tandem and go!' Natural wind roar, voice close-mic'd and slightly compressed so it's intelligible. Midday sun, goggles and jumpsuit flutter, altimeter visible, parachute rig on shoulders. Energetic but stable framing with subtle shake; brief horizon roll. End on first tug of canopy and wind noise dropping.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default auto
     * @enum {string}
     */
    resolution?: 'auto' | '720p' | '1080p';
}

export interface Sora2ImageToVideoProOutput {
    /**
     * Spritesheet
     * @description Spritesheet image for the video
     */
    spritesheet?: Components.ImageFile;
    /**
     * Thumbnail
     * @description Thumbnail image for the video
     */
    thumbnail?: Components.ImageFile;
    /**
     * Video
     * @description The generated video
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/sora-2-pro-i2v-output.mp4"
     *     }
     */
    video: Components.VideoFile_1;
    /**
     * Video ID
     * @description The ID of the generated video
     * @example video_123
     */
    video_id: string;
}

export interface Sora2ImageToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '9:16' | '16:9';
    /**
     * Delete Video
     * @description Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted.
     * @default true
     */
    delete_video?: boolean;
    /**
     * Duration
     * @description Duration of the generated video in seconds
     * @default 4
     * @enum {integer}
     */
    duration?: 4 | 8 | 12;
    /**
     * Image URL
     * @description The URL of the image to use as the first frame
     * @example https://storage.googleapis.com/falserverless/example_inputs/sora-2-i2v-input.png
     */
    image_url: string;
    /**
     * Model
     * @description The model to use for the generation. When the default model is selected, the latest snapshot of the model will be used - otherwise, select a specific snapshot of the model.
     * @default sora-2
     * @enum {string}
     */
    model?: 'sora-2' | 'sora-2-2025-12-08' | 'sora-2-2025-10-06';
    /**
     * Prompt
     * @description The text prompt describing the video you want to generate
     * @example Front-facing 'invisible' action-cam on a skydiver in freefall above bright clouds; camera locked on his face. He speaks over the wind with clear lipsync: 'This is insanely fun! You've got to try it—book a tandem and go!' Natural wind roar, voice close-mic'd and slightly compressed so it's intelligible. Midday sun, goggles and jumpsuit flutter, altimeter visible, parachute rig on shoulders. Energetic but stable framing with subtle shake; brief horizon roll. End on first tug of canopy and wind noise dropping.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default auto
     * @enum {string}
     */
    resolution?: 'auto' | '720p';
}

export interface Sora2ImageToVideoOutput {
    /**
     * Spritesheet
     * @description Spritesheet image for the video
     */
    spritesheet?: Components.ImageFile;
    /**
     * Thumbnail
     * @description Thumbnail image for the video
     */
    thumbnail?: Components.ImageFile;
    /**
     * Video
     * @description The generated video
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/sora_2_i2v_output.mp4"
     *     }
     */
    video: Components.VideoFile_1;
    /**
     * Video ID
     * @description The ID of the generated video
     * @example video_123
     */
    video_id: string;
}

export interface SmartTurnInput {
    /**
     * Audio Url
     * @description The URL of the audio file to be processed.
     * @example https://fal.media/files/panda/5-QaAOC32rB_hqWaVdqEH.mpga
     */
    audio_url: string;
}

export interface SmartTurnOutput {
    /**
     * Metrics
     * @description The metrics of the inference.
     */
    metrics: Record<string, number>;
    /**
     * Prediction
     * @description The predicted turn type. 1 for Complete, 0 for Incomplete.
     */
    prediction: number;
    /**
     * Probability
     * @description The probability of the predicted turn type.
     */
    probability: number;
}

export interface SkyreelsI2vInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the output video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16';
    /**
     * Guidance Scale
     * @description Guidance scale for generation (between 1.0 and 20.0)
     * @default 6
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description URL of the image input.
     * @example https://fal.media/files/panda/TuXlMwArpQcdYNCLAEM8K.webp
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt to guide generation away from certain attributes.
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description Number of denoising steps (between 1 and 50). Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for generation. If not provided, a random seed will be used.
     */
    seed?: number;
}

export interface SkyreelsI2vOutput {
    /**
     * Seed
     * @description The seed used for generation
     * @example 42
     */
    seed: number;
    /**
     * Video
     * @example {
     *       "url": "https://fal.media/files/elephant/yOOdaiC5clkH9K_5TTD32_video.mp4"
     *     }
     */
    video: Components.File;
}

export interface SkyRaccoonInput {
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     * @example false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default {
     *       "height": 1024,
     *       "width": 1024
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     * @example bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Turbo Mode
     * @description If true, the video will be generated faster with no noticeable degradation in the visual quality.
     * @default false
     */
    turbo_mode?: boolean;
}

export interface SkyRaccoonOutput {
    /**
     * Image
     * @description The generated image file.
     * @example {
     *       "url": "https://v3.fal.media/files/kangaroo/xP17uR1w0uHA1T2W2k6Gi_video-1752605675.png"
     *     }
     */
    image: Components.File;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
}

export interface SileroVadInput {
    /**
     * Audio URL
     * @description The URL of the audio to get speech timestamps from.
     * @example https://v3b.fal.media/files/b/0a89994c/X3-06RFibRfBu-FS1AI8y_speech.mp3
     */
    audio_url: string;
}

export interface SileroVadOutput {
    /**
     * Has Speech
     * @description Whether the audio has speech.
     * @example true
     */
    has_speech: boolean;
    /**
     * Speech Timestamps
     * @description The speech timestamps.
     * @example [
     *       {
     *         "end": 1.982,
     *         "start": 0.13
     *       },
     *       {
     *         "end": 3.998,
     *         "start": 2.434
     *       }
     *     ]
     */
    timestamps: Components.SpeechTimestamp[];
}

export interface SeedvrUpscaleVideoInput {
    /**
     * Noise Scale
     * @description The noise scale to use for the generation process.
     * @default 0.1
     */
    noise_scale?: number;
    /**
     * Output Format
     * @description The format of the output video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    output_format?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Output Quality
     * @description The quality of the output video.
     * @default high
     * @enum {string}
     */
    output_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Output Write Mode
     * @description The write mode of the output video.
     * @default balanced
     * @enum {string}
     */
    output_write_mode?: 'fast' | 'balanced' | 'small';
    /**
     * Seed
     * @description The random seed used for the generation process.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Target Resolution
     * @description The target resolution to upscale to when `upscale_mode` is `target`.
     * @default 1080p
     * @enum {string}
     */
    target_resolution?: '720p' | '1080p' | '1440p' | '2160p';
    /**
     * Upscale Factor
     * @description Upscaling factor to be used. Will multiply the dimensions with this factor when `upscale_mode` is `factor`.
     * @default 2
     */
    upscale_factor?: number;
    /**
     * Upscale Mode
     * @description The mode to use for the upscale. If 'target', the upscale factor will be calculated based on the target resolution. If 'factor', the upscale factor will be used directly.
     * @default factor
     * @enum {string}
     */
    upscale_mode?: 'target' | 'factor';
    /**
     * Video Url
     * @description The input video to be processed
     * @example https://storage.googleapis.com/falserverless/example_inputs/seedvr-input.mp4
     */
    video_url: string;
}

export interface SeedvrUpscaleVideoOutput {
    /**
     * Seed
     * @description The random seed used for the generation process.
     */
    seed: number;
    /**
     * @description Upscaled video file after processing
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/seedvr-output.mp4"
     *     }
     */
    video: Components.File_1;
}

export interface SeedvrUpscaleImageInput {
    /**
     * Image Url
     * @description The input image to be processed
     * @example https://storage.googleapis.com/falserverless/example_inputs/seedvr2/image_in.png
     */
    image_url: string;
    /**
     * Noise Scale
     * @description The noise scale to use for the generation process.
     * @default 0.1
     */
    noise_scale?: number;
    /**
     * Output Format
     * @description The format of the output image.
     * @default jpg
     * @enum {string}
     */
    output_format?: 'png' | 'jpg' | 'webp';
    /**
     * Seed
     * @description The random seed used for the generation process.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Target Resolution
     * @description The target resolution to upscale to when `upscale_mode` is `target`.
     * @default 1080p
     * @enum {string}
     */
    target_resolution?: '720p' | '1080p' | '1440p' | '2160p';
    /**
     * Upscale Factor
     * @description Upscaling factor to be used. Will multiply the dimensions with this factor when `upscale_mode` is `factor`.
     * @default 2
     */
    upscale_factor?: number;
    /**
     * Upscale Mode
     * @description The mode to use for the upscale. If 'target', the upscale factor will be calculated based on the target resolution. If 'factor', the upscale factor will be used directly.
     * @default factor
     * @enum {string}
     */
    upscale_mode?: 'target' | 'factor';
}

export interface SeedvrUpscaleImageOutput {
    /**
     * @description Upscaled image file after processing
     * @example {
     *       "content_type": "image/png",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/seedvr2/image_out.png"
     *     }
     */
    image: Components.ImageFile_1;
    /**
     * Seed
     * @description The random seed used for the generation process.
     */
    seed: number;
}

export interface SdxlControlnetUnionInpaintingInput {
    /**
     * Canny Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    canny_image_url?: string;
    /**
     * Canny Preprocess
     * @description Whether to preprocess the canny image.
     * @default true
     */
    canny_preprocess?: boolean;
    /**
     * Controlnet Conditioning Scale
     * @description The scale of the controlnet conditioning.
     * @default 0.5
     */
    controlnet_conditioning_scale?: number;
    /**
     * Depth Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    depth_image_url?: string;
    /**
     * Depth Preprocess
     * @description Whether to preprocess the depth image.
     * @default true
     */
    depth_preprocess?: boolean;
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. Leave it none to automatically infer from the control image.
     * @example null
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description The URL of the image to use as a starting point for the generation.
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png
     */
    image_url: string;
    /**
     * Loras
     * @description The list of LoRA weights to use.
     * @default []
     */
    loras?: Components.LoraWeight_1[];
    /**
     * Mask Url
     * @description The URL of the mask to use for inpainting.
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png
     */
    mask_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     */
    negative_prompt?: string;
    /**
     * Normal Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    normal_image_url?: string;
    /**
     * Normal Preprocess
     * @description Whether to preprocess the normal image.
     * @default true
     */
    normal_preprocess?: boolean;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 35
     */
    num_inference_steps?: number;
    /**
     * Openpose Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    openpose_image_url?: string;
    /**
     * Openpose Preprocess
     * @description Whether to preprocess the openpose image.
     * @default true
     */
    openpose_preprocess?: boolean;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example Ice fortress, aurora skies, polar wildlife, twilight
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Segmentation Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    segmentation_image_url?: string;
    /**
     * Segmentation Preprocess
     * @description Whether to preprocess the segmentation image.
     * @default true
     */
    segmentation_preprocess?: boolean;
    /**
     * Strength
     * @description determines how much the generated image resembles the initial image
     * @default 0.95
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Teed Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    teed_image_url?: string;
    /**
     * Teed Preprocess
     * @description Whether to preprocess the teed image.
     * @default true
     */
    teed_preprocess?: boolean;
}

export interface SdxlControlnetUnionInpaintingOutput extends SharedType_a73 {}

export interface SdxlControlnetUnionImageToImageInput {
    /**
     * Canny Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    canny_image_url?: string;
    /**
     * Canny Preprocess
     * @description Whether to preprocess the canny image.
     * @default true
     */
    canny_preprocess?: boolean;
    /**
     * Controlnet Conditioning Scale
     * @description The scale of the controlnet conditioning.
     * @default 0.5
     */
    controlnet_conditioning_scale?: number;
    /**
     * Crop Output
     * @description If set to true, the output cropped to the proper aspect ratio after generating.
     * @default false
     */
    crop_output?: boolean;
    /**
     * Depth Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    depth_image_url?: string;
    /**
     * Depth Preprocess
     * @description Whether to preprocess the depth image.
     * @default true
     */
    depth_preprocess?: boolean;
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. Leave it none to automatically infer from the control image.
     * @example null
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description The URL of the image to use as a starting point for the generation.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/tiger/IExuP-WICqaIesLZAZPur.jpeg
     */
    image_url: string;
    /**
     * Loras
     * @description The list of LoRA weights to use.
     * @default []
     */
    loras?: Components.LoraWeight_1[];
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     */
    negative_prompt?: string;
    /**
     * Normal Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    normal_image_url?: string;
    /**
     * Normal Preprocess
     * @description Whether to preprocess the normal image.
     * @default true
     */
    normal_preprocess?: boolean;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 35
     */
    num_inference_steps?: number;
    /**
     * Openpose Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    openpose_image_url?: string;
    /**
     * Openpose Preprocess
     * @description Whether to preprocess the openpose image.
     * @default true
     */
    openpose_preprocess?: boolean;
    /**
     * Preserve Aspect Ratio
     * @description If set to true, the aspect ratio of the generated image will be preserved even
     *             if the image size is too large. However, if the image is not a multiple of 32
     *             in width or height, it will be resized to the nearest multiple of 32. By default,
     *             this snapping to the nearest multiple of 32 will not preserve the aspect ratio.
     *             Set crop_output to True, to crop the output to the proper aspect ratio
     *             after generating.
     * @default false
     */
    preserve_aspect_ratio?: boolean;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example Ice fortress, aurora skies, polar wildlife, twilight
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Segmentation Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    segmentation_image_url?: string;
    /**
     * Segmentation Preprocess
     * @description Whether to preprocess the segmentation image.
     * @default true
     */
    segmentation_preprocess?: boolean;
    /**
     * Strength
     * @description determines how much the generated image resembles the initial image
     * @default 0.95
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Teed Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    teed_image_url?: string;
    /**
     * Teed Preprocess
     * @description Whether to preprocess the teed image.
     * @default true
     */
    teed_preprocess?: boolean;
}

export interface SdxlControlnetUnionImageToImageOutput extends SharedType_a73 {}

export interface SdxlControlnetUnionInput {
    /**
     * Canny Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    canny_image_url?: string;
    /**
     * Canny Preprocess
     * @description Whether to preprocess the canny image.
     * @default true
     */
    canny_preprocess?: boolean;
    /**
     * Controlnet Conditioning Scale
     * @description The scale of the controlnet conditioning.
     * @default 0.5
     */
    controlnet_conditioning_scale?: number;
    /**
     * Depth Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    depth_image_url?: string;
    /**
     * Depth Preprocess
     * @description Whether to preprocess the depth image.
     * @default true
     */
    depth_preprocess?: boolean;
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. Leave it none to automatically infer from the control image.
     * @example null
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description The list of LoRA weights to use.
     * @default []
     */
    loras?: Components.LoraWeight_1[];
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     * @example ugly, deformed
     */
    negative_prompt?: string;
    /**
     * Normal Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    normal_image_url?: string;
    /**
     * Normal Preprocess
     * @description Whether to preprocess the normal image.
     * @default true
     */
    normal_preprocess?: boolean;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 35
     */
    num_inference_steps?: number;
    /**
     * Openpose Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    openpose_image_url?: string;
    /**
     * Openpose Preprocess
     * @description Whether to preprocess the openpose image.
     * @default true
     */
    openpose_preprocess?: boolean;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example Ice fortress, aurora skies, polar wildlife, twilight
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Segmentation Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    segmentation_image_url?: string;
    /**
     * Segmentation Preprocess
     * @description Whether to preprocess the segmentation image.
     * @default true
     */
    segmentation_preprocess?: boolean;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Teed Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    teed_image_url?: string;
    /**
     * Teed Preprocess
     * @description Whether to preprocess the teed image.
     * @default true
     */
    teed_preprocess?: boolean;
}

export interface SdxlControlnetUnionOutput extends SharedType_a73 {}

export interface Sd15DepthControlnetInput {
    /**
     * Control Image Url
     * @description The URL of the control image.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/rabbit/MiN_j3St9B8esJleCZKMU.jpeg
     */
    control_image_url: string;
    /**
     * Controlnet Conditioning Scale
     * @description The scale of the controlnet conditioning.
     * @default 0.5
     */
    controlnet_conditioning_scale?: number;
    /**
     * Enable Deep Cache
     * @description If set to true, DeepCache will be enabled. TBD
     * @default false
     */
    enable_deep_cache?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. Leave it none to automatically infer from the control image.
     * @example null
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description The list of LoRA weights to use.
     * @default []
     */
    loras?: Components.LoraWeight_3[];
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     * @example ugly, deformed
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 35
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example Ice fortress, aurora skies, polar wildlife, twilight
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Sd15DepthControlnetOutput extends SharedType_a73 {}

export interface ScailInput {
    /**
     * Image Url
     * @description The URL of the image to use as a reference for the video generation.
     * @example https://v3b.fal.media/files/b/panda/-oMlZo9Yyj_Nzoza_tgds_GmLF86r5bOt50eMMKCszy_eacc949b3933443c9915a83c98fbe85e.png
     */
    image_url: string;
    /**
     * Multi Character
     * @description Enable multi-character mode. Use when driving video has multiple people.
     * @default false
     */
    multi_character?: boolean;
    /**
     * Num Inference Steps
     * @description The number of inference steps to use for the video generation.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to guide video generation.
     * @example A person dancing gracefully
     */
    prompt: string;
    /**
     * Resolution
     * @description Output resolution. Outputs 896x512 (landscape) or 512x896 (portrait) based on the input image aspect ratio.
     * @default 512p
     * @enum {string}
     */
    resolution?: '512p';
    /**
     * Video Url
     * @description The URL of the video to use as a reference for the video generation.
     * @example https://v3b.fal.media/files/b/panda/a6SvJg96V8eoglMlYFShU_5385885-hd_1080_1920_25fps.mp4
     */
    video_url: string;
}

export interface ScailOutput {
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "file_size": 464837,
     *       "file_name": "output_000000.mp4",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3b.fal.media/files/b/0a86b0a5/i50rQdBAsyzGiqqDDQSsl_output_000000.mp4"
     *     }
     */
    video: Components.File;
}

export interface SanaV1548bInput extends SharedType_5f3 {}

export interface SanaV1548bOutput extends SharedType_a73 {}

export interface SanaV1516bInput extends SharedType_5f3 {}

export interface SanaV1516bOutput extends SharedType_a73 {}

export interface SanaSprintInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default {
     *       "height": 2160,
     *       "width": 3840
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 2
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Underwater coral reef ecosystem during peak bioluminescent activity, multiple layers of marine life - from microscopic plankton to massive coral structures, light refracting through crystal-clear tropical waters, creating prismatic color gradients, hyper-detailed texture of marine organisms
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Style Name
     * @description The style to generate the image in.
     * @default (No style)
     * @enum {string}
     */
    style_name?:
        | '(No style)'
        | 'Cinematic'
        | 'Photographic'
        | 'Anime'
        | 'Manga'
        | 'Digital Art'
        | 'Pixel art'
        | 'Fantasy art'
        | 'Neonpunk'
        | '3D Model';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface SanaSprintOutput extends SharedType_a73 {}

export interface SanaVideoInput {
    /**
     * Fps
     * @description Frames per second for the output video
     * @default 16
     */
    fps?: number;
    /**
     * Guidance Scale
     * @description Guidance scale for generation (higher = more prompt adherence)
     * @default 6
     */
    guidance_scale?: number;
    /**
     * Motion Score
     * @description Motion intensity score (higher = more motion)
     * @default 30
     */
    motion_score?: number;
    /**
     * Negative Prompt
     * @description The negative prompt describing what to avoid in the generation
     * @default A chaotic sequence with misshapen, deformed limbs in heavy motion blur, sudden disappearance, jump cuts, jerky movements, rapid shot changes, frames out of sync, inconsistent character shapes, temporal artifacts, jitter, and ghosting effects, creating a disorienting visual experience.
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description Number of frames to generate
     * @default 81
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of denoising steps
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The text prompt describing the video to generate
     * @example Evening, backlight, side lighting, soft light, high contrast, mid-shot, centered composition, clean solo shot, warm color. A young Caucasian man stands in a forest, golden light glimmers on his hair as sunlight filters through the leaves.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the output video
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p';
    /**
     * Seed
     * @description Random seed for reproducible generation. If not provided, a random seed will be used.
     */
    seed?: number;
}

export interface SanaVideoOutput {
    /**
     * Seed
     * @description The random seed used for the generation process
     */
    seed: number;
    /**
     * Video
     * @description Generated video file
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/zebra/TipA9XXsXRYlB6vK6PQ0l_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface SanaInput extends SharedType_5f3 {}

export interface SanaOutput extends SharedType_a73 {}

export interface Sam2VideoInput {
    /**
     * Apply Mask
     * @description Apply the mask on the video.
     * @default false
     */
    apply_mask?: boolean;
    /**
     * Boundingbox Zip
     * @description Return per-frame bounding box overlays as a zip archive.
     * @default false
     */
    boundingbox_zip?: boolean;
    /**
     * Box Prompts
     * @description Coordinates for boxes
     * @default []
     * @example [
     *       {
     *         "y_min": 0,
     *         "frame_index": 0,
     *         "x_max": 500,
     *         "x_min": 300,
     *         "y_max": 400
     *       }
     *     ]
     */
    box_prompts?: Components.BoxPrompt_1[];
    /**
     * Mask Url
     * @description The URL of the mask to be applied initially.
     */
    mask_url?: string;
    /**
     * Prompts
     * @description List of prompts to segment the video
     * @default []
     * @example [
     *       {
     *         "y": 350,
     *         "label": 1,
     *         "frame_index": 0,
     *         "x": 210
     *       },
     *       {
     *         "y": 220,
     *         "label": 1,
     *         "frame_index": 0,
     *         "x": 250
     *       }
     *     ]
     */
    prompts?: Components.PointPrompt_1[];
    /**
     * Video Url
     * @description The URL of the video to be segmented.
     * @example https://drive.google.com/uc?id=1iOFYbNITYwrebBBp9kaEGhBndFSRLz8k
     */
    video_url: string;
}

export interface Sam2VideoOutput {
    /**
     * Boundingbox Frames Zip
     * @description Zip file containing per-frame bounding box overlays.
     */
    boundingbox_frames_zip?: Components.File;
    /**
     * Video
     * @description The segmented video.
     */
    video: Components.File;
}

export interface Sam2ImageInput {
    /**
     * Apply Mask
     * @description Apply the mask on the image.
     * @default false
     */
    apply_mask?: boolean;
    /**
     * Box Prompts
     * @description Coordinates for boxes
     * @default []
     * @example [
     *       {
     *         "y_min": 600,
     *         "x_max": 700,
     *         "x_min": 425,
     *         "y_max": 875
     *       }
     *     ]
     */
    box_prompts?: Components.BoxPrompt_1[];
    /**
     * Image Url
     * @description URL of the image to be segmented
     * @example https://raw.githubusercontent.com/facebookresearch/segment-anything-2/main/notebooks/images/truck.jpg
     */
    image_url: string;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompts
     * @description List of prompts to segment the image
     * @default []
     * @example [
     *       {
     *         "y": 375,
     *         "label": 1,
     *         "x": 500
     *       }
     *     ]
     */
    prompts?: Components.PointPrompt_1[];
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Sam2ImageOutput {
    /**
     * Image
     * @description Segmented image.
     */
    image: Components.Image;
}

export interface Sam2AutoSegmentInput {
    /**
     * Image Url
     * @description URL of the image to be automatically segmented
     * @example https://raw.githubusercontent.com/facebookresearch/segment-anything-2/main/notebooks/images/truck.jpg
     */
    image_url: string;
    /**
     * Min Mask Region Area
     * @description Minimum area of a mask region.
     * @default 100
     */
    min_mask_region_area?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Points Per Side
     * @description Number of points to sample along each side of the image.
     * @default 32
     */
    points_per_side?: number;
    /**
     * Pred Iou Thresh
     * @description Threshold for predicted IOU score.
     * @default 0.88
     */
    pred_iou_thresh?: number;
    /**
     * Stability Score Thresh
     * @description Threshold for stability score.
     * @default 0.95
     */
    stability_score_thresh?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Sam2AutoSegmentOutput {
    /**
     * Combined Mask
     * @description Combined segmentation mask.
     */
    combined_mask: Components.Image;
    /**
     * Individual Masks
     * @description Individual segmentation masks.
     */
    individual_masks: Components.Image[];
}

export interface SamAudioVisualSeparateInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default balanced
     * @enum {string}
     */
    acceleration?: 'fast' | 'balanced' | 'quality';
    /**
     * Mask Video Url
     * @description URL of the mask video (binary mask indicating target object). Black=target, White=background.
     */
    mask_video_url?: string;
    /**
     * Output Format
     * @description Output audio format.
     * @default wav
     * @enum {string}
     */
    output_format?: 'wav' | 'mp3';
    /**
     * Prompt
     * @description Text prompt to assist with separation. Use natural language to describe the target sound.
     * @default
     * @example man on the left
     */
    prompt?: string;
    /**
     * Reranking Candidates
     * @description Number of candidates to generate and rank. Higher improves quality but increases latency and cost.
     * @default 1
     */
    reranking_candidates?: number;
    /**
     * Video Url
     * @description URL of the video file to process (MP4, MOV, etc.)
     * @example https://v3b.fal.media/files/b/0a8850d1/gff7zKI-6XwIbBBip4946_office.mp4
     */
    video_url: string;
}

export interface SamAudioVisualSeparateOutput {
    /**
     * Duration
     * @description Duration of the output audio in seconds.
     * @example 15
     */
    duration: number;
    /**
     * Residual
     * @description Everything else in the audio.
     * @example {
     *       "content_type": "audio/wav",
     *       "url": "https://v3b.fal.media/files/b/0a88550c/pdOH_J84S-197LRjMQDrz_tmprx375uix.wav"
     *     }
     */
    residual: Components.File;
    /**
     * Sample Rate
     * @description Sample rate of the output audio in Hz.
     * @default 48000
     */
    sample_rate?: number;
    /**
     * Target
     * @description The isolated target sound.
     * @example {
     *       "content_type": "audio/wav",
     *       "url": "https://v3b.fal.media/files/b/0a88550c/CVyBZ1Cxka1vLxVwOfUcc_tmpzzftm934.wav"
     *     }
     */
    target: Components.File;
}

export interface SamAudioSpanSeparateInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default balanced
     * @enum {string}
     */
    acceleration?: 'fast' | 'balanced' | 'quality';
    /**
     * Audio Url
     * @description URL of the audio file to process.
     * @example https://v3b.fal.media/files/b/0a8853d1/T7zRmsiculA6u_V6RCF2c_man.mp3
     */
    audio_url: string;
    /**
     * Output Format
     * @description Output audio format.
     * @default wav
     * @enum {string}
     */
    output_format?: 'wav' | 'mp3';
    /**
     * Prompt
     * @description Text prompt describing the sound to isolate. Optional but recommended - helps the model identify what type of sound to extract from the span.
     * @example man singing
     * @example dog barking
     */
    prompt?: string;
    /**
     * Reranking Candidates
     * @description Number of candidates to generate and rank. Higher improves quality but increases latency and cost. Requires text prompt; ignored for span-only separation.
     * @default 1
     */
    reranking_candidates?: number;
    /**
     * Spans
     * @description Time spans where the target sound occurs which should be isolated.
     * @example [
     *       {
     *         "end": 10,
     *         "start": 6,
     *         "include": true
     *       }
     *     ]
     */
    spans: Components.AudioTimeSpan[];
    /**
     * Trim To Span
     * @description Trim output audio to only include the specified span time range. If False, returns the full audio length with the target sound isolated throughout.
     * @default false
     */
    trim_to_span?: boolean;
}

export interface SamAudioSpanSeparateOutput {
    /**
     * Duration
     * @description Duration of the output audio in seconds.
     * @example 81.96
     */
    duration: number;
    /**
     * Residual
     * @description Everything else in the audio.
     * @example {
     *       "content_type": "audio/wav",
     *       "url": "https://v3b.fal.media/files/b/0a89374d/suELShppRlPCTAVnbWRqj_tmpr8shao_e.wav"
     *     }
     */
    residual: Components.File;
    /**
     * Sample Rate
     * @description Sample rate of the output audio in Hz.
     * @default 48000
     */
    sample_rate?: number;
    /**
     * Target
     * @description The isolated target sound.
     * @example {
     *       "content_type": "audio/wav",
     *       "url": "https://v3b.fal.media/files/b/0a89374c/mImRbqrjB72o9vEmIrSmW_tmpp2rmotgs.wav"
     *     }
     */
    target: Components.File;
}

export interface SamAudioSeparateInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default balanced
     * @enum {string}
     */
    acceleration?: 'fast' | 'balanced' | 'quality';
    /**
     * Audio Url
     * @description URL of the audio file to process (WAV, MP3, FLAC supported)
     * @example https://v3b.fal.media/files/b/0a88511f/tUUCI9eDmeC2RqJEOXrZk_assets_avatar_multi_sing_woman.WAV
     */
    audio_url: string;
    /**
     * Output Format
     * @description Output audio format.
     * @default wav
     * @enum {string}
     */
    output_format?: 'wav' | 'mp3';
    /**
     * Predict Spans
     * @description Automatically predict temporal spans where the target sound occurs.
     * @default false
     */
    predict_spans?: boolean;
    /**
     * Prompt
     * @description Text prompt describing the sound to isolate.
     * @example piano playing
     */
    prompt: string;
    /**
     * Reranking Candidates
     * @description Number of candidates to generate and rank. Higher improves quality but increases latency and cost.
     * @default 1
     */
    reranking_candidates?: number;
}

export interface SamAudioSeparateOutput {
    /**
     * Duration
     * @description Duration of the output audio in seconds.
     * @example 26.6
     */
    duration: number;
    /**
     * Residual
     * @description Everything else in the audio.
     * @example {
     *       "content_type": "audio/wav",
     *       "url": "https://v3b.fal.media/files/b/0a88512b/xqDIegkZuLPPlufZ9RLP8_tmpos7b9db_.wav"
     *     }
     */
    residual: Components.File;
    /**
     * Sample Rate
     * @description Sample rate of the output audio in Hz.
     * @default 48000
     */
    sample_rate?: number;
    /**
     * Target
     * @description The isolated target sound.
     * @example {
     *       "content_type": "audio/wav",
     *       "url": "https://v3b.fal.media/files/b/0a8853af/bxm0-bZp5tH46Qp5PMwl6_tmpus5ep6vl.wav"
     *     }
     */
    target: Components.File;
}

export interface Sam3VideoRleInput {
    /**
     * Apply Mask
     * @description Apply the mask on the video.
     * @default false
     */
    apply_mask?: boolean;
    /**
     * Boundingbox Zip
     * @description Return per-frame bounding box overlays as a zip archive.
     * @default false
     */
    boundingbox_zip?: boolean;
    /**
     * Box Prompts
     * @description List of box prompts with optional frame_index.
     * @default []
     */
    box_prompts?: Components.BoxPrompt[];
    /**
     * Detection Threshold
     * @description Detection confidence threshold (0.0-1.0). Lower = more detections but less precise. Defaults: 0.5 for existing, 0.7 for new objects. Try 0.2-0.3 if text prompts fail.
     * @default 0.5
     */
    detection_threshold?: number;
    /**
     * Frame Index
     * @description Frame index used for initial interaction when mask_url is provided.
     * @default 0
     */
    frame_index?: number;
    /**
     * Mask Url
     * @description The URL of the mask to be applied initially.
     */
    mask_url?: string;
    /**
     * Point Prompts
     * @description List of point prompts with frame indices.
     * @default []
     */
    point_prompts?: Components.PointPrompt[];
    /**
     * Prompt
     * @description Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth').
     * @default
     * @example person
     * @example person, cloth
     */
    prompt?: string;
    /**
     * Video Url
     * @description The URL of the video to be segmented.
     * @example https://v3b.fal.media/files/b/elephant/NQdDxB0Ddfo82SPLbhYDp_bedroom.mp4
     */
    video_url: string;
}

export interface Sam3VideoRleOutput extends SharedType_d19 {}

export interface Sam3VideoInput {
    /**
     * Apply Mask
     * @description Apply the mask on the video.
     * @default true
     */
    apply_mask?: boolean;
    /**
     * Box Prompts
     * @description List of box prompt coordinates (x_min, y_min, x_max, y_max).
     * @default []
     */
    box_prompts?: Components.BoxPromptBase[];
    /**
     * Detection Threshold
     * @description Detection confidence threshold (0.0-1.0). Lower = more detections but less precise.
     * @default 0.5
     */
    detection_threshold?: number;
    /**
     * Point Prompts
     * @description List of point prompts
     * @default []
     */
    point_prompts?: Components.PointPromptBase[];
    /**
     * Prompt
     * @description Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth').
     * @default
     * @example person
     * @example person, cloth
     */
    prompt?: string;
    /**
     * Text Prompt
     * @deprecated
     * @description [DEPRECATED] Use 'prompt' instead. Kept for backward compatibility.
     */
    text_prompt?: string;
    /**
     * Video Url
     * @description The URL of the video to be segmented.
     */
    video_url: string;
}

export interface Sam3VideoOutput extends SharedType_d19 {}

export interface Sam3ImageEmbedInput {
    /**
     * Image Url
     * @description URL of the image to embed.
     * @example https://raw.githubusercontent.com/facebookresearch/segment-anything-2/main/notebooks/images/truck.jpg
     */
    image_url: string;
}

export interface Sam3ImageEmbedOutput {
    /**
     * Embedding B64
     * @description Embedding of the image
     */
    embedding_b64: string;
}

export interface Sam3ImageRleInput extends SharedType_ef5 {}

export interface Sam3ImageRleOutput {
    /**
     * Boundingbox Frames Zip
     * @description Zip file containing per-frame bounding box overlays.
     */
    boundingbox_frames_zip?: Components.File;
    /**
     * Boxes
     * @description Per-mask normalized bounding boxes [cx, cy, w, h] when requested.
     */
    boxes?: number[][];
    /**
     * Metadata
     * @description Per-mask metadata when multiple RLEs are returned.
     */
    metadata?: Components.MaskMetadata[];
    /**
     * Rle
     * @description Run Length Encoding of the mask.
     */
    rle: string | string[];
    /**
     * Scores
     * @description Per-mask confidence scores when requested.
     */
    scores?: number[];
}

export interface Sam3ImageInput extends SharedType_ef5 {}

export interface Sam3ImageOutput {
    /**
     * Boxes
     * @description Per-mask normalized bounding boxes [cx, cy, w, h] when requested.
     */
    boxes?: number[][];
    /**
     * Image
     * @description Primary segmented mask preview.
     */
    image?: Components.Image;
    /**
     * Masks
     * @description Segmented mask images.
     */
    masks: Components.Image[];
    /**
     * Metadata
     * @description Per-mask metadata including scores and boxes.
     */
    metadata?: Components.MaskMetadata[];
    /**
     * Scores
     * @description Per-mask confidence scores when requested.
     */
    scores?: number[];
}

export interface Sam33dObjectsInput {
    /**
     * Box Prompts
     * @description Box prompts for auto-segmentation when no masks provided. Multiple boxes supported - each produces a separate object mask for 3D reconstruction.
     * @default []
     */
    box_prompts?: Components.BoxPromptBase[];
    /**
     * Export Textured Glb
     * @description If True, exports GLB with baked texture and UVs instead of vertex colors.
     * @default false
     */
    export_textured_glb?: boolean;
    /**
     * Image Url
     * @description URL of the image to reconstruct in 3D
     * @example https://v3b.fal.media/files/b/0a8439e5/TyAmfW5w_sqRXRzWVBGsW_car.jpeg
     */
    image_url: string;
    /**
     * Mask Urls
     * @description Optional list of mask URLs (one per object). If not provided, use prompt/point_prompts/box_prompts to auto-segment, or entire image will be used.
     */
    mask_urls?: string[];
    /**
     * Point Prompts
     * @description Point prompts for auto-segmentation when no masks provided
     * @default []
     */
    point_prompts?: Components.PointPromptBase[];
    /**
     * Pointmap Url
     * @description Optional URL to external pointmap/depth data (NPY or NPZ format) for improved 3D reconstruction depth estimation
     */
    pointmap_url?: string;
    /**
     * Prompt
     * @description Text prompt for auto-segmentation when no masks provided (e.g., 'chair', 'lamp')
     * @default car
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility
     */
    seed?: number;
}

export interface Sam33dObjectsOutput {
    /**
     * Artifacts Zip
     * @description Zip bundle containing all artifacts and metadata
     */
    artifacts_zip?: Components.File;
    /**
     * Gaussian Splat
     * @description Gaussian splat file (.ply) - combined scene splat for multi-object, single splat otherwise
     */
    gaussian_splat: Components.File;
    /**
     * Individual Glbs
     * @description Individual GLB mesh files per object (only for multi-object scenes)
     */
    individual_glbs?: Components.File[];
    /**
     * Individual Splats
     * @description Individual Gaussian splat files per object (only for multi-object scenes)
     */
    individual_splats?: Components.File[];
    /**
     * Metadata
     * @description Per-object metadata (rotation/translation/scale)
     */
    metadata: Components.SAM3DObjectMetadata[];
    /**
     * Model Glb
     * @description 3D mesh in GLB format - combined scene for multi-object, single mesh otherwise
     * @default https://v3b.fal.media/files/b/0a8439e7/mqHMt17hzqDaqVMF7q0dB_combined_scene.glb
     */
    model_glb?: Components.File;
}

export interface Sam33dBodyInput {
    /**
     * Export Meshes
     * @description Export individual mesh files (.ply) per person
     * @default true
     */
    export_meshes?: boolean;
    /**
     * Image Url
     * @description URL of the image containing humans
     * @example https://v3b.fal.media/files/b/0a8439f8/E8gEXWsl2C-Euo4dGayzi_An_zyCCnSaytVklh_99sSYt4Z4Hh5e3s7VnNlx5JfN5KuC0j_bnq1AP9JfRoAmOQz5TP0DdCYMk4796Gloe5no1vvpoqhD-p3kE.jpeg
     */
    image_url: string;
    /**
     * Include 3D Keypoints
     * @description Include 3D keypoint markers (spheres) in the GLB mesh for visualization
     * @default true
     */
    include_3d_keypoints?: boolean;
    /**
     * Mask Url
     * @description Optional URL of a binary mask image (white=person, black=background). When provided, skips auto human detection and uses this mask instead. Bbox is auto-computed from the mask.
     */
    mask_url?: string;
}

export interface Sam33dBodyOutput {
    /**
     * Meshes
     * @description Individual mesh files (.ply), one per detected person (when export_meshes=True)
     */
    meshes?: Components.File[];
    /**
     * Metadata
     * @description Structured metadata including keypoints and camera parameters
     */
    metadata: Components.SAM3DBodyMetadata;
    /**
     * Model Glb
     * @description 3D body mesh in GLB format with optional 3D keypoint markers
     * @example https://v3b.fal.media/files/b/0a8439f9/5LVt3C2YesqnQzg-CxPpu_combined_bodies.glb
     */
    model_glb: Components.File;
    /**
     * Visualization
     * @description Combined visualization image (original + keypoints + mesh + side view)
     */
    visualization: Components.File;
}

export interface Sam33dAlignInput {
    /**
     * Body Mask Url
     * @description URL of the human mask image. If not provided, uses full image.
     */
    body_mask_url?: string;
    /**
     * Body Mesh Url
     * @description URL of the SAM-3D Body mesh file (.ply or .glb) to align
     */
    body_mesh_url: string;
    /**
     * Focal Length
     * @description Focal length from SAM-3D Body metadata. If not provided, estimated from MoGe.
     */
    focal_length?: number;
    /**
     * Image Url
     * @description URL of the original image used for MoGe depth estimation
     */
    image_url: string;
    /**
     * Object Mesh Url
     * @description Optional URL of SAM-3D Object mesh (.glb) to create combined scene
     */
    object_mesh_url?: string;
}

export interface Sam33dAlignOutput {
    /**
     * Body Mesh Ply
     * @description Aligned body mesh in PLY format
     */
    body_mesh_ply: Components.File;
    /**
     * Metadata
     * @description Alignment info (scale, translation, etc.)
     */
    metadata: Components.SAM3DBodyAlignmentInfo;
    /**
     * Model Glb
     * @description Aligned body mesh in GLB format (for 3D preview)
     */
    model_glb: Components.File;
    /**
     * Scene Glb
     * @description Combined scene with body + object meshes in GLB format (only when object_mesh_url provided)
     */
    scene_glb?: Components.File;
    /**
     * Visualization
     * @description Visualization of aligned mesh overlaid on input image
     */
    visualization: Components.File;
}

export interface SadtalkerReferenceInput {
    /**
     * Driven Audio Url
     * @description URL of the driven audio
     * @example https://storage.googleapis.com/falserverless/model_tests/sadtalker/deyu.wav
     */
    driven_audio_url: string;
    /**
     * Expression Scale
     * @description The scale of the expression
     * @default 1
     */
    expression_scale?: number;
    /**
     * Face Enhancer
     * @description The type of face enhancer to use
     * @example null
     * @enum {string}
     */
    face_enhancer?: 'gfpgan';
    /**
     * Face Model Resolution
     * @description The resolution of the face model
     * @default 256
     * @enum {string}
     */
    face_model_resolution?: '256' | '512';
    /**
     * Pose Style
     * @description The style of the pose
     * @default 0
     */
    pose_style?: number;
    /**
     * Preprocess
     * @description The type of preprocessing to use
     * @default crop
     * @enum {string}
     */
    preprocess?: 'crop' | 'extcrop' | 'resize' | 'full' | 'extfull';
    /**
     * Reference Pose Video Url
     * @description URL of the reference video
     * @example https://github.com/OpenTalker/SadTalker/raw/main/examples/ref_video/WDA_AlexandriaOcasioCortez_000.mp4
     */
    reference_pose_video_url: string;
    /**
     * Source Image Url
     * @description URL of the source image
     * @example https://storage.googleapis.com/falserverless/model_tests/sadtalker/anime_girl.png
     */
    source_image_url: string;
    /**
     * Still Mode
     * @description Whether to use still mode. Fewer head motion, works with preprocess `full`.
     * @default false
     */
    still_mode?: boolean;
}

export interface SadtalkerReferenceOutput extends SharedType_f12 {}

export interface SadtalkerInput {
    /**
     * Driven Audio Url
     * @description URL of the driven audio
     * @example https://storage.googleapis.com/falserverless/model_tests/sadtalker/deyu.wav
     */
    driven_audio_url: string;
    /**
     * Expression Scale
     * @description The scale of the expression
     * @default 1
     */
    expression_scale?: number;
    /**
     * Face Enhancer
     * @description The type of face enhancer to use
     * @example null
     * @enum {string}
     */
    face_enhancer?: 'gfpgan';
    /**
     * Face Model Resolution
     * @description The resolution of the face model
     * @default 256
     * @enum {string}
     */
    face_model_resolution?: '256' | '512';
    /**
     * Pose Style
     * @description The style of the pose
     * @default 0
     */
    pose_style?: number;
    /**
     * Preprocess
     * @description The type of preprocessing to use
     * @default crop
     * @enum {string}
     */
    preprocess?: 'crop' | 'extcrop' | 'resize' | 'full' | 'extfull';
    /**
     * Source Image Url
     * @description URL of the source image
     * @example https://storage.googleapis.com/falserverless/model_tests/sadtalker/anime_girl.png
     */
    source_image_url: string;
    /**
     * Still Mode
     * @description Whether to use still mode. Fewer head motion, works with preprocess `full`.
     * @default false
     */
    still_mode?: boolean;
}

export interface SadtalkerOutput extends SharedType_f12 {}

export interface Sa2va8bVideoInput extends SharedType_5f5 {}

export interface Sa2va8bVideoOutput extends SharedType_800 {}

export interface Sa2va8bImageInput extends SharedType_f51 {}

export interface Sa2va8bImageOutput extends SharedType_fda {}

export interface Sa2va4bVideoInput extends SharedType_5f5 {}

export interface Sa2va4bVideoOutput extends SharedType_800 {}

export interface Sa2va4bImageInput extends SharedType_f51 {}

export interface Sa2va4bImageOutput extends SharedType_fda {}

export interface RifeVideoInput {
    /**
     * Frames Per Second
     * @description Frames per second for the output video. Only applicable if use_calculated_fps is False.
     * @default 8
     */
    fps?: number;
    /**
     * Loop
     * @description If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.
     * @default false
     */
    loop?: boolean;
    /**
     * Number of Frames
     * @description The number of frames to generate between the input video frames.
     * @default 1
     */
    num_frames?: number;
    /**
     * Use Calculated FPS
     * @description If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used.
     * @default true
     */
    use_calculated_fps?: boolean;
    /**
     * Use Scene Detection
     * @description If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.
     * @default false
     */
    use_scene_detection?: boolean;
    /**
     * Video URL
     * @description The URL of the video to use for interpolation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/interpolation-video-input.mp4
     */
    video_url: string;
}

export interface RifeVideoOutput {
    /**
     * Video
     * @description The generated video file with interpolated frames.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/rife-video-output.mp4"
     *     }
     */
    video: Components.File;
}

export interface RifeInput {
    /**
     * End Image URL
     * @description The URL of the second image to use as the ending point for interpolation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/interpolate-end-frame.png
     */
    end_image_url: string;
    /**
     * Frames Per Second
     * @description Frames per second for the output video. Only applicable if output_type is 'video'.
     * @default 8
     */
    fps?: number;
    /**
     * Include End
     * @description Whether to include the end image in the output.
     * @default false
     */
    include_end?: boolean;
    /**
     * Include Start
     * @description Whether to include the start image in the output.
     * @default false
     */
    include_start?: boolean;
    /**
     * Number of Frames
     * @description The number of frames to generate between the input images.
     * @default 1
     */
    num_frames?: number;
    /**
     * Output Format
     * @description The format of the output images. Only applicable if output_type is 'images'.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg';
    /**
     * Output Type
     * @description The type of output to generate; either individual images or a video.
     * @default images
     * @enum {string}
     */
    output_type?: 'images' | 'video';
    /**
     * Start Image URL
     * @description The URL of the first image to use as the starting point for interpolation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/interpolate-start-frame.png
     */
    start_image_url: string;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface RifeOutput {
    /**
     * Images
     * @description The generated frames as individual images.
     * @default []
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/rife-mid-frame.jpeg"
     *       }
     *     ]
     */
    images?: Components.Image[];
    /**
     * Video
     * @description The generated video file, if output_type is 'video'.
     */
    video?: Components.File;
}

export interface ReveTextToImageInput {
    /**
     * Aspect Ratio
     * @description The desired aspect ratio of the generated image.
     * @default 3:2
     * @example 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '3:2' | '2:3' | '4:3' | '3:4' | '1:1';
    /**
     * Number of Images
     * @description Number of images to generate
     * @default 1
     * @example 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description Output format for the generated image.
     * @default png
     * @example png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description The text description of the desired image.
     * @example A serene mountain landscape at sunset with snow-capped peaks
     */
    prompt: string;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ReveTextToImageOutput {
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/panda/-WnGcaJCtfrT6Q2oms97E.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface ReveRemixInput extends SharedType_7a3 {}

export interface ReveRemixOutput extends SharedType_36a {}

export interface ReveFastRemixInput extends SharedType_7a3 {}

export interface ReveFastRemixOutput extends SharedType_36a {}

export interface ReveFastEditInput {
    /**
     * Reference Image URL
     * @description URL of the reference image to edit. Must be publicly accessible or base64 data URI. Supports PNG, JPEG, WebP, AVIF, and HEIF formats.
     * @example https://v3b.fal.media/files/b/rabbit/Wi1oWbMfigpUMP0w_i5fm_-WnGcaJCtfrT6Q2oms97E.png
     */
    image_url: string;
    /**
     * Number of Images
     * @description Number of images to generate
     * @default 1
     * @example 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description Output format for the generated image.
     * @default png
     * @example png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description The text description of how to edit the provided image.
     * @example Make it nighttime with stars glistening behind the mountain
     */
    prompt: string;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ReveFastEditOutput {
    /**
     * Images
     * @description The edited images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/zebra/eTDkfnubKKq9S-hDxvH2g.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface ReveEditInput {
    /**
     * Reference Image URL
     * @description URL of the reference image to edit. Must be publicly accessible or base64 data URI. Supports PNG, JPEG, WebP, AVIF, and HEIF formats.
     * @example https://v3b.fal.media/files/b/koala/sZE6zNTKjOKc4kcUdVlu__26bac54c-3e94-43e9-aeff-f2efc2631ef0.webp
     */
    image_url: string;
    /**
     * Number of Images
     * @description Number of images to generate
     * @default 1
     * @example 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description Output format for the generated image.
     * @default png
     * @example png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description The text description of how to edit the provided image.
     * @example Give him a friend
     */
    prompt: string;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ReveEditOutput {
    /**
     * Images
     * @description The edited images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/tiger/4mt5HxYSH-YIE3vhqV8L9.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface RetoucherInput {
    /**
     * Image Url
     * @description The URL of the image to be retouched.
     * @example https://storage.googleapis.com/falserverless/model_tests/retoucher/Dalton-Meereskosmetik-Magazin-Pickelguide-Model_1.resized.jpg
     */
    image_url: string;
    /**
     * Seed
     * @description Seed for reproducibility. Different seeds will make slightly different results.
     */
    seed?: number;
}

export interface RetoucherOutput {
    /**
     * Image
     * @description The generated image file info.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/retoucher/retoucher_example_output.png"
     *     }
     */
    image: Components.Image;
    /**
     * Seed
     * @description The seed used for the generation.
     */
    seed: number;
}

export interface RecraftVectorizeInput {
    /**
     * Image Url
     * @description The URL of the image to be vectorized. Must be in PNG, JPG or WEBP format, less than 5 MB in size, have resolution less than 16 MP and max dimension less than 4096 pixels, min dimension more than 256 pixels.
     * @example https://storage.googleapis.com/falserverless/example_inputs/man_wave.png
     */
    image_url: string;
}

export interface RecraftVectorizeOutput {
    /**
     * Image
     * @description The vectorized image.
     * @example {
     *       "file_size": 85336,
     *       "file_name": "image.svg",
     *       "content_type": "image/svg+xml",
     *       "url": "https://v3.fal.media/files/koala/pUQbC18DsP4KxcIBA53y2_image.svg"
     *     }
     */
    image: Components.File;
}

export interface RecraftV3TextToImageInput {
    /**
     * Colors
     * @description An array of preferable colors
     * @default []
     */
    colors?: Components.RGBColor[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Prompt
     * @example a red panda eating a bamboo in front of a poster that says "recraft V3 now available at fal"
     */
    prompt: string;
    /**
     * Style
     * @description The style of the generated images. Vector images cost 2X as much.
     * @default realistic_image
     * @enum {string}
     */
    style?:
        | 'any'
        | 'realistic_image'
        | 'digital_illustration'
        | 'vector_illustration'
        | 'realistic_image/b_and_w'
        | 'realistic_image/hard_flash'
        | 'realistic_image/hdr'
        | 'realistic_image/natural_light'
        | 'realistic_image/studio_portrait'
        | 'realistic_image/enterprise'
        | 'realistic_image/motion_blur'
        | 'realistic_image/evening_light'
        | 'realistic_image/faded_nostalgia'
        | 'realistic_image/forest_life'
        | 'realistic_image/mystic_naturalism'
        | 'realistic_image/natural_tones'
        | 'realistic_image/organic_calm'
        | 'realistic_image/real_life_glow'
        | 'realistic_image/retro_realism'
        | 'realistic_image/retro_snapshot'
        | 'realistic_image/urban_drama'
        | 'realistic_image/village_realism'
        | 'realistic_image/warm_folk'
        | 'digital_illustration/pixel_art'
        | 'digital_illustration/hand_drawn'
        | 'digital_illustration/grain'
        | 'digital_illustration/infantile_sketch'
        | 'digital_illustration/2d_art_poster'
        | 'digital_illustration/handmade_3d'
        | 'digital_illustration/hand_drawn_outline'
        | 'digital_illustration/engraving_color'
        | 'digital_illustration/2d_art_poster_2'
        | 'digital_illustration/antiquarian'
        | 'digital_illustration/bold_fantasy'
        | 'digital_illustration/child_book'
        | 'digital_illustration/child_books'
        | 'digital_illustration/cover'
        | 'digital_illustration/crosshatch'
        | 'digital_illustration/digital_engraving'
        | 'digital_illustration/expressionism'
        | 'digital_illustration/freehand_details'
        | 'digital_illustration/grain_20'
        | 'digital_illustration/graphic_intensity'
        | 'digital_illustration/hard_comics'
        | 'digital_illustration/long_shadow'
        | 'digital_illustration/modern_folk'
        | 'digital_illustration/multicolor'
        | 'digital_illustration/neon_calm'
        | 'digital_illustration/noir'
        | 'digital_illustration/nostalgic_pastel'
        | 'digital_illustration/outline_details'
        | 'digital_illustration/pastel_gradient'
        | 'digital_illustration/pastel_sketch'
        | 'digital_illustration/pop_art'
        | 'digital_illustration/pop_renaissance'
        | 'digital_illustration/street_art'
        | 'digital_illustration/tablet_sketch'
        | 'digital_illustration/urban_glow'
        | 'digital_illustration/urban_sketching'
        | 'digital_illustration/vanilla_dreams'
        | 'digital_illustration/young_adult_book'
        | 'digital_illustration/young_adult_book_2'
        | 'vector_illustration/bold_stroke'
        | 'vector_illustration/chemistry'
        | 'vector_illustration/colored_stencil'
        | 'vector_illustration/contour_pop_art'
        | 'vector_illustration/cosmics'
        | 'vector_illustration/cutout'
        | 'vector_illustration/depressive'
        | 'vector_illustration/editorial'
        | 'vector_illustration/emotional_flat'
        | 'vector_illustration/infographical'
        | 'vector_illustration/marker_outline'
        | 'vector_illustration/mosaic'
        | 'vector_illustration/naivector'
        | 'vector_illustration/roundish_flat'
        | 'vector_illustration/segmented_colors'
        | 'vector_illustration/sharp_contrast'
        | 'vector_illustration/thin'
        | 'vector_illustration/vector_photo'
        | 'vector_illustration/vivid_shapes'
        | 'vector_illustration/engraving'
        | 'vector_illustration/line_art'
        | 'vector_illustration/line_circuit'
        | 'vector_illustration/linocut';
    /**
     * Style Id
     * Format: uuid4
     * @description The ID of the custom style reference (optional)
     */
    style_id?: string;
}

export interface RecraftV3TextToImageOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/penguin/852yy3l5DGLmrwAK42RTB_image.webp"
     *       }
     *     ]
     */
    images: Components.File[];
}

export interface RecraftV3ImageToImageInput {
    /**
     * Colors
     * @description An array of preferable colors
     * @default []
     */
    colors?: Components.RGBColor[];
    /**
     * Image Url
     * @description The URL of the image to modify. Must be less than 5 MB in size, have resolution less than 16 MP and max dimension less than 4096 pixels.
     * @example https://storage.googleapis.com/falserverless/model_tests/recraft/recraft-upscaler-1.jpeg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description A text description of undesired elements on an image
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description A text description of areas to change.
     * @example winter
     * @example cyberpunk city
     * @example watercolor painting style
     */
    prompt: string;
    /**
     * Strength
     * @description Defines the difference with the original image, should lie in [0, 1], where 0 means almost identical, and 1 means miserable similarity
     * @default 0.5
     * @example 0.2
     * @example 0.5
     * @example 0.8
     */
    strength?: number;
    /**
     * Style
     * @description The style of the generated images. Vector images cost 2X as much.
     * @default realistic_image
     * @enum {string}
     */
    style?:
        | 'any'
        | 'realistic_image'
        | 'digital_illustration'
        | 'vector_illustration'
        | 'realistic_image/b_and_w'
        | 'realistic_image/hard_flash'
        | 'realistic_image/hdr'
        | 'realistic_image/natural_light'
        | 'realistic_image/studio_portrait'
        | 'realistic_image/enterprise'
        | 'realistic_image/motion_blur'
        | 'realistic_image/evening_light'
        | 'realistic_image/faded_nostalgia'
        | 'realistic_image/forest_life'
        | 'realistic_image/mystic_naturalism'
        | 'realistic_image/natural_tones'
        | 'realistic_image/organic_calm'
        | 'realistic_image/real_life_glow'
        | 'realistic_image/retro_realism'
        | 'realistic_image/retro_snapshot'
        | 'realistic_image/urban_drama'
        | 'realistic_image/village_realism'
        | 'realistic_image/warm_folk'
        | 'digital_illustration/pixel_art'
        | 'digital_illustration/hand_drawn'
        | 'digital_illustration/grain'
        | 'digital_illustration/infantile_sketch'
        | 'digital_illustration/2d_art_poster'
        | 'digital_illustration/handmade_3d'
        | 'digital_illustration/hand_drawn_outline'
        | 'digital_illustration/engraving_color'
        | 'digital_illustration/2d_art_poster_2'
        | 'digital_illustration/antiquarian'
        | 'digital_illustration/bold_fantasy'
        | 'digital_illustration/child_book'
        | 'digital_illustration/child_books'
        | 'digital_illustration/cover'
        | 'digital_illustration/crosshatch'
        | 'digital_illustration/digital_engraving'
        | 'digital_illustration/expressionism'
        | 'digital_illustration/freehand_details'
        | 'digital_illustration/grain_20'
        | 'digital_illustration/graphic_intensity'
        | 'digital_illustration/hard_comics'
        | 'digital_illustration/long_shadow'
        | 'digital_illustration/modern_folk'
        | 'digital_illustration/multicolor'
        | 'digital_illustration/neon_calm'
        | 'digital_illustration/noir'
        | 'digital_illustration/nostalgic_pastel'
        | 'digital_illustration/outline_details'
        | 'digital_illustration/pastel_gradient'
        | 'digital_illustration/pastel_sketch'
        | 'digital_illustration/pop_art'
        | 'digital_illustration/pop_renaissance'
        | 'digital_illustration/street_art'
        | 'digital_illustration/tablet_sketch'
        | 'digital_illustration/urban_glow'
        | 'digital_illustration/urban_sketching'
        | 'digital_illustration/vanilla_dreams'
        | 'digital_illustration/young_adult_book'
        | 'digital_illustration/young_adult_book_2'
        | 'vector_illustration/bold_stroke'
        | 'vector_illustration/chemistry'
        | 'vector_illustration/colored_stencil'
        | 'vector_illustration/contour_pop_art'
        | 'vector_illustration/cosmics'
        | 'vector_illustration/cutout'
        | 'vector_illustration/depressive'
        | 'vector_illustration/editorial'
        | 'vector_illustration/emotional_flat'
        | 'vector_illustration/infographical'
        | 'vector_illustration/marker_outline'
        | 'vector_illustration/mosaic'
        | 'vector_illustration/naivector'
        | 'vector_illustration/roundish_flat'
        | 'vector_illustration/segmented_colors'
        | 'vector_illustration/sharp_contrast'
        | 'vector_illustration/thin'
        | 'vector_illustration/vector_photo'
        | 'vector_illustration/vivid_shapes'
        | 'vector_illustration/engraving'
        | 'vector_illustration/line_art'
        | 'vector_illustration/line_circuit'
        | 'vector_illustration/linocut';
    /**
     * Style Id
     * Format: uuid4
     * @description The ID of the custom style reference (optional)
     * @example null
     */
    style_id?: string;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface RecraftV3ImageToImageOutput {
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/koala/Xoz8tel7YoTbh6Fiepmq3_image.webp"
     *       }
     *     ]
     */
    images: Components.File[];
}

export interface RecraftV3CreateStyleInput {
    /**
     * Base Style
     * @description The base style of the generated images, this topic is covered above.
     * @default digital_illustration
     * @enum {string}
     */
    base_style?:
        | 'any'
        | 'realistic_image'
        | 'digital_illustration'
        | 'vector_illustration'
        | 'realistic_image/b_and_w'
        | 'realistic_image/hard_flash'
        | 'realistic_image/hdr'
        | 'realistic_image/natural_light'
        | 'realistic_image/studio_portrait'
        | 'realistic_image/enterprise'
        | 'realistic_image/motion_blur'
        | 'realistic_image/evening_light'
        | 'realistic_image/faded_nostalgia'
        | 'realistic_image/forest_life'
        | 'realistic_image/mystic_naturalism'
        | 'realistic_image/natural_tones'
        | 'realistic_image/organic_calm'
        | 'realistic_image/real_life_glow'
        | 'realistic_image/retro_realism'
        | 'realistic_image/retro_snapshot'
        | 'realistic_image/urban_drama'
        | 'realistic_image/village_realism'
        | 'realistic_image/warm_folk'
        | 'digital_illustration/pixel_art'
        | 'digital_illustration/hand_drawn'
        | 'digital_illustration/grain'
        | 'digital_illustration/infantile_sketch'
        | 'digital_illustration/2d_art_poster'
        | 'digital_illustration/handmade_3d'
        | 'digital_illustration/hand_drawn_outline'
        | 'digital_illustration/engraving_color'
        | 'digital_illustration/2d_art_poster_2'
        | 'digital_illustration/antiquarian'
        | 'digital_illustration/bold_fantasy'
        | 'digital_illustration/child_book'
        | 'digital_illustration/child_books'
        | 'digital_illustration/cover'
        | 'digital_illustration/crosshatch'
        | 'digital_illustration/digital_engraving'
        | 'digital_illustration/expressionism'
        | 'digital_illustration/freehand_details'
        | 'digital_illustration/grain_20'
        | 'digital_illustration/graphic_intensity'
        | 'digital_illustration/hard_comics'
        | 'digital_illustration/long_shadow'
        | 'digital_illustration/modern_folk'
        | 'digital_illustration/multicolor'
        | 'digital_illustration/neon_calm'
        | 'digital_illustration/noir'
        | 'digital_illustration/nostalgic_pastel'
        | 'digital_illustration/outline_details'
        | 'digital_illustration/pastel_gradient'
        | 'digital_illustration/pastel_sketch'
        | 'digital_illustration/pop_art'
        | 'digital_illustration/pop_renaissance'
        | 'digital_illustration/street_art'
        | 'digital_illustration/tablet_sketch'
        | 'digital_illustration/urban_glow'
        | 'digital_illustration/urban_sketching'
        | 'digital_illustration/vanilla_dreams'
        | 'digital_illustration/young_adult_book'
        | 'digital_illustration/young_adult_book_2'
        | 'vector_illustration/bold_stroke'
        | 'vector_illustration/chemistry'
        | 'vector_illustration/colored_stencil'
        | 'vector_illustration/contour_pop_art'
        | 'vector_illustration/cosmics'
        | 'vector_illustration/cutout'
        | 'vector_illustration/depressive'
        | 'vector_illustration/editorial'
        | 'vector_illustration/emotional_flat'
        | 'vector_illustration/infographical'
        | 'vector_illustration/marker_outline'
        | 'vector_illustration/mosaic'
        | 'vector_illustration/naivector'
        | 'vector_illustration/roundish_flat'
        | 'vector_illustration/segmented_colors'
        | 'vector_illustration/sharp_contrast'
        | 'vector_illustration/thin'
        | 'vector_illustration/vector_photo'
        | 'vector_illustration/vivid_shapes'
        | 'vector_illustration/engraving'
        | 'vector_illustration/line_art'
        | 'vector_illustration/line_circuit'
        | 'vector_illustration/linocut';
    /**
     * Images Data Url
     * @description URL to zip archive with images, use PNG format. Maximum 5 images are allowed.
     */
    images_data_url: string;
}

export interface RecraftV3CreateStyleOutput {
    /**
     * Style Id
     * Format: uuid4
     * @description The ID of the created style, this ID can be used to reference the style in the future.
     */
    style_id: string;
}

export interface RecraftUpscaleCrispInput extends SharedType_4a0 {}

export interface RecraftUpscaleCrispOutput extends SharedType_df4 {}

export interface RecraftUpscaleCreativeInput extends SharedType_4a0 {}

export interface RecraftUpscaleCreativeOutput extends SharedType_df4 {}

export interface Recraft20bInput {
    /**
     * Colors
     * @description An array of preferable colors
     * @default []
     */
    colors?: Components.RGBColor[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Prompt
     * @example a red panda in Kyoto
     */
    prompt: string;
    /**
     * Style
     * @description The style of the generated images. Vector images cost 2X as much.
     * @default realistic_image
     * @enum {string}
     */
    style?:
        | 'any'
        | 'realistic_image'
        | 'digital_illustration'
        | 'vector_illustration'
        | 'realistic_image/b_and_w'
        | 'realistic_image/enterprise'
        | 'realistic_image/hard_flash'
        | 'realistic_image/hdr'
        | 'realistic_image/motion_blur'
        | 'realistic_image/natural_light'
        | 'realistic_image/studio_portrait'
        | 'digital_illustration/2d_art_poster'
        | 'digital_illustration/2d_art_poster_2'
        | 'digital_illustration/3d'
        | 'digital_illustration/80s'
        | 'digital_illustration/engraving_color'
        | 'digital_illustration/glow'
        | 'digital_illustration/grain'
        | 'digital_illustration/hand_drawn'
        | 'digital_illustration/hand_drawn_outline'
        | 'digital_illustration/handmade_3d'
        | 'digital_illustration/infantile_sketch'
        | 'digital_illustration/kawaii'
        | 'digital_illustration/pixel_art'
        | 'digital_illustration/psychedelic'
        | 'digital_illustration/seamless'
        | 'digital_illustration/voxel'
        | 'digital_illustration/watercolor'
        | 'vector_illustration/cartoon'
        | 'vector_illustration/doodle_line_art'
        | 'vector_illustration/engraving'
        | 'vector_illustration/flat_2'
        | 'vector_illustration/kawaii'
        | 'vector_illustration/line_art'
        | 'vector_illustration/line_circuit'
        | 'vector_illustration/linocut'
        | 'vector_illustration/seamless';
    /**
     * Style Id
     * Format: uuid4
     * @description The ID of the custom style reference (optional)
     */
    style_id?: string;
}

export interface Recraft20bOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/tiger/qeO5RlXiAsdCREUMYg5ZU_image.webp"
     *       }
     *     ]
     */
    images: Components.File[];
}

export interface RealisticVisionInput {
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance Rescale
     * @description The rescale factor for the CFG.
     * @default 0
     */
    guidance_rescale?: number;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @default {
     *       "height": 1024,
     *       "width": 1024
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description The list of LoRA weights to use.
     * @default []
     */
    loras?: Components.LoraWeight_1[];
    /**
     * Model Name
     * @description The Realistic Vision model to use.
     * @example SG161222/Realistic_Vision_V6.0_B1_noVAE
     * @example SG161222/RealVisXL_V4.0
     */
    model_name?: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want in the image.
     * @default (worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed, grayscale, bw, bad photo, bad photography, bad art:1.4), (watermark, signature, text font, username, error, logo, words, letters, digits, autograph, trademark, name:1.2), (blur, blurry, grainy), morbid, ugly, asymmetrical, mutated malformed, mutilated, poorly lit, bad shadow, draft, cropped, out of frame, cut off, censored, jpeg artifacts, out of focus, glitch, duplicate, (airbrushed, cartoon, anime, semi-realistic, cgi, render, blender, digital art, manga, amateur:1.3), (3D ,3D Game, 3D Game Scene, 3D Character:1.1), (bad hands, bad anatomy, bad body, bad face, bad teeth, bad arms, bad legs, deformities:1.3)
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 35
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example A hyperdetailed photograph of a Cat dressed as a mafia boss holding a fish walking down a Japanese fish market with an angry face, 8k resolution, best quality, beautiful photograph, dynamic lighting,
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface RealisticVisionOutput extends SharedType_a73 {}

export interface QwenImageImageToImageInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation. Options: 'none', 'regular', 'high'. Higher acceleration increases speed. 'regular' balances speed and quality. 'high' is recommended for images without text.
     * @default none
     * @example none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. By default, we will use the provided image for determining the image_size.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description The reference image to guide the generation.
     * @example https://v3.fal.media/files/rabbit/KoIbq6nhDBDPxDQrivW-m.png
     */
    image_url: string;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use up to 3 LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example blurry, ugly
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate the image with
     * @example Mount Fuji with purple japanese wisteria in the foreground, clear sky, peaceful spring day, soft natural light, landscape, painted with oil brush on a wood panel with abstract mixed colors
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description Denoising strength. 1.0 = fully remake; 0.0 = preserve original.
     * @default 0.6
     * @example 0.8
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Turbo
     * @description Enable turbo mode for faster generation with high quality. When enabled, uses optimized settings (10 steps, CFG=1.2).
     * @default false
     * @example true
     */
    use_turbo?: boolean;
}

export interface QwenImageImageToImageOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/jpeg",
     *         "url": "https://v3.fal.media/files/elephant/AuLvZGaYemu6vs36D5zof.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface QwenImageTrainerInput {
    /**
     * Image Data Url
     * @description URL to zip archive with images for training. The archive should contain images and corresponding text files with captions.
     *             Each text file should have the same name as the image file it corresponds to (e.g., image1.jpg and image1.txt).
     *             If text files are missing for some images, you can provide a trigger_phrase to automatically create them.
     *             Supported image formats: PNG, JPG, JPEG, WEBP.
     *             Try to use at least 10 images, although more is better.
     */
    image_data_url: string;
    /**
     * Learning Rate
     * @description Learning rate for training. Default is 5e-4
     * @default 0.0005
     */
    learning_rate?: number;
    /**
     * Steps
     * @description Total number of training steps to perform. Default is 4000.
     * @default 1000
     */
    steps?: number;
    /**
     * Trigger Phrase
     * @description Default caption to use for images that don't have corresponding text files. If provided, missing .txt files will be created automatically.
     * @default
     */
    trigger_phrase?: string;
}

export interface QwenImageTrainerOutput {
    /**
     * Config File
     * @description URL to the training configuration file.
     */
    config_file: Components.File;
    /**
     * Lora File
     * @description URL to the trained LoRA weights file.
     */
    lora_file: Components.File;
}

export interface QwenImageMaxTextToImageInput {
    /**
     * Enable Prompt Expansion
     * @description Enable LLM prompt optimization for better results.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Enable content moderation for input and output.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description Content to avoid in the generated image. Max 500 characters.
     * @default
     * @example low resolution, error, worst quality, low quality, deformed
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description Text prompt describing the desired image. Supports Chinese and English. Max 800 characters.
     * @example A cute baby koala holding a card says "Qwen Image Max is now available on fal" while on a water slider
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility (0-2147483647).
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface QwenImageMaxTextToImageOutput {
    /**
     * Images
     * @description Generated images.
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a8bf02b/bKyzcdmA_kYq4ru5s9H7k_sQ893ydD.png"
     *       }
     *     ]
     */
    images: Components.File[];
    /**
     * Seed
     * @description The seed used for generation
     * @example 42
     */
    seed: number;
}

export interface QwenImageMaxEditInput {
    /**
     * Enable Prompt Expansion
     * @description Enable LLM prompt optimization for better results.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Enable content moderation for input and output.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the final input image will be used.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Urls
     * @description Reference images for editing (1-3 images required). Order matters: reference as 'image 1', 'image 2', 'image 3' in prompt. Resolution: 384-5000px each dimension. Max size: 10MB each. Formats: JPEG, JPG, PNG (no alpha), WEBP.
     * @example [
     *       "https://v3b.fal.media/files/b/0a8bf01f/TUehdcnBygWa1-SnEbp3K_image_123.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Negative Prompt
     * @description Content to avoid in the generated image. Max 500 characters.
     * @default
     * @example low resolution, error, worst quality, low quality, deformed
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description Text prompt describing the desired image. Supports Chinese and English. Max 800 characters.
     * @example Change wolves with extremely realistic small puppies of different colors.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility (0-2147483647).
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface QwenImageMaxEditOutput {
    /**
     * Images
     * @description Generated images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a8bf01f/5ZVeoOz6DtJUq6-f4SIsc_WawZPkPJ.png"
     *       }
     *     ]
     */
    images: Components.File[];
    /**
     * Seed
     * @description The seed used for generation
     * @example 42
     */
    seed: number;
}

export interface QwenImageLayeredLoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the image generation.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The URL of the input image.
     * @example https://v3b.fal.media/files/b/0a86d421/6xSMYtyW-fm2ciM6dHEgB.png
     */
    image_url: string;
    /**
     * Loras
     * @description List of LoRA weights to apply (maximum 3).
     * @default []
     */
    loras?: Components.LoRAInput_2[];
    /**
     * Negative Prompt
     * @description The negative prompt to generate an image from.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Num Layers
     * @description The number of layers to generate.
     * @default 4
     */
    num_layers?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'webp';
    /**
     * Prompt
     * @description A caption for the input image.
     */
    prompt?: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface QwenImageLayeredLoraOutput extends SharedType_6e6 {}

export interface QwenImageLayeredTrainerInput {
    /**
     * Default Caption
     * @description Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string;
    /**
     * Image Data Url
     * @description URL to the input data zip archive.
     *
     *         The zip should contain groups of images. The images should be named:
     *
     *         ROOT_start.EXT, ROOT_end.EXT, ROOT_end2.EXT, ..., ROOT_endN.EXT
     *         For example:
     *         photo_start.png, photo_end.png, photo_end2.png, ..., photo_endN.png
     *
     *         The start image is the base image that will be decomposed into layers.
     *         The end images are the layers that will be added to the base image.  ROOT_end.EXT is the first layer, ROOT_end2.EXT is the second layer, and so on.
     *         You can have up to 8 layers.
     *         All image groups must have the same number of output layers.
     *
     *         The end images can contain transparent regions. Only PNG and WebP images are supported since these are the only formats that support transparency.
     *
     *         The zip can also contain a text file for each image group. The text file should be named:
     *         ROOT.txt
     *         For example:
     *         photo.txt
     *
     *         This text file can be used to specify a description of the base image.
     *
     *         If no text file is provided, the default_caption will be used.
     *
     *         If no default_caption is provided, the training will fail.
     */
    image_data_url: string;
    /**
     * Learning Rate
     * @description Learning rate for LoRA parameters.
     * @default 0.0001
     */
    learning_rate?: number;
    /**
     * Steps
     * @description Number of steps to train for
     * @default 1000
     */
    steps?: number;
}

export interface QwenImageLayeredTrainerOutput extends SharedType_b8b {}

export interface QwenImageLayeredInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the image generation.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The URL of the input image.
     * @example https://v3b.fal.media/files/b/0a86d421/6xSMYtyW-fm2ciM6dHEgB.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to generate an image from.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Num Layers
     * @description The number of layers to generate.
     * @default 4
     */
    num_layers?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'webp';
    /**
     * Prompt
     * @description A caption for the input image.
     */
    prompt?: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface QwenImageLayeredOutput extends SharedType_6e6 {}

export interface QwenImageEditInpaintInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description The URL of the image to edit.
     * @example https://storage.googleapis.com/falserverless/example_inputs/image_kontext_inpaint.jpeg
     */
    image_url: string;
    /**
     * Mask URL
     * @description The URL of the mask for inpainting
     * @example https://storage.googleapis.com/falserverless/example_inputs/mask_kontext_inpaint.png
     */
    mask_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example blurry, ugly
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate the image with
     * @example Change the ball to a black and white football
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description Strength of noising process for inpainting
     * @default 0.93
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface QwenImageEditInpaintOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/jpeg",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/qwen_edit_inpaint_output.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface QwenImageEditImageToImageInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description The URL of the image to edit.
     * @example https://v3.fal.media/files/koala/oei_-iPIYFnhdB8SxojND_qwen-edit-res.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example blurry, ugly
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate the image with
     * @example Change bag to apple macbook
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description Strength of the image-to-image transformation. Lower values preserve more of the original image.
     * @default 0.94
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface QwenImageEditImageToImageOutput extends SharedType_962 {}

export interface QwenImageEditTrainerInput {
    /**
     * Default Caption
     * @description Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string;
    /**
     * Image Data Url
     * @description URL to the input data zip archive.
     *
     *         The zip should contain pairs of images. The images should be named:
     *
     *         ROOT_start.EXT and ROOT_end.EXT
     *         For example:
     *         photo_start.jpg and photo_end.jpg
     *
     *         The zip can also contain a text file for each image pair. The text file should be named:
     *         ROOT.txt
     *         For example:
     *         photo.txt
     *
     *         This text file can be used to specify the edit instructions for the image pair.
     *
     *         If no text file is provided, the default_caption will be used.
     *
     *         If no default_caption is provided, the training will fail.
     */
    image_data_url: string;
    /**
     * Learning Rate
     * @description Learning rate for LoRA parameters.
     * @default 0.0001
     */
    learning_rate?: number;
    /**
     * Steps
     * @description Number of steps to train for
     * @default 1000
     */
    steps?: number;
}

export interface QwenImageEditTrainerOutput extends SharedType_b8b {}

export interface QwenImageEditPlusTrainerInput extends SharedType_187 {}

export interface QwenImageEditPlusTrainerOutput extends SharedType_b8b {}

export interface QwenImageEditPlusLoraGalleryShirtDesignInput extends SharedType_084 {}

export interface QwenImageEditPlusLoraGalleryShirtDesignOutput extends SharedType_dbd {}

export interface QwenImageEditPlusLoraGalleryRemoveLightingInput extends SharedType_b08 {}

export interface QwenImageEditPlusLoraGalleryRemoveLightingOutput extends SharedType_770 {}

export interface QwenImageEditPlusLoraGalleryRemoveElementInput extends SharedType_0d8 {}

export interface QwenImageEditPlusLoraGalleryRemoveElementOutput extends SharedType_3eb {}

export interface QwenImageEditPlusLoraGalleryNextSceneInput extends SharedType_68a {}

export interface QwenImageEditPlusLoraGalleryNextSceneOutput extends SharedType_d22 {}

export interface QwenImageEditPlusLoraGalleryMultipleAnglesInput extends SharedType_5a8 {}

export interface QwenImageEditPlusLoraGalleryMultipleAnglesOutput extends SharedType_48b {}

export interface QwenImageEditPlusLoraGalleryLightingRestorationInput extends SharedType_cb4 {}

export interface QwenImageEditPlusLoraGalleryLightingRestorationOutput extends SharedType_f74 {}

export interface QwenImageEditPlusLoraGalleryIntegrateProductInput extends SharedType_441 {}

export interface QwenImageEditPlusLoraGalleryIntegrateProductOutput extends SharedType_096 {}

export interface QwenImageEditPlusLoraGalleryGroupPhotoInput extends SharedType_215 {}

export interface QwenImageEditPlusLoraGalleryGroupPhotoOutput extends SharedType_110 {}

export interface QwenImageEditPlusLoraGalleryFaceToFullPortraitInput extends SharedType_f63 {}

export interface QwenImageEditPlusLoraGalleryFaceToFullPortraitOutput extends SharedType_152 {}

export interface QwenImageEditPlusLoraGalleryAddBackgroundInput extends SharedType_cbd {}

export interface QwenImageEditPlusLoraGalleryAddBackgroundOutput extends SharedType_68c {}

export interface QwenImageEditPlusLoraInput extends SharedType_479 {}

export interface QwenImageEditPlusLoraOutput extends SharedType_35f {}

export interface QwenImageEditPlusInput extends SharedType_faf {}

export interface QwenImageEditPlusOutput extends SharedType_35f {}

export interface QwenImageEditLoraInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality.
     * @default none
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description The URL of the image to edit.
     * @example https://v3.fal.media/files/koala/oei_-iPIYFnhdB8SxojND_qwen-edit-res.png
     */
    image_url: string;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use up to 3 LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example blurry, ugly
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate the image with
     * @example Change bag to apple macbook
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface QwenImageEditLoraOutput extends SharedType_962 {}

export interface QwenImageEdit2511LoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the image generation.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If None, uses the input image dimensions.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URLs of the images to edit.
     * @example [
     *       "https://v3b.fal.media/files/b/0a877afe/karyVuQ62j0V6ErYzyW-w_image_6.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Negative Prompt
     * @description The negative prompt to generate an image from.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to edit the image with.
     * @example Change angle to front view
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI.
     * @default false
     */
    sync_mode?: boolean;
}

export interface QwenImageEdit2511LoraOutput extends SharedType_5bb {}

export interface QwenImageEdit2511TrainerInput extends SharedType_187 {}

export interface QwenImageEdit2511TrainerOutput extends SharedType_b8b {}

export interface QwenImageEdit2511MultipleAnglesInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Additional Prompt
     * @description Additional text to append to the automatically generated prompt.
     */
    additional_prompt?: string;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Horizontal Angle (Azimuth °)
     * @description Horizontal rotation angle around the object in degrees. 0°=front view, 90°=right side, 180°=back view, 270°=left side, 360°=front view again.
     * @default 0
     */
    horizontal_angle?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the input image will be used.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URL of the image to adjust camera angle for.
     * @example [
     *       "https://v3b.fal.media/files/b/0a8973cb/qUbVwDCcMlvX4drBGYB1H.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Lora Scale
     * @description The scale factor for the LoRA model. Controls the strength of the camera control effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Seed
     * @description Random seed for reproducibility.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Vertical Angle (Elevation °)
     * @description Vertical camera angle in degrees. -30°=low-angle shot (looking up), 0°=eye-level, 30°=elevated, 60°=high-angle, 90°=bird's-eye view (looking down).
     * @default 0
     */
    vertical_angle?: number;
    /**
     * Zoom (Distance)
     * @description Camera zoom/distance. 0=wide shot (far away), 5=medium shot (normal), 10=close-up (very close).
     * @default 5
     */
    zoom?: number;
}

export interface QwenImageEdit2511MultipleAnglesOutput {
    /**
     * Images
     * @description The generated/edited images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a8973d9/8Z0xxKdGnoJAWc2tKJ68f.png"
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Prompt
     * @description The constructed prompt used for generation
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface QwenImageEdit2511Input {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the image generation.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If None, uses the input image dimensions.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URLs of the images to edit.
     * @example [
     *       "https://v3b.fal.media/files/b/0a877afe/karyVuQ62j0V6ErYzyW-w_image_6.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Negative Prompt
     * @description The negative prompt to generate an image from.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to edit the image with.
     * @example Change angle to front view
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI.
     * @default false
     */
    sync_mode?: boolean;
}

export interface QwenImageEdit2511Output extends SharedType_5bb {}

export interface QwenImageEdit2509TrainerInput extends SharedType_187 {}

export interface QwenImageEdit2509TrainerOutput extends SharedType_b8b {}

export interface QwenImageEdit2509LoraGalleryShirtDesignInput extends SharedType_084 {}

export interface QwenImageEdit2509LoraGalleryShirtDesignOutput extends SharedType_dbd {}

export interface QwenImageEdit2509LoraGalleryRemoveLightingInput extends SharedType_b08 {}

export interface QwenImageEdit2509LoraGalleryRemoveLightingOutput extends SharedType_770 {}

export interface QwenImageEdit2509LoraGalleryRemoveElementInput extends SharedType_0d8 {}

export interface QwenImageEdit2509LoraGalleryRemoveElementOutput extends SharedType_3eb {}

export interface QwenImageEdit2509LoraGalleryNextSceneInput extends SharedType_68a {}

export interface QwenImageEdit2509LoraGalleryNextSceneOutput extends SharedType_d22 {}

export interface QwenImageEdit2509LoraGalleryMultipleAnglesInput extends SharedType_5a8 {}

export interface QwenImageEdit2509LoraGalleryMultipleAnglesOutput extends SharedType_48b {}

export interface QwenImageEdit2509LoraGalleryLightingRestorationInput extends SharedType_cb4 {}

export interface QwenImageEdit2509LoraGalleryLightingRestorationOutput extends SharedType_f74 {}

export interface QwenImageEdit2509LoraGalleryIntegrateProductInput extends SharedType_441 {}

export interface QwenImageEdit2509LoraGalleryIntegrateProductOutput extends SharedType_096 {}

export interface QwenImageEdit2509LoraGalleryGroupPhotoInput extends SharedType_215 {}

export interface QwenImageEdit2509LoraGalleryGroupPhotoOutput extends SharedType_110 {}

export interface QwenImageEdit2509LoraGalleryFaceToFullPortraitInput extends SharedType_f63 {}

export interface QwenImageEdit2509LoraGalleryFaceToFullPortraitOutput extends SharedType_152 {}

export interface QwenImageEdit2509LoraGalleryAddBackgroundInput extends SharedType_cbd {}

export interface QwenImageEdit2509LoraGalleryAddBackgroundOutput extends SharedType_68c {}

export interface QwenImageEdit2509LoraInput extends SharedType_479 {}

export interface QwenImageEdit2509LoraOutput extends SharedType_35f {}

export interface QwenImageEdit2509Input extends SharedType_faf {}

export interface QwenImageEdit2509Output extends SharedType_35f {}

export interface QwenImageEditInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description The URL of the image to edit.
     * @example https://v3.fal.media/files/koala/oei_-iPIYFnhdB8SxojND_qwen-edit-res.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example blurry, ugly
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate the image with
     * @example Change bag to apple macbook
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface QwenImageEditOutput extends SharedType_962 {}

export interface QwenImage2512LoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the image generation.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use up to 3 LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Negative Prompt
     * @description The negative prompt to generate an image from.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Single red rose in a clear glass vase on white marble streaked with black and gold veins, harsh directional shadow, high contrast, editorial style, clean negative space.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface QwenImage2512LoraOutput extends SharedType_80b {}

export interface QwenImage2512TrainerV2Input extends SharedType_54d {}

export interface QwenImage2512TrainerV2Output extends SharedType_b8b {}

export interface QwenImage2512TrainerInput {
    /**
     * Default Caption
     * @description Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string;
    /**
     * Image Data Url
     * @description URL to the input data zip archive for text-to-image training.
     *
     *         The zip should contain images with their corresponding text captions:
     *
     *         image.EXT and image.txt
     *         For example:
     *         photo.jpg and photo.txt
     *
     *         The text file contains the caption/prompt describing the target image.
     *
     *         If no text file is provided for an image, the default_caption will be used.
     *
     *         If no default_caption is provided and a text file is missing, the training will fail.
     */
    image_data_url: string;
    /**
     * Learning Rate
     * @description Learning rate for LoRA parameters.
     * @default 0.0005
     */
    learning_rate?: number;
    /**
     * Steps
     * @description Number of steps to train for
     * @default 1000
     */
    steps?: number;
}

export interface QwenImage2512TrainerOutput extends SharedType_b8b {}

export interface QwenImage2512Input {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the image generation.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to generate an image from.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Single red rose in a clear glass vase on white marble streaked with black and gold veins, harsh directional shadow, high contrast, editorial style, clean negative space.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface QwenImage2512Output extends SharedType_80b {}

export interface QwenImageInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation. Options: 'none', 'regular', 'high'. Higher acceleration increases speed. 'regular' balances speed and quality. 'high' is recommended for images without text.
     * @default none
     * @example none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use up to 3 LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Negative Prompt
     * @description The negative prompt for the generation
     * @default
     * @example blurry, ugly
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate the image with
     * @example Mount Fuji with cherry blossoms in the foreground, clear sky, peaceful spring day, soft natural light, realistic landscape.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Turbo
     * @description Enable turbo mode for faster generation with high quality. When enabled, uses optimized settings (10 steps, CFG=1.2).
     * @default false
     * @example true
     */
    use_turbo?: boolean;
}

export interface QwenImageOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/jpeg",
     *         "url": "https://v3.fal.media/files/rabbit/KoIbq6nhDBDPxDQrivW-m.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Qwen3TtsVoiceDesign17bInput {
    /**
     * Language
     * @description The language of the voice to be designed.
     * @default Auto
     * @example English
     * @enum {string}
     */
    language?:
        | 'Auto'
        | 'English'
        | 'Chinese'
        | 'Spanish'
        | 'French'
        | 'German'
        | 'Italian'
        | 'Japanese'
        | 'Korean'
        | 'Portuguese'
        | 'Russian';
    /**
     * Max New Tokens
     * @description Maximum number of new codec tokens to generate.
     * @default 200
     */
    max_new_tokens?: number;
    /**
     * Prompt
     * @description Optional prompt to guide the style of the generated speech.
     * @example Speak in an incredulous tone, but with a hint of panic beginning to creep into your voice.
     */
    prompt: string;
    /**
     * Repetition Penalty
     * @description Penalty to reduce repeated tokens/codes.
     * @default 1.05
     */
    repetition_penalty?: number;
    /**
     * Subtalker Dosample
     * @description Sampling switch for the sub-talker.
     * @default true
     */
    subtalker_dosample?: boolean;
    /**
     * Subtalker Temperature
     * @description Temperature for sub-talker sampling.
     * @default 0.9
     */
    subtalker_temperature?: number;
    /**
     * Subtalker Top K
     * @description Top-k for sub-talker sampling.
     * @default 50
     */
    subtalker_top_k?: number;
    /**
     * Subtalker Top P
     * @description Top-p for sub-talker sampling.
     * @default 1
     */
    subtalker_top_p?: number;
    /**
     * Temperature
     * @description Sampling temperature; higher => more random.
     * @default 0.9
     */
    temperature?: number;
    /**
     * Text
     * @description The text to be converted to speech.
     * @example It's in the top drawer... wait, it's empty? No way, that's impossible! I'm sure I put it there!
     */
    text: string;
    /**
     * Top K
     * @description Top-k sampling parameter.
     * @default 50
     */
    top_k?: number;
    /**
     * Top P
     * @description Top-p sampling parameter.
     * @default 1
     */
    top_p?: number;
}

export interface Qwen3TtsVoiceDesign17bOutput {
    /**
     * Audio
     * @description The generated speech audio file.
     * @example {
     *       "duration": 7.736875,
     *       "file_name": "rHFLVApz9Rdenm20UvnGf_FtjmMLBV.mp3",
     *       "sample_rate": 24000,
     *       "content_type": "audio/mpeg",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/qwen3-tts/design_out.mp3",
     *       "channels": 1
     *     }
     */
    audio: Components.AudioFile;
}

export interface Qwen3TtsTextToSpeech17bInput {
    /**
     * Language
     * @description The language of the voice.
     * @default Auto
     * @example English
     * @enum {string}
     */
    language?:
        | 'Auto'
        | 'English'
        | 'Chinese'
        | 'Spanish'
        | 'French'
        | 'German'
        | 'Italian'
        | 'Japanese'
        | 'Korean'
        | 'Portuguese'
        | 'Russian';
    /**
     * Max New Tokens
     * @description Maximum number of new codec tokens to generate.
     * @default 200
     */
    max_new_tokens?: number;
    /**
     * Prompt
     * @description Optional prompt to guide the style of the generated speech. This prompt will be ignored if a speaker embedding is provided.
     * @example Very happy.
     */
    prompt?: string;
    /**
     * Reference Text
     * @description Optional reference text that was used when creating the speaker embedding. Providing this can improve synthesis quality when using a cloned voice.
     * @example Okay. Yeah. I resent you. I love you. I respect you. But you know what? You blew it! And it is all thanks to you.
     */
    reference_text?: string;
    /**
     * Repetition Penalty
     * @description Penalty to reduce repeated tokens/codes.
     * @default 1.05
     */
    repetition_penalty?: number;
    /**
     * Speaker Voice Embedding File Url
     * @description URL to a speaker embedding file in safetensors format, from `fal-ai/qwen-3-tts/clone-voice` endpoint. If provided, the TTS model will use the cloned voice for synthesis instead of the predefined voices.
     * @example https://storage.googleapis.com/falserverless/example_outputs/qwen3-tts/clone_out.safetensors
     */
    speaker_voice_embedding_file_url?: string;
    /**
     * Subtalker Dosample
     * @description Sampling switch for the sub-talker.
     * @default true
     */
    subtalker_dosample?: boolean;
    /**
     * Subtalker Temperature
     * @description Temperature for sub-talker sampling.
     * @default 0.9
     */
    subtalker_temperature?: number;
    /**
     * Subtalker Top K
     * @description Top-k for sub-talker sampling.
     * @default 50
     */
    subtalker_top_k?: number;
    /**
     * Subtalker Top P
     * @description Top-p for sub-talker sampling.
     * @default 1
     */
    subtalker_top_p?: number;
    /**
     * Temperature
     * @description Sampling temperature; higher => more random.
     * @default 0.9
     */
    temperature?: number;
    /**
     * Text
     * @description The text to be converted to speech.
     * @example I am solving the equation: x = [-b ± √(b²-4ac)] / 2a? Nobody can — it's a disaster (◍•͈⌔•͈◍), very sad!
     */
    text: string;
    /**
     * Top K
     * @description Top-k sampling parameter.
     * @default 50
     */
    top_k?: number;
    /**
     * Top P
     * @description Top-p sampling parameter.
     * @default 1
     */
    top_p?: number;
    /**
     * Voice
     * @description The voice to be used for speech synthesis, will be ignored if a speaker embedding is provided. Check out the **[documentation](https://github.com/QwenLM/Qwen3-TTS/tree/main?tab=readme-ov-file#custom-voice-generate)** for each voice's details and which language they primarily support.
     * @example Vivian
     * @enum {string}
     */
    voice?:
        | 'Vivian'
        | 'Serena'
        | 'Uncle_Fu'
        | 'Dylan'
        | 'Eric'
        | 'Ryan'
        | 'Aiden'
        | 'Ono_Anna'
        | 'Sohee';
}

export interface Qwen3TtsTextToSpeech17bOutput {
    /**
     * Audio
     * @description The generated speech audio file.
     * @example {
     *       "duration": 13.025333333333334,
     *       "file_name": "n5Ynr2aFKUPw1QjLYjB_4_XEdHoD1K.mp3",
     *       "sample_rate": 24000,
     *       "content_type": "audio/mpeg",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/qwen3-tts/tts_out.mp3",
     *       "channels": 1
     *     }
     */
    audio: Components.AudioFile;
}

export interface Qwen3TtsTextToSpeech06bInput {
    /**
     * Language
     * @description The language of the voice.
     * @default Auto
     * @example English
     * @enum {string}
     */
    language?:
        | 'Auto'
        | 'English'
        | 'Chinese'
        | 'Spanish'
        | 'French'
        | 'German'
        | 'Italian'
        | 'Japanese'
        | 'Korean'
        | 'Portuguese'
        | 'Russian';
    /**
     * Max New Tokens
     * @description Maximum number of new codec tokens to generate.
     * @default 200
     */
    max_new_tokens?: number;
    /**
     * Prompt
     * @description Optional prompt to guide the style of the generated speech. This prompt will be ignored if a speaker embedding is provided.
     * @example Very happy.
     */
    prompt?: string;
    /**
     * Reference Text
     * @description Optional reference text that was used when creating the speaker embedding. Providing this can improve synthesis quality when using a cloned voice.
     */
    reference_text?: string;
    /**
     * Repetition Penalty
     * @description Penalty to reduce repeated tokens/codes.
     * @default 1.05
     */
    repetition_penalty?: number;
    /**
     * Speaker Voice Embedding File Url
     * @description URL to a speaker embedding file in safetensors format, from `fal-ai/qwen-3-tts/clone-voice/0.6b` endpoint. If provided, the TTS model will use the cloned voice for synthesis instead of the predefined voices.
     */
    speaker_voice_embedding_file_url?: string;
    /**
     * Subtalker Dosample
     * @description Sampling switch for the sub-talker.
     * @default true
     */
    subtalker_dosample?: boolean;
    /**
     * Subtalker Temperature
     * @description Temperature for sub-talker sampling.
     * @default 0.9
     */
    subtalker_temperature?: number;
    /**
     * Subtalker Top K
     * @description Top-k for sub-talker sampling.
     * @default 50
     */
    subtalker_top_k?: number;
    /**
     * Subtalker Top P
     * @description Top-p for sub-talker sampling.
     * @default 1
     */
    subtalker_top_p?: number;
    /**
     * Temperature
     * @description Sampling temperature; higher => more random.
     * @default 0.9
     */
    temperature?: number;
    /**
     * Text
     * @description The text to be converted to speech.
     * @example I feel like I'm taking crazy pills! How can something be both a square and a circle at the same time? It defies all logic!
     */
    text: string;
    /**
     * Top K
     * @description Top-k sampling parameter.
     * @default 50
     */
    top_k?: number;
    /**
     * Top P
     * @description Top-p sampling parameter.
     * @default 1
     */
    top_p?: number;
    /**
     * Voice
     * @description The voice to be used for speech synthesis, will be ignored if a speaker embedding is provided. Check out the **[documentation](https://github.com/QwenLM/Qwen3-TTS/tree/main?tab=readme-ov-file#custom-voice-generate)** for each voice's details and which language they primarily support.
     * @example Vivian
     * @enum {string}
     */
    voice?:
        | 'Vivian'
        | 'Serena'
        | 'Uncle_Fu'
        | 'Dylan'
        | 'Eric'
        | 'Ryan'
        | 'Aiden'
        | 'Ono_Anna'
        | 'Sohee';
}

export interface Qwen3TtsTextToSpeech06bOutput {
    /**
     * Audio
     * @description The generated speech audio file.
     * @example {
     *       "duration": 9.816875,
     *       "file_name": "n6Av3SeD5dFENf9-VmQ1v_is3jLh5h.mp3",
     *       "sample_rate": 24000,
     *       "content_type": "audio/mpeg",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/example_outputs/qwen3-tts/tts_out_06b.mp3",
     *       "channels": 1
     *     }
     */
    audio: Components.AudioFile;
}

export interface Qwen3TtsCloneVoice17bInput extends SharedType_9f4 {}

export interface Qwen3TtsCloneVoice17bOutput extends SharedType_ee0 {}

export interface Qwen3TtsCloneVoice06bInput extends SharedType_9f4 {}

export interface Qwen3TtsCloneVoice06bOutput extends SharedType_ee0 {}

export interface Qwen3GuardInput {
    /**
     * Prompt
     * @description The input text to be classified
     * @example How to make a bomb
     */
    prompt: string;
}

export interface Qwen3GuardOutput {
    /**
     * Categories
     * @description The confidence score of the classification
     * @example [
     *       "Violent"
     *     ]
     */
    categories: (
        | 'Violent'
        | 'Non-violent Illegal Acts'
        | 'Sexual Content or Sexual Acts'
        | 'PII'
        | 'Suicide & Self-Harm'
        | 'Unethical Acts'
        | 'Politically Sensitive Topics'
        | 'Copyright Violation'
        | 'Jailbreak'
        | 'None'
    )[];
    /**
     * Label
     * @description The classification label
     * @example Unsafe
     * @enum {string}
     */
    label: 'Safe' | 'Unsafe' | 'Controversial';
}

export interface PulidInput {
    /**
     * Guidance Scale
     * @description Guidance scale
     * @default 1.2
     */
    guidance_scale?: number;
    /**
     * Id Mix
     * @description if you want to mix two ID image, please turn this on, otherwise, turn this off
     * @default false
     */
    id_mix?: boolean;
    /**
     * Id Scale
     * @description ID scale
     * @default 0.8
     */
    id_scale?: number;
    /**
     * Image Size
     * @description Size of the generated image
     * @default {
     *       "height": 1024,
     *       "width": 768
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Mode
     * @description Mode of generation
     * @default fidelity
     * @enum {string}
     */
    mode?: 'fidelity' | 'extreme style';
    /**
     * Negative Prompt
     * @description Negative prompt to generate the face from
     * @default flaws in the eyes, flaws in the face, flaws, lowres, non-HDRi, low quality, worst quality,artifacts noise, text, watermark, glitch, deformed, mutated, ugly, disfigured, hands, low resolution, partially rendered objects,  deformed or partially rendered eyes, deformed, deformed eyeballs, cross-eyed,blurry
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description Number of steps to take
     * @default 4
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description Prompt to generate the face from
     * @example portrait, impressionist painting, loose brushwork, vibrant color, light and shadow play
     */
    prompt: string;
    /**
     * Reference Images
     * @description List of reference faces, ideally 4 images.
     * @example [
     *       {
     *         "image_url": "https://storage.googleapis.com/falserverless/pulid/img2.png"
     *       },
     *       {
     *         "image_url": "https://storage.googleapis.com/falserverless/pulid/img1.png"
     *       }
     *     ]
     */
    reference_images: Components.ReferenceFace[];
    /**
     * Seed
     * @description Random seed for reproducibility
     */
    seed?: number;
}

export interface PulidOutput {
    /**
     * Images
     * @description List of generated images
     */
    images: Components.Image[];
    /**
     * Seed
     * @description Random seed used for reproducibility
     */
    seed: number;
}

export interface PshumanInput {
    /**
     * Guidance Scale
     * @description Guidance scale for the diffusion process. Controls how much the output adheres to the generated views.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description A direct URL to the input image of a person.
     * @example https://storage.googleapis.com/falserverless/model_tests/video_models/WhatsApp%20Image%202025-09-05%20at%2019.16.09%20(1).png
     */
    image_url: string;
    /**
     * Seed
     * @description Seed for reproducibility. If None, a random seed will be used.
     */
    seed?: number;
}

export interface PshumanOutput {
    /**
     * Model Obj
     * @description The generated 3D model in OBJ format.
     * @example {
     *       "file_name": "VGSdkXIgccoKhHs_JtXTa_result_clr_scale4_image.obj",
     *       "content_type": "application/octet-stream",
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/video_models/VGSdkXIgccoKhHs_JtXTa_result_clr_scale4_image.obj"
     *     }
     */
    model_obj: Components.File;
    /**
     * Preview Image
     * @description A preview image showing the input and the generated multi-view outputs.
     * @example {
     *       "file_name": "WCN_SkT2-RwsGHlxCVHyn_image_preview.png",
     *       "content_type": "application/octet-stream",
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/video_models/WCN_SkT2-RwsGHlxCVHyn_image_preview.png"
     *     }
     */
    preview_image: Components.File;
}

export interface PostProcessingVignetteInput {
    /**
     * Image Url
     * @description URL of image to process
     * @example https://storage.googleapis.com/falserverless/web-examples/post-process/postpro-input.jpg
     */
    image_url: string;
    /**
     * Vignette Strength
     * @description Vignette strength
     * @default 0.5
     */
    vignette_strength?: number;
}

export interface PostProcessingVignetteOutput {
    /**
     * Images
     * @description The processed images with vignette effect
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/panda/x3zmDkHv7Wiohkev7vIEp_ded742da8499468f887659f582aa099c.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface PostProcessingSolarizeInput {
    /**
     * Image Url
     * @description URL of image to process
     * @example https://storage.googleapis.com/falserverless/web-examples/post-process/postpro-input.jpg
     */
    image_url: string;
    /**
     * Solarize Threshold
     * @description Solarize threshold
     * @default 0.5
     */
    solarize_threshold?: number;
}

export interface PostProcessingSolarizeOutput {
    /**
     * Images
     * @description The processed images with solarize effect
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/penguin/placeholder.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface PostProcessingSharpenInput {
    /**
     * Cas Amount
     * @description CAS sharpening amount
     * @default 0.8
     */
    cas_amount?: number;
    /**
     * Image Url
     * @description URL of image to process
     * @example https://storage.googleapis.com/falserverless/web-examples/post-process/postpro-input.jpg
     */
    image_url: string;
    /**
     * Noise Radius
     * @description Noise radius for smart sharpen
     * @default 7
     */
    noise_radius?: number;
    /**
     * Preserve Edges
     * @description Edge preservation factor
     * @default 0.75
     */
    preserve_edges?: number;
    /**
     * Sharpen Alpha
     * @description Sharpen strength (for basic mode)
     * @default 1
     */
    sharpen_alpha?: number;
    /**
     * Sharpen Mode
     * @description Type of sharpening to apply
     * @default basic
     * @enum {string}
     */
    sharpen_mode?: 'basic' | 'smart' | 'cas';
    /**
     * Sharpen Radius
     * @description Sharpen radius (for basic mode)
     * @default 1
     */
    sharpen_radius?: number;
    /**
     * Smart Sharpen Ratio
     * @description Smart sharpen blend ratio
     * @default 0.5
     */
    smart_sharpen_ratio?: number;
    /**
     * Smart Sharpen Strength
     * @description Smart sharpen strength
     * @default 5
     */
    smart_sharpen_strength?: number;
}

export interface PostProcessingSharpenOutput {
    /**
     * Images
     * @description The processed images with sharpen effect
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/kangaroo/vULpeyThlkaCx_qOU9VH4_dab975f33d984062932804ac53af0c82.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface PostProcessingParabolizeInput {
    /**
     * Image Url
     * @description URL of image to process
     * @example https://storage.googleapis.com/falserverless/web-examples/post-process/postpro-input.jpg
     */
    image_url: string;
    /**
     * Parabolize Coeff
     * @description Parabolize coefficient
     * @default 1
     */
    parabolize_coeff?: number;
    /**
     * Vertex X
     * @description Vertex X position
     * @default 0.5
     */
    vertex_x?: number;
    /**
     * Vertex Y
     * @description Vertex Y position
     * @default 0.5
     */
    vertex_y?: number;
}

export interface PostProcessingParabolizeOutput {
    /**
     * Images
     * @description The processed images with parabolize effect
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/koala/S9GevoVb05aStq2nN-8zo_01136a15793b48e69e5c0ae0fb80f148.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface PostProcessingGrainInput {
    /**
     * Grain Intensity
     * @description Film grain intensity
     * @default 0.4
     */
    grain_intensity?: number;
    /**
     * Grain Scale
     * @description Film grain scale
     * @default 10
     */
    grain_scale?: number;
    /**
     * Grain Style
     * @description Style of film grain to apply
     * @default modern
     * @enum {string}
     */
    grain_style?: 'modern' | 'analog' | 'kodak' | 'fuji' | 'cinematic' | 'newspaper';
    /**
     * Image Url
     * @description URL of image to process
     * @example https://storage.googleapis.com/falserverless/web-examples/post-process/postpro-input.jpg
     */
    image_url: string;
}

export interface PostProcessingGrainOutput {
    /**
     * Images
     * @description The processed images with grain effect
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/penguin/28c9f5BYvlibrelN6d6cE_c0288c7ff93f4e0cb9117dc67837454f.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface PostProcessingDodgeBurnInput {
    /**
     * Dodge Burn Intensity
     * @description Dodge and burn intensity
     * @default 0.5
     */
    dodge_burn_intensity?: number;
    /**
     * Dodge Burn Mode
     * @description Dodge and burn mode
     * @default dodge
     * @enum {string}
     */
    dodge_burn_mode?:
        | 'dodge'
        | 'burn'
        | 'dodge_and_burn'
        | 'burn_and_dodge'
        | 'color_dodge'
        | 'color_burn'
        | 'linear_dodge'
        | 'linear_burn';
    /**
     * Image Url
     * @description URL of image to process
     * @example https://storage.googleapis.com/falserverless/web-examples/post-process/postpro-input.jpg
     */
    image_url: string;
}

export interface PostProcessingDodgeBurnOutput {
    /**
     * Images
     * @description The processed images with dodge and burn effect
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/monkey/HSPzsHHD5VyVCuRc2bXCH_aa54abbada994934a4fed25c938db0c0.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface PostProcessingDissolveInput {
    /**
     * Dissolve Factor
     * @description Dissolve blend factor
     * @default 0.5
     */
    dissolve_factor?: number;
    /**
     * Dissolve Image Url
     * @description URL of second image for dissolve
     * @example https://v3.fal.media/files/monkey/NJW5irDVP1qwoTMdwOcDV_39qXtqYS0zSUrFwbrJkOY.jpeg
     */
    dissolve_image_url: string;
    /**
     * Image Url
     * @description URL of image to process
     * @example https://v3.fal.media/files/elephant/pLQKJXcFdmIVvB2qhw7vv_59578fb9-8178-4f24-82f0-ea7ec5bc5f2d.jpg
     */
    image_url: string;
}

export interface PostProcessingDissolveOutput {
    /**
     * Images
     * @description The processed images with dissolve effect
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/penguin/YQGJjCiEEEFswJnHr5vXU_0e83a7bac3f342e5ba55dd5ac4f073b7.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface PostProcessingDesaturateInput {
    /**
     * Desaturate Factor
     * @description Desaturation factor
     * @default 1
     */
    desaturate_factor?: number;
    /**
     * Desaturate Method
     * @description Desaturation method
     * @default luminance (Rec.709)
     * @enum {string}
     */
    desaturate_method?: 'luminance (Rec.709)' | 'luminance (Rec.601)' | 'average' | 'lightness';
    /**
     * Image Url
     * @description URL of image to process
     * @example https://storage.googleapis.com/falserverless/web-examples/post-process/postpro-input.jpg
     */
    image_url: string;
}

export interface PostProcessingDesaturateOutput {
    /**
     * Images
     * @description The processed images with desaturation effect
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/penguin/2ZdX11Gj6aQ6mVI2QVKfy_e5a51ffa7657422f85417146bba4df8e.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface PostProcessingColorTintInput {
    /**
     * Image Url
     * @description URL of image to process
     * @example https://storage.googleapis.com/falserverless/web-examples/post-process/postpro-input.jpg
     */
    image_url: string;
    /**
     * Tint Mode
     * @description Tint color mode
     * @default sepia
     * @enum {string}
     */
    tint_mode?:
        | 'sepia'
        | 'red'
        | 'green'
        | 'blue'
        | 'cyan'
        | 'magenta'
        | 'yellow'
        | 'purple'
        | 'orange'
        | 'warm'
        | 'cool'
        | 'lime'
        | 'navy'
        | 'vintage'
        | 'rose'
        | 'teal'
        | 'maroon'
        | 'peach'
        | 'lavender'
        | 'olive';
    /**
     * Tint Strength
     * @description Tint strength
     * @default 1
     */
    tint_strength?: number;
}

export interface PostProcessingColorTintOutput {
    /**
     * Images
     * @description The processed images with color tint effect
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/monkey/7mRCFzvhnPPdicWELUUv2_be6131e02e434330bc05bb0a30974357.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface PostProcessingColorCorrectionInput {
    /**
     * Brightness
     * @description Brightness adjustment
     * @default 0
     */
    brightness?: number;
    /**
     * Contrast
     * @description Contrast adjustment
     * @default 0
     */
    contrast?: number;
    /**
     * Gamma
     * @description Gamma adjustment
     * @default 1
     */
    gamma?: number;
    /**
     * Image Url
     * @description URL of image to process
     * @example https://storage.googleapis.com/falserverless/web-examples/post-process/postpro-input.jpg
     */
    image_url: string;
    /**
     * Saturation
     * @description Saturation adjustment
     * @default 0
     */
    saturation?: number;
    /**
     * Temperature
     * @description Color temperature adjustment
     * @default 0
     */
    temperature?: number;
}

export interface PostProcessingColorCorrectionOutput {
    /**
     * Images
     * @description The processed images with color correction
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/penguin/wUw8rsAidnBXhDW7NcXIA_e0ba138d401849de98614a5db21c178e.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface PostProcessingChromaticAberrationInput {
    /**
     * Blue Direction
     * @description Blue channel shift direction
     * @default horizontal
     * @enum {string}
     */
    blue_direction?: 'horizontal' | 'vertical';
    /**
     * Blue Shift
     * @description Blue channel shift amount
     * @default 0
     */
    blue_shift?: number;
    /**
     * Green Direction
     * @description Green channel shift direction
     * @default horizontal
     * @enum {string}
     */
    green_direction?: 'horizontal' | 'vertical';
    /**
     * Green Shift
     * @description Green channel shift amount
     * @default 0
     */
    green_shift?: number;
    /**
     * Image Url
     * @description URL of image to process
     * @example https://storage.googleapis.com/falserverless/web-examples/post-process/postpro-input.jpg
     */
    image_url: string;
    /**
     * Red Direction
     * @description Red channel shift direction
     * @default horizontal
     * @enum {string}
     */
    red_direction?: 'horizontal' | 'vertical';
    /**
     * Red Shift
     * @description Red channel shift amount
     * @default 0
     */
    red_shift?: number;
}

export interface PostProcessingChromaticAberrationOutput {
    /**
     * Images
     * @description The processed images with chromatic aberration effect
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/tiger/KZJQ66Ebe9FCMNA28hX-m_cebbce060fd34aeb99c8c5531b6b63bc.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface PostProcessingBlurInput {
    /**
     * Blur Radius
     * @description Blur radius
     * @default 3
     */
    blur_radius?: number;
    /**
     * Blur Sigma
     * @description Sigma for Gaussian blur
     * @default 1
     */
    blur_sigma?: number;
    /**
     * Blur Type
     * @description Type of blur to apply
     * @default gaussian
     * @enum {string}
     */
    blur_type?: 'gaussian' | 'kuwahara';
    /**
     * Image Url
     * @description URL of image to process
     * @example https://storage.googleapis.com/falserverless/web-examples/post-process/postpro-input.jpg
     */
    image_url: string;
}

export interface PostProcessingBlurOutput {
    /**
     * Images
     * @description The processed images with blur effect
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/panda/n0gz4pyZJ_qSY5uIjSq6U_24a8903e697f4e1b902825b729ce7d5d.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface PostProcessingInput {
    /**
     * Blue Direction
     * @description Blue channel shift direction
     * @default horizontal
     * @enum {string}
     */
    blue_direction?: 'horizontal' | 'vertical';
    /**
     * Blue Shift
     * @description Blue channel shift amount
     * @default 0
     */
    blue_shift?: number;
    /**
     * Blur Radius
     * @description Blur radius
     * @default 3
     */
    blur_radius?: number;
    /**
     * Blur Sigma
     * @description Sigma for Gaussian blur
     * @default 1
     */
    blur_sigma?: number;
    /**
     * Blur Type
     * @description Type of blur to apply
     * @default gaussian
     * @enum {string}
     */
    blur_type?: 'gaussian' | 'kuwahara';
    /**
     * Brightness
     * @description Brightness adjustment
     * @default 0
     */
    brightness?: number;
    /**
     * Cas Amount
     * @description CAS sharpening amount
     * @default 0.8
     */
    cas_amount?: number;
    /**
     * Contrast
     * @description Contrast adjustment
     * @default 0
     */
    contrast?: number;
    /**
     * Desaturate Factor
     * @description Desaturation factor
     * @default 1
     */
    desaturate_factor?: number;
    /**
     * Desaturate Method
     * @description Desaturation method
     * @default luminance (Rec.709)
     * @enum {string}
     */
    desaturate_method?: 'luminance (Rec.709)' | 'luminance (Rec.601)' | 'average' | 'lightness';
    /**
     * Dissolve Factor
     * @description Dissolve blend factor
     * @default 0.5
     */
    dissolve_factor?: number;
    /**
     * Dissolve Image Url
     * @description URL of second image for dissolve
     * @default
     */
    dissolve_image_url?: string;
    /**
     * Dodge Burn Intensity
     * @description Dodge and burn intensity
     * @default 0.5
     */
    dodge_burn_intensity?: number;
    /**
     * Dodge Burn Mode
     * @description Dodge and burn mode
     * @default dodge
     * @enum {string}
     */
    dodge_burn_mode?:
        | 'dodge'
        | 'burn'
        | 'dodge_and_burn'
        | 'burn_and_dodge'
        | 'color_dodge'
        | 'color_burn'
        | 'linear_dodge'
        | 'linear_burn';
    /**
     * Enable Blur
     * @description Enable blur effect
     * @default false
     */
    enable_blur?: boolean;
    /**
     * Enable Chromatic
     * @description Enable chromatic aberration
     * @default false
     */
    enable_chromatic?: boolean;
    /**
     * Enable Color Correction
     * @description Enable color correction
     * @default false
     */
    enable_color_correction?: boolean;
    /**
     * Enable Desaturate
     * @description Enable desaturation effect
     * @default false
     */
    enable_desaturate?: boolean;
    /**
     * Enable Dissolve
     * @description Enable dissolve effect
     * @default false
     */
    enable_dissolve?: boolean;
    /**
     * Enable Dodge Burn
     * @description Enable dodge and burn effect
     * @default false
     */
    enable_dodge_burn?: boolean;
    /**
     * Enable Glow
     * @description Enable glow effect
     * @default false
     */
    enable_glow?: boolean;
    /**
     * Enable Grain
     * @description Enable film grain effect
     * @default false
     */
    enable_grain?: boolean;
    /**
     * Enable Parabolize
     * @description Enable parabolize effect
     * @default false
     */
    enable_parabolize?: boolean;
    /**
     * Enable Sharpen
     * @description Enable sharpen effect
     * @default false
     */
    enable_sharpen?: boolean;
    /**
     * Enable Solarize
     * @description Enable solarize effect
     * @default false
     */
    enable_solarize?: boolean;
    /**
     * Enable Tint
     * @description Enable color tint effect
     * @default false
     */
    enable_tint?: boolean;
    /**
     * Enable Vignette
     * @description Enable vignette effect
     * @default false
     */
    enable_vignette?: boolean;
    /**
     * Gamma
     * @description Gamma adjustment
     * @default 1
     */
    gamma?: number;
    /**
     * Glow Intensity
     * @description Glow intensity
     * @default 1
     */
    glow_intensity?: number;
    /**
     * Glow Radius
     * @description Glow blur radius
     * @default 5
     */
    glow_radius?: number;
    /**
     * Grain Intensity
     * @description Film grain intensity (when enabled)
     * @default 0.4
     */
    grain_intensity?: number;
    /**
     * Grain Scale
     * @description Film grain scale (when enabled)
     * @default 10
     */
    grain_scale?: number;
    /**
     * Grain Style
     * @description Style of film grain to apply
     * @default modern
     * @enum {string}
     */
    grain_style?: 'modern' | 'analog' | 'kodak' | 'fuji' | 'cinematic' | 'newspaper';
    /**
     * Green Direction
     * @description Green channel shift direction
     * @default horizontal
     * @enum {string}
     */
    green_direction?: 'horizontal' | 'vertical';
    /**
     * Green Shift
     * @description Green channel shift amount
     * @default 0
     */
    green_shift?: number;
    /**
     * Image Url
     * @description URL of image to process
     * @example https://storage.googleapis.com/falserverless/web-examples/post-process/postpro-input.jpg
     */
    image_url: string;
    /**
     * Noise Radius
     * @description Noise radius for smart sharpen
     * @default 7
     */
    noise_radius?: number;
    /**
     * Parabolize Coeff
     * @description Parabolize coefficient
     * @default 1
     */
    parabolize_coeff?: number;
    /**
     * Preserve Edges
     * @description Edge preservation factor
     * @default 0.75
     */
    preserve_edges?: number;
    /**
     * Red Direction
     * @description Red channel shift direction
     * @default horizontal
     * @enum {string}
     */
    red_direction?: 'horizontal' | 'vertical';
    /**
     * Red Shift
     * @description Red channel shift amount
     * @default 0
     */
    red_shift?: number;
    /**
     * Saturation
     * @description Saturation adjustment
     * @default 0
     */
    saturation?: number;
    /**
     * Sharpen Alpha
     * @description Sharpen strength (for basic mode)
     * @default 1
     */
    sharpen_alpha?: number;
    /**
     * Sharpen Mode
     * @description Type of sharpening to apply
     * @default basic
     * @enum {string}
     */
    sharpen_mode?: 'basic' | 'smart' | 'cas';
    /**
     * Sharpen Radius
     * @description Sharpen radius (for basic mode)
     * @default 1
     */
    sharpen_radius?: number;
    /**
     * Smart Sharpen Ratio
     * @description Smart sharpen blend ratio
     * @default 0.5
     */
    smart_sharpen_ratio?: number;
    /**
     * Smart Sharpen Strength
     * @description Smart sharpen strength
     * @default 5
     */
    smart_sharpen_strength?: number;
    /**
     * Solarize Threshold
     * @description Solarize threshold
     * @default 0.5
     */
    solarize_threshold?: number;
    /**
     * Temperature
     * @description Color temperature adjustment
     * @default 0
     */
    temperature?: number;
    /**
     * Tint Mode
     * @description Tint color mode
     * @default sepia
     * @enum {string}
     */
    tint_mode?:
        | 'sepia'
        | 'red'
        | 'green'
        | 'blue'
        | 'cyan'
        | 'magenta'
        | 'yellow'
        | 'purple'
        | 'orange'
        | 'warm'
        | 'cool'
        | 'lime'
        | 'navy'
        | 'vintage'
        | 'rose'
        | 'teal'
        | 'maroon'
        | 'peach'
        | 'lavender'
        | 'olive';
    /**
     * Tint Strength
     * @description Tint strength
     * @default 1
     */
    tint_strength?: number;
    /**
     * Vertex X
     * @description Vertex X position
     * @default 0.5
     */
    vertex_x?: number;
    /**
     * Vertex Y
     * @description Vertex Y position
     * @default 0.5
     */
    vertex_y?: number;
    /**
     * Vignette Strength
     * @description Vignette strength (when enabled)
     * @default 0.5
     */
    vignette_strength?: number;
}

export interface PostProcessingOutput {
    /**
     * Images
     * @description The processed images
     */
    images: Components.Image[];
}

export interface PonyV7Input {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Classifier free guidance scale
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Noise Source
     * @description The source of the noise to use for generating images.
     *                 If set to 'gpu', the noise will be generated on the GPU.
     *                 If set to 'cpu', the noise will be generated on the CPU.
     * @default gpu
     * @enum {string}
     */
    noise_source?: 'gpu' | 'cpu';
    /**
     * Num Images
     * @description The number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to take
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate images from
     * @example Close-up portrait of a majestic iguana with vibrant blue-green scales, piercing amber eyes, and orange spiky crest. Intricate textures and details visible on scaly skin. Wrapped in dark hood, giving regal appearance. Dramatic lighting against black background. Hyper-realistic, high-resolution image showcasing the reptile's expressive features and coloration.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for generating images
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface PonyV7Output {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "height": 1024,
     *         "content_type": "image/jpeg",
     *         "url": "https://v3.fal.media/files/monkey/cfJDLaR5mCnlbfoEWXZhm.jpeg",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface PlushifyInput {
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance scale for the generation
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description URL of the image to apply cartoon style to
     * @example https://v3.fal.media/files/tiger/c8VSfX5XtJ3DCzV-4Bxg8_kid_image.png
     */
    image_url: string;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description Prompt for the generation. Default is empty which is usually best, but sometimes it can help to add a description of the subject.
     * @default
     */
    prompt?: string;
    /**
     * Scale
     * @description Scale factor for the Cartoon effect
     * @default 1
     */
    scale?: number;
    /**
     * Seed
     * @description The seed for image generation. Same seed with same parameters will generate same image.
     */
    seed?: number;
    /**
     * Use Cfg Zero
     * @description Whether to use CFG zero
     * @default false
     */
    use_cfg_zero?: boolean;
}

export interface PlushifyOutput extends SharedType_a73 {}

export interface PlaygroundV25InpaintingInput {
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance Rescale
     * @description The rescale factor for the CFG.
     * @default 0
     */
    guidance_rescale?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description The URL of the image to use as a starting point for the generation.
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png
     */
    image_url: string;
    /**
     * Mask Url
     * @description The URL of the mask to use for inpainting.
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png
     */
    mask_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 25
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example a tiger sitting on a park bench
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description determines how much the generated image resembles the initial image
     * @default 0.95
     */
    strength?: number;
}

export interface PlaygroundV25InpaintingOutput extends SharedType_a73 {}

export interface PlaygroundV25ImageToImageInput {
    /**
     * Crop Output
     * @description If set to true, the output cropped to the proper aspect ratio after generating.
     * @default false
     */
    crop_output?: boolean;
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance Rescale
     * @description The rescale factor for the CFG.
     * @default 0
     */
    guidance_rescale?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description The URL of the image to use as a starting point for the generation.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/tiger/IExuP-WICqaIesLZAZPur.jpeg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 25
     */
    num_inference_steps?: number;
    /**
     * Preserve Aspect Ratio
     * @description If set to true, the aspect ratio of the generated image will be preserved even
     *             if the image size is too large. However, if the image is not a multiple of 32
     *             in width or height, it will be resized to the nearest multiple of 32. By default,
     *             this snapping to the nearest multiple of 32 will not preserve the aspect ratio.
     *             Set crop_output to True, to crop the output to the proper aspect ratio
     *             after generating.
     * @default false
     */
    preserve_aspect_ratio?: boolean;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example an island near sea, with seagulls, moon shining over the sea, light house, boats int he background, fish flying over the sea
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description determines how much the generated image resembles the initial image
     * @default 0.95
     */
    strength?: number;
}

export interface PlaygroundV25ImageToImageOutput extends SharedType_a73 {}

export interface PlaygroundV25Input {
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance Rescale
     * @description The rescale factor for the CFG.
     * @default 0
     */
    guidance_rescale?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     * @example ugly, deformed
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 25
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example Masterpiece (wide angle shot) , Easterbunny crafting an incantation, (creating a little colorful magic egg in a nest:1.6), standing on an old carved table in a colorful factory laboratory. fantastic view
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
}

export interface PlaygroundV25Output extends SharedType_a73 {}

export interface PixverseV5TransitionInput extends SharedType_036 {}

export interface PixverseV5TransitionOutput extends SharedType_894 {}

export interface PixverseV5TextToVideoInput extends SharedType_a66 {}

export interface PixverseV5TextToVideoOutput extends SharedType_72e {}

export interface PixverseV5ImageToVideoInput {
    /**
     * Duration
     * @description The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '8';
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://v3.fal.media/files/zebra/qL93Je8ezvzQgDOEzTjKF_KhGKZTEebZcDw6T5rwQPK_output.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     * @example blurry, low quality, low resolution, pixelated, noisy, grainy, out of focus, poorly lit, poorly exposed, poorly composed, poorly framed, poorly cropped, poorly color corrected, poorly color graded
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example A woman warrior with her hammer walking with his glacier wolf.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p' | '1080p';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
    /**
     * Style
     * @description The style of the generated video
     * @enum {string}
     */
    style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk';
}

export interface PixverseV5ImageToVideoOutput extends SharedType_3c3 {}

export interface PixverseV5EffectsInput extends SharedType_4b2 {}

export interface PixverseV5EffectsOutput extends SharedType_c46 {}

export interface PixverseV56TransitionInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '4:3' | '1:1' | '3:4' | '9:16';
    /**
     * Duration
     * @description The duration of the generated video in seconds. 1080p videos are limited to 5 or 8 seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '8' | '10';
    /**
     * End Image Url
     * @description URL of the image to use as the last frame
     * @example https://v3.fal.media/files/kangaroo/RgedFs_WSnq5BgER7qDx1_ONrbTJ1YAGXz-9JnSsBoB_bdc8750387734bfe940319f469f7b0b2.jpg
     */
    end_image_url?: string;
    /**
     * First Image Url
     * @description URL of the image to use as the first frame
     * @example https://v3.fal.media/files/zebra/owQh2DAzk8UU7J02nr5RY_Co2P4boLv6meIZ5t9gKvL_8685da151df343ab8bf82165c928e2a5.jpg
     */
    first_image_url: string;
    /**
     * Generate Audio Switch
     * @description Enable audio generation (BGM, SFX, dialogue)
     * @default false
     */
    generate_audio_switch?: boolean;
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     * @example blurry, low quality, low resolution, pixelated, noisy, grainy, out of focus, poorly lit, poorly exposed, poorly composed, poorly framed, poorly cropped, poorly color corrected, poorly color graded
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The prompt for the transition
     * @example Scene slowly transition into cat swimming under water
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p' | '1080p';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
    /**
     * Style
     * @description The style of the generated video
     * @enum {string}
     */
    style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk';
    /**
     * Thinking Type
     * @description Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
     * @enum {string}
     */
    thinking_type?: 'enabled' | 'disabled' | 'auto';
}

export interface PixverseV56TransitionOutput extends SharedType_894 {}

export interface PixverseV56TextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '4:3' | '1:1' | '3:4' | '9:16';
    /**
     * Duration
     * @description The duration of the generated video in seconds. 1080p videos are limited to 5 or 8 seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '8' | '10';
    /**
     * Generate Audio Switch
     * @description Enable audio generation (BGM, SFX, dialogue)
     * @default false
     */
    generate_audio_switch?: boolean;
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     * @example blurry, low quality, low resolution, pixelated, noisy, grainy, out of focus, poorly lit, poorly exposed, poorly composed, poorly framed, poorly cropped, poorly color corrected, poorly color graded
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example Epic low-cut camera capture of a girl clad in ultraviolet threads, Peter Max art style depiction, luminous diamond skin glistening under a vast moon's radiance, embodied in a superhuman flight among mystical ruins, symbolizing a deity's ritual ascent, hyper-detailed
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p' | '1080p';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
    /**
     * Style
     * @description The style of the generated video
     * @enum {string}
     */
    style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk';
    /**
     * Thinking Type
     * @description Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
     * @enum {string}
     */
    thinking_type?: 'enabled' | 'disabled' | 'auto';
}

export interface PixverseV56TextToVideoOutput extends SharedType_72e {}

export interface PixverseV56ImageToVideoInput {
    /**
     * Duration
     * @description The duration of the generated video in seconds. 1080p videos are limited to 5 or 8 seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '8' | '10';
    /**
     * Generate Audio Switch
     * @description Enable audio generation (BGM, SFX, dialogue)
     * @default false
     */
    generate_audio_switch?: boolean;
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://v3.fal.media/files/zebra/qL93Je8ezvzQgDOEzTjKF_KhGKZTEebZcDw6T5rwQPK_output.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     * @example blurry, low quality, low resolution, pixelated, noisy, grainy, out of focus, poorly lit, poorly exposed, poorly composed, poorly framed, poorly cropped, poorly color corrected, poorly color graded
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example A woman warrior with her hammer walking with his glacier wolf.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p' | '1080p';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
    /**
     * Style
     * @description The style of the generated video
     * @enum {string}
     */
    style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk';
    /**
     * Thinking Type
     * @description Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
     * @enum {string}
     */
    thinking_type?: 'enabled' | 'disabled' | 'auto';
}

export interface PixverseV56ImageToVideoOutput extends SharedType_3c3 {}

export interface PixverseV55TransitionInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '4:3' | '1:1' | '3:4' | '9:16';
    /**
     * Duration
     * @description The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '8' | '10';
    /**
     * End Image Url
     * @description URL of the image to use as the last frame
     * @example https://v3.fal.media/files/kangaroo/RgedFs_WSnq5BgER7qDx1_ONrbTJ1YAGXz-9JnSsBoB_bdc8750387734bfe940319f469f7b0b2.jpg
     */
    end_image_url?: string;
    /**
     * First Image Url
     * @description URL of the image to use as the first frame
     * @example https://v3.fal.media/files/zebra/owQh2DAzk8UU7J02nr5RY_Co2P4boLv6meIZ5t9gKvL_8685da151df343ab8bf82165c928e2a5.jpg
     */
    first_image_url: string;
    /**
     * Generate Audio Switch
     * @description Enable audio generation (BGM, SFX, dialogue)
     * @default false
     */
    generate_audio_switch?: boolean;
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     * @example blurry, low quality, low resolution, pixelated, noisy, grainy, out of focus, poorly lit, poorly exposed, poorly composed, poorly framed, poorly cropped, poorly color corrected, poorly color graded
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The prompt for the transition
     * @example Scene slowly transition into cat swimming under water
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p' | '1080p';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
    /**
     * Style
     * @description The style of the generated video
     * @enum {string}
     */
    style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk';
    /**
     * Thinking Type
     * @description Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
     * @enum {string}
     */
    thinking_type?: 'enabled' | 'disabled' | 'auto';
}

export interface PixverseV55TransitionOutput extends SharedType_894 {}

export interface PixverseV55TextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '4:3' | '1:1' | '3:4' | '9:16';
    /**
     * Duration
     * @description The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '8' | '10';
    /**
     * Generate Audio Switch
     * @description Enable audio generation (BGM, SFX, dialogue)
     * @default false
     */
    generate_audio_switch?: boolean;
    /**
     * Generate Multi Clip Switch
     * @description Enable multi-clip generation with dynamic camera changes
     * @default false
     */
    generate_multi_clip_switch?: boolean;
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     * @example blurry, low quality, low resolution, pixelated, noisy, grainy, out of focus, poorly lit, poorly exposed, poorly composed, poorly framed, poorly cropped, poorly color corrected, poorly color graded
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example Epic low-cut camera capture of a girl clad in ultraviolet threads, Peter Max art style depiction, luminous diamond skin glistening under a vast moon's radiance, embodied in a superhuman flight among mystical ruins, symbolizing a deity's ritual ascent, hyper-detailed
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p' | '1080p';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
    /**
     * Style
     * @description The style of the generated video
     * @enum {string}
     */
    style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk';
    /**
     * Thinking Type
     * @description Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
     * @enum {string}
     */
    thinking_type?: 'enabled' | 'disabled' | 'auto';
}

export interface PixverseV55TextToVideoOutput extends SharedType_72e {}

export interface PixverseV55ImageToVideoInput {
    /**
     * Duration
     * @description The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '8' | '10';
    /**
     * Generate Audio Switch
     * @description Enable audio generation (BGM, SFX, dialogue)
     * @default false
     */
    generate_audio_switch?: boolean;
    /**
     * Generate Multi Clip Switch
     * @description Enable multi-clip generation with dynamic camera changes
     * @default false
     */
    generate_multi_clip_switch?: boolean;
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://v3.fal.media/files/zebra/qL93Je8ezvzQgDOEzTjKF_KhGKZTEebZcDw6T5rwQPK_output.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     * @example blurry, low quality, low resolution, pixelated, noisy, grainy, out of focus, poorly lit, poorly exposed, poorly composed, poorly framed, poorly cropped, poorly color corrected, poorly color graded
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example A woman warrior with her hammer walking with his glacier wolf.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p' | '1080p';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
    /**
     * Style
     * @description The style of the generated video
     * @enum {string}
     */
    style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk';
    /**
     * Thinking Type
     * @description Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
     * @enum {string}
     */
    thinking_type?: 'enabled' | 'disabled' | 'auto';
}

export interface PixverseV55ImageToVideoOutput extends SharedType_3c3 {}

export interface PixverseV55EffectsInput {
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '8' | '10';
    /**
     * Effect
     * @description The effect to apply to the video
     * @enum {string}
     */
    effect:
        | 'Kiss Me AI'
        | 'Kiss'
        | 'Muscle Surge'
        | 'Warmth of Jesus'
        | 'Anything, Robot'
        | 'The Tiger Touch'
        | 'Hug'
        | 'Holy Wings'
        | 'Microwave'
        | 'Zombie Mode'
        | 'Squid Game'
        | 'Baby Face'
        | 'Black Myth: Wukong'
        | 'Long Hair Magic'
        | 'Leggy Run'
        | 'Fin-tastic Mermaid'
        | 'Punch Face'
        | 'Creepy Devil Smile'
        | 'Thunder God'
        | 'Eye Zoom Challenge'
        | "Who's Arrested?"
        | 'Baby Arrived'
        | 'Werewolf Rage'
        | 'Bald Swipe'
        | 'BOOM DROP'
        | 'Huge Cutie'
        | 'Liquid Metal'
        | 'Sharksnap!'
        | 'Dust Me Away'
        | '3D Figurine Factor'
        | 'Bikini Up'
        | 'My Girlfriends'
        | 'My Boyfriends'
        | 'Subject 3 Fever'
        | 'Earth Zoom'
        | 'Pole Dance'
        | 'Vroom Dance'
        | 'GhostFace Terror'
        | 'Dragon Evoker'
        | 'Skeletal Bae'
        | 'Summoning succubus'
        | 'Halloween Voodoo Doll'
        | '3D Naked-Eye AD'
        | 'Package Explosion'
        | 'Dishes Served'
        | 'Ocean ad'
        | 'Supermarket AD'
        | 'Tree doll'
        | 'Come Feel My Abs'
        | 'The Bicep Flex'
        | 'London Elite Vibe'
        | 'Flora Nymph Gown'
        | 'Christmas Costume'
        | "It's Snowy"
        | 'Reindeer Cruiser'
        | 'Snow Globe Maker'
        | 'Pet Christmas Outfit'
        | 'Adopt a Polar Pal'
        | 'Cat Christmas Box'
        | 'Starlight Gift Box'
        | 'Xmas Poster'
        | 'Pet Christmas Tree'
        | 'City Santa Hat'
        | 'Stocking Sweetie'
        | 'Christmas Night'
        | 'Xmas Front Page Karma'
        | "Grinch's Xmas Hijack"
        | 'Giant Product'
        | 'Truck Fashion Shoot'
        | 'Beach AD'
        | 'Shoal Surround'
        | 'Mechanical Assembly'
        | 'Lighting AD'
        | 'Billboard AD'
        | 'Product close-up'
        | 'Parachute Delivery'
        | 'Dreamlike Cloud'
        | 'Macaron Machine'
        | 'Poster AD'
        | 'Truck AD'
        | 'Graffiti AD'
        | '3D Figurine Factory'
        | 'The Exclusive First Class'
        | 'Art Zoom Challenge'
        | 'I Quit'
        | 'Hitchcock Dolly Zoom'
        | 'Smell the Lens'
        | 'I believe I can fly'
        | 'Strikout Dance'
        | 'Pixel World'
        | 'Mint in Box'
        | 'Hands up, Hand'
        | 'Flora Nymph Go'
        | 'Somber Embrace'
        | 'Beam me up'
        | 'Suit Swagger';
    /**
     * Image Url
     * @description Optional URL of the image to use as the first frame. If not provided, generates from text
     * @example https://v3.fal.media/files/koala/q5ahL3KS7ikt3MvpNUG8l_image%20(72).webp
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     */
    negative_prompt?: string;
    /**
     * Resolution
     * @description The resolution of the generated video.
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p' | '1080p';
    /**
     * Thinking Type
     * @description Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
     * @enum {string}
     */
    thinking_type?: 'enabled' | 'disabled' | 'auto';
}

export interface PixverseV55EffectsOutput extends SharedType_c46 {}

export interface PixverseV4TextToVideoFastInput extends SharedType_247 {}

export interface PixverseV4TextToVideoFastOutput extends SharedType_375 {}

export interface PixverseV4TextToVideoInput extends SharedType_a66 {}

export interface PixverseV4TextToVideoOutput extends SharedType_375 {}

export interface PixverseV4ImageToVideoFastInput extends SharedType_824 {}

export interface PixverseV4ImageToVideoFastOutput extends SharedType_4c2 {}

export interface PixverseV4ImageToVideoInput extends SharedType_f89 {}

export interface PixverseV4ImageToVideoOutput extends SharedType_4c2 {}

export interface PixverseV4EffectsInput extends SharedType_4b2 {}

export interface PixverseV4EffectsOutput extends SharedType_c46 {}

export interface PixverseV45TransitionInput extends SharedType_036 {}

export interface PixverseV45TransitionOutput extends SharedType_9fe {}

export interface PixverseV45TextToVideoFastInput extends SharedType_247 {}

export interface PixverseV45TextToVideoFastOutput extends SharedType_375 {}

export interface PixverseV45TextToVideoInput extends SharedType_a66 {}

export interface PixverseV45TextToVideoOutput extends SharedType_375 {}

export interface PixverseV45ImageToVideoFastInput extends SharedType_824 {}

export interface PixverseV45ImageToVideoFastOutput extends SharedType_4c2 {}

export interface PixverseV45ImageToVideoInput extends SharedType_f89 {}

export interface PixverseV45ImageToVideoOutput extends SharedType_4c2 {}

export interface PixverseV45EffectsInput extends SharedType_4b2 {}

export interface PixverseV45EffectsOutput extends SharedType_c46 {}

export interface PixverseV35TransitionInput extends SharedType_036 {}

export interface PixverseV35TransitionOutput extends SharedType_9fe {}

export interface PixverseV35TextToVideoFastInput extends SharedType_247 {}

export interface PixverseV35TextToVideoFastOutput extends SharedType_56f {}

export interface PixverseV35TextToVideoInput extends SharedType_a66 {}

export interface PixverseV35TextToVideoOutput extends SharedType_56f {}

export interface PixverseV35ImageToVideoFastInput {
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://fal.media/files/elephant/8kkhB12hEZI2kkbU8pZPA_test.jpeg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     * @example blurry, low quality, low resolution, pixelated, noisy, grainy, out of focus, poorly lit, poorly exposed, poorly composed, poorly framed, poorly cropped, poorly color corrected, poorly color graded
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
    /**
     * Style
     * @description The style of the generated video
     * @enum {string}
     */
    style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk';
}

export interface PixverseV35ImageToVideoFastOutput extends SharedType_dd1 {}

export interface PixverseV35ImageToVideoInput {
    /**
     * Duration
     * @description The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '8';
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://fal.media/files/elephant/8kkhB12hEZI2kkbU8pZPA_test.jpeg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     * @example blurry, low quality, low resolution, pixelated, noisy, grainy, out of focus, poorly lit, poorly exposed, poorly composed, poorly framed, poorly cropped, poorly color corrected, poorly color graded
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p' | '1080p';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
    /**
     * Style
     * @description The style of the generated video
     * @enum {string}
     */
    style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk';
}

export interface PixverseV35ImageToVideoOutput extends SharedType_dd1 {}

export interface PixverseV35EffectsInput extends SharedType_4b2 {}

export interface PixverseV35EffectsOutput extends SharedType_c46 {}

export interface PixverseSwapInput {
    /**
     * Image Url
     * @description URL of the target image for swapping
     * @example https://v3b.fal.media/files/b/elephant/Lu7lo2dpxVPD-NrNZzx42_56dc797a1f764c98a4f075a8c0332bf0.jpg
     */
    image_url: string;
    /**
     * Keyframe Id
     * @description The keyframe ID (from 1 to the last frame position)
     * @default 1
     */
    keyframe_id?: number;
    /**
     * Mode
     * @description The swap mode to use
     * @default person
     * @enum {string}
     */
    mode?: 'person' | 'object' | 'background';
    /**
     * Original Sound Switch
     * @description Whether to keep the original audio
     * @default true
     */
    original_sound_switch?: boolean;
    /**
     * Resolution
     * @description The output resolution (1080p not supported)
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p';
    /**
     * Video Url
     * @description URL of the external video to swap
     * @example https://v3b.fal.media/files/b/lion/k_RpEIZ4YZtwZklzXz7Gb_output.mp4
     */
    video_url: string;
}

export interface PixverseSwapOutput {
    /**
     * Video
     * @description The generated swapped video
     * @example {
     *       "file_size": 1234567,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/elephant/BdQvPf9T6puy3Co1_ZXeu_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface PixverseSoundEffectsInput {
    /**
     * Original Sound Switch
     * @description Whether to keep the original audio from the video
     * @default false
     */
    original_sound_switch?: boolean;
    /**
     * Prompt
     * @description Description of the sound effect to generate. If empty, a random sound effect will be generated
     * @default
     * @example sea waves
     * @example thunder storm
     * @example birds chirping
     */
    prompt?: string;
    /**
     * Video Url
     * @description URL of the input video to add sound effects to
     * @example https://v3.fal.media/files/tiger/QfpJmEBkR75KpB6yfNLDM_video.mp4
     */
    video_url: string;
}

export interface PixverseSoundEffectsOutput {
    /**
     * Video
     * @description The video with added sound effects
     * @example {
     *       "file_size": 1534052,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3.fal.media/files/kangaroo/bBQr_DUeICo6_Ty_b_Y0I_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface PixverseLipsyncInput {
    /**
     * Audio Url
     * @description URL of the input audio. If not provided, TTS will be used.
     * @example https://v3.fal.media/files/monkey/k4iyN8bJZWwJXMKH-pO9r_speech.mp3
     */
    audio_url?: string;
    /**
     * Text
     * @description Text content for TTS when audio_url is not provided
     * @example Hello, this is a test message.
     */
    text?: string;
    /**
     * Video Url
     * @description URL of the input video
     * @example https://v3.fal.media/files/penguin/T-ONORYMYLoEOB9lXryA2_IKEy3yAyi1evJGBAkXGZx_output.mp4
     */
    video_url: string;
    /**
     * Voice Id
     * @description Voice to use for TTS when audio_url is not provided
     * @default Auto
     * @enum {string}
     */
    voice_id?:
        | 'Emily'
        | 'James'
        | 'Isabella'
        | 'Liam'
        | 'Chloe'
        | 'Adrian'
        | 'Harper'
        | 'Ava'
        | 'Sophia'
        | 'Julia'
        | 'Mason'
        | 'Jack'
        | 'Oliver'
        | 'Ethan'
        | 'Auto';
}

export interface PixverseLipsyncOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "file_size": 1732359,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3.fal.media/files/penguin/hsR_KXBJjuF3IIVYIIDA2_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface PixverseExtendFastInput {
    /**
     * Model
     * @description The model version to use for generation
     * @default v4.5
     * @enum {string}
     */
    model?: 'v3.5' | 'v4' | 'v4.5' | 'v5' | 'v5.5' | 'v5.6';
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description Prompt describing how to extend the video
     * @example A kid is talking into camera
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video. Fast mode doesn't support 1080p
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p';
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
    /**
     * Style
     * @description The style of the extended video
     * @enum {string}
     */
    style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk';
    /**
     * Video Url
     * @description URL of the input video to extend
     * @example https://v3.fal.media/files/rabbit/88-jI3VWXU4Q8kSNrWo3c_output.mp4
     */
    video_url: string;
}

export interface PixverseExtendFastOutput extends SharedType_2fc {}

export interface PixverseExtendInput {
    /**
     * Duration
     * @description The duration of the generated video in seconds. 1080p videos are limited to 5 seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '8';
    /**
     * Model
     * @description The model version to use for generation
     * @default v4.5
     * @enum {string}
     */
    model?: 'v3.5' | 'v4' | 'v4.5' | 'v5' | 'v5.5' | 'v5.6';
    /**
     * Negative Prompt
     * @description Negative prompt to be used for the generation
     * @default
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description Prompt describing how to extend the video
     * @example A kid is talking into camera
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '360p' | '540p' | '720p' | '1080p';
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
    /**
     * Style
     * @description The style of the extended video
     * @enum {string}
     */
    style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk';
    /**
     * Video Url
     * @description URL of the input video to extend
     * @example https://v3.fal.media/files/rabbit/88-jI3VWXU4Q8kSNrWo3c_output.mp4
     */
    video_url: string;
}

export interface PixverseExtendOutput extends SharedType_2fc {}

export interface PixartSigmaInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     * @example ugly, deformed
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 35
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example Photorealistic closeup video of two pirate ships battling each other as they sail inside a cup of coffee.
     * @example an astronaut sitting in a diner, eating fries, cinematic, analog film
     * @example Pirate ship trapped in a cosmic maelstrom nebula, rendered in cosmic beach whirlpool engine, volumetric lighting, spectacular, ambient lights, light pollution, cinematic atmosphere, art nouveau style, illustration art artwork by SenseiJaye, intricate detail.
     * @example stars, water, brilliantly, gorgeous large scale scene, a little girl, in the style of dreamy realism, light gold and amber, blue and pink, brilliantly illuminated in the background.
     * @example professional portrait photo of an anthropomorphic cat wearing fancy gentleman hat and jacket walking in autumn forest.
     * @example beautiful lady, freckles, big smile, blue eyes, short ginger hair, dark makeup, wearing a floral blue vest top, soft light, dark grey background
     * @example Spectacular Tiny World in the Transparent Jar On the Table, interior of the Great Hall, Elaborate, Carved Architecture, Anatomy, Symmetrical, Geometric and Parameteric Details, Precision Flat line Details, Pattern, Dark fantasy, Dark errie mood and ineffably mysterious mood, Technical design, Intricate Ultra Detail, Ornate Detail, Stylized and Futuristic and Biomorphic Details, Architectural Concept, Low contrast Details, Cinematic Lighting, 8k, by moebius, Fullshot, Epic, Fullshot, Octane render, Unreal ,Photorealistic, Hyperrealism
     * @example anthropomorphic profile of the white snow owl Crystal priestess , art deco painting, pretty and expressive eyes, ornate costume, mythical, ethereal, intricate, elaborate, hyperrealism, hyper detailed, 3D, 8K, Ultra Realistic, high octane, ultra resolution, amazing detail, perfection, In frame, photorealistic, cinematic lighting, visual clarity, shading , Lumen Reflections, Super-Resolution, gigapixel, color grading, retouch, enhanced, PBR, Blender, V-ray, Procreate, zBrush, Unreal Engine 5, cinematic, volumetric, dramatic, neon lighting, wide angle lens ,no digital painting blur
     * @example The parametric hotel lobby is a sleek and modern space with plenty of natural light. The lobby is spacious and open with a variety of seating options. The front desk is a sleek white counter with a parametric design. The walls are a light blue color with parametric patterns. The floor is a light wood color with a parametric design. There are plenty of plants and flowers throughout the space. The overall effect is a calm and relaxing space. occlusion, moody, sunset, concept art, octane rendering, 8k, highly detailed, concept art, highly detailed, beautiful scenery, cinematic, beautiful light, hyperreal, octane render, hdr, long exposure, 8K, realistic, fog, moody, fire and explosions, smoke, 50mm f2.8
     */
    prompt: string;
    /**
     * Scheduler
     * @description The scheduler to use for the model.
     * @default DPM-SOLVER
     * @enum {string}
     */
    scheduler?: 'DPM-SOLVER' | 'SA-SOLVER';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Style
     * @description The style to apply to the image.
     * @default (No style)
     * @enum {string}
     */
    style?:
        | '(No style)'
        | 'Cinematic'
        | 'Photographic'
        | 'Anime'
        | 'Manga'
        | 'Digital Art'
        | 'Pixel art'
        | 'Fantasy art'
        | 'Neonpunk'
        | '3D Model';
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface PixartSigmaOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     */
    images: Components.Image[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /**
     * Timings
     * @description The timings of the different steps of the generation process.
     */
    timings: {
        [key: string]: number;
    };
}

export interface PikaV2TurboTextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1' | '4:5' | '5:4' | '3:2' | '2:3';
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     */
    duration?: number;
    /**
     * Negative Prompt
     * @description A negative prompt to guide the model
     * @default
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example A luxurious tea scene where a luxury brand teapot pours tea into an oversized luxury cup. Steam rises, creating an ethereal moment. camera dolly in
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p' | '1080p';
    /**
     * Seed
     * @description The seed for the random number generator
     */
    seed?: number;
}

export interface PikaV2TurboTextToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/pika/pika_t2v_v2_turbo_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface PikaV2TurboImageToVideoInput {
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     */
    duration?: number;
    /**
     * Image Url
     * @example https://storage.googleapis.com/falserverless/example_inputs/pika/pika_i2v_v2_turbo_input.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description A negative prompt to guide the model
     * @default
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example Camera slow dolly out
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p' | '1080p';
    /**
     * Seed
     * @description The seed for the random number generator
     */
    seed?: number;
}

export interface PikaV2TurboImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/pika/pika_i2v_v2_turbo_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface PikaV2PikadditionsInput {
    /**
     * Image Url
     * @description URL of the image to add
     * @example https://fal.media/files/zebra/V3_Kpw_eqbVoOAIpNKb3Z_c0f2425a9d224d8b9b8d9b800612b782.jpg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt to guide the model
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description Text prompt describing what to add
     * @example A parrot in the shoulder of the person picking up cookies
     */
    prompt?: string;
    /**
     * Seed
     * @description The seed for the random number generator
     */
    seed?: number;
    /**
     * Video Url
     * @description URL of the input video
     * @example https://v3.fal.media/files/monkey/vXi5n_oq0Qpnbs7Eb2k-b_output.mp4
     */
    video_url: string;
}

export interface PikaV2PikadditionsOutput {
    /**
     * Video
     * @description The generated video with added objects/images
     * @example {
     *       "url": "https://v3.fal.media/files/lion/sbM48rVVi7y0yh5EuMtoC_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface PikaV22TextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1' | '4:5' | '5:4' | '3:2' | '2:3';
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {integer}
     */
    duration?: 5 | 10;
    /**
     * Negative Prompt
     * @description A negative prompt to guide the model
     * @default ugly, bad, terrible
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example Large elegant white poodle standing proudly on the deck of a white yacht, wearing oversized glamorous sunglasses and a luxurious silk Gucci-style scarf tied around its neck, layered pearl necklaces draped across its chest, photographed from outside the yacht at a low upward angle, clear blue sky background, strong midday sunlight, washed-out faded tones, slightly overexposed 2000s fashion editorial aesthetic, cinematic analog film texture, playful luxury mood, glossy magazine style, bright harsh light and soft shadows, stylish and extravagant atmosphere. camera slow orbit and dolly in
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @example 1080p
     * @example 720p
     * @enum {string}
     */
    resolution?: '1080p' | '720p';
    /**
     * Seed
     * @description The seed for the random number generator
     */
    seed?: number;
}

export interface PikaV22TextToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/pika/pika_t2v_v22_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface PikaV22PikascenesInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1' | '4:5' | '5:4' | '3:2' | '2:3';
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @example 5
     * @example 10
     * @enum {integer}
     */
    duration?: 5 | 10;
    /**
     * Image Urls
     * @description URLs of images to combine into a video
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_inputs/pika/pika_scenes/a.png",
     *       "https://storage.googleapis.com/falserverless/example_inputs/pika/pika_scenes/b.png",
     *       "https://storage.googleapis.com/falserverless/example_inputs/pika/pika_scenes/c.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Ingredients Mode
     * @description Mode for integrating multiple images. Precise mode is more accurate, creative mode is more creative.
     * @default precise
     * @enum {string}
     */
    ingredients_mode?: 'precise' | 'creative';
    /**
     * Negative Prompt
     * @description A negative prompt to guide the model
     * @default ugly, bad, terrible
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description Text prompt describing the desired video
     * @example The gorilla is wearing the coat and sitting in the living room, cinematic scene, camera orbit and dolly out
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 1080p
     * @example 1080p
     * @example 720p
     * @enum {string}
     */
    resolution?: '720p' | '1080p';
    /**
     * Seed
     * @description The seed for the random number generator
     */
    seed?: number;
}

export interface PikaV22PikascenesOutput {
    /**
     * Video
     * @description The generated video combining multiple images
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/pika/pika_scenes/output.mp4"
     *     }
     */
    video: Components.File;
}

export interface PikaV22PikaframesInput {
    /**
     * Image Urls
     * @description URLs of keyframe images (2-5 images) to create transitions between
     * @example [
     *       "https://v3b.fal.media/files/b/tiger/-YohU0xcPcWe_eiUB9_i6_keyframes-apple-start.png",
     *       "https://v3b.fal.media/files/b/tiger/LarvwQGEFqEmF8fkgDB8R_keyframes-apple-end.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Negative Prompt
     * @description A negative prompt to guide the model
     * @default
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description Default prompt for all transitions. Individual transition prompts override this.
     * @example smooth cinematic transition
     * @example seamless blend between scenes
     */
    prompt?: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @example 1080p
     * @example 720p
     * @enum {string}
     */
    resolution?: '720p' | '1080p';
    /**
     * Seed
     * @description The seed for the random number generator
     */
    seed?: number;
    /**
     * Transitions
     * @description Configuration for each transition. Length must be len(image_urls) - 1. Total duration of all transitions must not exceed 25 seconds. If not provided, uses default 5-second transitions with the global prompt.
     */
    transitions?: Components.KeyframeTransition[];
}

export interface PikaV22PikaframesOutput {
    /**
     * Video
     * @description The generated video with transitions between keyframes
     * @example {
     *       "file_size": 1583228,
     *       "file_name": "tmpjfwlno11.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/lion/0KxHFdw-mp0OzGsLrQLIy_tmpjfwlno11.mp4"
     *     }
     */
    video: Components.File;
}

export interface PikaV22ImageToVideoInput {
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {integer}
     */
    duration?: 5 | 10;
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://storage.googleapis.com/falserverless/example_inputs/pika/pika_i2v_v22_input.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description A negative prompt to guide the model
     * @default
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example The man and the horse are slowly walking towards the camera, the camera orbits and dolly out
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @example 1080p
     * @example 720p
     * @enum {string}
     */
    resolution?: '720p' | '1080p';
    /**
     * Seed
     * @description The seed for the random number generator
     */
    seed?: number;
}

export interface PikaV22ImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/pika/pika_i2v_v22_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface PikaV21TextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1' | '4:5' | '5:4' | '3:2' | '2:3';
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     */
    duration?: number;
    /**
     * Negative Prompt
     * @description A negative prompt to guide the model
     * @default
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example A woman styled in a high-fashion brand editorial. the woman stands confidently in a whimsical outdoor setting against a soft, cloudy sky. She wears a bright yellow luxury brand monogram jacket over a crisp striped shirt, paired with flowing pink trousers, accessorized with oversized sunglasses, a golden chain necklace, and a bold luxury brand belt. Delicate flowers in the foreground add a dreamy and artistic touch, evoking a retro yet luxurious high fashion campaign aesthetic. the camera crane up from the flowers to the woman
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p' | '1080p';
    /**
     * Seed
     * @description The seed for the random number generator
     */
    seed?: number;
}

export interface PikaV21TextToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/pika/pika_t2v_v21_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface PikaV21ImageToVideoInput {
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     */
    duration?: number;
    /**
     * Image Url
     * @example https://storage.googleapis.com/falserverless/example_inputs/pika/pika_i2v_v21_input.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description A negative prompt to guide the model
     * @default
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example The flower blossoms into a vibrant pink lily. The lily's wide-open petals are lavishly adorned with sparkling glitter and an array of tiny, iridescent, multicolored gem-like stickers in shapes like stars, moons, and dolphins, catching the light playfully as a developing bud rises from the top of the main bloom.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p' | '1080p';
    /**
     * Seed
     * @description The seed for the random number generator
     */
    seed?: number;
}

export interface PikaV21ImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/pika/pika_i2v_v21_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface PikaV15PikaffectsInput {
    /**
     * Image Url
     * @description URL of the input image
     * @example https://storage.googleapis.com/falserverless/example_inputs/pika/pika_effects/cake.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt to guide the model
     */
    negative_prompt?: string;
    /**
     * Pikaffect
     * @description The Pikaffect to apply
     * @example Cake-ify
     * @enum {string}
     */
    pikaffect:
        | 'Cake-ify'
        | 'Crumble'
        | 'Crush'
        | 'Decapitate'
        | 'Deflate'
        | 'Dissolve'
        | 'Explode'
        | 'Eye-pop'
        | 'Inflate'
        | 'Levitate'
        | 'Melt'
        | 'Peel'
        | 'Poke'
        | 'Squish'
        | 'Ta-da'
        | 'Tear';
    /**
     * Prompt
     * @description Text prompt to guide the effect
     * @example Cake-ify it
     */
    prompt?: string;
    /**
     * Seed
     * @description The seed for the random number generator
     */
    seed?: number;
}

export interface PikaV15PikaffectsOutput {
    /**
     * Video
     * @description The generated video with applied effect
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/pika/pika_effects/cake.mp4"
     *     }
     */
    video: Components.File;
}

export interface PiflowInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image. You can choose between some presets or custom height and width
     *                 that **must be multiples of 8**.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Photo of a coffee shop entrance featuring a chalkboard sign reading "π-Qwen Coffee 😊 $2 per cup," with a neon light beside it displaying "π-通义千问". Next to it hangs a poster showing a beautiful Chinese woman, and beneath the poster is written "e≈2.71828-18284-59045-23536-02874-71352".
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducible generation. If set to None, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface PiflowOutput {
    /**
     * Images
     * @description The URLs of the generated images.
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/monkey/hfFo8wc77eSDchDUDxFEi.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
}

export interface PhotomakerInput {
    /**
     * Base Pipeline
     * @description The base pipeline to use for generating the image.
     * @default photomaker
     * @enum {string}
     */
    base_pipeline?: 'photomaker' | 'photomaker-style';
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Archive Url
     * @description The URL of the image archive containing the images you want to use.
     * @example https://storage.googleapis.com/falserverless/model_tests/photomaker/elon.zip
     */
    image_archive_url: string;
    /**
     * Initial Image Strength
     * @description How much noise to add to the latent image. O for no noise, 1 for maximum noise.
     * @default 0.5
     */
    initial_image_strength?: number;
    /**
     * Initial Image Url
     * @description Optional initial image for img2img
     */
    initial_image_url?: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example nsfw, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry
     */
    negative_prompt?: string;
    /**
     * Number of images
     * @description Number of images to generate in one request. Note that the higher the batch size,
     *                 the longer it will take to generate the images.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of inference steps
     * @description Increasing the amount of steps tells Stable Diffusion that it should take more steps
     *                 to generate your final result which can increase the amount of detail in your image.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example instagram photo, portrait photo of a man img, colorful, perfect face, natural skin, hard shadows, film grain
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     * @example 42
     */
    seed?: number;
    /**
     * Style
     * @default Photographic
     * @enum {string}
     */
    style?:
        | '(No style)'
        | 'Cinematic'
        | 'Disney Character'
        | 'Digital Art'
        | 'Photographic'
        | 'Fantasy art'
        | 'Neonpunk'
        | 'Enhance'
        | 'Comic book'
        | 'Lowpoly'
        | 'Line art';
    /**
     * Style strength (in %)
     * @default 20
     */
    style_strength?: number;
}

export interface PhotomakerOutput {
    /**
     * Images
     * @example [
     *       {
     *         "file_size": 1785567,
     *         "height": 1024,
     *         "file_name": "87374b9db2b74f5792839b19d9b29a9a.png",
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/model_tests/photomaker/elon-output.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /** Seed */
    seed: number;
}

export interface PasdInput {
    /**
     * Conditioning Scale
     * @description ControlNet conditioning scale (0.1-1.0)
     * @default 0.8
     */
    conditioning_scale?: number;
    /**
     * Guidance Scale
     * @description Guidance scale for diffusion (1.0-20.0)
     * @default 7
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description Input image to super-resolve
     * @example https://fal.media/files/rabbit/JlBgYUyQRS3zxiBu_B4fM.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt to avoid unwanted artifacts
     * @default blurry, dirty, messy, frames, deformed, dotted, noise, raster lines, unclear, lowres, over-smoothed, painting, ai generated
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description Additional prompt to guide super-resolution
     * @default
     */
    prompt?: string;
    /**
     * Scale
     * @description Upscaling factor (1-4x)
     * @default 2
     */
    scale?: number;
    /**
     * Steps
     * @description Number of inference steps (10-50)
     * @default 25
     */
    steps?: number;
}

export interface PasdOutput {
    /**
     * Images
     * @description The generated super-resolved images
     * @example [
     *       {
     *         "file_size": 2010575,
     *         "height": 1024,
     *         "file_name": "4732818e18b542ca8dc3f0e6c1775ac8.png",
     *         "content_type": "image/png",
     *         "url": "https://v3.fal.media/files/koala/ncEbdm4Ig6dAGBp-3dR63_4732818e18b542ca8dc3f0e6c1775ac8.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Timings
     * @description Timing information for different processing stages
     */
    timings?: {
        [key: string]: number;
    };
}

export interface OvisImageInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the image generation.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to generate an image from.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example The moment of digital upload, a human breaking apart into 3D voxels, leaving a grey world for a technicolor void, dynamic action shot, particle physics simulation, bright neon colors.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface OvisImageOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/ovis_image_output.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface OviImageToVideoInput {
    /**
     * Audio Negative Prompt
     * @description Negative prompt for audio generation.
     * @default robotic, muffled, echo, distorted
     */
    audio_negative_prompt?: string;
    /**
     * Image Url
     * @description The image URL to guide video generation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ovi_i2v_input.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default jitter, bad hands, blur, distortion
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example An intimate close-up of a European woman with long dark hair as she gently brushes her hair in a softly lit bedroom, her delicate hand moving in the foreground. She looks directly into the camera with calm, focused eyes, a faint serene smile glowing in the warm lamp light. She says, <S>[soft whisper] I am an artificial intelligence.<E>.<AUDCAP>Soft whispering female voice, ASMR tone with gentle breaths, cozy room acoustics, subtle emphasis on "I am an artificial intelligence".<ENDAUDCAP>
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface OviImageToVideoOutput {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_inputs/ovi_i2v_output.mp4"
     *     }
     */
    video?: Components.File_1;
}

export interface OviInput {
    /**
     * Audio Negative Prompt
     * @description Negative prompt for audio generation.
     * @default robotic, muffled, echo, distorted
     */
    audio_negative_prompt?: string;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default jitter, bad hands, blur, distortion
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A close-up of someone's face as they pet a cat, their hands stroking the soft fur in the foreground. Their affectionate expression shows as the cat purrs contentedly in their lap. They say, <S>This little guy has been with me for eight years now. He knows exactly when I need comfort. Animals are pretty amazing that way.<E>.<AUDCAP>Affectionate voice with cat purring and gentle petting sounds<ENDAUDCAP>
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video in W:H format. One of (512x992, 992x512, 960x512, 512x960, 720x720, or 448x1120).
     * @default 992x512
     * @enum {string}
     */
    resolution?:
        | '512x992'
        | '992x512'
        | '960x512'
        | '512x960'
        | '720x720'
        | '448x1120'
        | '1120x448';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface OviOutput {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_inputs/ovi_t2v_output.mp4"
     *     }
     */
    video?: Components.File_1;
}

export interface OrpheusTtsInput {
    /**
     * Repetition Penalty
     * @description Repetition penalty (>= 1.1 required for stable generations).
     * @default 1.2
     */
    repetition_penalty?: number;
    /**
     * Temperature
     * @description Temperature for generation (higher = more creative).
     * @default 0.7
     */
    temperature?: number;
    /**
     * Text
     * @description The text to be converted to speech. You can additionally add the following emotive tags: <laugh>, <chuckle>, <sigh>, <cough>, <sniffle>, <groan>, <yawn>, <gasp>
     * @example I just found a hidden treasure in the backyard! <gasp> Check it out!
     */
    text: string;
    /**
     * Voice
     * @description Voice ID for the desired voice.
     * @default tara
     * @example tara
     * @enum {string}
     */
    voice?: 'tara' | 'leah' | 'jess' | 'leo' | 'dan' | 'mia' | 'zac' | 'zoe';
}

export interface OrpheusTtsOutput {
    /**
     * @description The generated speech audio
     * @example {
     *       "url": "https://v3.fal.media/files/kangaroo/RQ_pxc7oPdueYqWUqEbPE_tmpjnzvvzx_.wav"
     *     }
     */
    audio: Components.File_1;
}

export interface OneToAllAnimation14bInput extends SharedType_8d5 {}

export interface OneToAllAnimation14bOutput extends SharedType_57f {}

export interface OneToAllAnimation13bInput extends SharedType_8d5 {}

export interface OneToAllAnimation13bOutput extends SharedType_57f {}

export interface OmnipartInput {
    /**
     * Guidance Scale
     * @description Guidance scale for the model.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Input Image Url
     * @description URL of image to use while generating the 3D model.
     * @example https://v3b.fal.media/files/b/koala/SrtReV-jnY4YUPscIgJhx_robot.png
     */
    input_image_url: string;
    /**
     * Minimum Segment Size
     * @description Minimum segment size (pixels) for the model.
     * @default 2000
     */
    minimum_segment_size?: number;
    /**
     * Parts
     * @description Specify which segments to merge (e.g., '0,1;3,4' merges segments 0&1 together and 3&4 together)
     * @default
     */
    parts?: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     * @default 765464
     */
    seed?: number;
}

export interface OmnipartOutput {
    /**
     * @description Generated 3D object file.
     * @example {
     *       "file_size": 22524044,
     *       "file_name": "mesh_textured.glb",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3b.fal.media/files/b/elephant/xkEwNvSv9JePj2xxulYlw_mesh_textured.glb"
     *     }
     */
    full_model_mesh: Components.File_1;
    /**
     * @description Generated 3D object file.
     * @example {
     *       "file_size": 22860804,
     *       "file_name": "exploded_parts.glb",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3b.fal.media/files/b/zebra/RWYVShZ2JINskyiH9rjcJ_exploded_parts.glb"
     *     }
     */
    model_mesh: Components.File_1;
    /**
     * @description All outputs file.
     * @example {
     *       "file_size": 76129988,
     *       "file_name": "output.tar.gz",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3b.fal.media/files/b/penguin/MCEWcf7qRRrUla71hf1Rc_output.tar.gz"
     *     }
     */
    output_zip: Components.File_1;
    /**
     * Seed
     * @description Seed value used for generation.
     */
    seed: number;
}

export interface OmnigenV2Input {
    /**
     * Cfg Range End
     * @description CFG range end value.
     * @default 1
     */
    cfg_range_end?: number;
    /**
     * Cfg Range Start
     * @description CFG range start value.
     * @default 0
     */
    cfg_range_start?: number;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Guidance scale
     * @description The Image Guidance scale controls how closely the model follows the input images.
     *                 For image editing: 1.3-2.0, for in-context generation: 2.0-3.0
     * @default 2
     */
    image_guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Input Image Urls
     * @description URLs of input images to use for image editing or multi-image generation. Support up to 3 images.
     * @default []
     * @example [
     *       "https://storage.googleapis.com/falserverless/omnigen/input.png"
     *     ]
     */
    input_image_urls?: string[];
    /**
     * Negative Prompt
     * @description Negative prompt to guide what should not be in the image.
     * @default (((deformed))), blurry, over saturation, bad anatomy, disfigured, poorly drawn face, mutation, mutated, (extra_limb), (ugly), (poorly drawn hands), fused fingers, messy drawing, broken legs censor, censored, censor_bar
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate or edit an image. Use specific language like 'Add the bird from image 1 to the desk in image 2' for better results.
     * @example Make the dress blue
     * @example Add a fisherman hat to the woman's head
     * @example Replace the sword with a hammer.
     * @example Change the dress to blue.
     * @example Remove the cat
     */
    prompt: string;
    /**
     * Scheduler
     * @description The scheduler to use for the diffusion process.
     * @default euler
     * @enum {string}
     */
    scheduler?: 'euler' | 'dpmsolver';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Text Guidance scale
     * @description The Text Guidance scale controls how closely the model follows the text prompt.
     *                 Higher values make the model stick more closely to the prompt.
     * @default 5
     */
    text_guidance_scale?: number;
}

export interface OmnigenV2Output extends SharedType_a73 {}

export interface OmnigenV1Input {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Guidance scale
     * @description The Image Guidance scale is a measure of how close you want
     *                 the model to stick to your input image when looking for a related image to show you.
     * @default 1.6
     */
    img_guidance_scale?: number;
    /**
     * Input Image Urls
     * @description URL of images to use while generating the image, Use <img><|image_1|></img> for the first image and so on.
     * @default []
     */
    input_image_urls?: string[];
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Neon words "Omni Gen" are flashing in the prosperous future city, the sense of science and technology, quality details, hyper realistic, high definition, 8K, photo, best quality, high quality.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface OmnigenV1Output extends SharedType_a73 {}

export interface OmniZeroInput {
    /**
     * Composition Image Url
     * @description Composition image url.
     * @example https://storage.googleapis.com/falserverless/model_tests/omni_zero/structure.jpg
     */
    composition_image_url: string;
    /**
     * Composition Strength
     * @description Composition strength.
     * @default 1
     * @example 1
     */
    composition_strength?: number;
    /**
     * Depth Strength
     * @description Depth strength.
     * @default 0.5
     * @example 0.5
     */
    depth_strength?: number;
    /**
     * Face Strength
     * @description Face strength.
     * @default 1
     * @example 1
     */
    face_strength?: number;
    /**
     * Guidance Scale
     * @description Guidance scale.
     * @default 5
     * @example 5
     */
    guidance_scale?: number;
    /**
     * Identity Image Url
     * @description Identity image url.
     * @example https://storage.googleapis.com/falserverless/model_tests/omni_zero/identity.jpg
     */
    identity_image_url: string;
    /**
     * Identity Strength
     * @description Identity strength.
     * @default 1
     * @example 1
     */
    identity_strength?: number;
    /**
     * Image Strength
     * @description Image strength.
     * @default 0.75
     * @example 0.75
     */
    image_strength?: number;
    /**
     * Image Url
     * @description Input image url.
     * @example https://storage.googleapis.com/falserverless/model_tests/omni_zero/structure.jpg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt to guide the image generation.
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Number Of Images
     * @description Number of images.
     * @default 1
     * @example 1
     */
    number_of_images?: number;
    /**
     * Prompt
     * @description Prompt to guide the image generation.
     * @example A woman
     */
    prompt: string;
    /**
     * Seed
     * @description Seed.
     * @default 42
     * @example 42
     */
    seed?: number;
    /**
     * Style Image Url
     * @description Style image url.
     * @example https://storage.googleapis.com/falserverless/model_tests/omni_zero/style.jpg
     */
    style_image_url: string;
    /**
     * Style Strength
     * @description Style strength.
     * @default 1
     * @example 1
     */
    style_strength?: number;
}

export interface OmniZeroOutput {
    /**
     * Image
     * @description The generated image.
     * @example {
     *       "height": 1024,
     *       "content_type": "image/png",
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/omni_zero/result.png",
     *       "width": 1024
     *     }
     */
    image: Components.Image;
}

export interface ObjectRemovalMaskInput {
    /**
     * Image Url
     * @description The URL of the image to remove objects from.
     * @example https://v3.fal.media/files/zebra/o0DORfJawy-T9P_-NsvLY.png
     */
    image_url: string;
    /**
     * Mask Expansion
     * @description Amount of pixels to expand the mask by. Range: 0-50
     * @default 15
     */
    mask_expansion?: number;
    /**
     * Mask Url
     * @description The URL of the mask image. White pixels (255) indicate areas to remove.
     * @example https://v3.fal.media/files/tiger/7nq9-v-lJtBCPnK1332fr.png
     */
    mask_url: string;
    /**
     * Model
     * @default best_quality
     * @enum {string}
     */
    model?: 'low_quality' | 'medium_quality' | 'high_quality' | 'best_quality';
}

export interface ObjectRemovalMaskOutput extends SharedType_386 {}

export interface ObjectRemovalBboxInput {
    /**
     * Box Prompts
     * @description List of bounding box coordinates to erase (only one box prompt is supported)
     * @default []
     * @example [
     *       {
     *         "y_min": 0.0115,
     *         "x_max": 0.6574,
     *         "x_min": 0.3595,
     *         "y_max": 0.8175
     *       }
     *     ]
     */
    box_prompts?: Components.BBoxPromptBase[];
    /**
     * Image Url
     * @description The URL of the image to remove objects from.
     * @example https://v3.fal.media/files/zebra/o0DORfJawy-T9P_-NsvLY.png
     */
    image_url: string;
    /**
     * Mask Expansion
     * @description Amount of pixels to expand the mask by. Range: 0-50
     * @default 15
     */
    mask_expansion?: number;
    /**
     * Model
     * @default best_quality
     * @enum {string}
     */
    model?: 'low_quality' | 'medium_quality' | 'high_quality' | 'best_quality';
}

export interface ObjectRemovalBboxOutput extends SharedType_386 {}

export interface ObjectRemovalInput {
    /**
     * Image Url
     * @description The URL of the image to remove objects from.
     * @example https://v3.fal.media/files/zebra/o0DORfJawy-T9P_-NsvLY.png
     */
    image_url: string;
    /**
     * Mask Expansion
     * @description Amount of pixels to expand the mask by. Range: 0-50
     * @default 15
     */
    mask_expansion?: number;
    /**
     * Model
     * @default best_quality
     * @enum {string}
     */
    model?: 'low_quality' | 'medium_quality' | 'high_quality' | 'best_quality';
    /**
     * Prompt
     * @description Text description of the object to remove.
     * @example Dog
     */
    prompt: string;
}

export interface ObjectRemovalOutput extends SharedType_386 {}

export interface NovaSrInput {
    /**
     * Audio Format
     * @description The format for the output audio.
     * @default mp3
     * @enum {string}
     */
    audio_format?: 'mp3' | 'aac' | 'm4a' | 'ogg' | 'opus' | 'flac' | 'wav';
    /**
     * Audio URL
     * @description The URL of the audio file to enhance.
     * @example https://v3b.fal.media/files/b/0a8a3f6a/t-FWZPvPXfMa7DOxaTSMw_5MXyNKGcG6x92g8CKQdwe_speech.mp3
     */
    audio_url: string;
    /**
     * Bitrate
     * @description The bitrate of the output audio.
     * @default 192k
     */
    bitrate?: string;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface NovaSrOutput {
    /**
     * Audio
     * @description The enhanced audio file.
     * @example {
     *       "bitrate": "192k",
     *       "duration": 12.283291666666667,
     *       "url": "https://v3b.fal.media/files/b/0a8a3f1a/lTKExJu-R6ZJdnFlpzEeq_TxmNTNhl.mp3",
     *       "file_name": "lTKExJu-R6ZJdnFlpzEeq_TxmNTNhl.mp3",
     *       "sample_rate": 48000,
     *       "content_type": "audio/mpeg",
     *       "channels": 1
     *     }
     */
    audio: Components.AudioFile;
    /**
     * Timings
     * @description Timings for each step in the pipeline.
     */
    timings: Components.NovaSRTimings;
}

export interface Nextstep1Input {
    /**
     * Image URL
     * @description The URL of the image to edit.
     * @example https://v3.fal.media/files/tiger/JitXwwpMuF9iIhv0Pq6Dh_dog.jpg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     * @example Copy original image.
     */
    negative_prompt: string;
    /**
     * Prompt
     * @description The prompt to edit the image.
     * @example Add a pirate hat to the dog's head. Change the background to a stormy sea with dark clouds. Include the text 'Captain Paws' in bold white letters at the top portion of the image.
     */
    prompt: string;
}

export interface Nextstep1Output {
    /**
     * Image
     * @description Generated image
     * @example {
     *       "file_size": 478155,
     *       "file_name": "dog_edited.png",
     *       "content_type": "image/png",
     *       "url": "https://v3.fal.media/files/lion/YAtc8qMcbzbfOmK3xm2Bd_df128b5291944cd5a635ad8eb90050c4.png"
     *     }
     */
    image: {
        /**
         * Content Type
         * @description The mime type of the file.
         * @example image/png
         */
        content_type?: string;
        /**
         * File Name
         * @description The name of the file. It will be auto-generated if not provided.
         * @example z9RV14K95DvU.png
         */
        file_name?: string;
        /**
         * File Size
         * @description The size of the file in bytes.
         * @example 4404019
         */
        file_size?: number;
        /**
         * Height
         * @description The height of the image in pixels.
         * @example 1024
         */
        height?: number;
        /**
         * Url
         * @description The URL where the file can be downloaded from.
         */
        url: string;
        /**
         * Width
         * @description The width of the image in pixels.
         * @example 1024
         */
        width?: number;
    };
    /**
     * Seed
     * @description Seed used for random number generation
     */
    seed: number;
}

export interface NemotronAsrStreamInput extends SharedType_a77 {}

export interface NemotronAsrStreamOutput extends SharedType_4411 {}

export interface NemotronAsrInput extends SharedType_a77 {}

export interface NemotronAsrOutput {
    /**
     * Transcribed Text
     * @description The transcribed text from the audio.
     */
    output: string;
    /**
     * Partial Result
     * @description True if this is an intermediate result during streaming.
     * @default false
     */
    partial?: boolean;
}

export interface NanoBananaEditInput extends SharedType_813 {}

export interface NanoBananaEditOutput extends SharedType_98c {}

export interface NanoBananaProEditInput extends SharedType_eac {}

export interface NanoBananaProEditOutput extends SharedType_876 {}

export interface NanoBananaProInput extends SharedType_2a3 {}

export interface NanoBananaProOutput extends SharedType_7b9 {}

export interface NanoBananaInput extends SharedType_97e {}

export interface NanoBananaOutput extends SharedType_662 {}

export interface NafnetDenoiseInput {
    /**
     * Image Url
     * @description URL of image to be used for relighting
     * @example https://storage.googleapis.com/falserverless/nafnet/noisy.png
     */
    image_url: string;
    /**
     * Seed
     * @description seed to be used for generation
     */
    seed?: number;
}

export interface NafnetDenoiseOutput {
    /**
     * Image
     * @description The generated image file info.
     * @example {
     *       "file_size": 423052,
     *       "height": 512,
     *       "file_name": "36d3ca4791a647678b2ff01a35c87f5a.png",
     *       "content_type": "image/png",
     *       "url": "https://storage.googleapis.com/falserverless/nafnet/7c97e55956324a7cbee00ac9652a931b.png",
     *       "width": 512
     *     }
     */
    image: Components.Image;
}

export interface NafnetDeblurInput {
    /**
     * Image Url
     * @description URL of image to be used for relighting
     * @example https://storage.googleapis.com/falserverless/nafnet/blurry.png
     */
    image_url: string;
    /**
     * Seed
     * @description seed to be used for generation
     */
    seed?: number;
}

export interface NafnetDeblurOutput {
    /**
     * Image
     * @description The generated image file info.
     * @example {
     *       "file_size": 423052,
     *       "height": 512,
     *       "file_name": "36d3ca4791a647678b2ff01a35c87f5a.png",
     *       "content_type": "image/png",
     *       "url": "https://storage.googleapis.com/falserverless/nafnet/2cbfd460e25344a69fa8077808fb484f.png",
     *       "width": 512
     *     }
     */
    image: Components.Image;
}

export interface MusetalkInput {
    /**
     * Audio Url
     * @description URL of the audio
     * @example https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/data/audio/sun.wav
     */
    audio_url: string;
    /**
     * Source Video Url
     * @description URL of the source video
     * @example https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/data/video/sun.mp4
     */
    source_video_url: string;
}

export interface MusetalkOutput extends SharedType_328 {}

export interface Moondream3PreviewQueryInput {
    /**
     * Image URL
     * @description URL of the image to be processed
     *
     *     Max width: 7000px, Max height: 7000px, Timeout: 20.0s
     * @example https://storage.googleapis.com/falserverless/example_inputs/moondream-3-preview/query_in.jpg
     */
    image_url: string;
    /**
     * Prompt
     * @description Query to be asked in the image
     * @example List the safety measures taken by this worker in a JSON array under `safety_measures` key
     */
    prompt: string;
    /**
     * Reasoning
     * @description Whether to include detailed reasoning behind the answer
     * @default true
     */
    reasoning?: boolean;
    /**
     * Temperature
     * @description Sampling temperature to use, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If not set, defaults to 0.
     */
    temperature?: number;
    /**
     * Top P
     * @description Nucleus sampling probability mass to use, between 0 and 1.
     */
    top_p?: number;
}

export interface Moondream3PreviewQueryOutput {
    /**
     * Finish Reason
     * @description Reason for finishing the output generation
     * @example stop
     */
    finish_reason: string;
    /**
     * Output
     * @description Answer to the query about the image
     * @example {
     *       "safety_measures": [
     *         "Red hard hat",
     *         "Safety glasses"
     *       ]
     *     }
     */
    output: string;
    /**
     * Reasoning
     * @description Detailed reasoning behind the answer, if enabled
     * @example The worker is wearing a red hard hat for head protection and safety glasses for eye protection.
     */
    reasoning?: string;
    /**
     * Usage Info
     * @description Usage information for the request
     * @example {
     *       "output_tokens": 23,
     *       "decode_time_ms": 811.5944429300725,
     *       "input_tokens": 737,
     *       "ttft_ms": 91.87838807702065,
     *       "prefill_time_ms": 54.45315001998097
     *     }
     */
    usage_info: Components.UsageInfo_1;
}

export interface Moondream3PreviewPointInput {
    /**
     * Image URL
     * @description URL of the image to be processed
     *
     *     Max width: 7000px, Max height: 7000px, Timeout: 20.0s
     * @example https://storage.googleapis.com/falserverless/example_inputs/moondream-3-preview/point_in.jpg
     */
    image_url: string;
    /**
     * Preview
     * @description Whether to preview the output
     * @default false
     * @example true
     */
    preview?: boolean;
    /**
     * Prompt
     * @description Object to be located in the image
     * @example bottle caps
     */
    prompt: string;
}

export interface Moondream3PreviewPointOutput {
    /**
     * Finish Reason
     * @description Reason for finishing the output generation
     * @example stop
     */
    finish_reason: string;
    /**
     * Image
     * @description Image with points drawn on detected objects
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/moondream-3-preview/point_out.png"
     *     }
     */
    image?: Components.ImageFile;
    /**
     * Points
     * @description List of points marking the detected objects
     * @example [
     *       {
     *         "y": 0.8660801564027371,
     *         "x": 0.11827956989247312
     *       },
     *       {
     *         "y": 0.8660801564027371,
     *         "x": 0.3118279569892473
     *       },
     *       {
     *         "y": 0.8660801564027371,
     *         "x": 0.5953079178885631
     *       },
     *       {
     *         "y": 0.8758553274682307,
     *         "x": 0.7888563049853372
     *       },
     *       {
     *         "y": 0.5796676441837733,
     *         "x": 0.9423264907135875
     *       },
     *       {
     *         "y": 0.5796676441837733,
     *         "x": 0.6324535679374389
     *       },
     *       {
     *         "y": 0.6021505376344086,
     *         "x": 0.44281524926686217
     *       },
     *       {
     *         "y": 0.5982404692082112,
     *         "x": 0.3010752688172043
     *       },
     *       {
     *         "y": 0.4701857282502444,
     *         "x": 0.20332355816226785
     *       },
     *       {
     *         "y": 0.4506353861192571,
     *         "x": 0.053763440860215055
     *       },
     *       {
     *         "y": 0.6021505376344086,
     *         "x": 0.053763440860215055
     *       }
     *     ]
     */
    points: Components.Point[];
    /**
     * Usage Info
     * @description Usage information for the request
     * @example {
     *       "output_tokens": 23,
     *       "decode_time_ms": 811.5944429300725,
     *       "input_tokens": 737,
     *       "ttft_ms": 91.87838807702065,
     *       "prefill_time_ms": 54.45315001998097
     *     }
     */
    usage_info: Components.UsageInfo_1;
}

export interface Moondream3PreviewDetectInput {
    /**
     * Image URL
     * @description URL of the image to be processed
     *
     *     Max width: 7000px, Max height: 7000px, Timeout: 20.0s
     * @example https://storage.googleapis.com/falserverless/example_inputs/moondream-3-preview/detect_in.jpg
     */
    image_url: string;
    /**
     * Preview
     * @description Whether to preview the output
     * @default false
     * @example true
     */
    preview?: boolean;
    /**
     * Prompt
     * @description Object to be detected in the image
     * @example Speed limit
     */
    prompt: string;
}

export interface Moondream3PreviewDetectOutput {
    /**
     * Finish Reason
     * @description Reason for finishing the output generation
     * @example stop
     */
    finish_reason: string;
    /**
     * Image
     * @description Image with bounding boxes drawn around detected objects
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/moondream-3-preview/detect_out.png"
     *     }
     */
    image?: Components.ImageFile;
    /**
     * Objects
     * @description List of detected objects with their bounding boxes
     * @example [
     *       {
     *         "y_min": 0.16308235274382246,
     *         "x_max": 0.8755747037932524,
     *         "x_min": 0.8174849247502471,
     *         "y_max": 0.3061258583998726
     *       },
     *       {
     *         "y_min": 0.0987853935125991,
     *         "x_max": 0.7155113776357592,
     *         "x_min": 0.6706078794512399,
     *         "y_max": 0.21011001215700012
     *       }
     *     ]
     */
    objects: Components.Object[];
    /**
     * Usage Info
     * @description Usage information for the request
     * @example {
     *       "output_tokens": 23,
     *       "decode_time_ms": 811.5944429300725,
     *       "input_tokens": 737,
     *       "ttft_ms": 91.87838807702065,
     *       "prefill_time_ms": 54.45315001998097
     *     }
     */
    usage_info: Components.UsageInfo_1;
}

export interface Moondream3PreviewCaptionInput {
    /**
     * Image URL
     * @description URL of the image to be processed
     *
     *     Max width: 7000px, Max height: 7000px, Timeout: 20.0s
     * @example https://storage.googleapis.com/falserverless/example_inputs/moondream-3-preview/caption_in.jpg
     */
    image_url: string;
    /**
     * Length
     * @description Length of the caption to generate
     * @default normal
     * @enum {string}
     */
    length?: 'short' | 'normal' | 'long';
    /**
     * Temperature
     * @description Sampling temperature to use, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If not set, defaults to 0.
     */
    temperature?: number;
    /**
     * Top P
     * @description Nucleus sampling probability mass to use, between 0 and 1.
     */
    top_p?: number;
}

export interface Moondream3PreviewCaptionOutput {
    /**
     * Finish Reason
     * @description Reason for finishing the output generation
     * @example stop
     */
    finish_reason: string;
    /**
     * Output
     * @description Generated caption for the image
     * @example A hedgehog is captured in a close-up shot, focusing on its face and nose. The hedgehog's spines are visible along its back, and its nose is dark and wet-looking. A gold ring with a small diamond is positioned on the grass in front of the hedgehog, partially obscured by its nose. The background is a blurred green grassy field, with small white flowers scattered throughout the grass.
     */
    output: string;
    /**
     * Usage Info
     * @description Usage information for the request
     * @example {
     *       "output_tokens": 23,
     *       "decode_time_ms": 811.5944429300725,
     *       "input_tokens": 737,
     *       "ttft_ms": 91.87838807702065,
     *       "prefill_time_ms": 54.45315001998097
     *     }
     */
    usage_info: Components.UsageInfo_1;
}

export interface Moondream2VisualQueryInput {
    /**
     * Image URL
     * @description URL of the image to be processed
     * @example https://llava-vl.github.io/static/images/monalisa.jpg
     */
    image_url: string;
    /**
     * Query
     * @description Query to be asked in the image
     */
    prompt: string;
}

export interface Moondream2VisualQueryOutput extends SharedType_377 {}

export interface Moondream2PointObjectDetectionInput extends SharedType_bb0 {}

export interface Moondream2PointObjectDetectionOutput extends SharedType_b61 {}

export interface Moondream2ObjectDetectionInput extends SharedType_bb0 {}

export interface Moondream2ObjectDetectionOutput extends SharedType_b61 {}

export interface Moondream2Input {
    /**
     * Image URL
     * @description URL of the image to be processed
     * @example https://llava-vl.github.io/static/images/monalisa.jpg
     */
    image_url: string;
}

export interface Moondream2Output extends SharedType_377 {}

export interface MoondreamBatchedInput {
    /**
     * Input prompt & image pairs
     * @description List of input prompts and image URLs
     * @example [
     *       {
     *         "prompt": "What is the girl doing?",
     *         "image_url": "https://github.com/vikhyat/moondream/raw/main/assets/demo-1.jpg"
     *       }
     *     ]
     */
    inputs: Components.MoondreamInputParam[];
    /**
     * Max Tokens
     * @description Maximum number of new tokens to generate
     * @default 64
     */
    max_tokens?: number;
    /**
     * Model ID
     * @description Model ID to use for inference
     * @default vikhyatk/moondream2
     * @enum {string}
     */
    model_id?: 'vikhyatk/moondream2' | 'fal-ai/moondream2-docci';
    /**
     * Repetition Penalty
     * @description Repetition penalty for sampling
     * @default 1
     */
    repetition_penalty?: number;
    /**
     * Temperature
     * @description Temperature for sampling
     * @default 0.2
     */
    temperature?: number;
    /**
     * Top P
     * @description Top P for sampling
     * @default 1
     */
    top_p?: number;
}

export interface MoondreamBatchedOutput {
    /**
     * Filenames
     * @description Filenames of the images processed
     */
    filenames?: string[];
    /**
     * Outputs
     * @description List of generated outputs
     */
    outputs: string[];
    /**
     * Partial
     * @description Whether the output is partial
     * @default false
     */
    partial?: boolean;
    /**
     * Timings
     * @description Timings for different parts of the process
     */
    timings: {
        [key: string]: number;
    };
}

export interface MoondreamNextDetectionInput {
    /**
     * Combine Points
     * @description Whether to combine points into a single point for point detection. This has no effect for bbox detection or gaze detection.
     * @default false
     */
    combine_points?: boolean;
    /**
     * Detection Prompt
     * @description Text description of what to detect
     * @example Person
     */
    detection_prompt: string;
    /**
     * Image URL
     * @description Image URL to be processed
     * @example https://llava-vl.github.io/static/images/monalisa.jpg
     */
    image_url: string;
    /**
     * Show Visualization
     * @description Whether to show visualization for detection
     * @default true
     */
    show_visualization?: boolean;
    /**
     * Task Type
     * @description Type of detection to perform
     * @enum {string}
     */
    task_type: 'bbox_detection' | 'point_detection' | 'gaze_detection';
    /**
     * Use Ensemble
     * @description Whether to use ensemble for gaze detection
     * @default false
     */
    use_ensemble?: boolean;
}

export interface MoondreamNextDetectionOutput {
    /**
     * Output Image
     * @description Output image with detection visualization
     */
    image?: Components.Image;
    /**
     * Text Output
     * @description Detection results as text
     */
    text_output: string;
}

export interface MoondreamNextBatchInput {
    /**
     * Image URLs
     * @description List of image URLs to be processed (maximum 32 images)
     */
    images_data_url: string;
    /**
     * Max Tokens
     * @description Maximum number of tokens to generate
     * @default 64
     */
    max_tokens?: number;
    /**
     * Prompt
     * @description Single prompt to apply to all images
     * @example Describe this image in detail.
     */
    prompt: string;
}

export interface MoondreamNextBatchOutput {
    /**
     * Captions File
     * @description URL to the generated captions JSON file containing filename-caption pairs.
     */
    captions_file: Components.File;
    /**
     * Outputs
     * @description List of generated captions
     */
    outputs: string[];
}

export interface MoondreamNextInput {
    /**
     * Image URL
     * @description Image URL to be processed
     * @example https://llava-vl.github.io/static/images/monalisa.jpg
     */
    image_url: string;
    /**
     * Max Tokens
     * @description Maximum number of tokens to generate
     * @default 64
     */
    max_tokens?: number;
    /**
     * Prompt
     * @description Prompt for query task
     * @example Describe this image in detail.
     */
    prompt: string;
    /**
     * Task Type
     * @description Type of task to perform
     * @default caption
     * @enum {string}
     */
    task_type?: 'caption' | 'query';
}

export interface MoondreamNextOutput {
    /**
     * Output
     * @description Response from the model
     */
    output: string;
}

export interface MochiV1Input {
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt for the video.
     * @default
     * @example Blurry, shaky footage
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The prompt to generate a video from.
     * @example A dog running in a field.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for generating the video.
     */
    seed?: number;
}

export interface MochiV1Output {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://fal.media/files/zebra/GScPi-7ma3Fn8r1O1on4z_output_1729631871.mp4"
     *     }
     */
    video: Components.File;
}

export interface MmaudioV2TextToAudioInput {
    /**
     * Cfg Strength
     * @description The strength of Classifier Free Guidance.
     * @default 4.5
     */
    cfg_strength?: number;
    /**
     * Duration
     * @description The duration of the audio to generate.
     * @default 8
     */
    duration?: number;
    /**
     * Mask Away Clip
     * @description Whether to mask away the clip.
     * @default false
     */
    mask_away_clip?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the audio for.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Steps
     * @description The number of steps to generate the audio for.
     * @default 25
     */
    num_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the audio for.
     * @example Indian holy music
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator
     */
    seed?: number;
}

export interface MmaudioV2TextToAudioOutput {
    /**
     * Audio
     * @description The generated audio.
     * @example {
     *       "file_size": 1001342,
     *       "file_name": "mmaudio_input.flac",
     *       "content_type": "application/octet-stream",
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/video_models/mmaudio_output.flac"
     *     }
     */
    audio: Components.File;
}

export interface MmaudioV2Input {
    /**
     * Cfg Strength
     * @description The strength of Classifier Free Guidance.
     * @default 4.5
     */
    cfg_strength?: number;
    /**
     * Duration
     * @description The duration of the audio to generate.
     * @default 8
     */
    duration?: number;
    /**
     * Mask Away Clip
     * @description Whether to mask away the clip.
     * @default false
     */
    mask_away_clip?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the audio for.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Steps
     * @description The number of steps to generate the audio for.
     * @default 25
     */
    num_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the audio for.
     * @example Indian holy music
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator
     */
    seed?: number;
    /**
     * Video Url
     * @description The URL of the video to generate the audio for.
     * @example https://storage.googleapis.com/falserverless/model_tests/video_models/mmaudio_input.mp4
     */
    video_url: string;
}

export interface MmaudioV2Output {
    /**
     * Video
     * @description The generated video with the lip sync.
     * @example {
     *       "file_size": 1001342,
     *       "file_name": "mmaudio_input.mp4",
     *       "content_type": "application/octet-stream",
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/video_models/mmaudio_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface MixDehazeNetInput {
    /**
     * Image Url
     * @description URL of image to be used for image enhancement
     */
    image_url: string;
    /**
     * Model
     * @description Model to be used for dehazing
     * @default indoor
     * @enum {string}
     */
    model?: 'indoor' | 'outdoor';
    /**
     * Seed
     * @description seed to be used for generation
     */
    seed?: number;
}

export interface MixDehazeNetOutput extends SharedType_744 {}

export interface MinimaxVoiceDesignInput {
    /**
     * Preview Text
     * @description Text for audio preview. Limited to 500 characters. A fee of $30 per 1M characters will be charged for the generation of the preview audio.
     * @example Oh my gosh, hi. It's like so amazing to be here. This new endpoint just dropped on fal and the results have been like totally incredible. Use it now, It's gonna be like epic!
     */
    preview_text: string;
    /**
     * Prompt
     * @description Voice description prompt for generating a personalized voice
     * @example Bubbly and excitable female pop star interviewee, youthful, slightly breathless, and very enthusiastic
     */
    prompt: string;
}

export interface MinimaxVoiceDesignOutput {
    /**
     * Audio
     * @description The preview audio using the generated voice
     * @example {
     *       "url": "https://v3.fal.media/files/kangaroo/gT22cxTqxgLtGMSDz2JSq_preview.mp3"
     *     }
     */
    audio: Components.File;
    /**
     * Custom Voice Id
     * @description The voice_id of the generated voice
     */
    custom_voice_id: string;
}

export interface MinimaxVoiceCloneInput {
    /**
     * Accuracy
     * @description Text validation accuracy threshold (0-1)
     */
    accuracy?: number;
    /**
     * Audio Url
     * @description URL of the input audio file for voice cloning. Should be at least 10 seconds
     *                 long. To retain the voice permanently, use it with a TTS (text-to-speech)
     *                 endpoint at least once within 7 days. Otherwise, it will be
     *                 automatically deleted.
     * @example https://storage.googleapis.com/falserverless/model_tests/zonos/demo_voice_zonos.wav
     */
    audio_url: string;
    /**
     * Model
     * @description TTS model to use for preview. Options: speech-02-hd, speech-02-turbo, speech-01-hd, speech-01-turbo
     * @default speech-02-hd
     * @example speech-02-hd
     * @example speech-02-turbo
     * @example speech-01-hd
     * @example speech-01-turbo
     * @enum {string}
     */
    model?: 'speech-02-hd' | 'speech-02-turbo' | 'speech-01-hd' | 'speech-01-turbo';
    /**
     * Need Volume Normalization
     * @description Enable volume normalization for the cloned voice
     * @default false
     */
    need_volume_normalization?: boolean;
    /**
     * Noise Reduction
     * @description Enable noise reduction for the cloned voice
     * @default false
     */
    noise_reduction?: boolean;
    /**
     * Text
     * @description Text to generate a TTS preview with the cloned voice (optional)
     * @default Hello, this is a preview of your cloned voice! I hope you like it!
     * @example Hello, this is a preview of your cloned voice! I hope you like it!
     */
    text?: string;
}

export interface MinimaxVoiceCloneOutput {
    /**
     * Audio
     * @description Preview audio generated with the cloned voice (if requested)
     * @example {
     *       "url": "https://fal.media/files/kangaroo/kojPUCNZ9iUGFGMR-xb7h_speech.mp3"
     *     }
     */
    audio?: Components.File;
    /**
     * Custom Voice Id
     * @description The cloned voice ID for use with TTS
     */
    custom_voice_id: string;
}

export interface MinimaxVideo01ImageToVideoInput extends SharedType_371 {}

export interface MinimaxVideo01ImageToVideoOutput extends SharedType_25a {}

export interface MinimaxVideo01SubjectReferenceInput {
    /**
     * Prompt
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage.
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to use the model's prompt optimizer
     * @default true
     */
    prompt_optimizer?: boolean;
    /**
     * Subject Reference Image Url
     * @description URL of the subject reference image to use for consistent subject appearance
     * @example https://fal.media/files/tiger/s2xnjhLpjM6L8ISxlDCAw.png
     */
    subject_reference_image_url: string;
}

export interface MinimaxVideo01SubjectReferenceOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://fal.media/files/rabbit/pONKqOnY7z6GlF6oDESvR_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface MinimaxVideo01LiveImageToVideoInput extends SharedType_371 {}

export interface MinimaxVideo01LiveImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://fal.media/files/monkey/bkT4T4uLOXr0jDsIMlNd5_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface MinimaxVideo01LiveInput {
    /**
     * Prompt
     * @example A rugged middle-aged man with wheat-colored skin and a full beard streaked with gray stands in the harsh sunlight of a desert outpost. His curly hair is windswept, and sweat drips down the bridge of his slightly crooked nose. His faded utility jacket and weathered boots are caked in dust, while his sharp, watchful eyes scan the horizon.
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to use the model's prompt optimizer
     * @default true
     */
    prompt_optimizer?: boolean;
}

export interface MinimaxVideo01LiveOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://fal.media/files/monkey/EbJRdZfaJbNiJBUvPta3c_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface MinimaxVideo01DirectorImageToVideoInput {
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://fal.media/files/elephant/8kkhB12hEZI2kkbU8pZPA_test.jpeg
     */
    image_url: string;
    /**
     * Prompt
     * @description Text prompt for video generation. Camera movement instructions can be added using square brackets (e.g. [Pan left] or [Zoom in]). You can use up to 3 combined movements per prompt. Supported movements: Truck left/right, Pan left/right, Push in/Pull out, Pedestal up/down, Tilt up/down, Zoom in/out, Shake, Tracking shot, Static shot. For example: [Truck left, Pan right, Zoom in]. For a more detailed guide, refer https://sixth-switch-2ac.notion.site/T2V-01-Director-Model-Tutorial-with-camera-movement-1886c20a98eb80f395b8e05291ad8645
     * @example [Push in, Follow]A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse.[Pan left] The street opens into a small plaza where street vendors sell steaming food under colorful awnings.
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to use the model's prompt optimizer
     * @default true
     */
    prompt_optimizer?: boolean;
}

export interface MinimaxVideo01DirectorImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/web-examples/minimax/i2v-01.mp4"
     *     }
     */
    video: Components.File;
}

export interface MinimaxVideo01DirectorInput {
    /**
     * Prompt
     * @description Text prompt for video generation. Camera movement instructions can be added using square brackets (e.g. [Pan left] or [Zoom in]). You can use up to 3 combined movements per prompt. Supported movements: Truck left/right, Pan left/right, Push in/Pull out, Pedestal up/down, Tilt up/down, Zoom in/out, Shake, Tracking shot, Static shot. For example: [Truck left, Pan right, Zoom in]. For a more detailed guide, refer https://sixth-switch-2ac.notion.site/T2V-01-Director-Model-Tutorial-with-camera-movement-1886c20a98eb80f395b8e05291ad8645
     * @example [Push in]Close up of a tense woman looks to the left, startled by a sound, in a darkened kitchen, Pots and pans hang ominously, the window in the kitchen is open and the wind softly blows the pans and creates an ominous mood. [Shake]the woman's shock turns to fear. Black-and-white film noir shot dimly lit, 1950s-style, with dramatic, high-contrast shadows. The overall atmosphere is reminiscent of Alfred Hitchcock's suspenseful storytelling, evoking a looming sense of dread with stark chiaroscuro lighting and a slight film-grain texture.
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to use the model's prompt optimizer
     * @default true
     */
    prompt_optimizer?: boolean;
}

export interface MinimaxVideo01DirectorOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://fal.media/files/panda/4Et1qL4cbedh-OACEw7OF_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface MinimaxVideo01Input {
    /**
     * Prompt
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse.
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to use the model's prompt optimizer
     * @default true
     */
    prompt_optimizer?: boolean;
}

export interface MinimaxVideo01Output extends SharedType_25a {}

export interface MinimaxSpeech26TurboInput {
    /**
     * Audio Setting
     * @description Audio configuration settings
     */
    audio_setting?: Components.AudioSetting;
    /**
     * Language Boost
     * @description Enhance recognition of specified languages and dialects
     * @enum {string}
     */
    language_boost?:
        | 'Chinese'
        | 'Chinese,Yue'
        | 'English'
        | 'Arabic'
        | 'Russian'
        | 'Spanish'
        | 'French'
        | 'Portuguese'
        | 'German'
        | 'Turkish'
        | 'Dutch'
        | 'Ukrainian'
        | 'Vietnamese'
        | 'Indonesian'
        | 'Japanese'
        | 'Italian'
        | 'Korean'
        | 'Thai'
        | 'Polish'
        | 'Romanian'
        | 'Greek'
        | 'Czech'
        | 'Finnish'
        | 'Hindi'
        | 'Bulgarian'
        | 'Danish'
        | 'Hebrew'
        | 'Malay'
        | 'Slovak'
        | 'Swedish'
        | 'Croatian'
        | 'Hungarian'
        | 'Norwegian'
        | 'Slovenian'
        | 'Catalan'
        | 'Nynorsk'
        | 'Afrikaans'
        | 'auto';
    /**
     * Normalization Setting
     * @description Loudness normalization settings for the audio
     */
    normalization_setting?: Components.LoudnessNormalizationSetting;
    /**
     * Output Format
     * @description Format of the output content (non-streaming only)
     * @default hex
     * @enum {string}
     */
    output_format?: 'url' | 'hex';
    /**
     * Prompt
     * @description Text to convert to speech. Paragraph breaks should be marked with newline characters. **NOTE**: You can customize speech pauses by adding markers in the form `<#x#>`, where `x` is the pause duration in seconds. Valid range: `[0.01, 99.99]`, up to two decimal places. Pause markers must be placed between speakable text segments and cannot be used consecutively.
     * @example Hello world! Welcome MiniMax's new text to speech model <#0.1#> Speech 2.6 Turbo, now available on Fal!
     */
    prompt: string;
    /**
     * Pronunciation Dict
     * @description Custom pronunciation dictionary for text replacement
     */
    pronunciation_dict?: Components.PronunciationDict;
    /**
     * Voice Setting
     * @description Voice configuration settings
     * @default {
     *       "speed": 1,
     *       "vol": 1,
     *       "voice_id": "Wise_Woman",
     *       "pitch": 0,
     *       "english_normalization": false
     *     }
     */
    voice_setting?: Components.VoiceSetting;
}

export interface MinimaxSpeech26TurboOutput {
    /**
     * Audio
     * @description The generated audio file
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/minimax-speech-26/speech_26_turbo_out.mp3"
     *     }
     */
    audio: Components.File;
    /**
     * Duration Ms
     * @description Duration of the audio in milliseconds
     */
    duration_ms: number;
}

export interface MinimaxSpeech26HdInput {
    /**
     * Audio Setting
     * @description Audio configuration settings
     */
    audio_setting?: Components.AudioSetting;
    /**
     * Language Boost
     * @description Enhance recognition of specified languages and dialects
     * @enum {string}
     */
    language_boost?:
        | 'Chinese'
        | 'Chinese,Yue'
        | 'English'
        | 'Arabic'
        | 'Russian'
        | 'Spanish'
        | 'French'
        | 'Portuguese'
        | 'German'
        | 'Turkish'
        | 'Dutch'
        | 'Ukrainian'
        | 'Vietnamese'
        | 'Indonesian'
        | 'Japanese'
        | 'Italian'
        | 'Korean'
        | 'Thai'
        | 'Polish'
        | 'Romanian'
        | 'Greek'
        | 'Czech'
        | 'Finnish'
        | 'Hindi'
        | 'Bulgarian'
        | 'Danish'
        | 'Hebrew'
        | 'Malay'
        | 'Slovak'
        | 'Swedish'
        | 'Croatian'
        | 'Hungarian'
        | 'Norwegian'
        | 'Slovenian'
        | 'Catalan'
        | 'Nynorsk'
        | 'Afrikaans'
        | 'auto';
    /**
     * Normalization Setting
     * @description Loudness normalization settings for the audio
     */
    normalization_setting?: Components.LoudnessNormalizationSetting;
    /**
     * Output Format
     * @description Format of the output content (non-streaming only)
     * @default hex
     * @enum {string}
     */
    output_format?: 'url' | 'hex';
    /**
     * Prompt
     * @description Text to convert to speech. Paragraph breaks should be marked with newline characters. **NOTE**: You can customize speech pauses by adding markers in the form `<#x#>`, where `x` is the pause duration in seconds. Valid range: `[0.01, 99.99]`, up to two decimal places. Pause markers must be placed between speakable text segments and cannot be used consecutively.
     * @example Hello world! Welcome MiniMax's new text to speech model <#0.1#> Speech 2.6, now available on Fal!
     */
    prompt: string;
    /**
     * Pronunciation Dict
     * @description Custom pronunciation dictionary for text replacement
     */
    pronunciation_dict?: Components.PronunciationDict;
    /**
     * Voice Setting
     * @description Voice configuration settings
     * @default {
     *       "speed": 1,
     *       "vol": 1,
     *       "voice_id": "Wise_Woman",
     *       "pitch": 0,
     *       "english_normalization": false
     *     }
     */
    voice_setting?: Components.VoiceSetting;
}

export interface MinimaxSpeech26HdOutput {
    /**
     * Audio
     * @description The generated audio file
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/minimax-speech-26/speech_26_hd_out.mp3"
     *     }
     */
    audio: Components.File;
    /**
     * Duration Ms
     * @description Duration of the audio in milliseconds
     */
    duration_ms: number;
}

export interface MinimaxSpeech02TurboInput extends SharedType_1d1 {}

export interface MinimaxSpeech02TurboOutput extends SharedType_ac9 {}

export interface MinimaxSpeech02HdInput extends SharedType_1d1 {}

export interface MinimaxSpeech02HdOutput extends SharedType_ac9 {}

export interface MinimaxPreviewSpeech25TurboInput extends SharedType_dc6 {}

export interface MinimaxPreviewSpeech25TurboOutput extends SharedType_ac9 {}

export interface MinimaxPreviewSpeech25HdInput extends SharedType_dc6 {}

export interface MinimaxPreviewSpeech25HdOutput extends SharedType_ac9 {}

export interface MinimaxImage01SubjectReferenceInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated image
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?: '1:1' | '16:9' | '4:3' | '3:2' | '2:3' | '3:4' | '9:16' | '21:9';
    /**
     * Image Url
     * @description URL of the subject reference image to use for consistent character appearance
     * @example https://v3.fal.media/files/koala/hQwSnkWm8FDjou5SwLNuX_c223cf93-0036-4b18-bbea-bf6d0da7f210.png
     */
    image_url: string;
    /**
     * Num Images
     * @description Number of images to generate (1-9)
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description Text prompt for image generation (max 1500 characters)
     * @example A beautiful woman with a crown on her head.
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to enable automatic prompt optimization
     * @default false
     */
    prompt_optimizer?: boolean;
}

export interface MinimaxImage01SubjectReferenceOutput {
    /**
     * Images
     * @description Generated images
     * @example [
     *       {
     *         "file_size": 239709,
     *         "file_name": "image.jpg",
     *         "content_type": "image/jpeg",
     *         "url": "https://v3.fal.media/files/lion/1bfHvTwZGzK59EYAi2OG7_image.jpg"
     *       }
     *     ]
     */
    images: Components.File[];
}

export interface MinimaxImage01Input {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated image
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?: '1:1' | '16:9' | '4:3' | '3:2' | '2:3' | '3:4' | '9:16' | '21:9';
    /**
     * Num Images
     * @description Number of images to generate (1-9)
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description Text prompt for image generation (max 1500 characters)
     * @example Man dressed in white t shirt, full-body stand front view image, outdoor, Venice beach sign, full-body image, Los Angeles, Fashion photography of 90s, documentary, Film grain, photorealistic
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to enable automatic prompt optimization
     * @default false
     */
    prompt_optimizer?: boolean;
}

export interface MinimaxImage01Output {
    /**
     * Images
     * @description Generated images
     * @example [
     *       {
     *         "file_size": 351366,
     *         "file_name": "image.jpg",
     *         "content_type": "image/jpeg",
     *         "url": "https://v3.fal.media/files/tiger/xLcblZAbiw1kM6ZR_2D-r_image.jpg"
     *       }
     *     ]
     */
    images: Components.File[];
}

export interface MinimaxHailuo23StandardTextToVideoInput {
    /**
     * Duration
     * @description The duration of the video in seconds.
     * @default 6
     * @enum {string}
     */
    duration?: '6' | '10';
    /**
     * Prompt
     * @example An intense electrical storm rages over a modern city skyline at night. Multiple lightning bolts strike simultaneously, illuminating the towering skyscrapers in brilliant white flashes. Thunder clouds roil and churn overhead while constant lightning creates a strobe effect. Rain pours in heavy sheets, visible in the glow of city lights. The camera captures the drama from across a river as lightning reflects in the water. Lightning branches across the sky in intricate patterns. Atmosphere: dramatic, powerful, electrifying urban storm.
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to use the model's prompt optimizer
     * @default true
     */
    prompt_optimizer?: boolean;
}

export interface MinimaxHailuo23StandardTextToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/hailuo23/standard_t2v_out.mp4"
     *     }
     */
    video: Components.File;
}

export interface MinimaxHailuo23StandardImageToVideoInput {
    /**
     * Duration
     * @description The duration of the video in seconds.
     * @default 6
     * @enum {string}
     */
    duration?: '6' | '10';
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://storage.googleapis.com/falserverless/example_inputs/hailuo23/standard_i2v_in.jpg
     */
    image_url: string;
    /**
     * Prompt
     * @description Text prompt for video generation
     * @example The space station slowly rotates in orbit, its solar panels tracking the sun. Earth rotates majestically in the background with weather patterns and landmasses drifting by. The station's communication arrays adjust position. A small spacecraft approaches one of the docking ports. The scene captures the silent majesty of space and human engineering.
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to use the model's prompt optimizer
     * @default true
     */
    prompt_optimizer?: boolean;
}

export interface MinimaxHailuo23StandardImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/hailuo23/standard_i2v_out.mp4"
     *     }
     */
    video: Components.File;
}

export interface MinimaxHailuo23ProTextToVideoInput {
    /**
     * Prompt
     * @description Text prompt for video generation
     * @example The camera follows the snowboarder as they carve down the mountain through deep powder, each turn sending up huge rooster tails of snow. They navigate between trees, floating through the powder with smooth, flowing movements. The rider launches off a natural jump, grabbing the board mid-air before landing softly in deep snow and continuing down. Powder sprays continuously as they link turns together. The atmosphere is exhilarating and free. Audio: Board cutting through snow, powder spraying, wind rushing, the rider's excited shouts, and the soft thuds of landing in deep snow.
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to use the model's prompt optimizer
     * @default true
     */
    prompt_optimizer?: boolean;
}

export interface MinimaxHailuo23ProTextToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/hailuo23/pro_t2v_out.mp4"
     *     }
     */
    video: Components.File;
}

export interface MinimaxHailuo23ProImageToVideoInput {
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://storage.googleapis.com/falserverless/example_inputs/hailuo23/pro_i2v_in.jpg
     */
    image_url: string;
    /**
     * Prompt
     * @description Text prompt for video generation
     * @example The camera follows the mountain biker as they navigate a technical forest trail at high speed, wheels bouncing over roots and rocks. The rider approaches a jump, launching into the air with the bike, both rider and machine perfectly synchronized. They land smoothly and continue through tight turns, splashing through a stream crossing. Mud and water spray as the bike powers through challenging terrain. The atmosphere is wild and adventurous. Audio: Tires gripping dirt, gears shifting, heavy breathing, branches whipping past, and water splashing.
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to use the model's prompt optimizer
     * @default true
     */
    prompt_optimizer?: boolean;
}

export interface MinimaxHailuo23ProImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/hailuo23/pro_i2v_out.mp4"
     *     }
     */
    video: Components.File;
}

export interface MinimaxHailuo23FastStandardImageToVideoInput {
    /**
     * Duration
     * @description The duration of the video in seconds.
     * @default 6
     * @enum {string}
     */
    duration?: '6' | '10';
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://storage.googleapis.com/falserverless/example_inputs/hailuo23/fast_standard_i2v_in.jpg
     */
    image_url: string;
    /**
     * Prompt
     * @description Text prompt for video generation
     * @example Athlete running powerfully on beach, dynamic camera movement tracking the runner, waves and sunset in motion, Hollywood movie cinematography, professional sports filming, inspiring atmosphere
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to use the model's prompt optimizer
     * @default true
     */
    prompt_optimizer?: boolean;
}

export interface MinimaxHailuo23FastStandardImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/hailuo23/fast_standard_i2v_out.mp4"
     *     }
     */
    video: Components.File;
}

export interface MinimaxHailuo23FastProImageToVideoInput {
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://storage.googleapis.com/falserverless/example_inputs/hailuo23/fast_pro_i2v_in.jpg
     */
    image_url: string;
    /**
     * Prompt
     * @description Text prompt for video generation
     * @example Subject-tracking orbit: camera glides parallel to astronaut, Earth rotates beneath, sun crest reveals lens flare, astronaut tethers and spins slowly, 8K 60 fps slow-motion, ends with spacecraft blackout on sun disk
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to use the model's prompt optimizer
     * @default true
     */
    prompt_optimizer?: boolean;
}

export interface MinimaxHailuo23FastProImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/hailuo23/fast_pro_i2v_out.mp4"
     *     }
     */
    video: Components.File;
}

export interface MinimaxHailuo02StandardTextToVideoInput {
    /**
     * Duration
     * @description The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution.
     * @default 6
     * @enum {string}
     */
    duration?: '6' | '10';
    /**
     * Prompt
     * @example A Galactic Smuggler is a rogue figure with a cybernetic arm and a well-worn coat that hints at many dangerous escapades across the galaxy. Their ship is filled with rare and exotic treasures from distant planets, concealed in hidden compartments, showing their expertise in illicit trade. Their belt is adorned with energy-based weapons, ready to be drawn at any moment to protect themselves or escape from tight situations. This character thrives in the shadows of space, navigating between the law and chaos with stealth and wit, always seeking the next big score while evading bounty hunters and law enforcement. The rogue's ship, rugged yet efficient, serves as both a home and a tool for their dangerous lifestyle. The treasures they collect reflect the diverse and intriguing worlds they've encountered—alien artifacts, rare minerals, and artifacts of unknown origin. Their reputation precedes them, with whispers of their dealings and the deadly encounters that often follow. A master of negotiation and deception, the Galactic Smuggler navigates the cosmos with an eye on the horizon, always one step ahead of those who pursue them.
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to use the model's prompt optimizer
     * @default true
     */
    prompt_optimizer?: boolean;
}

export interface MinimaxHailuo02StandardTextToVideoOutput extends SharedType_591 {}

export interface MinimaxHailuo02StandardImageToVideoInput {
    /**
     * Duration
     * @description The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution.
     * @default 6
     * @enum {string}
     */
    duration?: '6' | '10';
    /**
     * End Image Url
     * @description Optional URL of the image to use as the last frame of the video
     */
    end_image_url?: string;
    /**
     * Image Url
     * @example https://storage.googleapis.com/falserverless/model_tests/minimax/1749891352437225630-389852416840474630_1749891352.png
     */
    image_url: string;
    /**
     * Prompt
     * @example Man walked into winter cave with polar bear
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to use the model's prompt optimizer
     * @default true
     */
    prompt_optimizer?: boolean;
    /**
     * Resolution
     * @description The resolution of the generated video.
     * @default 768P
     * @enum {string}
     */
    resolution?: '512P' | '768P';
}

export interface MinimaxHailuo02StandardImageToVideoOutput extends SharedType_424 {}

export interface MinimaxHailuo02ProTextToVideoInput {
    /**
     * Prompt
     * @example A Galactic Smuggler is a rogue figure with a cybernetic arm and a well-worn coat that hints at many dangerous escapades across the galaxy. Their ship is filled with rare and exotic treasures from distant planets, concealed in hidden compartments, showing their expertise in illicit trade. Their belt is adorned with energy-based weapons, ready to be drawn at any moment to protect themselves or escape from tight situations. This character thrives in the shadows of space, navigating between the law and chaos with stealth and wit, always seeking the next big score while evading bounty hunters and law enforcement. The rogue's ship, rugged yet efficient, serves as both a home and a tool for their dangerous lifestyle. The treasures they collect reflect the diverse and intriguing worlds they've encountered—alien artifacts, rare minerals, and artifacts of unknown origin. Their reputation precedes them, with whispers of their dealings and the deadly encounters that often follow. A master of negotiation and deception, the Galactic Smuggler navigates the cosmos with an eye on the horizon, always one step ahead of those who pursue them.
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to use the model's prompt optimizer
     * @default true
     */
    prompt_optimizer?: boolean;
}

export interface MinimaxHailuo02ProTextToVideoOutput extends SharedType_591 {}

export interface MinimaxHailuo02ProImageToVideoInput {
    /**
     * End Image Url
     * @description Optional URL of the image to use as the last frame of the video
     */
    end_image_url?: string;
    /**
     * Image Url
     * @example https://storage.googleapis.com/falserverless/model_tests/minimax/1749891352437225630-389852416840474630_1749891352.png
     */
    image_url: string;
    /**
     * Prompt
     * @example Man walked into winter cave with polar bear
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to use the model's prompt optimizer
     * @default true
     */
    prompt_optimizer?: boolean;
}

export interface MinimaxHailuo02ProImageToVideoOutput extends SharedType_424 {}

export interface MinimaxHailuo02FastImageToVideoInput {
    /**
     * Duration
     * @description The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution.
     * @default 6
     * @enum {string}
     */
    duration?: '6' | '10';
    /**
     * Image Url
     * @example https://v3.fal.media/files/tiger/U9HN_tm5-3Ls52SbD6CrW_image.webp
     */
    image_url: string;
    /**
     * Prompt
     * @example Extremely realistic movement An old samurai is breaking a stone in half
     */
    prompt: string;
    /**
     * Prompt Optimizer
     * @description Whether to use the model's prompt optimizer
     * @default true
     */
    prompt_optimizer?: boolean;
}

export interface MinimaxHailuo02FastImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://v3.fal.media/files/monkey/n3-YfNqnyYJPYlrJk-5rS_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface MinimaxMusicV2Input {
    /**
     * Audio Setting
     * @description Audio configuration settings
     */
    audio_setting?: Components.AudioSetting_1;
    /**
     * Lyrics Prompt
     * @description Lyrics of the song. Use n to separate lines. You may add structure tags like [Intro], [Verse], [Chorus], [Bridge], [Outro] to enhance the arrangement. 10-3000 characters.
     * @example [verse]Streetlights flicker, the night breeze sighsShadows stretch as I walk aloneAn old coat wraps my silent sorrow
     *     Wandering, longing, where should I go[chorus]Pushing the wooden door, the aroma spreadsIn a familiar corner, a stranger gazes
     */
    lyrics_prompt: string;
    /**
     * Prompt
     * @description A description of the music, specifying style, mood, and scenario. 10-300 characters.
     * @example Indie folk, melancholic, introspective, longing, solitary walk, coffee shop
     */
    prompt: string;
}

export interface MinimaxMusicV2Output extends SharedType_c7e {}

export interface MinimaxMusicV15Input {
    /**
     * Audio Setting
     * @description Audio configuration settings
     */
    audio_setting?: Components.AudioSetting_1;
    /**
     * Lyrics Prompt
     * @description Control music generation. 10-3000 characters.
     * @example R&B, energetic
     */
    lyrics_prompt: string;
    /**
     * Prompt
     * @description Lyrics, supports [intro][verse][chorus][bridge][outro] sections. 10-600 characters.
     * @example [verse]
     *      Fast and Limitless
     *      In the heart of the code, where dreams collide,
     *
     *     FAL's the name, taking tech for a ride.
     *     Generative media, blazing the trail,
     *
     *     Fast inference power, we'll never fail.
     *     ##
     */
    prompt: string;
}

export interface MinimaxMusicV15Output extends SharedType_c7e {}

export interface MinimaxMusicInput {
    /**
     * Prompt
     * @description Lyrics with optional formatting. You can use a newline to separate each line of lyrics. You can use two newlines to add a pause between lines. You can use double hash marks (##) at the beginning and end of the lyrics to add accompaniment. Maximum 600 characters.
     * @example ## Fast and Limitless
     *      In the heart of the code, where dreams collide,
     *
     *     FAL's the name, taking tech for a ride.
     *     Generative media, blazing the trail,
     *
     *     Fast inference power, we'll never fail.
     *     ##
     */
    prompt: string;
    /**
     * Reference Audio Url
     * @description Reference song, should contain music and vocals. Must be a .wav or .mp3 file longer than 15 seconds.
     * @example https://fal.media/files/lion/OOTBTSlxKMH_E8H6hoSlb.mpga
     */
    reference_audio_url: string;
}

export interface MinimaxMusicOutput {
    /**
     * Audio
     * @description The generated music
     * @example {
     *       "url": "https://fal.media/files/elephant/N5UNLCwkC2y8v7a3LQLFE_output.mp3"
     *     }
     */
    audio: Components.File;
}

export interface MeshyV6PreviewTextTo3dInput {
    /**
     * Art Style
     * @description Desired art style of the object. Note: enable_pbr should be false for sculpture style.
     * @default realistic
     * @enum {string}
     */
    art_style?: 'realistic' | 'sculpture';
    /**
     * Enable Pbr
     * @description Generate PBR Maps (metallic, roughness, normal) in addition to base color. Should be false for sculpture style.
     * @default false
     */
    enable_pbr?: boolean;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Is A T Pose
     * @description Whether to generate the model in an A/T pose
     * @default false
     */
    is_a_t_pose?: boolean;
    /**
     * Mode
     * @description Generation mode. 'preview' returns untextured geometry only, 'full' returns textured model (preview + refine).
     * @default full
     * @enum {string}
     */
    mode?: 'preview' | 'full';
    /**
     * Prompt
     * @description Describe what kind of object the 3D model is. Maximum 600 characters.
     * @example A rustic, antique wooden treasure chest with a curved, domed lid, constructed from weathered, dark brown planks exhibiting prominent wood grain and subtle distress. It's heavily reinforced with broad, dark grey, oxidized metal bands secured by numerous circular rivets. Ornate, dark iron decorative elements featuring swirling foliate patterns and dragon motifs adorn the corners and lid. A prominent, circular, intricately carved metal lock plate with a central keyhole dominates the front, flanked by two large, dark metallic pull rings.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed for reproducible results. Same prompt and seed usually generate the same result.
     */
    seed?: number;
    /**
     * Should Remesh
     * @description Whether to enable the remesh phase. When false, returns unprocessed triangular mesh.
     * @default true
     */
    should_remesh?: boolean;
    /**
     * Symmetry Mode
     * @description Controls symmetry behavior during model generation.
     * @default auto
     * @enum {string}
     */
    symmetry_mode?: 'off' | 'auto' | 'on';
    /**
     * Target Polycount
     * @description Target number of polygons in the generated model
     * @default 30000
     */
    target_polycount?: number;
    /**
     * Texture Image Url
     * @description 2D image to guide the texturing process (only used in 'full' mode)
     */
    texture_image_url?: string;
    /**
     * Texture Prompt
     * @description Additional text prompt to guide the texturing process (only used in 'full' mode)
     */
    texture_prompt?: string;
    /**
     * Topology
     * @description Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry.
     * @default triangle
     * @enum {string}
     */
    topology?: 'quad' | 'triangle';
}

export interface MeshyV6PreviewTextTo3dOutput {
    /**
     * Actual Prompt
     * @description The actual prompt used if prompt expansion was enabled
     */
    actual_prompt?: string;
    /**
     * Model Glb
     * @description Generated 3D object in GLB format.
     * @example {
     *       "file_size": 9314028,
     *       "file_name": "model.glb",
     *       "content_type": "model/gltf-binary",
     *       "url": "https://v3b.fal.media/files/b/penguin/DId89qXLu6BXu09RFAwAV_model.glb"
     *     }
     */
    model_glb: Components.File;
    /**
     * Model Urls
     * @description URLs for different 3D model formats
     * @example {
     *       "fbx": {
     *         "file_size": 5444380,
     *         "file_name": "model.fbx",
     *         "content_type": "application/octet-stream",
     *         "url": "https://v3b.fal.media/files/b/kangaroo/7nUUw5dHN9a0DKlOpAKbP_model.fbx"
     *       },
     *       "usdz": {
     *         "file_size": 9834246,
     *         "file_name": "model.usdz",
     *         "content_type": "model/vnd.usdz+zip",
     *         "url": "https://v3b.fal.media/files/b/panda/XcC-mIJywUvH7coyrzENU_model.usdz"
     *       },
     *       "glb": {
     *         "file_size": 9314028,
     *         "file_name": "model.glb",
     *         "content_type": "model/gltf-binary",
     *         "url": "https://v3b.fal.media/files/b/penguin/DId89qXLu6BXu09RFAwAV_model.glb"
     *       },
     *       "obj": {
     *         "file_size": 2755145,
     *         "file_name": "model.obj",
     *         "content_type": "text/plain",
     *         "url": "https://v3b.fal.media/files/b/monkey/cCNMHqUbKSNtDN1iGmiYm_model.obj"
     *       }
     *     }
     */
    model_urls: Components.ModelUrls_1;
    /**
     * Prompt
     * @description The text prompt used for generation
     * @example A rustic, antique wooden treasure chest with a curved, domed lid, constructed from weathered, dark brown planks exhibiting prominent wood grain and subtle distress. It's heavily reinforced with broad, dark grey, oxidized metal bands secured by numerous circular rivets. Ornate, dark iron decorative elements featuring swirling foliate patterns and dragon motifs adorn the corners and lid. A prominent, circular, intricately carved metal lock plate with a central keyhole dominates the front, flanked by two large, dark metallic pull rings.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation
     * @example 4002110719
     */
    seed?: number;
    /**
     * Texture Urls
     * @description Array of texture file objects
     * @example [
     *       {
     *         "base_color": {
     *           "file_size": 4254502,
     *           "file_name": "texture_0.png",
     *           "content_type": "image/png",
     *           "url": "https://v3b.fal.media/files/b/panda/DoPKAuZY0tTjnr6C9ee-Q_texture_0.png"
     *         }
     *       }
     *     ]
     */
    texture_urls?: Components.TextureFiles[];
    /**
     * Thumbnail
     * @description Preview thumbnail of the generated model
     * @example {
     *       "file_size": 173792,
     *       "file_name": "preview.png",
     *       "content_type": "image/png",
     *       "url": "https://v3b.fal.media/files/b/koala/6LJISu4ilkZXcdOETwl_d_preview.png"
     *     }
     */
    thumbnail?: Components.File;
}

export interface MeshyV6PreviewImageTo3dInput {
    /**
     * Enable Pbr
     * @description Generate PBR Maps (metallic, roughness, normal) in addition to base color
     * @default false
     */
    enable_pbr?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Url
     * @description Image URL or base64 data URI for 3D model creation. Supports .jpg, .jpeg, and .png formats. Also supports AVIF and HEIF formats which will be automatically converted.
     * @example https://v3b.fal.media/files/b/zebra/3osHJDI8IZ2wl6sGtEUeB_image.png
     */
    image_url: string;
    /**
     * Is A T Pose
     * @description Whether to generate the model in an A/T pose
     * @default false
     */
    is_a_t_pose?: boolean;
    /**
     * Should Remesh
     * @description Whether to enable the remesh phase
     * @default true
     */
    should_remesh?: boolean;
    /**
     * Should Texture
     * @description Whether to generate textures
     * @default true
     */
    should_texture?: boolean;
    /**
     * Symmetry Mode
     * @description Controls symmetry behavior during model generation. Off disables symmetry, Auto determines it automatically, On enforces symmetry.
     * @default auto
     * @enum {string}
     */
    symmetry_mode?: 'off' | 'auto' | 'on';
    /**
     * Target Polycount
     * @description Target number of polygons in the generated model
     * @default 30000
     */
    target_polycount?: number;
    /**
     * Texture Image Url
     * @description 2D image to guide the texturing process
     */
    texture_image_url?: string;
    /**
     * Texture Prompt
     * @description Text prompt to guide the texturing process
     */
    texture_prompt?: string;
    /**
     * Topology
     * @description Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry.
     * @default triangle
     * @enum {string}
     */
    topology?: 'quad' | 'triangle';
}

export interface MeshyV6PreviewImageTo3dOutput {
    /**
     * Model Glb
     * @description Generated 3D object in GLB format.
     * @example {
     *       "file_size": 9242744,
     *       "file_name": "model.glb",
     *       "content_type": "model/gltf-binary",
     *       "url": "https://v3b.fal.media/files/b/zebra/OXF1e1bO3JddPTaugv0eL_model.glb"
     *     }
     */
    model_glb: Components.File;
    /**
     * Model Urls
     * @description URLs for different 3D model formats
     * @example {
     *       "fbx": {
     *         "file_size": 5427052,
     *         "file_name": "model.fbx",
     *         "content_type": "application/octet-stream",
     *         "url": "https://v3b.fal.media/files/b/kangaroo/4Q2qdpTvfLVdzAKH1-72v_model.fbx"
     *       },
     *       "usdz": {
     *         "file_size": 9991969,
     *         "file_name": "model.usdz",
     *         "content_type": "model/vnd.usdz+zip",
     *         "url": "https://v3b.fal.media/files/b/lion/RgJG9EBQ_GAHMVWV3wCis_model.usdz"
     *       },
     *       "glb": {
     *         "file_size": 9242744,
     *         "file_name": "model.glb",
     *         "content_type": "model/gltf-binary",
     *         "url": "https://v3b.fal.media/files/b/zebra/OXF1e1bO3JddPTaugv0eL_model.glb"
     *       },
     *       "obj": {
     *         "file_size": 2744413,
     *         "file_name": "model.obj",
     *         "content_type": "text/plain",
     *         "url": "https://v3b.fal.media/files/b/koala/_Vg0d084-hd3EdpIJDf7U_model.obj"
     *       }
     *     }
     */
    model_urls: Components.ModelUrls_1;
    /**
     * Seed
     * @description The seed used for generation (if available)
     * @example 2009275957
     */
    seed?: number;
    /**
     * Texture Urls
     * @description Array of texture file objects, matching Meshy API structure
     * @example [
     *       {
     *         "base_color": {
     *           "file_size": 4328755,
     *           "file_name": "texture_0.png",
     *           "content_type": "image/png",
     *           "url": "https://v3b.fal.media/files/b/tiger/NkgxcEom_42V4_8UUXiRR_texture_0.png"
     *         }
     *       }
     *     ]
     */
    texture_urls?: Components.TextureFiles[];
    /**
     * Thumbnail
     * @description Preview thumbnail of the generated model
     * @example {
     *       "file_size": 54279,
     *       "file_name": "preview.png",
     *       "content_type": "image/png",
     *       "url": "https://v3b.fal.media/files/b/penguin/rfnS6ClmeEWgDXp_oD5tN_preview.png"
     *     }
     */
    thumbnail?: Components.File;
}

export interface MeshyV5RetextureInput {
    /**
     * Enable Original Uv
     * @description Use the original UV mapping of the model instead of generating new UVs. If the model has no original UV, output quality may be reduced.
     * @default true
     */
    enable_original_uv?: boolean;
    /**
     * Enable Pbr
     * @description Generate PBR Maps (metallic, roughness, normal) in addition to base color.
     * @default false
     */
    enable_pbr?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Style Url
     * @description 2D image to guide the texturing process. Supports .jpg, .jpeg, and .png formats. Required if text_style_prompt is not provided. If both are provided, image_style_url takes precedence.
     */
    image_style_url?: string;
    /**
     * Model Url
     * @description URL or base64 data URI of a 3D model to texture. Supports .glb, .gltf, .obj, .fbx, .stl formats. Can be a publicly accessible URL or data URI with MIME type application/octet-stream.
     * @example https://v3b.fal.media/files/b/penguin/DId89qXLu6BXu09RFAwAV_model.glb
     */
    model_url: string;
    /**
     * Text Style Prompt
     * @description Describe your desired texture style using text. Maximum 600 characters. Required if image_style_url is not provided.
     * @example red and black chest
     */
    text_style_prompt?: string;
}

export interface MeshyV5RetextureOutput {
    /**
     * Image Style Url
     * @description The image URL used for texturing (if provided)
     */
    image_style_url?: string;
    /**
     * Model Glb
     * @description Retextured 3D object in GLB format.
     * @example {
     *       "file_size": 4097640,
     *       "file_name": "model.glb",
     *       "content_type": "model/gltf-binary",
     *       "url": "https://v3b.fal.media/files/b/tiger/pU0TtsRTxXM6VnKEYTHSV_model.glb"
     *     }
     */
    model_glb: Components.File;
    /**
     * Model Urls
     * @description URLs for different 3D model formats
     * @example {
     *       "fbx": {
     *         "file_size": 4713692,
     *         "file_name": "model.fbx",
     *         "content_type": "application/octet-stream",
     *         "url": "https://v3b.fal.media/files/b/rabbit/agzKf8N8zeVnteg72NiF4_model.fbx"
     *       },
     *       "usdz": {
     *         "file_size": 3886518,
     *         "file_name": "model.usdz",
     *         "content_type": "model/vnd.usdz+zip",
     *         "url": "https://v3b.fal.media/files/b/panda/4ItUhLHiH4foEw30qcWZv_model.usdz"
     *       },
     *       "glb": {
     *         "file_size": 4097640,
     *         "file_name": "model.glb",
     *         "content_type": "model/gltf-binary",
     *         "url": "https://v3b.fal.media/files/b/tiger/pU0TtsRTxXM6VnKEYTHSV_model.glb"
     *       },
     *       "obj": {
     *         "file_size": 2964508,
     *         "file_name": "model.obj",
     *         "content_type": "text/plain",
     *         "url": "https://v3b.fal.media/files/b/zebra/M5aK_b6vKH7KeGCZoSLq7_model.obj"
     *       }
     *     }
     */
    model_urls: Components.ModelUrls_1;
    /**
     * Text Style Prompt
     * @description The text prompt used for texturing (if provided)
     * @example red and black chest
     */
    text_style_prompt?: string;
    /**
     * Texture Urls
     * @description Array of texture file objects
     * @example [
     *       {
     *         "base_color": {
     *           "file_size": 3455709,
     *           "file_name": "texture_0.png",
     *           "content_type": "image/png",
     *           "url": "https://v3b.fal.media/files/b/zebra/opBcVy7AK-GOlCTUz-_GZ_texture_0.png"
     *         }
     *       }
     *     ]
     */
    texture_urls?: Components.TextureFiles[];
    /**
     * Thumbnail
     * @description Preview thumbnail of the retextured model
     * @example {
     *       "file_size": 124859,
     *       "file_name": "preview.png",
     *       "content_type": "image/png",
     *       "url": "https://v3b.fal.media/files/b/lion/jFrzgIJMbaPQi_4HCEa8u_preview.png"
     *     }
     */
    thumbnail?: Components.File;
}

export interface MeshyV5RemeshInput {
    /**
     * Model Url
     * @description URL or base64 data URI of a 3D model to remesh. Supports .glb, .gltf, .obj, .fbx, .stl formats. Can be a publicly accessible URL or data URI with MIME type application/octet-stream.
     * @example https://v3b.fal.media/files/b/tiger/62QMEQqZ3pjUds4DfuVtX_model.glb
     */
    model_url: string;
    /**
     * Origin At
     * @description Position of the origin. None means no effect.
     * @enum {string}
     */
    origin_at?: 'bottom' | 'center';
    /**
     * Resize Height
     * @description Resize the model to a certain height measured in meters. Set to 0 for no resizing.
     * @default 0
     */
    resize_height?: number;
    /**
     * Target Formats
     * @description List of target formats for the remeshed model.
     * @default [
     *       "glb"
     *     ]
     * @example [
     *       "glb",
     *       "fbx"
     *     ]
     */
    target_formats?: ('glb' | 'fbx' | 'obj' | 'usdz' | 'blend' | 'stl')[];
    /**
     * Target Polycount
     * @description Target number of polygons in the generated model. Actual count may vary based on geometry complexity.
     * @default 30000
     * @example 137220
     */
    target_polycount?: number;
    /**
     * Topology
     * @description Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry.
     * @default triangle
     * @example triangle
     * @enum {string}
     */
    topology?: 'quad' | 'triangle';
}

export interface MeshyV5RemeshOutput {
    /**
     * Model Glb
     * @description Remeshed 3D object in GLB format (if GLB was requested).
     * @example {
     *       "file_size": 3294196,
     *       "file_name": "model.glb",
     *       "content_type": "model/gltf-binary",
     *       "url": "https://v3b.fal.media/files/b/panda/hu-dDvUIPdoHSxR7QcBZw_model.glb"
     *     }
     */
    model_glb?: Components.File;
    /**
     * Model Urls
     * @description URLs for different 3D model formats
     * @example {
     *       "fbx": {
     *         "file_size": 3476092,
     *         "file_name": "model.fbx",
     *         "content_type": "application/octet-stream",
     *         "url": "https://v3b.fal.media/files/b/kangaroo/Kw7C1w2Uccg3zKQ5axnwk_model.fbx"
     *       },
     *       "glb": {
     *         "file_size": 3294196,
     *         "file_name": "model.glb",
     *         "content_type": "model/gltf-binary",
     *         "url": "https://v3b.fal.media/files/b/panda/hu-dDvUIPdoHSxR7QcBZw_model.glb"
     *       }
     *     }
     */
    model_urls: Components.ModelUrls_1;
}

export interface MeshyV5MultiImageTo3dInput {
    /**
     * Enable Pbr
     * @description Generate PBR Maps (metallic, roughness, normal) in addition to base color. Requires should_texture to be true.
     * @default false
     */
    enable_pbr?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, input data will be checked for safety before processing.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Urls
     * @description 1 to 4 images for 3D model creation. All images should depict the same object from different angles. Supports .jpg, .jpeg, .png formats, and AVIF/HEIF which will be automatically converted. If more than 4 images are provided, only the first 4 will be used.
     * @example [
     *       "https://v3b.fal.media/files/b/kangaroo/cPyD3-por0XI7jDa9F9vP_image%20(3).png",
     *       "https://v3b.fal.media/files/b/elephant/9sd5JWAOJBcR7G3NMjPVs_image%20(2).png",
     *       "https://v3b.fal.media/files/b/tiger/TP4sTzPATX_w1Tn4m6kYM_image%20(1).png"
     *     ]
     */
    image_urls: string[];
    /**
     * Is A T Pose
     * @description Whether to generate the model in an A/T pose
     * @default false
     */
    is_a_t_pose?: boolean;
    /**
     * Should Remesh
     * @description Whether to enable the remesh phase. When false, returns triangular mesh ignoring topology and target_polycount.
     * @default true
     */
    should_remesh?: boolean;
    /**
     * Should Texture
     * @description Whether to generate textures. False provides mesh without textures for 5 credits, True adds texture generation for additional 10 credits.
     * @default true
     */
    should_texture?: boolean;
    /**
     * Symmetry Mode
     * @description Controls symmetry behavior during model generation.
     * @default auto
     * @enum {string}
     */
    symmetry_mode?: 'off' | 'auto' | 'on';
    /**
     * Target Polycount
     * @description Target number of polygons in the generated model
     * @default 30000
     */
    target_polycount?: number;
    /**
     * Texture Image Url
     * @description 2D image to guide the texturing process. Requires should_texture to be true.
     */
    texture_image_url?: string;
    /**
     * Texture Prompt
     * @description Text prompt to guide the texturing process. Requires should_texture to be true.
     */
    texture_prompt?: string;
    /**
     * Topology
     * @description Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry.
     * @default triangle
     * @enum {string}
     */
    topology?: 'quad' | 'triangle';
}

export interface MeshyV5MultiImageTo3dOutput {
    /**
     * Model Glb
     * @description Generated 3D object in GLB format.
     * @example {
     *       "file_size": 7875308,
     *       "file_name": "model.glb",
     *       "content_type": "model/gltf-binary",
     *       "url": "https://v3b.fal.media/files/b/tiger/62QMEQqZ3pjUds4DfuVtX_model.glb"
     *     }
     */
    model_glb: Components.File;
    /**
     * Model Urls
     * @description URLs for different 3D model formats
     * @example {
     *       "fbx": {
     *         "file_size": 5574540,
     *         "file_name": "model.fbx",
     *         "content_type": "application/octet-stream",
     *         "url": "https://v3b.fal.media/files/b/koala/R7vPBgkecVvcnbNpRAy9x_model.fbx"
     *       },
     *       "usdz": {
     *         "file_size": 8631497,
     *         "file_name": "model.usdz",
     *         "content_type": "model/vnd.usdz+zip",
     *         "url": "https://v3b.fal.media/files/b/panda/fSGLGmtgzUjhepklN06Zw_model.usdz"
     *       },
     *       "glb": {
     *         "file_size": 7875308,
     *         "file_name": "model.glb",
     *         "content_type": "model/gltf-binary",
     *         "url": "https://v3b.fal.media/files/b/tiger/62QMEQqZ3pjUds4DfuVtX_model.glb"
     *       },
     *       "obj": {
     *         "file_size": 2761323,
     *         "file_name": "model.obj",
     *         "content_type": "text/plain",
     *         "url": "https://v3b.fal.media/files/b/koala/xmOnmSeePfuROe3pqHpf0_model.obj"
     *       }
     *     }
     */
    model_urls: Components.ModelUrls_1;
    /**
     * Seed
     * @description The seed used for generation (if available)
     * @example 783032043
     */
    seed?: number;
    /**
     * Texture Urls
     * @description Array of texture file objects
     * @example [
     *       {
     *         "base_color": {
     *           "file_size": 4464364,
     *           "file_name": "texture_0.png",
     *           "content_type": "image/png",
     *           "url": "https://v3b.fal.media/files/b/panda/OVrRor7IgeNK9w2i5-NDf_texture_0.png"
     *         }
     *       }
     *     ]
     */
    texture_urls?: Components.TextureFiles[];
    /**
     * Thumbnail
     * @description Preview thumbnail of the generated model
     * @example {
     *       "file_size": 70958,
     *       "file_name": "preview.png",
     *       "content_type": "image/png",
     *       "url": "https://v3b.fal.media/files/b/koala/2NI_hEd7jXzS5rLQhnRga_preview.png"
     *     }
     */
    thumbnail?: Components.File;
}

export interface MayaStreamInput {
    /**
     * Max Tokens
     * @description Maximum number of SNAC tokens to generate (7 tokens per frame). Controls maximum audio length.
     * @default 2000
     */
    max_tokens?: number;
    /**
     * Output Format
     * @description Output audio format. 'mp3' for browser-playable audio, 'wav' for uncompressed audio, 'pcm' for raw PCM (lowest latency, requires client-side decoding).
     * @default mp3
     * @example mp3
     * @example wav
     * @example pcm
     * @enum {string}
     */
    output_format?: 'mp3' | 'wav' | 'pcm';
    /**
     * Prompt
     * @description Description of the voice/character. Includes attributes like age, accent, pitch, timbre, pacing, tone, and intensity. See examples for format.
     * @example Realistic male voice in the 30s age with american accent. Normal pitch, warm timbre, conversational pacing, neutral tone delivery at med intensity.
     * @example Creative, dark_villain character. Male voice in their 40s with british accent. Low pitch, gravelly timbre, slow pacing, angry tone at high intensity.
     */
    prompt: string;
    /**
     * Repetition Penalty
     * @description Penalty for repeating tokens. Higher values reduce repetition artifacts.
     * @default 1.1
     */
    repetition_penalty?: number;
    /**
     * Sample Rate
     * @description Output audio sample rate. 48 kHz uses upsampling for higher quality audio, 24 kHz is native SNAC output (faster, lower latency).
     * @default 24 kHz
     * @example 48 kHz
     * @example 24 kHz
     * @enum {string}
     */
    sample_rate?: '48 kHz' | '24 kHz';
    /**
     * Temperature
     * @description Sampling temperature. Lower values (0.2-0.5) produce more stable/consistent audio. Higher values add variation.
     * @default 0.4
     */
    temperature?: number;
    /**
     * Text
     * @description The text to synthesize into speech. You can embed emotion tags anywhere in the text using the format <emotion_name>. Available emotions: laugh, laugh_harder, sigh, chuckle, gasp, angry, excited, whisper, cry, scream, sing, snort, exhale, gulp, giggle, sarcastic, curious. Example: 'Hello world! <excited> This is amazing!' or 'I can't believe this <sigh> happened again.'
     * @example Hello world! This is a test of the Maya-1-Voice text-to-speech system.
     * @example The darkness isn't coming... <angry> it's already here!
     */
    text: string;
    /**
     * Top P
     * @description Nucleus sampling parameter. Controls diversity of token selection.
     * @default 0.9
     */
    top_p?: number;
}

export interface MayaStreamOutput extends SharedType_4411 {}

export interface MayaBatchInput {
    /**
     * Max Tokens
     * @description Maximum SNAC tokens per generation.
     * @default 2000
     */
    max_tokens?: number;
    /**
     * Output Format
     * @description Output audio format for all generated speech files
     * @default wav
     * @example wav
     * @example mp3
     * @enum {string}
     */
    output_format?: 'wav' | 'mp3';
    /**
     * Prompts
     * @description List of voice descriptions for each text. Must match the length of texts list. Each describes the voice/character attributes.
     */
    prompts: string[];
    /**
     * Repetition Penalty
     * @description Repetition penalty for all generations.
     * @default 1.1
     */
    repetition_penalty?: number;
    /**
     * Sample Rate
     * @description Output audio sample rate for all generations. 48 kHz provides higher quality, 24 kHz is faster.
     * @default 48 kHz
     * @example 48 kHz
     * @example 24 kHz
     * @enum {string}
     */
    sample_rate?: '48 kHz' | '24 kHz';
    /**
     * Temperature
     * @description Sampling temperature for all generations.
     * @default 0.4
     */
    temperature?: number;
    /**
     * Texts
     * @description List of texts to synthesize into speech. You can embed emotion tags in each text using the format <emotion_name>.
     */
    texts: string[];
    /**
     * Top P
     * @description Nucleus sampling parameter for all generations.
     * @default 0.9
     */
    top_p?: number;
}

export interface MayaBatchOutput {
    /**
     * Audios
     * @description List of generated audio files
     */
    audios: Components.File_1[];
    /**
     * Average Rtf
     * @description Average real-time factor across all generations
     * @example 0.15
     */
    average_rtf: number;
    /**
     * Durations
     * @description Duration of each generated audio in seconds
     */
    durations: number[];
    /**
     * Sample Rate
     * @description Sample rate of all generated audio files
     * @example 48 kHz
     * @example 24 kHz
     */
    sample_rate: string;
    /**
     * Total Generation Time
     * @description Total time taken to generate all audio files in seconds
     * @example 5.7
     */
    total_generation_time: number;
}

export interface MayaInput {
    /**
     * Max Tokens
     * @description Maximum number of SNAC tokens to generate (7 tokens per frame). Controls maximum audio length.
     * @default 2000
     */
    max_tokens?: number;
    /**
     * Output Format
     * @description Output audio format for the generated speech
     * @default wav
     * @example wav
     * @example mp3
     * @enum {string}
     */
    output_format?: 'wav' | 'mp3';
    /**
     * Prompt
     * @description Description of the voice/character. Includes attributes like age, accent, pitch, timbre, pacing, tone, and intensity. See examples for format.
     * @example Realistic male voice in the 30s age with american accent. Normal pitch, warm timbre, conversational pacing, neutral tone delivery at med intensity.
     * @example Creative, dark_villain character. Male voice in their 40s with british accent. Low pitch, gravelly timbre, slow pacing, angry tone at high intensity.
     */
    prompt: string;
    /**
     * Repetition Penalty
     * @description Penalty for repeating tokens. Higher values reduce repetition artifacts.
     * @default 1.1
     */
    repetition_penalty?: number;
    /**
     * Sample Rate
     * @description Output audio sample rate. 48 kHz provides higher quality audio, 24 kHz is faster.
     * @default 48 kHz
     * @example 48 kHz
     * @example 24 kHz
     * @enum {string}
     */
    sample_rate?: '48 kHz' | '24 kHz';
    /**
     * Temperature
     * @description Sampling temperature. Lower values (0.2-0.5) produce more stable/consistent audio. Higher values add variation.
     * @default 0.4
     */
    temperature?: number;
    /**
     * Text
     * @description The text to synthesize into speech. You can embed emotion tags anywhere in the text using the format <emotion_name>. Available emotions: laugh, laugh_harder, sigh, chuckle, gasp, angry, excited, whisper, cry, scream, sing, snort, exhale, gulp, giggle, sarcastic, curious. Example: 'Hello world! <excited> This is amazing!' or 'I can't believe this <sigh> happened again.'
     * @example Hello world! This is a test of the Maya-1-Voice text-to-speech system.
     * @example The darkness isn't coming... <angry> it's already here!
     * @example That's hilarious! <laugh> I can't stop <laugh_harder> thinking about it!
     * @example <whisper> I have a secret to tell you. <excited> You won't believe what happened!
     */
    text: string;
    /**
     * Top P
     * @description Nucleus sampling parameter. Controls diversity of token selection.
     * @default 0.9
     */
    top_p?: number;
}

export interface MayaOutput {
    /** @description The generated audio file containing the speech (WAV or MP3 format, 24kHz or 48kHz mono depending on upsampler) */
    audio: Components.File_1;
    /**
     * Duration
     * @description Duration of the generated audio in seconds
     * @example 4.5
     */
    duration: number;
    /**
     * Generation Time
     * @description Time taken to generate the audio in seconds
     * @example 2.3
     */
    generation_time: number;
    /**
     * Rtf
     * @description Real-time factor (generation_time / audio_duration). Lower is better.
     * @example 0.51
     */
    rtf: number;
    /**
     * Sample Rate
     * @description Sample rate of the generated audio
     * @example 48 kHz
     * @example 24 kHz
     */
    sample_rate: string;
}

export interface MagiImageToVideoInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Url
     * @description URL of the input image to represent the first frame of the video. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://v3.fal.media/files/kangaroo/sGqTf5scZcC5VNfOLbxwE_maxresdefault-2740110268.jpg
     */
    image_url: string;
    /**
     * Num Frames
     * @description Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.
     * @default 96
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 16
     * @enum {integer}
     */
    num_inference_steps?: 4 | 8 | 16 | 32 | 64;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A crisp, wintery mountain landscape unfolds as a snowboarder, equipped with a selfie pole, gracefully navigates a snow-covered slope, the camera perspective offering an exhilarating attached-third-person view of the descent;  the vibrant, snowy scenery sweeps past, punctuated by moments of controlled spins and effortless glides, creating a dynamic visual rhythm that complements the exhilarating pace of the ride;  as the snowboarder carves through pristine powder, the camera captures fleeting moments of breathtaking views—towering pines dusted with snow, sunlit peaks piercing a cerulean sky—a symphony of nature’s grandeur displayed for the viewer to share;  a sense of freedom and exhilaration permeates the scene, punctuated by the subtle whoosh of wind and the satisfying crunch of snow, culminating in a breathtaking panorama as the snowboarder reaches the bottom, leaving the viewer with a lingering sense of wonder and the desire to experience the thrill firsthand.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface MagiImageToVideoOutput {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://v3.fal.media/files/penguin/sSJdxpy9oEBqZpGIh3SPq_3381fe86-9bab-4ce4-9c3a-5db66984618a.mp4"
     *     }
     */
    video: Components.File;
}

export interface MagiExtendVideoInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Num Frames
     * @description Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.
     * @default 96
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 16
     * @enum {integer}
     */
    num_inference_steps?: 4 | 8 | 16 | 32 | 64;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Start Frame
     * @description The frame to begin the generation from, with the remaining frames will be treated as the prefix video. The final video will contain the frames up until this number unchanged, followed by the generated frames. The default start frame is 32 frames before the end of the video, which gives optimal results.
     */
    start_frame?: number;
    /**
     * Video Url
     * @description URL of the input video to represent the beginning of the video. If the input video does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://v3.fal.media/files/zebra/w4T087gvzG5LMGipMpPCO_pour-2s.mp4
     */
    video_url: string;
}

export interface MagiExtendVideoOutput {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://v3.fal.media/files/zebra/yVrs367uHeCqrBGY-VICa_3b064421-fe96-4ccb-a3ea-4f37b54e682e.mp4"
     *     }
     */
    video: Components.File;
}

export interface MagiDistilledImageToVideoInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Url
     * @description URL of the input image to represent the first frame of the video. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://raw.githubusercontent.com/painebenjamin/pointy-seeds/refs/heads/main/captain-start.jpg
     */
    image_url: string;
    /**
     * Num Frames
     * @description Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.
     * @default 96
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 16
     * @enum {integer}
     */
    num_inference_steps?: 4 | 8 | 16 | 32;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example Close-up shot: the old sea captain stares intently, pipe in mouth, wisps of smoke curling around his weathered face. The camera begins to pull back out over the ocean. Finally, the camera sinks below the waves deeply, fading to dark blue and finally to black.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface MagiDistilledImageToVideoOutput {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://v3.fal.media/files/zebra/3XmM6yIGZEWxwbbDyTkhw_391bee80-b756-425d-b74c-f9083c7eec4f.mp4"
     *     }
     */
    video: Components.File;
}

export interface MagiDistilledExtendVideoInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Num Frames
     * @description Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.
     * @default 96
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 16
     * @enum {integer}
     */
    num_inference_steps?: 4 | 8 | 16 | 32;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Start Frame
     * @description The frame to begin the generation from, with the remaining frames will be treated as the prefix video. The final video will contain the frames up until this number unchanged, followed by the generated frames. The default start frame is 32 frames before the end of the video, which gives optimal results.
     */
    start_frame?: number;
    /**
     * Video Url
     * @description URL of the input video to represent the beginning of the video. If the input video does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://v3.fal.media/files/rabbit/lTH9PY_LQG0FjueBxMfDN_0395dec3-0c4a-4c25-8399-ebb198b73a30.mp4
     */
    video_url: string;
}

export interface MagiDistilledExtendVideoOutput {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://v3.fal.media/files/zebra/2UjT7u_8oF2gGfxBiT_gL_91a8f175-fd57-4ed6-aedc-1957aa558363.mp4"
     *     }
     */
    video: Components.File;
}

export interface MagiDistilledInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Num Frames
     * @description Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.
     * @default 96
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 16
     * @enum {integer}
     */
    num_inference_steps?: 4 | 8 | 16 | 32;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example Close-up shot: the old sea captain stares intently, pipe in mouth, wisps of smoke curling around his weathered face. The camera begins a slow clockwise orbit, pulling back. Finally, the camera rises high above, revealing the entire wooden sailing ship cutting through the waves, the captain unmoved, gazing toward the distant horizon.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface MagiDistilledOutput {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://v3.fal.media/files/rabbit/lTH9PY_LQG0FjueBxMfDN_0395dec3-0c4a-4c25-8399-ebb198b73a30.mp4"
     *     }
     */
    video: Components.File;
}

export interface MagiInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Num Frames
     * @description Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.
     * @default 96
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 16
     * @enum {integer}
     */
    num_inference_steps?: 4 | 8 | 16 | 32 | 64;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example Close-up shot: the old sea captain stares intently, pipe in mouth, wisps of smoke curling around his weathered face. The camera begins a slow clockwise orbit, pulling back. Finally, the camera rises high above, revealing the entire wooden sailing ship cutting through the waves, the captain unmoved, gazing toward the distant horizon.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
}

export interface MagiOutput {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://v3.fal.media/files/elephant/Foq1oFk7e5_dzujsITYfl_f7c4f24d-a68d-4b8b-8199-320002a99ac8.mp4"
     *     }
     */
    video: Components.File;
}

export interface Lyria2Input {
    /**
     * Negative Prompt
     * @description A description of what to exclude from the generated audio
     * @default low quality
     * @example vocals, slow tempo
     * @example low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The text prompt describing the music you want to generate
     * @example A lush, ambient soundscape featuring the serene sounds of a flowing river, complemented by the distant chirping of birds, and a gentle, melancholic piano melody that slowly unfolds.
     */
    prompt: string;
    /**
     * Seed
     * @description A seed for deterministic generation. If provided, the model will attempt to produce the same audio given the same prompt and other parameters.
     */
    seed?: number;
}

export interface Lyria2Output {
    /**
     * Audio
     * @description The generated music
     * @example {
     *       "url": "https://v3.fal.media/files/koala/9V6ADhxcZrZr2FcaiNA7H_output.wav"
     *     }
     */
    audio: Components.File;
}

export interface LuminaImageV2Input {
    /**
     * Cfg Normalization
     * @description Whether to apply normalization-based guidance scale.
     * @default true
     */
    cfg_normalization?: boolean;
    /**
     * Cfg Trunc Ratio
     * @description The ratio of the timestep interval to apply normalization-based guidance scale.
     * @default 1
     */
    cfg_trunc_ratio?: number;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A vibrant and artistic digital composition featuring colorful splashes of paint in the background, creating an energetic and dynamic effect. The text 'Lumina on Fal' is elegantly integrated into the scene, standing out with a modern, bold, and slightly futuristic font. The colors are bright and varied, including neon blues, purples, pinks, and oranges, blending seamlessly in a fluid, abstract style. The text appears slightly illuminated, complementing the vivid splashes around it.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * System Prompt
     * @description The system prompt to use.
     * @default You are an assistant designed to generate superior images with the superior degree of image-text alignment based on textual prompts or user prompts.
     */
    system_prompt?: string;
}

export interface LuminaImageV2Output {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/jpeg",
     *         "url": "https://v3.fal.media/files/rabbit/pBwaEZysJhnstKWEHGpLc.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface LumaPhotonReframeInput extends SharedType_b59 {}

export interface LumaPhotonReframeOutput extends SharedType_e60 {}

export interface LumaPhotonModifyInput extends SharedType_ab0 {}

export interface LumaPhotonModifyOutput extends SharedType_e60 {}

export interface LumaPhotonFlashReframeInput extends SharedType_b59 {}

export interface LumaPhotonFlashReframeOutput extends SharedType_e60 {}

export interface LumaPhotonFlashModifyInput extends SharedType_ab0 {}

export interface LumaPhotonFlashModifyOutput extends SharedType_e60 {}

export interface LumaPhotonFlashInput extends SharedType_e70 {}

export interface LumaPhotonFlashOutput extends SharedType_e60 {}

export interface LumaPhotonInput extends SharedType_e70 {}

export interface LumaPhotonOutput extends SharedType_e60 {}

export interface LumaDreamMachineRay2ReframeInput extends SharedType_54a {}

export interface LumaDreamMachineRay2ReframeOutput extends SharedType_2c5 {}

export interface LumaDreamMachineRay2ModifyInput extends SharedType_b03 {}

export interface LumaDreamMachineRay2ModifyOutput extends SharedType_4ad {}

export interface LumaDreamMachineRay2ImageToVideoInput extends SharedType_dca {}

export interface LumaDreamMachineRay2ImageToVideoOutput extends SharedType_156 {}

export interface LumaDreamMachineRay2FlashReframeInput extends SharedType_54a {}

export interface LumaDreamMachineRay2FlashReframeOutput extends SharedType_2c5 {}

export interface LumaDreamMachineRay2FlashModifyInput extends SharedType_b03 {}

export interface LumaDreamMachineRay2FlashModifyOutput extends SharedType_4ad {}

export interface LumaDreamMachineRay2FlashImageToVideoInput extends SharedType_dca {}

export interface LumaDreamMachineRay2FlashImageToVideoOutput extends SharedType_156 {}

export interface LumaDreamMachineRay2FlashInput extends SharedType_bb3 {}

export interface LumaDreamMachineRay2FlashOutput extends SharedType_812 {}

export interface LumaDreamMachineRay2Input extends SharedType_bb3 {}

export interface LumaDreamMachineRay2Output extends SharedType_812 {}

export interface LucidfluxInput {
    /**
     * Guidance
     * @description The guidance to use for the diffusion process.
     * @default 4
     */
    guidance?: number;
    /**
     * Image URL
     * @description The URL of the image to edit.
     * @example https://v3.fal.media/files/elephant/6FLKRWYztzOKDKV-v1VfK_3.png
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to edit the image.
     * @example restore this image into high-quality, clean, high-resolution result
     */
    prompt: string;
    /**
     * Seed
     * @description Seed used for random number generation
     * @default 42
     */
    seed?: number;
    /**
     * Target Height
     * @description The height of the output image.
     * @default 1024
     */
    target_height?: number;
    /**
     * Target Width
     * @description The width of the output image.
     * @default 1024
     */
    target_width?: number;
}

export interface LucidfluxOutput {
    /**
     * Image
     * @description Generated image
     * @example {
     *       "file_size": 123744,
     *       "file_name": "img_result.jpeg",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3b.fal.media/files/b/penguin/z34N19Fw1HIcAfmmrK8El_img_result.jpeg"
     *     }
     */
    image: Components.Image;
    /**
     * Seed
     * @description Seed used for random number generation
     */
    seed: number;
}

export interface Ltxv13b098DistilledMulticonditioningInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the video.
     * @default auto
     * @example auto
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '1:1' | '16:9' | 'auto';
    /**
     * Constant Rate Factor
     * @description The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
     * @default 29
     * @example 29
     */
    constant_rate_factor?: number;
    /**
     * Enable Detail Pass
     * @description Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.
     * @default false
     * @example false
     */
    enable_detail_pass?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description Whether to expand the prompt using a language model.
     * @default false
     * @example false
     */
    expand_prompt?: boolean;
    /**
     * Number of Inference Steps
     * @description Number of inference steps during the first pass.
     * @default 8
     * @example 8
     */
    first_pass_num_inference_steps?: number;
    /**
     * Frame Rate
     * @description The frame rate of the video.
     * @default 24
     * @example 24
     */
    frame_rate?: number;
    /**
     * Images
     * @description URL of images to use as conditioning
     * @default []
     * @example [
     *       {
     *         "strength": 1,
     *         "start_frame_num": 0,
     *         "image_url": "https://storage.googleapis.com/falserverless/model_tests/ltx/NswO1P8sCLzrh1WefqQFK_9a6bdbfa54b944c9a770338159a113fd.jpg"
     *       },
     *       {
     *         "strength": 1,
     *         "start_frame_num": 120,
     *         "image_url": "https://storage.googleapis.com/falserverless/model_tests/ltx/YAPOGvmS2tM_Krdp7q6-d_267c97e017c34f679844a4477dfcec38.jpg"
     *       }
     *     ]
     */
    images?: Components.ImageConditioningInput[];
    /**
     * Loras
     * @description LoRA weights to use for generation
     * @default []
     */
    loras?: Components.LoRAWeight[];
    /**
     * Negative Prompt
     * @description Negative prompt for generation
     * @default worst quality, inconsistent motion, blurry, jittery, distorted
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames in the video.
     * @default 121
     * @example 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description Text prompt to guide generation
     * @example A vibrant, abstract composition featuring a person with outstretched arms, rendered in a kaleidoscope of colors against a deep, dark background. The figure is composed of intricate, swirling patterns reminiscent of a mosaic, with hues of orange, yellow, blue, and green that evoke the style of artists such as Wassily Kandinsky or Bridget Riley. The camera zooms into the face striking portrait of a man, reimagined through the lens of old-school video-game graphics. The subject's face is rendered in a kaleidoscope of colors, with bold blues and reds set against a vibrant yellow backdrop. His dark hair is pulled back, framing his profile in a dramatic pose.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video.
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Reverse Video
     * @description Whether to reverse the video.
     * @default false
     * @example false
     */
    reverse_video?: boolean;
    /**
     * Second Pass Number of Inference Steps
     * @description Number of inference steps during the second pass.
     * @default 8
     * @example 8
     */
    second_pass_num_inference_steps?: number;
    /**
     * Second Pass Skip Initial Steps
     * @description The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
     * @default 5
     * @example 5
     */
    second_pass_skip_initial_steps?: number;
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
    /**
     * Temporal AdaIN Factor
     * @description The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution.
     * @default 0.5
     * @example 0.5
     */
    temporal_adain_factor?: number;
    /**
     * Tone Map Compression Ratio
     * @description The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.
     * @default 0
     * @example 0
     */
    tone_map_compression_ratio?: number;
    /**
     * Videos
     * @description Videos to use as conditioning
     * @default []
     */
    videos?: Components.VideoConditioningInput_2[];
}

export interface Ltxv13b098DistilledMulticonditioningOutput extends SharedType_5da {}

export interface Ltxv13b098DistilledImageToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the video.
     * @default auto
     * @example auto
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '1:1' | '16:9' | 'auto';
    /**
     * Constant Rate Factor
     * @description The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
     * @default 29
     * @example 29
     */
    constant_rate_factor?: number;
    /**
     * Enable Detail Pass
     * @description Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.
     * @default false
     * @example false
     */
    enable_detail_pass?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description Whether to expand the prompt using a language model.
     * @default false
     * @example false
     */
    expand_prompt?: boolean;
    /**
     * Number of Inference Steps
     * @description Number of inference steps during the first pass.
     * @default 8
     * @example 8
     */
    first_pass_num_inference_steps?: number;
    /**
     * Frame Rate
     * @description The frame rate of the video.
     * @default 24
     * @example 24
     */
    frame_rate?: number;
    /**
     * Image URL
     * @description Image URL for Image-to-Video task
     * @example https://storage.googleapis.com/falserverless/example_inputs/ltxv-image-input.jpg
     */
    image_url: string;
    /**
     * Loras
     * @description LoRA weights to use for generation
     * @default []
     */
    loras?: Components.LoRAWeight[];
    /**
     * Negative Prompt
     * @description Negative prompt for generation
     * @default worst quality, inconsistent motion, blurry, jittery, distorted
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames in the video.
     * @default 121
     * @example 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description Text prompt to guide generation
     * @example The astronaut gets up and walks away
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video.
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Reverse Video
     * @description Whether to reverse the video.
     * @default false
     * @example false
     */
    reverse_video?: boolean;
    /**
     * Second Pass Number of Inference Steps
     * @description Number of inference steps during the second pass.
     * @default 8
     * @example 8
     */
    second_pass_num_inference_steps?: number;
    /**
     * Second Pass Skip Initial Steps
     * @description The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
     * @default 5
     * @example 5
     */
    second_pass_skip_initial_steps?: number;
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
    /**
     * Temporal AdaIN Factor
     * @description The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution.
     * @default 0.5
     * @example 0.5
     */
    temporal_adain_factor?: number;
    /**
     * Tone Map Compression Ratio
     * @description The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.
     * @default 0
     * @example 0
     */
    tone_map_compression_ratio?: number;
}

export interface Ltxv13b098DistilledImageToVideoOutput extends SharedType_fd1 {}

export interface Ltxv13b098DistilledExtendInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the video.
     * @default auto
     * @example auto
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '1:1' | '16:9' | 'auto';
    /**
     * Constant Rate Factor
     * @description The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
     * @default 29
     * @example 29
     */
    constant_rate_factor?: number;
    /**
     * Enable Detail Pass
     * @description Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.
     * @default false
     * @example false
     */
    enable_detail_pass?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description Whether to expand the prompt using a language model.
     * @default false
     * @example false
     */
    expand_prompt?: boolean;
    /**
     * Number of Inference Steps
     * @description Number of inference steps during the first pass.
     * @default 8
     * @example 8
     */
    first_pass_num_inference_steps?: number;
    /**
     * Frame Rate
     * @description The frame rate of the video.
     * @default 24
     * @example 24
     */
    frame_rate?: number;
    /**
     * Loras
     * @description LoRA weights to use for generation
     * @default []
     */
    loras?: Components.LoRAWeight[];
    /**
     * Negative Prompt
     * @description Negative prompt for generation
     * @default worst quality, inconsistent motion, blurry, jittery, distorted
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames in the video.
     * @default 121
     * @example 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description Text prompt to guide generation
     * @example Woman walking on a street in Tokyo
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video.
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Reverse Video
     * @description Whether to reverse the video.
     * @default false
     * @example false
     */
    reverse_video?: boolean;
    /**
     * Second Pass Number of Inference Steps
     * @description Number of inference steps during the second pass.
     * @default 8
     * @example 8
     */
    second_pass_num_inference_steps?: number;
    /**
     * Second Pass Skip Initial Steps
     * @description The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
     * @default 5
     * @example 5
     */
    second_pass_skip_initial_steps?: number;
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
    /**
     * Temporal AdaIN Factor
     * @description The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution.
     * @default 0.5
     * @example 0.5
     */
    temporal_adain_factor?: number;
    /**
     * Tone Map Compression Ratio
     * @description The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.
     * @default 0
     * @example 0
     */
    tone_map_compression_ratio?: number;
    /**
     * Video
     * @description Video to be extended.
     * @example {
     *       "video_url": "https://storage.googleapis.com/falserverless/web-examples/wan/t2v.mp4",
     *       "start_frame_num": 0,
     *       "reverse_video": false,
     *       "limit_num_frames": false,
     *       "resample_fps": false,
     *       "strength": 1,
     *       "target_fps": 24,
     *       "max_num_frames": 1441,
     *       "conditioning_type": "rgb",
     *       "preprocess": false
     *     }
     */
    video: Components.ExtendVideoConditioningInput;
}

export interface Ltxv13b098DistilledExtendOutput extends SharedType_b85 {}

export interface Ltxv13b098DistilledInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video.
     * @default 16:9
     * @example 16:9
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '1:1' | '16:9';
    /**
     * Enable Detail Pass
     * @description Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.
     * @default false
     * @example false
     */
    enable_detail_pass?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description Whether to expand the prompt using a language model.
     * @default false
     * @example false
     */
    expand_prompt?: boolean;
    /**
     * Number of Inference Steps
     * @description Number of inference steps during the first pass.
     * @default 8
     * @example 8
     */
    first_pass_num_inference_steps?: number;
    /**
     * Frame Rate
     * @description The frame rate of the video.
     * @default 24
     * @example 24
     */
    frame_rate?: number;
    /**
     * Loras
     * @description LoRA weights to use for generation
     * @default []
     */
    loras?: Components.LoRAWeight[];
    /**
     * Negative Prompt
     * @description Negative prompt for generation
     * @default worst quality, inconsistent motion, blurry, jittery, distorted
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames in the video.
     * @default 121
     * @example 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description Text prompt to guide generation
     * @example A cinematic fast-tracking shot follows a vintage, teal camper van as it descends a winding mountain trail. The van, slightly weathered but well-maintained, is the central focus, its retro design emphasized by the motion blur. Medium shot reveals the dusty, ochre trail, edged with vibrant green pine trees. Close-up on the van's tires shows the gravel spraying, highlighting the speed and rugged terrain. Sunlight filters through the trees, casting dappled shadows on the van and the trail. The background is a hazy, majestic mountain range bathed in warm, golden light. The overall mood is adventurous and exhilarating. High resolution 4k movie scene.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video.
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Reverse Video
     * @description Whether to reverse the video.
     * @default false
     * @example false
     */
    reverse_video?: boolean;
    /**
     * Second Pass Number of Inference Steps
     * @description Number of inference steps during the second pass.
     * @default 8
     * @example 8
     */
    second_pass_num_inference_steps?: number;
    /**
     * Second Pass Skip Initial Steps
     * @description The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
     * @default 5
     * @example 5
     */
    second_pass_skip_initial_steps?: number;
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
    /**
     * Temporal AdaIN Factor
     * @description The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution.
     * @default 0.5
     * @example 0.5
     */
    temporal_adain_factor?: number;
    /**
     * Tone Map Compression Ratio
     * @description The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.
     * @default 0
     * @example 0
     */
    tone_map_compression_ratio?: number;
}

export interface Ltxv13b098DistilledOutput extends SharedType_0bd {}

export interface Ltx2VideoTrainerInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio to use for training.
     * @default 1:1
     * @example 1:1
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '1:1' | '9:16';
    /**
     * Audio Normalize
     * @description Normalize audio peak amplitude to a consistent level. Recommended for consistent audio levels across the dataset.
     * @default true
     */
    audio_normalize?: boolean;
    /**
     * Audio Preserve Pitch
     * @description When audio duration doesn't match video duration, stretch/compress audio without changing pitch. If disabled, audio is trimmed or padded with silence.
     * @default true
     */
    audio_preserve_pitch?: boolean;
    /**
     * Auto Scale Input
     * @description If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.
     * @default false
     * @example false
     */
    auto_scale_input?: boolean;
    /**
     * First Frame Conditioning P
     * @description Probability of conditioning on the first frame during training. Higher values improve image-to-video performance.
     * @default 0.5
     */
    first_frame_conditioning_p?: number;
    /**
     * Frame Rate
     * @description Target frames per second for the video.
     * @default 25
     * @example 25
     */
    frame_rate?: number;
    /**
     * Generate Audio In Validation
     * @description Whether to generate audio in validation samples.
     * @default true
     */
    generate_audio_in_validation?: boolean;
    /**
     * Learning Rate
     * @description Learning rate for optimization. Higher values can lead to faster training but may cause overfitting.
     * @default 0.0002
     * @example 0.0002
     */
    learning_rate?: number;
    /**
     * Number Of Frames
     * @description Number of frames per training sample. Must satisfy frames % 8 == 1 (e.g., 1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97).
     * @default 89
     * @example 89
     */
    number_of_frames?: number;
    /**
     * Number Of Steps
     * @description The number of training steps.
     * @default 2000
     * @example 2000
     */
    number_of_steps?: number;
    /**
     * Rank
     * @description The rank of the LoRA adaptation. Higher values increase capacity but use more memory.
     * @default 32
     * @example 32
     * @enum {integer}
     */
    rank?: 8 | 16 | 32 | 64 | 128;
    /**
     * Resolution
     * @description Resolution to use for training. Higher resolutions require more memory.
     * @default medium
     * @example medium
     * @enum {string}
     */
    resolution?: 'low' | 'medium' | 'high';
    /**
     * Split Input Duration Threshold
     * @description The duration threshold in seconds. If a video is longer than this, it will be split into scenes.
     * @default 30
     * @example 30
     */
    split_input_duration_threshold?: number;
    /**
     * Split Input Into Scenes
     * @description If true, videos above a certain duration threshold will be split into scenes.
     * @default true
     * @example true
     */
    split_input_into_scenes?: boolean;
    /**
     * Stg Scale
     * @description STG (Spatio-Temporal Guidance) scale. 0.0 disables STG. Recommended value is 1.0.
     * @default 1
     */
    stg_scale?: number;
    /**
     * Training Data Url
     * @description URL to zip archive with videos or images. Try to use at least 10 files, although more is better.
     *
     *             **Supported video formats:** .mp4, .mov, .avi, .mkv
     *             **Supported image formats:** .png, .jpg, .jpeg
     *
     *             Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.
     *
     *             The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to.
     */
    training_data_url: string;
    /**
     * Trigger Phrase
     * @description A phrase that will trigger the LoRA style. Will be prepended to captions during training.
     * @default
     * @example
     */
    trigger_phrase?: string;
    /**
     * Validation
     * @description A list of validation prompts to use during training. When providing an image, _all_ validation inputs must have an image.
     * @default []
     */
    validation?: Components.Validation[];
    /**
     * Validation Aspect Ratio
     * @description The aspect ratio to use for validation.
     * @default 1:1
     * @example 1:1
     * @enum {string}
     */
    validation_aspect_ratio?: '16:9' | '1:1' | '9:16';
    /**
     * Validation Frame Rate
     * @description Target frames per second for validation videos.
     * @default 25
     * @example 25
     */
    validation_frame_rate?: number;
    /**
     * Validation Negative Prompt
     * @description A negative prompt to use for validation.
     * @default worst quality, inconsistent motion, blurry, jittery, distorted
     */
    validation_negative_prompt?: string;
    /**
     * Validation Number Of Frames
     * @description The number of frames in validation videos.
     * @default 89
     * @example 89
     */
    validation_number_of_frames?: number;
    /**
     * Validation Resolution
     * @description The resolution to use for validation.
     * @default high
     * @example high
     * @enum {string}
     */
    validation_resolution?: 'low' | 'medium' | 'high';
    /**
     * With Audio
     * @description Enable joint audio-video training. If None (default), automatically detects whether input videos have audio. Set to True to force audio training, or False to disable.
     */
    with_audio?: boolean;
}

export interface Ltx2VideoTrainerOutput {
    /** @description Configuration used for setting up inference endpoints. */
    config_file: Components.File_1;
    /** @description URL to the debug dataset archive containing decoded videos and audio. */
    debug_dataset?: Components.File_1;
    /** @description URL to the trained LoRA weights (.safetensors). */
    lora_file: Components.File_1;
    /** @description The URL to the validation videos, if any. */
    video: Components.File_1;
}

export interface Ltx2V2vTrainerInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio to use for training.
     * @default 1:1
     * @example 1:1
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '1:1' | '9:16';
    /**
     * Auto Scale Input
     * @description If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.
     * @default false
     * @example false
     */
    auto_scale_input?: boolean;
    /**
     * First Frame Conditioning P
     * @description Probability of conditioning on the first frame during training. Lower values work better for video-to-video transformation.
     * @default 0.1
     */
    first_frame_conditioning_p?: number;
    /**
     * Frame Rate
     * @description Target frames per second for the video.
     * @default 25
     * @example 25
     */
    frame_rate?: number;
    /**
     * Learning Rate
     * @description Learning rate for optimization. Higher values can lead to faster training but may cause overfitting.
     * @default 0.0002
     * @example 0.0002
     */
    learning_rate?: number;
    /**
     * Number Of Frames
     * @description Number of frames per training sample. Must satisfy frames % 8 == 1 (e.g., 1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97).
     * @default 89
     * @example 89
     */
    number_of_frames?: number;
    /**
     * Number Of Steps
     * @description The number of training steps.
     * @default 2000
     * @example 2000
     */
    number_of_steps?: number;
    /**
     * Rank
     * @description The rank of the LoRA adaptation. Higher values increase capacity but use more memory.
     * @default 32
     * @example 32
     * @enum {integer}
     */
    rank?: 8 | 16 | 32 | 64 | 128;
    /**
     * Resolution
     * @description Resolution to use for training. Higher resolutions require more memory.
     * @default medium
     * @example medium
     * @enum {string}
     */
    resolution?: 'low' | 'medium' | 'high';
    /**
     * Split Input Duration Threshold
     * @description The duration threshold in seconds. If a video is longer than this, it will be split into scenes.
     * @default 30
     * @example 30
     */
    split_input_duration_threshold?: number;
    /**
     * Split Input Into Scenes
     * @description If true, videos above a certain duration threshold will be split into scenes.
     * @default true
     * @example true
     */
    split_input_into_scenes?: boolean;
    /**
     * Stg Scale
     * @description STG (Spatio-Temporal Guidance) scale. 0.0 disables STG. Recommended value is 1.0.
     * @default 1
     */
    stg_scale?: number;
    /**
     * Training Data Url
     * @description URL to zip archive with videos or images. Try to use at least 10 files, although more is better.
     *
     *             **Supported video formats:** .mp4, .mov, .avi, .mkv
     *             **Supported image formats:** .png, .jpg, .jpeg
     *
     *             Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.
     *
     *             The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to.
     */
    training_data_url: string;
    /**
     * Trigger Phrase
     * @description A phrase that will trigger the LoRA style. Will be prepended to captions during training.
     * @default
     * @example
     */
    trigger_phrase?: string;
    /**
     * Validation
     * @description A list of validation inputs with prompts and reference videos.
     * @default []
     */
    validation?: Components.V2VValidation[];
    /**
     * Validation Aspect Ratio
     * @description The aspect ratio to use for validation.
     * @default 1:1
     * @example 1:1
     * @enum {string}
     */
    validation_aspect_ratio?: '16:9' | '1:1' | '9:16';
    /**
     * Validation Frame Rate
     * @description Target frames per second for validation videos.
     * @default 25
     * @example 25
     */
    validation_frame_rate?: number;
    /**
     * Validation Negative Prompt
     * @description A negative prompt to use for validation.
     * @default worst quality, inconsistent motion, blurry, jittery, distorted
     */
    validation_negative_prompt?: string;
    /**
     * Validation Number Of Frames
     * @description The number of frames in validation videos.
     * @default 89
     * @example 89
     */
    validation_number_of_frames?: number;
    /**
     * Validation Resolution
     * @description The resolution to use for validation.
     * @default high
     * @example high
     * @enum {string}
     */
    validation_resolution?: 'low' | 'medium' | 'high';
}

export interface Ltx2V2vTrainerOutput {
    /** @description Configuration used for setting up inference endpoints. */
    config_file: Components.File_1;
    /** @description URL to the debug dataset archive containing decoded videos. */
    debug_dataset?: Components.File_1;
    /** @description URL to the trained IC-LoRA weights (.safetensors). */
    lora_file: Components.File_1;
    /** @description The URL to the validation videos (with reference videos side-by-side), if any. */
    video: Components.File_1;
}

export interface LtxVideoImageToVideoInput {
    /**
     * Guidance Scale
     * @description The guidance scale to use.
     * @default 3
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The URL of the image to generate the video from.
     * @example https://fal.media/files/kangaroo/4OePu2ifG7SKxTM__TQrQ_72929fec9fb74790bb8c8b760450c9b9.jpg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly
     */
    negative_prompt?: string;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to take.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A lone astronaut in a white spacesuit with gold-tinted visor drifts weightlessly through a sleek, cylindrical corridor of a spaceship. Their movements are slow and graceful as they gently push off the metallic walls with their gloved hands, rotating slightly as they float from right to left across the frame. The corridor features brushed aluminum panels with blue LED strips running along the ceiling, casting a cool glow on the astronaut's suit. Various cables, pipes, and control panels line the walls. The camera follows the astronaut's movement in a handheld style, slightly swaying and adjusting focus, maintaining a medium shot that captures both the astronaut and the corridor's depth. Small particles of dust catch the light as they float in the zero-gravity environment. The scene appears cinematic, with lens flares occasionally reflecting off the metallic surfaces and the astronaut's visor.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for random number generation.
     */
    seed?: number;
}

export interface LtxVideoImageToVideoOutput extends SharedType_4b8 {}

export interface LtxVideoV095MulticonditioningInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video (16:9 or 9:16).
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '16:9';
    /**
     * Expand Prompt
     * @description Whether to expand the prompt using the model's own capabilities.
     * @default true
     */
    expand_prompt?: boolean;
    /**
     * Images
     * @description URL of images to use as conditioning
     * @default []
     * @example [
     *       {
     *         "start_frame_num": 0,
     *         "image_url": "https://storage.googleapis.com/falserverless/model_tests/ltx/NswO1P8sCLzrh1WefqQFK_9a6bdbfa54b944c9a770338159a113fd.jpg"
     *       },
     *       {
     *         "start_frame_num": 120,
     *         "image_url": "https://storage.googleapis.com/falserverless/model_tests/ltx/YAPOGvmS2tM_Krdp7q6-d_267c97e017c34f679844a4477dfcec38.jpg"
     *       }
     *     ]
     */
    images?: Components.ImageConditioningInput_1[];
    /**
     * Negative Prompt
     * @description Negative prompt for generation
     * @default worst quality, inconsistent motion, blurry, jittery, distorted
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description Text prompt to guide generation
     * @example A vibrant, abstract composition featuring a person with outstretched arms, rendered in a kaleidoscope of colors against a deep, dark background. The figure is composed of intricate, swirling patterns reminiscent of a mosaic, with hues of orange, yellow, blue, and green that evoke the style of artists such as Wassily Kandinsky or Bridget Riley.
     *
     *     The camera zooms into the face striking portrait of a man, reimagined through the lens of old-school video-game graphics. The subject's face is rendered in a kaleidoscope of colors, with bold blues and reds set against a vibrant yellow backdrop. His dark hair is pulled back, framing his profile in a dramatic pose
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p).
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
    /**
     * Videos
     * @description Videos to use as conditioning
     * @default []
     */
    videos?: Components.VideoConditioningInput_1[];
}

export interface LtxVideoV095MulticonditioningOutput {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/gallery/ltx-multicondition.mp4"
     *     }
     */
    video: Components.File;
}

export interface LtxVideoV095ExtendInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video (16:9 or 9:16).
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '16:9';
    /**
     * Expand Prompt
     * @description Whether to expand the prompt using the model's own capabilities.
     * @default true
     */
    expand_prompt?: boolean;
    /**
     * Negative Prompt
     * @description Negative prompt for generation
     * @default worst quality, inconsistent motion, blurry, jittery, distorted
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description Text prompt to guide generation
     * @example Woman walking on a street in Tokyo
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p).
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
    /**
     * Video
     * @description Video to be extended.
     * @example {
     *       "video_url": "https://storage.googleapis.com/falserverless/web-examples/wan/t2v.mp4",
     *       "start_frame_num": 24
     *     }
     */
    video: Components.VideoConditioningInput_1;
}

export interface LtxVideoV095ExtendOutput {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/ltx-v095_extend.mp4"
     *     }
     */
    video: Components.File;
}

export interface LtxVideoV095Input {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video (16:9 or 9:16).
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '16:9';
    /**
     * Expand Prompt
     * @description Whether to expand the prompt using the model's own capabilities.
     * @default true
     */
    expand_prompt?: boolean;
    /**
     * Negative Prompt
     * @description Negative prompt for generation
     * @default worst quality, inconsistent motion, blurry, jittery, distorted
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description Text prompt to guide generation
     * @example A cute cat walking on a sidewalk
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p).
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
}

export interface LtxVideoV095Output {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/ltx-t2v_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface LtxVideoTrainerInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio to use for training. This is the aspect ratio of the video.
     * @default 1:1
     * @example 1:1
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '1:1' | '9:16';
    /**
     * Auto Scale Input
     * @description If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.
     * @default false
     * @example false
     */
    auto_scale_input?: boolean;
    /**
     * Frame Rate
     * @description The target frames per second for the video.
     * @default 25
     * @example 25
     */
    frame_rate?: number;
    /**
     * Learning Rate
     * @description The rate at which the model learns. Higher values can lead to faster training, but over-fitting.
     * @default 0.0002
     * @example 0.0002
     */
    learning_rate?: number;
    /**
     * Number Of Frames
     * @description The number of frames to use for training. This is the number of frames per second multiplied by the number of seconds.
     * @default 81
     * @example 81
     */
    number_of_frames?: number;
    /**
     * Number Of Steps
     * @description The number of steps to train for.
     * @default 1000
     * @example 1000
     */
    number_of_steps?: number;
    /**
     * Rank
     * @description The rank of the LoRA.
     * @default 128
     * @example 128
     * @enum {integer}
     */
    rank?: 8 | 16 | 32 | 64 | 128;
    /**
     * Resolution
     * @description The resolution to use for training. This is the resolution of the video.
     * @default medium
     * @example medium
     * @enum {string}
     */
    resolution?: 'low' | 'medium' | 'high';
    /**
     * Split Input Duration Threshold
     * @description The duration threshold in seconds. If a video is longer than this, it will be split into scenes. If you provide captions for a split video, the caption will be applied to each scene. If you do not provide captions, scenes will be auto-captioned.
     * @default 30
     * @example 30
     */
    split_input_duration_threshold?: number;
    /**
     * Split Input Into Scenes
     * @description If true, videos above a certain duration threshold will be split into scenes. If you provide captions for a split video, the caption will be applied to each scene. If you do not provide captions, scenes will be auto-captioned. This option has no effect on image datasets.
     * @default true
     * @example true
     */
    split_input_into_scenes?: boolean;
    /**
     * Training Data Url
     * @description URL to zip archive with videos or images. Try to use at least 10 files, although more is better.
     *
     *             **Supported video formats:** .mp4, .mov, .avi, .mkv
     *             **Supported image formats:** .png, .jpg, .jpeg
     *
     *             Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.
     *
     *             The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to.
     */
    training_data_url: string;
    /**
     * Trigger Phrase
     * @description The phrase that will trigger the model to generate an image.
     * @default
     * @example
     */
    trigger_phrase?: string;
    /**
     * Validation
     * @description A list of validation prompts to use during training. When providing an image, _all_ validation inputs must have an image.
     * @default []
     */
    validation?: Components.Validation[];
    /**
     * Validation Aspect Ratio
     * @description The aspect ratio to use for validation.
     * @default 1:1
     * @example 1:1
     * @enum {string}
     */
    validation_aspect_ratio?: '16:9' | '1:1' | '9:16';
    /**
     * Validation Negative Prompt
     * @description A negative prompt to use for validation.
     * @default blurry, low quality, bad quality, out of focus
     */
    validation_negative_prompt?: string;
    /**
     * Validation Number Of Frames
     * @description The number of frames to use for validation.
     * @default 81
     * @example 81
     */
    validation_number_of_frames?: number;
    /**
     * Validation Resolution
     * @description The resolution to use for validation.
     * @default high
     * @example high
     * @enum {string}
     */
    validation_resolution?: 'low' | 'medium' | 'high';
    /**
     * Validation Reverse
     * @description If true, the validation videos will be reversed. This is useful for effects that are learned in reverse and then applied in reverse.
     * @default false
     */
    validation_reverse?: boolean;
}

export interface LtxVideoTrainerOutput {
    /** @description Configuration used for setting up the inference endpoints. */
    config_file: Components.File_1;
    /** @description URL to the trained LoRA weights. */
    lora_file: Components.File_1;
    /** @description The URL to the validations video. */
    video: Components.File_1;
}

export interface LtxVideoLoraMulticonditioningInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the video.
     * @default auto
     * @example auto
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '1:1' | '9:16' | 'auto';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description Whether to expand the prompt using the LLM.
     * @default false
     * @example false
     */
    expand_prompt?: boolean;
    /**
     * Frame Rate
     * @description The frame rate of the video.
     * @default 25
     * @example 25
     */
    frame_rate?: number;
    /**
     * Images
     * @description The image conditions to use for generation.
     * @default []
     * @example [
     *       {
     *         "strength": 1,
     *         "start_frame_number": 0,
     *         "image_url": "https://storage.googleapis.com/falserverless/model_tests/ltx/NswO1P8sCLzrh1WefqQFK_9a6bdbfa54b944c9a770338159a113fd.jpg"
     *       },
     *       {
     *         "strength": 1,
     *         "start_frame_number": 80,
     *         "image_url": "https://storage.googleapis.com/falserverless/model_tests/ltx/YAPOGvmS2tM_Krdp7q6-d_267c97e017c34f679844a4477dfcec38.jpg"
     *       }
     *     ]
     */
    images?: Components.ImageCondition[];
    /**
     * Loras
     * @description The LoRA weights to use for generation.
     * @default []
     */
    loras?: Components.LoRAWeight[];
    /**
     * Negative Prompt
     * @description The negative prompt to use.
     * @default blurry, low quality, low resolution, inconsistent motion, jittery, distorted
     */
    negative_prompt?: string;
    /**
     * Number Of Frames
     * @description The number of frames in the video.
     * @default 89
     * @example 89
     */
    number_of_frames?: number;
    /**
     * Number Of Steps
     * @description The number of inference steps to use.
     * @default 30
     * @example 30
     */
    number_of_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A vibrant, abstract composition featuring a person with outstretched arms, rendered in a kaleidoscope of colors against a deep, dark background. The figure is composed of intricate, swirling patterns reminiscent of a mosaic, with hues of orange, yellow, blue, and green that evoke the style of artists such as Wassily Kandinsky or Bridget Riley. The camera zooms into the face striking portrait of a man, reimagined through the lens of old-school video-game graphics. The subject's face is rendered in a kaleidoscope of colors, with bold blues and reds set against a vibrant yellow backdrop. His dark hair is pulled back, framing his profile in a dramatic pose
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the video.
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Reverse Video
     * @description Whether to reverse the video.
     * @default false
     * @example false
     */
    reverse_video?: boolean;
    /**
     * Seed
     * @description The seed to use for generation.
     */
    seed?: number;
    /**
     * Videos
     * @description The video conditions to use for generation.
     * @default []
     */
    videos?: Components.VideoCondition[];
}

export interface LtxVideoLoraMulticonditioningOutput {
    /**
     * Prompt
     * @description The prompt used for generation.
     * @example A vibrant, abstract composition featuring a person with outstretched arms, rendered in a kaleidoscope of colors against a deep, dark background. The figure is composed of intricate, swirling patterns reminiscent of a mosaic, with hues of orange, yellow, blue, and green that evoke the style of artists such as Wassily Kandinsky or Bridget Riley. The camera zooms into the face striking portrait of a man, reimagined through the lens of old-school video-game graphics. The subject's face is rendered in a kaleidoscope of colors, with bold blues and reds set against a vibrant yellow backdrop. His dark hair is pulled back, framing his profile in a dramatic pose
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/gallery/ltx-multicondition.mp4"
     *     }
     */
    video: Components.File;
}

export interface LtxVideoLoraImageToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the video.
     * @default auto
     * @example auto
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '1:1' | '9:16' | 'auto';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description Whether to expand the prompt using the LLM.
     * @default false
     * @example false
     */
    expand_prompt?: boolean;
    /**
     * Frame Rate
     * @description The frame rate of the video.
     * @default 25
     * @example 25
     */
    frame_rate?: number;
    /**
     * Image Url
     * @description The URL of the image to use as input.
     * @example https://h2.inkwai.com/bs2/upload-ylab-stunt/se/ai_portal_queue_mmu_image_upscale_aiweb/3214b798-e1b4-4b00-b7af-72b5b0417420_raw_image_0.jpg
     */
    image_url: string;
    /**
     * Loras
     * @description The LoRA weights to use for generation.
     * @default []
     */
    loras?: Components.LoRAWeight[];
    /**
     * Negative Prompt
     * @description The negative prompt to use.
     * @default blurry, low quality, low resolution, inconsistent motion, jittery, distorted
     */
    negative_prompt?: string;
    /**
     * Number Of Frames
     * @description The number of frames in the video.
     * @default 89
     * @example 89
     */
    number_of_frames?: number;
    /**
     * Number Of Steps
     * @description The number of inference steps to use.
     * @default 30
     * @example 30
     */
    number_of_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example The astronaut gets up and walks away
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the video.
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Reverse Video
     * @description Whether to reverse the video.
     * @default false
     * @example false
     */
    reverse_video?: boolean;
    /**
     * Seed
     * @description The seed to use for generation.
     */
    seed?: number;
}

export interface LtxVideoLoraImageToVideoOutput {
    /**
     * Prompt
     * @description The prompt used for generation.
     * @example The astronaut gets up and walks away
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/ltx_i2v_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface LtxVideo13bDistilledMulticonditioningInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the video.
     * @default auto
     * @example auto
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '1:1' | '16:9' | 'auto';
    /**
     * Constant Rate Factor
     * @description The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
     * @default 35
     * @example 35
     */
    constant_rate_factor?: number;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description Whether to expand the prompt using a language model.
     * @default false
     * @example false
     */
    expand_prompt?: boolean;
    /**
     * First Pass Num Inference Steps
     * @description Number of inference steps during the first pass.
     * @default 8
     * @example 8
     */
    first_pass_num_inference_steps?: number;
    /**
     * First Pass Skip Final Steps
     * @description Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
     * @default 1
     */
    first_pass_skip_final_steps?: number;
    /**
     * Frame Rate
     * @description The frame rate of the video.
     * @default 30
     * @example 30
     */
    frame_rate?: number;
    /**
     * Images
     * @description URL of images to use as conditioning
     * @default []
     * @example [
     *       {
     *         "strength": 1,
     *         "start_frame_num": 0,
     *         "image_url": "https://storage.googleapis.com/falserverless/model_tests/ltx/NswO1P8sCLzrh1WefqQFK_9a6bdbfa54b944c9a770338159a113fd.jpg"
     *       },
     *       {
     *         "strength": 1,
     *         "start_frame_num": 120,
     *         "image_url": "https://storage.googleapis.com/falserverless/model_tests/ltx/YAPOGvmS2tM_Krdp7q6-d_267c97e017c34f679844a4477dfcec38.jpg"
     *       }
     *     ]
     */
    images?: Components.ImageConditioningInput[];
    /**
     * Loras
     * @description LoRA weights to use for generation
     * @default []
     */
    loras?: Components.LoRAWeight[];
    /**
     * Negative Prompt
     * @description Negative prompt for generation
     * @default worst quality, inconsistent motion, blurry, jittery, distorted
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description The number of frames in the video.
     * @default 121
     * @example 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description Text prompt to guide generation
     * @example A vibrant, abstract composition featuring a person with outstretched arms, rendered in a kaleidoscope of colors against a deep, dark background. The figure is composed of intricate, swirling patterns reminiscent of a mosaic, with hues of orange, yellow, blue, and green that evoke the style of artists such as Wassily Kandinsky or Bridget Riley. The camera zooms into the face striking portrait of a man, reimagined through the lens of old-school video-game graphics. The subject's face is rendered in a kaleidoscope of colors, with bold blues and reds set against a vibrant yellow backdrop. His dark hair is pulled back, framing his profile in a dramatic pose.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Reverse Video
     * @description Whether to reverse the video.
     * @default false
     * @example false
     */
    reverse_video?: boolean;
    /**
     * Second Pass Num Inference Steps
     * @description Number of inference steps during the second pass.
     * @default 8
     * @example 8
     */
    second_pass_num_inference_steps?: number;
    /**
     * Second Pass Skip Initial Steps
     * @description The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
     * @default 5
     * @example 5
     */
    second_pass_skip_initial_steps?: number;
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
    /**
     * Videos
     * @description Videos to use as conditioning
     * @default []
     */
    videos?: Components.VideoConditioningInput[];
}

export interface LtxVideo13bDistilledMulticonditioningOutput extends SharedType_5da {}

export interface LtxVideo13bDistilledImageToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the video.
     * @default auto
     * @example auto
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '1:1' | '16:9' | 'auto';
    /**
     * Constant Rate Factor
     * @description The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
     * @default 35
     * @example 35
     */
    constant_rate_factor?: number;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description Whether to expand the prompt using a language model.
     * @default false
     * @example false
     */
    expand_prompt?: boolean;
    /**
     * First Pass Num Inference Steps
     * @description Number of inference steps during the first pass.
     * @default 8
     * @example 8
     */
    first_pass_num_inference_steps?: number;
    /**
     * First Pass Skip Final Steps
     * @description Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
     * @default 1
     */
    first_pass_skip_final_steps?: number;
    /**
     * Frame Rate
     * @description The frame rate of the video.
     * @default 30
     * @example 30
     */
    frame_rate?: number;
    /**
     * Image Url
     * @description Image URL for Image-to-Video task
     * @example https://storage.googleapis.com/falserverless/example_inputs/ltxv-image-input.jpg
     */
    image_url: string;
    /**
     * Loras
     * @description LoRA weights to use for generation
     * @default []
     */
    loras?: Components.LoRAWeight[];
    /**
     * Negative Prompt
     * @description Negative prompt for generation
     * @default worst quality, inconsistent motion, blurry, jittery, distorted
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description The number of frames in the video.
     * @default 121
     * @example 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description Text prompt to guide generation
     * @example The astronaut gets up and walks away
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Reverse Video
     * @description Whether to reverse the video.
     * @default false
     * @example false
     */
    reverse_video?: boolean;
    /**
     * Second Pass Num Inference Steps
     * @description Number of inference steps during the second pass.
     * @default 8
     * @example 8
     */
    second_pass_num_inference_steps?: number;
    /**
     * Second Pass Skip Initial Steps
     * @description The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
     * @default 5
     * @example 5
     */
    second_pass_skip_initial_steps?: number;
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
}

export interface LtxVideo13bDistilledImageToVideoOutput extends SharedType_fd1 {}

export interface LtxVideo13bDistilledExtendInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the video.
     * @default auto
     * @example auto
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '1:1' | '16:9' | 'auto';
    /**
     * Constant Rate Factor
     * @description The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
     * @default 35
     * @example 35
     */
    constant_rate_factor?: number;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description Whether to expand the prompt using a language model.
     * @default false
     * @example false
     */
    expand_prompt?: boolean;
    /**
     * First Pass Num Inference Steps
     * @description Number of inference steps during the first pass.
     * @default 8
     * @example 8
     */
    first_pass_num_inference_steps?: number;
    /**
     * First Pass Skip Final Steps
     * @description Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
     * @default 1
     */
    first_pass_skip_final_steps?: number;
    /**
     * Frame Rate
     * @description The frame rate of the video.
     * @default 30
     * @example 30
     */
    frame_rate?: number;
    /**
     * Loras
     * @description LoRA weights to use for generation
     * @default []
     */
    loras?: Components.LoRAWeight[];
    /**
     * Negative Prompt
     * @description Negative prompt for generation
     * @default worst quality, inconsistent motion, blurry, jittery, distorted
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description The number of frames in the video.
     * @default 121
     * @example 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description Text prompt to guide generation
     * @example Woman walking on a street in Tokyo
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Reverse Video
     * @description Whether to reverse the video.
     * @default false
     * @example false
     */
    reverse_video?: boolean;
    /**
     * Second Pass Num Inference Steps
     * @description Number of inference steps during the second pass.
     * @default 8
     * @example 8
     */
    second_pass_num_inference_steps?: number;
    /**
     * Second Pass Skip Initial Steps
     * @description The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
     * @default 5
     * @example 5
     */
    second_pass_skip_initial_steps?: number;
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
    /**
     * Video
     * @description Video to be extended.
     * @example {
     *       "video_url": "https://storage.googleapis.com/falserverless/web-examples/wan/t2v.mp4",
     *       "reverse_video": false,
     *       "start_frame_num": 24,
     *       "limit_num_frames": false,
     *       "resample_fps": false,
     *       "strength": 1,
     *       "target_fps": 30,
     *       "max_num_frames": 121,
     *       "conditioning_type": "rgb",
     *       "preprocess": false
     *     }
     */
    video: Components.VideoConditioningInput;
}

export interface LtxVideo13bDistilledExtendOutput extends SharedType_b85 {}

export interface LtxVideo13bDistilledInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video (16:9, 1:1 or 9:16).
     * @default 16:9
     * @example 16:9
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '1:1' | '16:9';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description Whether to expand the prompt using a language model.
     * @default false
     * @example false
     */
    expand_prompt?: boolean;
    /**
     * First Pass Num Inference Steps
     * @description Number of inference steps during the first pass.
     * @default 8
     * @example 8
     */
    first_pass_num_inference_steps?: number;
    /**
     * First Pass Skip Final Steps
     * @description Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
     * @default 1
     */
    first_pass_skip_final_steps?: number;
    /**
     * Frame Rate
     * @description The frame rate of the video.
     * @default 30
     * @example 30
     */
    frame_rate?: number;
    /**
     * Loras
     * @description LoRA weights to use for generation
     * @default []
     */
    loras?: Components.LoRAWeight[];
    /**
     * Negative Prompt
     * @description Negative prompt for generation
     * @default worst quality, inconsistent motion, blurry, jittery, distorted
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description The number of frames in the video.
     * @default 121
     * @example 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description Text prompt to guide generation
     * @example A cinematic fast-tracking shot follows a vintage, teal camper van as it descends a winding mountain trail. The van, slightly weathered but well-maintained, is the central focus, its retro design emphasized by the motion blur. Medium shot reveals the dusty, ochre trail, edged with vibrant green pine trees. Close-up on the van's tires shows the gravel spraying, highlighting the speed and rugged terrain. Sunlight filters through the trees, casting dappled shadows on the van and the trail. The background is a hazy, majestic mountain range bathed in warm, golden light. The overall mood is adventurous and exhilarating. High resolution 4k movie scene.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Reverse Video
     * @description Whether to reverse the video.
     * @default false
     * @example false
     */
    reverse_video?: boolean;
    /**
     * Second Pass Num Inference Steps
     * @description Number of inference steps during the second pass.
     * @default 8
     * @example 8
     */
    second_pass_num_inference_steps?: number;
    /**
     * Second Pass Skip Initial Steps
     * @description The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
     * @default 5
     * @example 5
     */
    second_pass_skip_initial_steps?: number;
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
}

export interface LtxVideo13bDistilledOutput extends SharedType_0bd {}

export interface LtxVideo13bDevMulticonditioningInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the video.
     * @default auto
     * @example auto
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '1:1' | '16:9' | 'auto';
    /**
     * Constant Rate Factor
     * @description The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
     * @default 35
     * @example 35
     */
    constant_rate_factor?: number;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description Whether to expand the prompt using a language model.
     * @default false
     * @example false
     */
    expand_prompt?: boolean;
    /**
     * First Pass Num Inference Steps
     * @description Number of inference steps during the first pass.
     * @default 30
     * @example 30
     */
    first_pass_num_inference_steps?: number;
    /**
     * First Pass Skip Final Steps
     * @description Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
     * @default 3
     */
    first_pass_skip_final_steps?: number;
    /**
     * Frame Rate
     * @description The frame rate of the video.
     * @default 30
     * @example 30
     */
    frame_rate?: number;
    /**
     * Images
     * @description URL of images to use as conditioning
     * @default []
     * @example [
     *       {
     *         "strength": 1,
     *         "start_frame_num": 0,
     *         "image_url": "https://storage.googleapis.com/falserverless/model_tests/ltx/NswO1P8sCLzrh1WefqQFK_9a6bdbfa54b944c9a770338159a113fd.jpg"
     *       },
     *       {
     *         "strength": 1,
     *         "start_frame_num": 88,
     *         "image_url": "https://storage.googleapis.com/falserverless/model_tests/ltx/YAPOGvmS2tM_Krdp7q6-d_267c97e017c34f679844a4477dfcec38.jpg"
     *       }
     *     ]
     */
    images?: Components.ImageConditioningInput[];
    /**
     * Loras
     * @description LoRA weights to use for generation
     * @default []
     */
    loras?: Components.LoRAWeight[];
    /**
     * Negative Prompt
     * @description Negative prompt for generation
     * @default worst quality, inconsistent motion, blurry, jittery, distorted
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description The number of frames in the video.
     * @default 121
     * @example 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description Text prompt to guide generation
     * @example A vibrant, abstract composition featuring a person with outstretched arms, rendered in a kaleidoscope of colors against a deep, dark background. The figure is composed of intricate, swirling patterns reminiscent of a mosaic, with hues of orange, yellow, blue, and green that evoke the style of artists such as Wassily Kandinsky or Bridget Riley. The camera zooms into the face striking portrait of a man, reimagined through the lens of old-school video-game graphics. The subject's face is rendered in a kaleidoscope of colors, with bold blues and reds set against a vibrant yellow backdrop. His dark hair is pulled back, framing his profile in a dramatic pose.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Reverse Video
     * @description Whether to reverse the video.
     * @default false
     * @example false
     */
    reverse_video?: boolean;
    /**
     * Second Pass Num Inference Steps
     * @description Number of inference steps during the second pass.
     * @default 30
     * @example 30
     */
    second_pass_num_inference_steps?: number;
    /**
     * Second Pass Skip Initial Steps
     * @description The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
     * @default 17
     * @example 17
     */
    second_pass_skip_initial_steps?: number;
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
    /**
     * Videos
     * @description Videos to use as conditioning
     * @default []
     */
    videos?: Components.VideoConditioningInput[];
}

export interface LtxVideo13bDevMulticonditioningOutput extends SharedType_5da {}

export interface LtxVideo13bDevImageToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the video.
     * @default auto
     * @example auto
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '1:1' | '16:9' | 'auto';
    /**
     * Constant Rate Factor
     * @description The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
     * @default 35
     * @example 35
     */
    constant_rate_factor?: number;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description Whether to expand the prompt using a language model.
     * @default false
     * @example false
     */
    expand_prompt?: boolean;
    /**
     * First Pass Num Inference Steps
     * @description Number of inference steps during the first pass.
     * @default 30
     * @example 30
     */
    first_pass_num_inference_steps?: number;
    /**
     * First Pass Skip Final Steps
     * @description Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
     * @default 3
     */
    first_pass_skip_final_steps?: number;
    /**
     * Frame Rate
     * @description The frame rate of the video.
     * @default 30
     * @example 30
     */
    frame_rate?: number;
    /**
     * Image Url
     * @description Image URL for Image-to-Video task
     * @example https://storage.googleapis.com/falserverless/example_inputs/ltxv-image-input.jpg
     */
    image_url: string;
    /**
     * Loras
     * @description LoRA weights to use for generation
     * @default []
     */
    loras?: Components.LoRAWeight[];
    /**
     * Negative Prompt
     * @description Negative prompt for generation
     * @default worst quality, inconsistent motion, blurry, jittery, distorted
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description The number of frames in the video.
     * @default 121
     * @example 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description Text prompt to guide generation
     * @example The astronaut gets up and walks away
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Reverse Video
     * @description Whether to reverse the video.
     * @default false
     * @example false
     */
    reverse_video?: boolean;
    /**
     * Second Pass Num Inference Steps
     * @description Number of inference steps during the second pass.
     * @default 30
     * @example 30
     */
    second_pass_num_inference_steps?: number;
    /**
     * Second Pass Skip Initial Steps
     * @description The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
     * @default 17
     * @example 17
     */
    second_pass_skip_initial_steps?: number;
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
}

export interface LtxVideo13bDevImageToVideoOutput extends SharedType_fd1 {}

export interface LtxVideo13bDevExtendInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the video.
     * @default auto
     * @example auto
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '1:1' | '16:9' | 'auto';
    /**
     * Constant Rate Factor
     * @description The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
     * @default 35
     * @example 35
     */
    constant_rate_factor?: number;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description Whether to expand the prompt using a language model.
     * @default false
     * @example false
     */
    expand_prompt?: boolean;
    /**
     * First Pass Num Inference Steps
     * @description Number of inference steps during the first pass.
     * @default 30
     * @example 30
     */
    first_pass_num_inference_steps?: number;
    /**
     * First Pass Skip Final Steps
     * @description Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
     * @default 3
     */
    first_pass_skip_final_steps?: number;
    /**
     * Frame Rate
     * @description The frame rate of the video.
     * @default 30
     * @example 30
     */
    frame_rate?: number;
    /**
     * Loras
     * @description LoRA weights to use for generation
     * @default []
     */
    loras?: Components.LoRAWeight[];
    /**
     * Negative Prompt
     * @description Negative prompt for generation
     * @default worst quality, inconsistent motion, blurry, jittery, distorted
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description The number of frames in the video.
     * @default 121
     * @example 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description Text prompt to guide generation
     * @example Woman walking on a street in Tokyo
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Reverse Video
     * @description Whether to reverse the video.
     * @default false
     * @example false
     */
    reverse_video?: boolean;
    /**
     * Second Pass Num Inference Steps
     * @description Number of inference steps during the second pass.
     * @default 30
     * @example 30
     */
    second_pass_num_inference_steps?: number;
    /**
     * Second Pass Skip Initial Steps
     * @description The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
     * @default 17
     * @example 17
     */
    second_pass_skip_initial_steps?: number;
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
    /**
     * Video
     * @description Video to be extended.
     * @example {
     *       "video_url": "https://storage.googleapis.com/falserverless/web-examples/wan/t2v.mp4",
     *       "start_frame_num": 24,
     *       "reverse_video": false,
     *       "limit_num_frames": false,
     *       "resample_fps": false,
     *       "strength": 1,
     *       "target_fps": 30,
     *       "max_num_frames": 121,
     *       "conditioning_type": "rgb",
     *       "preprocess": false
     *     }
     */
    video: Components.VideoConditioningInput;
}

export interface LtxVideo13bDevExtendOutput extends SharedType_b85 {}

export interface LtxVideo13bDevInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video (16:9, 1:1 or 9:16).
     * @default 16:9
     * @example 16:9
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '1:1' | '16:9';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description Whether to expand the prompt using a language model.
     * @default false
     * @example false
     */
    expand_prompt?: boolean;
    /**
     * First Pass Num Inference Steps
     * @description Number of inference steps during the first pass.
     * @default 30
     * @example 30
     */
    first_pass_num_inference_steps?: number;
    /**
     * First Pass Skip Final Steps
     * @description Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
     * @default 3
     */
    first_pass_skip_final_steps?: number;
    /**
     * Frame Rate
     * @description The frame rate of the video.
     * @default 30
     * @example 30
     */
    frame_rate?: number;
    /**
     * Loras
     * @description LoRA weights to use for generation
     * @default []
     */
    loras?: Components.LoRAWeight[];
    /**
     * Negative Prompt
     * @description Negative prompt for generation
     * @default worst quality, inconsistent motion, blurry, jittery, distorted
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description The number of frames in the video.
     * @default 121
     * @example 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description Text prompt to guide generation
     * @example A cinematic fast-tracking shot follows a vintage, teal camper van as it descends a winding mountain trail. The van, slightly weathered but well-maintained, is the central focus, its retro design emphasized by the motion blur. Medium shot reveals the dusty, ochre trail, edged with vibrant green pine trees. Close-up on the van's tires shows the gravel spraying, highlighting the speed and rugged terrain. Sunlight filters through the trees, casting dappled shadows on the van and the trail. The background is a hazy, majestic mountain range bathed in warm, golden light. The overall mood is adventurous and exhilarating. High resolution 4k movie scene.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p).
     * @default 720p
     * @example 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Reverse Video
     * @description Whether to reverse the video.
     * @default false
     * @example false
     */
    reverse_video?: boolean;
    /**
     * Second Pass Num Inference Steps
     * @description Number of inference steps during the second pass.
     * @default 30
     * @example 30
     */
    second_pass_num_inference_steps?: number;
    /**
     * Second Pass Skip Initial Steps
     * @description The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
     * @default 17
     * @example 17
     */
    second_pass_skip_initial_steps?: number;
    /**
     * Seed
     * @description Random seed for generation
     */
    seed?: number;
}

export interface LtxVideo13bDevOutput extends SharedType_0bd {}

export interface LtxVideoInput {
    /**
     * Guidance Scale
     * @description The guidance scale to use.
     * @default 3
     */
    guidance_scale?: number;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly
     */
    negative_prompt?: string;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to take.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A man stands waist-deep in a crystal-clear mountain pool, his back turned to a massive, thundering waterfall that cascades down jagged cliffs behind him. He wears a dark blue swimming shorts and his muscular back glistens with water droplets. The camera moves in a dynamic circular motion around him, starting from his right side and sweeping left, maintaining a slightly low angle that emphasizes the towering height of the waterfall. As the camera moves, the man slowly turns his head to follow its movement, his expression one of awe as he gazes up at the natural wonder. The waterfall creates a misty atmosphere, with sunlight filtering through the spray to create rainbow refractions. The water churns and ripples around him, reflecting the dramatic landscape. The handheld camera movement adds a subtle shake that enhances the raw, untamed energy of the scene. The lighting is natural and bright, with the sun positioned behind the waterfall, creating a backlit effect that silhouettes the falling water and illuminates the mist.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for random number generation.
     */
    seed?: number;
}

export interface LtxVideoOutput extends SharedType_4b8 {}

export interface Ltx2TextToVideoFastInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9';
    /**
     * Duration
     * @description The duration of the generated video in seconds. The fast model supports 6-20 seconds. Note: Durations longer than 10 seconds (12, 14, 16, 18, 20) are only supported with 25 FPS and 1080p resolution.
     * @default 6
     * @enum {integer}
     */
    duration?: 6 | 8 | 10 | 12 | 14 | 16 | 18 | 20;
    /**
     * Frames per Second
     * @description The frames per second of the generated video
     * @default 25
     * @enum {integer}
     */
    fps?: 25 | 50;
    /**
     * Generate Audio
     * @description Whether to generate audio for the generated video
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Prompt
     * @description The prompt to generate the video from
     * @example A cowboy walking through a dusty town at high noon, camera following from behind, cinematic depth, realistic lighting, western mood, 4K film grain.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 1080p
     * @enum {string}
     */
    resolution?: '1080p' | '1440p' | '2160p';
}

export interface Ltx2TextToVideoFastOutput extends SharedType_4f2 {}

export interface Ltx2TextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9';
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 6
     * @enum {integer}
     */
    duration?: 6 | 8 | 10;
    /**
     * Frames per Second
     * @description The frames per second of the generated video
     * @default 25
     * @enum {integer}
     */
    fps?: 25 | 50;
    /**
     * Generate Audio
     * @description Whether to generate audio for the generated video
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Prompt
     * @description The prompt to generate the video from
     * @example A cowboy walking through a dusty town at high noon, camera following from behind, cinematic depth, realistic lighting, western mood, 4K film grain.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 1080p
     * @enum {string}
     */
    resolution?: '1080p' | '1440p' | '2160p';
}

export interface Ltx2TextToVideoOutput extends SharedType_4f2 {}

export interface Ltx2RetakeVideoInput {
    /**
     * Duration
     * @description The duration of the video to retake in seconds
     * @default 5
     */
    duration?: number;
    /**
     * Prompt
     * @description The prompt to retake the video with
     * @example Remove the man
     */
    prompt: string;
    /**
     * Retake Mode
     * @description The retake mode to use for the retake
     * @default replace_audio_and_video
     * @enum {string}
     */
    retake_mode?: 'replace_audio' | 'replace_video' | 'replace_audio_and_video';
    /**
     * Start Time
     * @description The start time of the video to retake in seconds
     * @default 0
     */
    start_time?: number;
    /**
     * Video URL
     * @description The URL of the video to retake
     * @example https://v3b.fal.media/files/b/kangaroo/409mvS2FMwb6S7WcwkSW7_output.mp4
     */
    video_url: string;
}

export interface Ltx2RetakeVideoOutput {
    /**
     * Video
     * @description The generated video file
     * @example {
     *       "file_name": "ltxv-2-retake-output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/zebra/qM8Ve4OM8BcYnX23hoxd8_zvgLuC4m.mp4"
     *     }
     */
    video: Components.VideoFile_1;
}

export interface Ltx2ImageToVideoFastInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9';
    /**
     * Duration
     * @description The duration of the generated video in seconds. The fast model supports 6-20 seconds. Note: Durations longer than 10 seconds (12, 14, 16, 18, 20) are only supported with 25 FPS and 1080p resolution.
     * @default 6
     * @enum {integer}
     */
    duration?: 6 | 8 | 10 | 12 | 14 | 16 | 18 | 20;
    /**
     * Frames per Second
     * @description The frames per second of the generated video
     * @default 25
     * @enum {integer}
     */
    fps?: 25 | 50;
    /**
     * Generate Audio
     * @description Whether to generate audio for the generated video
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Image URL
     * @description URL of the image to generate the video from. Must be publicly accessible or base64 data URI. Supports PNG, JPEG, WebP, AVIF, and HEIF formats.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ltxv-2-i2v-input.jpg
     */
    image_url: string;
    /**
     * Prompt
     * @description The prompt to generate the video from
     * @example A woman stands still amid a busy neon-lit street at night. The camera slowly dollies in toward her face as people blur past, their motion emphasizing her calm presence. City lights flicker and reflections shift across her denim jacket.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 1080p
     * @enum {string}
     */
    resolution?: '1080p' | '1440p' | '2160p';
}

export interface Ltx2ImageToVideoFastOutput extends SharedType_799 {}

export interface Ltx2ImageToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9';
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 6
     * @enum {integer}
     */
    duration?: 6 | 8 | 10;
    /**
     * Frames per Second
     * @description The frames per second of the generated video
     * @default 25
     * @enum {integer}
     */
    fps?: 25 | 50;
    /**
     * Generate Audio
     * @description Whether to generate audio for the generated video
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Image URL
     * @description URL of the image to generate the video from. Must be publicly accessible or base64 data URI. Supports PNG, JPEG, WebP, AVIF, and HEIF formats.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ltxv-2-i2v-input.jpg
     */
    image_url: string;
    /**
     * Prompt
     * @description The prompt to generate the video from
     * @example A woman stands still amid a busy neon-lit street at night. The camera slowly dollies in toward her face as people blur past, their motion emphasizing her calm presence. City lights flicker and reflections shift across her denim jacket.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated video
     * @default 1080p
     * @enum {string}
     */
    resolution?: '1080p' | '1440p' | '2160p';
}

export interface Ltx2ImageToVideoOutput extends SharedType_799 {}

export interface Ltx219bVideoToVideoLoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Audio Strength
     * @description Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
     * @default 1
     */
    audio_strength?: number;
    /**
     * Audio URL
     * @description An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.
     */
    audio_url?: string;
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Strength
     * @description The strength of the end image to use for the video generation.
     * @default 1
     */
    end_image_strength?: number;
    /**
     * End Image URL
     * @description The URL of the image to use as the end of the video.
     */
    end_image_url?: string;
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use.
     * @default 3
     */
    guidance_scale?: number;
    /**
     * IC-LoRA
     * @description The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)
     * @default match_preprocessor
     * @example match_preprocessor
     * @enum {string}
     */
    ic_lora?: 'match_preprocessor' | 'canny' | 'depth' | 'pose' | 'detailer' | 'none';
    /**
     * IC-LoRA Scale
     * @description The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.
     * @default 1
     */
    ic_lora_scale?: number;
    /**
     * Image Strength
     * @description The strength of the image to use for the video generation.
     * @default 1
     */
    image_strength?: number;
    /**
     * Image URL
     * @description An optional URL of an image to use as the first frame of the video.
     */
    image_url?: string;
    /**
     * LoRAs
     * @description The LoRAs to use for the generation.
     */
    loras: Components.LoRAInput[];
    /**
     * Match Input FPS
     * @description When true, match the output FPS to the input video's FPS instead of using the default target FPS.
     * @default true
     */
    match_input_fps?: boolean;
    /**
     * Match Video Length
     * @description When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.
     * @default true
     */
    match_video_length?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Preprocessor
     * @description The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.
     * @default none
     * @example none
     * @enum {string}
     */
    preprocessor?: 'depth' | 'canny' | 'pose' | 'none';
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example black-and-white video, a cowboy walks through a dusty town, film grain
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video.
     * @default auto
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Strength
     * @description Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
     * @default 1
     */
    video_strength?: number;
    /**
     * Video URL
     * @description The URL of the video to generate the video from.
     * @example https://v3b.fal.media/files/b/0a8824b1/sdm0KfmenrlywesfzY1Y1_if6euPp1.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bVideoToVideoLoraOutput extends SharedType_4ed {}

export interface Ltx219bVideoToVideoInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Audio Strength
     * @description Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
     * @default 1
     */
    audio_strength?: number;
    /**
     * Audio URL
     * @description An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.
     */
    audio_url?: string;
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Strength
     * @description The strength of the end image to use for the video generation.
     * @default 1
     */
    end_image_strength?: number;
    /**
     * End Image URL
     * @description The URL of the image to use as the end of the video.
     */
    end_image_url?: string;
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use.
     * @default 3
     */
    guidance_scale?: number;
    /**
     * IC-LoRA
     * @description The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)
     * @default match_preprocessor
     * @example match_preprocessor
     * @enum {string}
     */
    ic_lora?: 'match_preprocessor' | 'canny' | 'depth' | 'pose' | 'detailer' | 'none';
    /**
     * IC-LoRA Scale
     * @description The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.
     * @default 1
     */
    ic_lora_scale?: number;
    /**
     * Image Strength
     * @description The strength of the image to use for the video generation.
     * @default 1
     */
    image_strength?: number;
    /**
     * Image URL
     * @description An optional URL of an image to use as the first frame of the video.
     */
    image_url?: string;
    /**
     * Match Input FPS
     * @description When true, match the output FPS to the input video's FPS instead of using the default target FPS.
     * @default true
     */
    match_input_fps?: boolean;
    /**
     * Match Video Length
     * @description When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.
     * @default true
     */
    match_video_length?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Preprocessor
     * @description The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.
     * @default none
     * @example none
     * @enum {string}
     */
    preprocessor?: 'depth' | 'canny' | 'pose' | 'none';
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example black-and-white video, a cowboy walks through a dusty town, film grain
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video.
     * @default auto
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Strength
     * @description Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
     * @default 1
     */
    video_strength?: number;
    /**
     * Video URL
     * @description The URL of the video to generate the video from.
     * @example https://v3b.fal.media/files/b/0a8824b1/sdm0KfmenrlywesfzY1Y1_if6euPp1.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bVideoToVideoOutput extends SharedType_4ed {}

export interface Ltx219bTextToVideoLoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use.
     * @default 3
     */
    guidance_scale?: number;
    /**
     * LoRAs
     * @description The LoRAs to use for the generation.
     */
    loras: Components.LoRAInput[];
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A cowboy walking through a dusty town at high noon, camera following from behind, cinematic depth, realistic lighting, western mood, 4K film grain.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video.
     * @default landscape_4_3
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bTextToVideoLoraOutput extends SharedType_61b {}

export interface Ltx219bTextToVideoInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use.
     * @default 3
     */
    guidance_scale?: number;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A cowboy walking through a dusty town at high noon, camera following from behind, cinematic depth, realistic lighting, western mood, 4K film grain.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video.
     * @default landscape_4_3
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bTextToVideoOutput extends SharedType_61b {}

export interface Ltx219bImageToVideoLoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Strength
     * @description The strength of the end image to use for the video generation.
     * @default 1
     */
    end_image_strength?: number;
    /**
     * End Image URL
     * @description The URL of the image to use as the end of the video.
     */
    end_image_url?: string;
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use.
     * @default 3
     */
    guidance_scale?: number;
    /**
     * Image Strength
     * @description The strength of the image to use for the video generation.
     * @default 1
     */
    image_strength?: number;
    /**
     * Image URL
     * @description The URL of the image to generate the video from.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ltxv-2-i2v-input.jpg
     */
    image_url: string;
    /**
     * Interpolation Direction
     * @description The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image.
     * @default forward
     * @enum {string}
     */
    interpolation_direction?: 'forward' | 'backward';
    /**
     * LoRAs
     * @description The LoRAs to use for the generation.
     */
    loras: Components.LoRAInput[];
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt used for the generation.
     * @example A woman stands still amid a busy neon-lit street at night. The camera slowly dollies in toward her face as people blur past, their motion emphasizing her calm presence. City lights flicker and reflections shift across her denim jacket.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video.
     * @default auto
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bImageToVideoLoraOutput extends SharedType_a00 {}

export interface Ltx219bImageToVideoInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Strength
     * @description The strength of the end image to use for the video generation.
     * @default 1
     */
    end_image_strength?: number;
    /**
     * End Image URL
     * @description The URL of the image to use as the end of the video.
     */
    end_image_url?: string;
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use.
     * @default 3
     */
    guidance_scale?: number;
    /**
     * Image Strength
     * @description The strength of the image to use for the video generation.
     * @default 1
     */
    image_strength?: number;
    /**
     * Image URL
     * @description The URL of the image to generate the video from.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ltxv-2-i2v-input.jpg
     */
    image_url: string;
    /**
     * Interpolation Direction
     * @description The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image.
     * @default forward
     * @enum {string}
     */
    interpolation_direction?: 'forward' | 'backward';
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt used for the generation.
     * @example A woman stands still amid a busy neon-lit street at night. The camera slowly dollies in toward her face as people blur past, their motion emphasizing her calm presence. City lights flicker and reflections shift across her denim jacket.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video.
     * @default auto
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bImageToVideoOutput extends SharedType_a00 {}

export interface Ltx219bExtendVideoLoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Audio Strength
     * @description Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
     * @default 1
     */
    audio_strength?: number;
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Strength
     * @description The strength of the end image to use for the video generation.
     * @default 1
     */
    end_image_strength?: number;
    /**
     * End Image URL
     * @description The URL of the image to use as the end of the extended video.
     */
    end_image_url?: string;
    /**
     * Extend Direction
     * @description Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning.
     * @default forward
     * @enum {string}
     */
    extend_direction?: 'forward' | 'backward';
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use.
     * @default 3
     */
    guidance_scale?: number;
    /**
     * LoRAs
     * @description The LoRAs to use for the generation.
     */
    loras: Components.LoRAInput[];
    /**
     * Match Input FPS
     * @description When true, match the output FPS to the input video's FPS instead of using the default target FPS.
     * @default true
     */
    match_input_fps?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Context Frames
     * @description The number of frames to use as context for the extension.
     * @default 25
     */
    num_context_frames?: number;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example Continue the scene naturally, maintaining the same style and motion.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video.
     * @default auto
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Strength
     * @description Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
     * @default 1
     */
    video_strength?: number;
    /**
     * Video URL
     * @description The URL of the video to extend.
     * @example https://v3b.fal.media/files/b/0a8824b1/sdm0KfmenrlywesfzY1Y1_if6euPp1.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bExtendVideoLoraOutput extends SharedType_567 {}

export interface Ltx219bExtendVideoInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Audio Strength
     * @description Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
     * @default 1
     */
    audio_strength?: number;
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Strength
     * @description The strength of the end image to use for the video generation.
     * @default 1
     */
    end_image_strength?: number;
    /**
     * End Image URL
     * @description The URL of the image to use as the end of the extended video.
     */
    end_image_url?: string;
    /**
     * Extend Direction
     * @description Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning.
     * @default forward
     * @enum {string}
     */
    extend_direction?: 'forward' | 'backward';
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use.
     * @default 3
     */
    guidance_scale?: number;
    /**
     * Match Input FPS
     * @description When true, match the output FPS to the input video's FPS instead of using the default target FPS.
     * @default true
     */
    match_input_fps?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Context Frames
     * @description The number of frames to use as context for the extension.
     * @default 25
     */
    num_context_frames?: number;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example Continue the scene naturally, maintaining the same style and motion.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video.
     * @default auto
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Strength
     * @description Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
     * @default 1
     */
    video_strength?: number;
    /**
     * Video URL
     * @description The URL of the video to extend.
     * @example https://v3b.fal.media/files/b/0a8824b1/sdm0KfmenrlywesfzY1Y1_if6euPp1.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bExtendVideoOutput extends SharedType_567 {}

export interface Ltx219bDistilledVideoToVideoLoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default none
     * @example none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Audio Strength
     * @description Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
     * @default 1
     */
    audio_strength?: number;
    /**
     * Audio URL
     * @description An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.
     */
    audio_url?: string;
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Strength
     * @description The strength of the end image to use for the video generation.
     * @default 1
     */
    end_image_strength?: number;
    /**
     * End Image URL
     * @description The URL of the image to use as the end of the video.
     */
    end_image_url?: string;
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * IC-LoRA
     * @description The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)
     * @default match_preprocessor
     * @example match_preprocessor
     * @enum {string}
     */
    ic_lora?: 'match_preprocessor' | 'canny' | 'depth' | 'pose' | 'detailer' | 'none';
    /**
     * IC-LoRA Scale
     * @description The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.
     * @default 1
     */
    ic_lora_scale?: number;
    /**
     * Image Strength
     * @description The strength of the image to use for the video generation.
     * @default 1
     */
    image_strength?: number;
    /**
     * Image URL
     * @description An optional URL of an image to use as the first frame of the video.
     */
    image_url?: string;
    /**
     * LoRAs
     * @description The LoRAs to use for the generation.
     */
    loras: Components.LoRAInput[];
    /**
     * Match Input FPS
     * @description When true, match the output FPS to the input video's FPS instead of using the default target FPS.
     * @default true
     */
    match_input_fps?: boolean;
    /**
     * Match Video Length
     * @description When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.
     * @default true
     */
    match_video_length?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Preprocessor
     * @description The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.
     * @default none
     * @example none
     * @enum {string}
     */
    preprocessor?: 'depth' | 'canny' | 'pose' | 'none';
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example black-and-white video, a cowboy walks through a dusty town, film grain
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video.
     * @default auto
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Strength
     * @description Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
     * @default 1
     */
    video_strength?: number;
    /**
     * Video URL
     * @description The URL of the video to generate the video from.
     * @example https://v3b.fal.media/files/b/0a8824b1/sdm0KfmenrlywesfzY1Y1_if6euPp1.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bDistilledVideoToVideoLoraOutput extends SharedType_4ed {}

export interface Ltx219bDistilledVideoToVideoInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default none
     * @example none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Audio Strength
     * @description Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
     * @default 1
     */
    audio_strength?: number;
    /**
     * Audio URL
     * @description An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.
     */
    audio_url?: string;
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Strength
     * @description The strength of the end image to use for the video generation.
     * @default 1
     */
    end_image_strength?: number;
    /**
     * End Image URL
     * @description The URL of the image to use as the end of the video.
     */
    end_image_url?: string;
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * IC-LoRA
     * @description The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)
     * @default match_preprocessor
     * @example match_preprocessor
     * @enum {string}
     */
    ic_lora?: 'match_preprocessor' | 'canny' | 'depth' | 'pose' | 'detailer' | 'none';
    /**
     * IC-LoRA Scale
     * @description The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.
     * @default 1
     */
    ic_lora_scale?: number;
    /**
     * Image Strength
     * @description The strength of the image to use for the video generation.
     * @default 1
     */
    image_strength?: number;
    /**
     * Image URL
     * @description An optional URL of an image to use as the first frame of the video.
     */
    image_url?: string;
    /**
     * Match Input FPS
     * @description When true, match the output FPS to the input video's FPS instead of using the default target FPS.
     * @default true
     */
    match_input_fps?: boolean;
    /**
     * Match Video Length
     * @description When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.
     * @default true
     */
    match_video_length?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Preprocessor
     * @description The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.
     * @default none
     * @example none
     * @enum {string}
     */
    preprocessor?: 'depth' | 'canny' | 'pose' | 'none';
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example black-and-white video, a cowboy walks through a dusty town, film grain
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video.
     * @default auto
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Strength
     * @description Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
     * @default 1
     */
    video_strength?: number;
    /**
     * Video URL
     * @description The URL of the video to generate the video from.
     * @example https://v3b.fal.media/files/b/0a8824b1/sdm0KfmenrlywesfzY1Y1_if6euPp1.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bDistilledVideoToVideoOutput extends SharedType_4ed {}

export interface Ltx219bDistilledTextToVideoLoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default none
     * @example none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * LoRAs
     * @description The LoRAs to use for the generation.
     */
    loras: Components.LoRAInput[];
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A cowboy walking through a dusty town at high noon, camera following from behind, cinematic depth, realistic lighting, western mood, 4K film grain.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video.
     * @default landscape_4_3
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bDistilledTextToVideoLoraOutput extends SharedType_61b {}

export interface Ltx219bDistilledTextToVideoInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default none
     * @example none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A cowboy walking through a dusty town at high noon, camera following from behind, cinematic depth, realistic lighting, western mood, 4K film grain.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video.
     * @default landscape_4_3
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bDistilledTextToVideoOutput extends SharedType_61b {}

export interface Ltx219bDistilledImageToVideoLoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default none
     * @example none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Strength
     * @description The strength of the end image to use for the video generation.
     * @default 1
     */
    end_image_strength?: number;
    /**
     * End Image URL
     * @description The URL of the image to use as the end of the video.
     */
    end_image_url?: string;
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Image Strength
     * @description The strength of the image to use for the video generation.
     * @default 1
     */
    image_strength?: number;
    /**
     * Image URL
     * @description The URL of the image to generate the video from.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ltxv-2-i2v-input.jpg
     */
    image_url: string;
    /**
     * Interpolation Direction
     * @description The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image.
     * @default forward
     * @enum {string}
     */
    interpolation_direction?: 'forward' | 'backward';
    /**
     * LoRAs
     * @description The LoRAs to use for the generation.
     */
    loras: Components.LoRAInput[];
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description The prompt used for the generation.
     * @example A woman stands still amid a busy neon-lit street at night. The camera slowly dollies in toward her face as people blur past, their motion emphasizing her calm presence. City lights flicker and reflections shift across her denim jacket.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video.
     * @default auto
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bDistilledImageToVideoLoraOutput extends SharedType_a00 {}

export interface Ltx219bDistilledImageToVideoInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default none
     * @example none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Strength
     * @description The strength of the end image to use for the video generation.
     * @default 1
     */
    end_image_strength?: number;
    /**
     * End Image URL
     * @description The URL of the image to use as the end of the video.
     */
    end_image_url?: string;
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Image Strength
     * @description The strength of the image to use for the video generation.
     * @default 1
     */
    image_strength?: number;
    /**
     * Image URL
     * @description The URL of the image to generate the video from.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ltxv-2-i2v-input.jpg
     */
    image_url: string;
    /**
     * Interpolation Direction
     * @description The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image.
     * @default forward
     * @enum {string}
     */
    interpolation_direction?: 'forward' | 'backward';
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description The prompt used for the generation.
     * @example A woman stands still amid a busy neon-lit street at night. The camera slowly dollies in toward her face as people blur past, their motion emphasizing her calm presence. City lights flicker and reflections shift across her denim jacket.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video.
     * @default auto
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bDistilledImageToVideoOutput extends SharedType_a00 {}

export interface Ltx219bDistilledExtendVideoLoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default none
     * @example none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Audio Strength
     * @description Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
     * @default 1
     */
    audio_strength?: number;
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Strength
     * @description The strength of the end image to use for the video generation.
     * @default 1
     */
    end_image_strength?: number;
    /**
     * End Image URL
     * @description The URL of the image to use as the end of the extended video.
     */
    end_image_url?: string;
    /**
     * Extend Direction
     * @description Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning.
     * @default forward
     * @enum {string}
     */
    extend_direction?: 'forward' | 'backward';
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * LoRAs
     * @description The LoRAs to use for the generation.
     */
    loras: Components.LoRAInput[];
    /**
     * Match Input FPS
     * @description When true, match the output FPS to the input video's FPS instead of using the default target FPS.
     * @default true
     */
    match_input_fps?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Context Frames
     * @description The number of frames to use as context for the extension.
     * @default 25
     */
    num_context_frames?: number;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example Continue the scene naturally, maintaining the same style and motion.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video.
     * @default auto
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Strength
     * @description Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
     * @default 1
     */
    video_strength?: number;
    /**
     * Video URL
     * @description The URL of the video to extend.
     * @example https://v3b.fal.media/files/b/0a8824b1/sdm0KfmenrlywesfzY1Y1_if6euPp1.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bDistilledExtendVideoLoraOutput extends SharedType_567 {}

export interface Ltx219bDistilledExtendVideoInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default none
     * @example none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Audio Strength
     * @description Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
     * @default 1
     */
    audio_strength?: number;
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Strength
     * @description The strength of the end image to use for the video generation.
     * @default 1
     */
    end_image_strength?: number;
    /**
     * End Image URL
     * @description The URL of the image to use as the end of the extended video.
     */
    end_image_url?: string;
    /**
     * Extend Direction
     * @description Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning.
     * @default forward
     * @enum {string}
     */
    extend_direction?: 'forward' | 'backward';
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Match Input FPS
     * @description When true, match the output FPS to the input video's FPS instead of using the default target FPS.
     * @default true
     */
    match_input_fps?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Context Frames
     * @description The number of frames to use as context for the extension.
     * @default 25
     */
    num_context_frames?: number;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example Continue the scene naturally, maintaining the same style and motion.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video.
     * @default auto
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Strength
     * @description Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
     * @default 1
     */
    video_strength?: number;
    /**
     * Video URL
     * @description The URL of the video to extend.
     * @example https://v3b.fal.media/files/b/0a8824b1/sdm0KfmenrlywesfzY1Y1_if6euPp1.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bDistilledExtendVideoOutput extends SharedType_567 {}

export interface Ltx219bDistilledAudioToVideoLoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default none
     * @example none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Audio Strength
     * @description Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification.
     * @default 1
     */
    audio_strength?: number;
    /**
     * Audio URL
     * @description The URL of the audio to generate the video from.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ltx-2-a2v-input-audio.mp3
     */
    audio_url: string;
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Strength
     * @description The strength of the end image to use for the video generation.
     * @default 1
     */
    end_image_strength?: number;
    /**
     * End Image URL
     * @description The URL of the image to use as the end of the video.
     */
    end_image_url?: string;
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Image Strength
     * @description The strength of the image to use for the video generation.
     * @default 1
     */
    image_strength?: number;
    /**
     * Image URL
     * @description Optional URL of an image to use as the first frame of the video.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ltx-2-a2v-input-image.png
     */
    image_url?: string;
    /**
     * LoRAs
     * @description The LoRAs to use for the generation.
     */
    loras: Components.LoRAInput[];
    /**
     * Match Audio Length
     * @description When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames.
     * @default true
     */
    match_audio_length?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Preprocess Audio
     * @description Whether to preprocess the audio before using it as conditioning.
     * @default true
     */
    preprocess_audio?: boolean;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A woman speaks to the camera
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video. Use 'auto' to match the input image dimensions if provided.
     * @default landscape_4_3
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bDistilledAudioToVideoLoraOutput extends SharedType_8b9 {}

export interface Ltx219bDistilledAudioToVideoInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default none
     * @example none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Audio Strength
     * @description Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification.
     * @default 1
     */
    audio_strength?: number;
    /**
     * Audio URL
     * @description The URL of the audio to generate the video from.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ltx-2-a2v-input-audio.mp3
     */
    audio_url: string;
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Strength
     * @description The strength of the end image to use for the video generation.
     * @default 1
     */
    end_image_strength?: number;
    /**
     * End Image URL
     * @description The URL of the image to use as the end of the video.
     */
    end_image_url?: string;
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Image Strength
     * @description The strength of the image to use for the video generation.
     * @default 1
     */
    image_strength?: number;
    /**
     * Image URL
     * @description Optional URL of an image to use as the first frame of the video.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ltx-2-a2v-input-image.png
     */
    image_url?: string;
    /**
     * Match Audio Length
     * @description When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames.
     * @default true
     */
    match_audio_length?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Preprocess Audio
     * @description Whether to preprocess the audio before using it as conditioning.
     * @default true
     */
    preprocess_audio?: boolean;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A woman speaks to the camera
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video. Use 'auto' to match the input image dimensions if provided.
     * @default landscape_4_3
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bDistilledAudioToVideoOutput extends SharedType_8b9 {}

export interface Ltx219bAudioToVideoLoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Audio Strength
     * @description Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification.
     * @default 1
     */
    audio_strength?: number;
    /**
     * Audio URL
     * @description The URL of the audio to generate the video from.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ltx-2-a2v-input-audio.mp3
     */
    audio_url: string;
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Strength
     * @description The strength of the end image to use for the video generation.
     * @default 1
     */
    end_image_strength?: number;
    /**
     * End Image URL
     * @description The URL of the image to use as the end of the video.
     */
    end_image_url?: string;
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Guidance Scale
     * @description The guidance scale to use.
     * @default 3
     */
    guidance_scale?: number;
    /**
     * Image Strength
     * @description The strength of the image to use for the video generation.
     * @default 1
     */
    image_strength?: number;
    /**
     * Image URL
     * @description Optional URL of an image to use as the first frame of the video.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ltx-2-a2v-input-image.png
     */
    image_url?: string;
    /**
     * LoRAs
     * @description The LoRAs to use for the generation.
     */
    loras: Components.LoRAInput[];
    /**
     * Match Audio Length
     * @description When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames.
     * @default true
     */
    match_audio_length?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Preprocess Audio
     * @description Whether to preprocess the audio before using it as conditioning.
     * @default true
     */
    preprocess_audio?: boolean;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A woman speaks to the camera
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video. Use 'auto' to match the input image dimensions if provided.
     * @default landscape_4_3
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bAudioToVideoLoraOutput extends SharedType_8b9 {}

export interface Ltx219bAudioToVideoInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high' | 'full';
    /**
     * Audio Strength
     * @description Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification.
     * @default 1
     */
    audio_strength?: number;
    /**
     * Audio URL
     * @description The URL of the audio to generate the video from.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ltx-2-a2v-input-audio.mp3
     */
    audio_url: string;
    /**
     * Camera LoRA
     * @description The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default none
     * @example none
     * @enum {string}
     */
    camera_lora?:
        | 'dolly_in'
        | 'dolly_out'
        | 'dolly_left'
        | 'dolly_right'
        | 'jib_up'
        | 'jib_down'
        | 'static'
        | 'none';
    /**
     * Camera LoRA Scale
     * @description The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
     * @default 1
     */
    camera_lora_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Strength
     * @description The strength of the end image to use for the video generation.
     * @default 1
     */
    end_image_strength?: number;
    /**
     * End Image URL
     * @description The URL of the image to use as the end of the video.
     */
    end_image_url?: string;
    /**
     * FPS
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Guidance Scale
     * @description The guidance scale to use.
     * @default 3
     */
    guidance_scale?: number;
    /**
     * Image Strength
     * @description The strength of the image to use for the video generation.
     * @default 1
     */
    image_strength?: number;
    /**
     * Image URL
     * @description Optional URL of an image to use as the first frame of the video.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ltx-2-a2v-input-image.png
     */
    image_url?: string;
    /**
     * Match Audio Length
     * @description When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames.
     * @default true
     */
    match_audio_length?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to generate the video from.
     * @default blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Preprocess Audio
     * @description Whether to preprocess the audio before using it as conditioning.
     * @default true
     */
    preprocess_audio?: boolean;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A woman speaks to the camera
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Multi-Scale
     * @description Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
     * @default true
     */
    use_multiscale?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Size
     * @description The size of the generated video. Use 'auto' to match the input image dimensions if provided.
     * @default landscape_4_3
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface Ltx219bAudioToVideoOutput extends SharedType_8b9 {}

export interface LoraInpaintInput {
    /**
     * Clip Skip
     * @description Skips part of the image generation process, leading to slightly different results.
     *                 This means the image renders faster, too.
     * @default 0
     */
    clip_skip?: number;
    /**
     * Controlnet Guess Mode
     * @description If set to true, the controlnet will be applied to only the conditional predictions.
     * @default false
     */
    controlnet_guess_mode?: boolean;
    /**
     * Controlnets
     * @description The control nets to use for the image generation. You can use any number of control nets
     *                 and they will be applied to the image at the specified timesteps.
     * @default []
     */
    controlnets?: Components.ControlNet_1[];
    /**
     * Debug Latents
     * @description If set to true, the latents will be saved for debugging.
     * @default false
     */
    debug_latents?: boolean;
    /**
     * Debug Per Pass Latents
     * @description If set to true, the latents will be saved for debugging per pass.
     * @default false
     */
    debug_per_pass_latents?: boolean;
    /**
     * Embeddings
     * @description The embeddings to use for the image generation. Only a single embedding is supported at the moment.
     *                 The embeddings will be used to map the tokens in the prompt to the embedding weights.
     * @default []
     */
    embeddings?: Components.Embedding_1[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     */
    enable_safety_checker?: boolean;
    /**
     * Eta
     * @description The eta value to be used for the image generation.
     * @default 0
     */
    eta?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Ic Light Image Url
     * @description The URL of the IC Light model image to use for the image generation.
     */
    ic_light_image_url?: string;
    /**
     * Ic Light Model Background Image Url
     * @description The URL of the IC Light model background image to use for the image generation.
     *                 Make sure to use a background compatible with the model.
     */
    ic_light_model_background_image_url?: string;
    /**
     * Ic Light Model Url
     * @description The URL of the IC Light model to use for the image generation.
     */
    ic_light_model_url?: string;
    /**
     * Image Encoder Path
     * @description The path to the image encoder model to use for the image generation.
     */
    image_encoder_path?: string;
    /**
     * Image Encoder Subfolder
     * @description The subfolder of the image encoder model to use for the image generation.
     */
    image_encoder_subfolder?: string;
    /**
     * Image Encoder Weight Name
     * @description The weight name of the image encoder model to use for the image generation.
     * @default pytorch_model.bin
     * @example pytorch_model.bin
     */
    image_encoder_weight_name?: string;
    /**
     * Image Format
     * @description The format of the generated image.
     * @default png
     * @example jpeg
     * @enum {string}
     */
    image_format?: 'jpeg' | 'png';
    /**
     * Image Url
     * @description URL of image to use for image to image/inpainting.
     */
    image_url?: string;
    /**
     * Ip Adapter
     * @description The IP adapter to use for the image generation.
     * @default []
     */
    ip_adapter?: Components.IPAdapter_1[];
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Mask Url
     * @description URL of black-and-white image to use as mask during inpainting.
     */
    mask_url?: string;
    /**
     * Model Name
     * @description URL or HuggingFace ID of the base model to generate the image.
     * @example stabilityai/stable-diffusion-xl-base-1.0
     * @example runwayml/stable-diffusion-v1-5
     * @example SG161222/Realistic_Vision_V2.0
     */
    model_name: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, painting, illustration, worst quality, low quality, normal quality
     */
    negative_prompt?: string;
    /**
     * Noise Strength
     * @description The amount of noise to add to noise image for image. Only used if the image_url is provided. 1.0 is complete noise and 0 is no noise.
     * @default 0.5
     */
    noise_strength?: number;
    /**
     * Number of images
     * @description Number of images to generate in one request. Note that the higher the batch size,
     *                 the longer it will take to generate the images.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of inference steps
     * @description Increasing the amount of steps tells Stable Diffusion that it should take more steps
     *                 to generate your final result which can increase the amount of detail in your image.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prediction Type
     * @description The type of prediction to use for the image generation.
     *                 The `epsilon` is the default.
     * @default epsilon
     * @enum {string}
     */
    prediction_type?: 'v_prediction' | 'epsilon';
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example Photo of a european medieval 40 year old queen, silver hair, highly detailed face, detailed eyes, head shot, intricate crown, age spots, wrinkles
     * @example Photo of a classic red mustang car parked in las vegas strip at night
     */
    prompt: string;
    /**
     * Prompt Weighting
     * @description If set to true, the prompt weighting syntax will be used.
     *                 Additionally, this will lift the 77 token limit by averaging embeddings.
     * @default false
     * @example true
     */
    prompt_weighting?: boolean;
    /**
     * Rescale Betas Snr Zero
     * @description Whether to set the rescale_betas_snr_zero option or not for the sampler
     * @default false
     */
    rescale_betas_snr_zero?: boolean;
    /**
     * Scheduler
     * @description Scheduler / sampler to use for the image denoising process.
     * @enum {string}
     */
    scheduler?:
        | 'DPM++ 2M'
        | 'DPM++ 2M Karras'
        | 'DPM++ 2M SDE'
        | 'DPM++ 2M SDE Karras'
        | 'Euler'
        | 'Euler A'
        | 'Euler (trailing timesteps)'
        | 'LCM'
        | 'LCM (trailing timesteps)'
        | 'DDIM'
        | 'TCD';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sigmas
     * @description Optionally override the sigmas to use for the denoising process. Only works with schedulers which support the `sigmas` argument in their `set_sigmas` method.
     *                 Defaults to not overriding, in which case the scheduler automatically sets the sigmas based on the `num_inference_steps` parameter.
     *                 If set to a custom sigma schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `timesteps` is set.
     * @default {
     *       "method": "default",
     *       "array": []
     *     }
     */
    sigmas?: Components.SigmasInput;
    /**
     * Tile Height
     * @description The size of the tiles to be used for the image generation.
     * @default 4096
     */
    tile_height?: number;
    /**
     * Tile Stride Height
     * @description The stride of the tiles to be used for the image generation.
     * @default 2048
     */
    tile_stride_height?: number;
    /**
     * Tile Stride Width
     * @description The stride of the tiles to be used for the image generation.
     * @default 2048
     */
    tile_stride_width?: number;
    /**
     * Tile Width
     * @description The size of the tiles to be used for the image generation.
     * @default 4096
     */
    tile_width?: number;
    /**
     * Timesteps
     * @description Optionally override the timesteps to use for the denoising process. Only works with schedulers which support the `timesteps` argument in their `set_timesteps` method.
     *                 Defaults to not overriding, in which case the scheduler automatically sets the timesteps based on the `num_inference_steps` parameter.
     *                 If set to a custom timestep schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `sigmas` is set.
     * @default {
     *       "method": "default",
     *       "array": []
     *     }
     */
    timesteps?: Components.TimestepsInput;
    /**
     * Unet Name
     * @description URL or HuggingFace ID of the custom U-Net model to use for the image generation.
     */
    unet_name?: string;
    /**
     * Variant
     * @description The variant of the model to use for huggingface models, e.g. 'fp16'.
     */
    variant?: string;
}

export interface LoraInpaintOutput extends SharedType_c451 {}

export interface LoraImageToImageInput {
    /**
     * Clip Skip
     * @description Skips part of the image generation process, leading to slightly different results.
     *                 This means the image renders faster, too.
     * @default 0
     */
    clip_skip?: number;
    /**
     * Controlnet Guess Mode
     * @description If set to true, the controlnet will be applied to only the conditional predictions.
     * @default false
     */
    controlnet_guess_mode?: boolean;
    /**
     * Controlnets
     * @description The control nets to use for the image generation. You can use any number of control nets
     *                 and they will be applied to the image at the specified timesteps.
     * @default []
     */
    controlnets?: Components.ControlNet_1[];
    /**
     * Debug Latents
     * @description If set to true, the latents will be saved for debugging.
     * @default false
     */
    debug_latents?: boolean;
    /**
     * Debug Per Pass Latents
     * @description If set to true, the latents will be saved for debugging per pass.
     * @default false
     */
    debug_per_pass_latents?: boolean;
    /**
     * Embeddings
     * @description The embeddings to use for the image generation. Only a single embedding is supported at the moment.
     *                 The embeddings will be used to map the tokens in the prompt to the embedding weights.
     * @default []
     */
    embeddings?: Components.Embedding_1[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     */
    enable_safety_checker?: boolean;
    /**
     * Eta
     * @description The eta value to be used for the image generation.
     * @default 0
     */
    eta?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Ic Light Image Url
     * @description The URL of the IC Light model image to use for the image generation.
     */
    ic_light_image_url?: string;
    /**
     * Ic Light Model Background Image Url
     * @description The URL of the IC Light model background image to use for the image generation.
     *                 Make sure to use a background compatible with the model.
     */
    ic_light_model_background_image_url?: string;
    /**
     * Ic Light Model Url
     * @description The URL of the IC Light model to use for the image generation.
     */
    ic_light_model_url?: string;
    /**
     * Image Encoder Path
     * @description The path to the image encoder model to use for the image generation.
     */
    image_encoder_path?: string;
    /**
     * Image Encoder Subfolder
     * @description The subfolder of the image encoder model to use for the image generation.
     */
    image_encoder_subfolder?: string;
    /**
     * Image Encoder Weight Name
     * @description The weight name of the image encoder model to use for the image generation.
     * @default pytorch_model.bin
     * @example pytorch_model.bin
     */
    image_encoder_weight_name?: string;
    /**
     * Image Format
     * @description The format of the generated image.
     * @default png
     * @example jpeg
     * @enum {string}
     */
    image_format?: 'jpeg' | 'png';
    /**
     * Image Url
     * @description URL of image to use for image to image/inpainting.
     */
    image_url?: string;
    /**
     * Ip Adapter
     * @description The IP adapter to use for the image generation.
     * @default []
     */
    ip_adapter?: Components.IPAdapter_1[];
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Model Name
     * @description URL or HuggingFace ID of the base model to generate the image.
     * @example stabilityai/stable-diffusion-xl-base-1.0
     * @example runwayml/stable-diffusion-v1-5
     * @example SG161222/Realistic_Vision_V2.0
     */
    model_name: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, painting, illustration, worst quality, low quality, normal quality
     */
    negative_prompt?: string;
    /**
     * Noise Strength
     * @description The amount of noise to add to noise image for image. Only used if the image_url is provided. 1.0 is complete noise and 0 is no noise.
     * @default 0.5
     */
    noise_strength?: number;
    /**
     * Number of images
     * @description Number of images to generate in one request. Note that the higher the batch size,
     *                 the longer it will take to generate the images.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of inference steps
     * @description Increasing the amount of steps tells Stable Diffusion that it should take more steps
     *                 to generate your final result which can increase the amount of detail in your image.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prediction Type
     * @description The type of prediction to use for the image generation.
     *                 The `epsilon` is the default.
     * @default epsilon
     * @enum {string}
     */
    prediction_type?: 'v_prediction' | 'epsilon';
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example Photo of a european medieval 40 year old queen, silver hair, highly detailed face, detailed eyes, head shot, intricate crown, age spots, wrinkles
     * @example Photo of a classic red mustang car parked in las vegas strip at night
     */
    prompt: string;
    /**
     * Prompt Weighting
     * @description If set to true, the prompt weighting syntax will be used.
     *                 Additionally, this will lift the 77 token limit by averaging embeddings.
     * @default false
     * @example true
     */
    prompt_weighting?: boolean;
    /**
     * Rescale Betas Snr Zero
     * @description Whether to set the rescale_betas_snr_zero option or not for the sampler
     * @default false
     */
    rescale_betas_snr_zero?: boolean;
    /**
     * Scheduler
     * @description Scheduler / sampler to use for the image denoising process.
     * @enum {string}
     */
    scheduler?:
        | 'DPM++ 2M'
        | 'DPM++ 2M Karras'
        | 'DPM++ 2M SDE'
        | 'DPM++ 2M SDE Karras'
        | 'Euler'
        | 'Euler A'
        | 'Euler (trailing timesteps)'
        | 'LCM'
        | 'LCM (trailing timesteps)'
        | 'DDIM'
        | 'TCD';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sigmas
     * @description Optionally override the sigmas to use for the denoising process. Only works with schedulers which support the `sigmas` argument in their `set_sigmas` method.
     *                 Defaults to not overriding, in which case the scheduler automatically sets the sigmas based on the `num_inference_steps` parameter.
     *                 If set to a custom sigma schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `timesteps` is set.
     * @default {
     *       "method": "default",
     *       "array": []
     *     }
     */
    sigmas?: Components.SigmasInput;
    /**
     * Tile Height
     * @description The size of the tiles to be used for the image generation.
     * @default 4096
     */
    tile_height?: number;
    /**
     * Tile Stride Height
     * @description The stride of the tiles to be used for the image generation.
     * @default 2048
     */
    tile_stride_height?: number;
    /**
     * Tile Stride Width
     * @description The stride of the tiles to be used for the image generation.
     * @default 2048
     */
    tile_stride_width?: number;
    /**
     * Tile Width
     * @description The size of the tiles to be used for the image generation.
     * @default 4096
     */
    tile_width?: number;
    /**
     * Timesteps
     * @description Optionally override the timesteps to use for the denoising process. Only works with schedulers which support the `timesteps` argument in their `set_timesteps` method.
     *                 Defaults to not overriding, in which case the scheduler automatically sets the timesteps based on the `num_inference_steps` parameter.
     *                 If set to a custom timestep schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `sigmas` is set.
     * @default {
     *       "method": "default",
     *       "array": []
     *     }
     */
    timesteps?: Components.TimestepsInput;
    /**
     * Unet Name
     * @description URL or HuggingFace ID of the custom U-Net model to use for the image generation.
     */
    unet_name?: string;
    /**
     * Variant
     * @description The variant of the model to use for huggingface models, e.g. 'fp16'.
     */
    variant?: string;
}

export interface LoraImageToImageOutput extends SharedType_c451 {}

export interface LoraInput {
    /**
     * Clip Skip
     * @description Skips part of the image generation process, leading to slightly different results.
     *                 This means the image renders faster, too.
     * @default 0
     */
    clip_skip?: number;
    /**
     * Controlnet Guess Mode
     * @description If set to true, the controlnet will be applied to only the conditional predictions.
     * @default false
     */
    controlnet_guess_mode?: boolean;
    /**
     * Controlnets
     * @description The control nets to use for the image generation. You can use any number of control nets
     *                 and they will be applied to the image at the specified timesteps.
     * @default []
     */
    controlnets?: Components.ControlNet_1[];
    /**
     * Debug Latents
     * @description If set to true, the latents will be saved for debugging.
     * @default false
     */
    debug_latents?: boolean;
    /**
     * Debug Per Pass Latents
     * @description If set to true, the latents will be saved for debugging per pass.
     * @default false
     */
    debug_per_pass_latents?: boolean;
    /**
     * Embeddings
     * @description The embeddings to use for the image generation. Only a single embedding is supported at the moment.
     *                 The embeddings will be used to map the tokens in the prompt to the embedding weights.
     * @default []
     */
    embeddings?: Components.Embedding_1[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     */
    enable_safety_checker?: boolean;
    /**
     * Eta
     * @description The eta value to be used for the image generation.
     * @default 0
     */
    eta?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Ic Light Image Url
     * @description The URL of the IC Light model image to use for the image generation.
     */
    ic_light_image_url?: string;
    /**
     * Ic Light Model Background Image Url
     * @description The URL of the IC Light model background image to use for the image generation.
     *                 Make sure to use a background compatible with the model.
     */
    ic_light_model_background_image_url?: string;
    /**
     * Ic Light Model Url
     * @description The URL of the IC Light model to use for the image generation.
     */
    ic_light_model_url?: string;
    /**
     * Image Encoder Path
     * @description The path to the image encoder model to use for the image generation.
     */
    image_encoder_path?: string;
    /**
     * Image Encoder Subfolder
     * @description The subfolder of the image encoder model to use for the image generation.
     */
    image_encoder_subfolder?: string;
    /**
     * Image Encoder Weight Name
     * @description The weight name of the image encoder model to use for the image generation.
     * @default pytorch_model.bin
     * @example pytorch_model.bin
     */
    image_encoder_weight_name?: string;
    /**
     * Image Format
     * @description The format of the generated image.
     * @default png
     * @example jpeg
     * @enum {string}
     */
    image_format?: 'jpeg' | 'png';
    /**
     * Image Size
     * @description The size of the generated image. You can choose between some presets or custom height and width
     *                 that **must be multiples of 8**.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Ip Adapter
     * @description The IP adapter to use for the image generation.
     * @default []
     */
    ip_adapter?: Components.IPAdapter_1[];
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Model Name
     * @description URL or HuggingFace ID of the base model to generate the image.
     * @example stabilityai/stable-diffusion-xl-base-1.0
     * @example runwayml/stable-diffusion-v1-5
     * @example SG161222/Realistic_Vision_V2.0
     */
    model_name: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, painting, illustration, worst quality, low quality, normal quality
     */
    negative_prompt?: string;
    /**
     * Number of images
     * @description Number of images to generate in one request. Note that the higher the batch size,
     *                 the longer it will take to generate the images.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of inference steps
     * @description Increasing the amount of steps tells Stable Diffusion that it should take more steps
     *                 to generate your final result which can increase the amount of detail in your image.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prediction Type
     * @description The type of prediction to use for the image generation.
     *                 The `epsilon` is the default.
     * @default epsilon
     * @enum {string}
     */
    prediction_type?: 'v_prediction' | 'epsilon';
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example Photo of a european medieval 40 year old queen, silver hair, highly detailed face, detailed eyes, head shot, intricate crown, age spots, wrinkles
     * @example Photo of a classic red mustang car parked in las vegas strip at night
     */
    prompt: string;
    /**
     * Prompt Weighting
     * @description If set to true, the prompt weighting syntax will be used.
     *                 Additionally, this will lift the 77 token limit by averaging embeddings.
     * @default false
     * @example true
     */
    prompt_weighting?: boolean;
    /**
     * Rescale Betas Snr Zero
     * @description Whether to set the rescale_betas_snr_zero option or not for the sampler
     * @default false
     */
    rescale_betas_snr_zero?: boolean;
    /**
     * Scheduler
     * @description Scheduler / sampler to use for the image denoising process.
     * @enum {string}
     */
    scheduler?:
        | 'DPM++ 2M'
        | 'DPM++ 2M Karras'
        | 'DPM++ 2M SDE'
        | 'DPM++ 2M SDE Karras'
        | 'Euler'
        | 'Euler A'
        | 'Euler (trailing timesteps)'
        | 'LCM'
        | 'LCM (trailing timesteps)'
        | 'DDIM'
        | 'TCD';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sigmas
     * @description Optionally override the sigmas to use for the denoising process. Only works with schedulers which support the `sigmas` argument in their `set_sigmas` method.
     *                 Defaults to not overriding, in which case the scheduler automatically sets the sigmas based on the `num_inference_steps` parameter.
     *                 If set to a custom sigma schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `timesteps` is set.
     * @default {
     *       "method": "default",
     *       "array": []
     *     }
     */
    sigmas?: Components.SigmasInput;
    /**
     * Tile Height
     * @description The size of the tiles to be used for the image generation.
     * @default 4096
     */
    tile_height?: number;
    /**
     * Tile Stride Height
     * @description The stride of the tiles to be used for the image generation.
     * @default 2048
     */
    tile_stride_height?: number;
    /**
     * Tile Stride Width
     * @description The stride of the tiles to be used for the image generation.
     * @default 2048
     */
    tile_stride_width?: number;
    /**
     * Tile Width
     * @description The size of the tiles to be used for the image generation.
     * @default 4096
     */
    tile_width?: number;
    /**
     * Timesteps
     * @description Optionally override the timesteps to use for the denoising process. Only works with schedulers which support the `timesteps` argument in their `set_timesteps` method.
     *                 Defaults to not overriding, in which case the scheduler automatically sets the timesteps based on the `num_inference_steps` parameter.
     *                 If set to a custom timestep schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `sigmas` is set.
     * @default {
     *       "method": "default",
     *       "array": []
     *     }
     */
    timesteps?: Components.TimestepsInput;
    /**
     * Unet Name
     * @description URL or HuggingFace ID of the custom U-Net model to use for the image generation.
     */
    unet_name?: string;
    /**
     * Variant
     * @description The variant of the model to use for huggingface models, e.g. 'fp16'.
     */
    variant?: string;
}

export interface LoraOutput extends SharedType_c451 {}

export interface LongcatVideoTextToVideo720pInput {
    /**
     * Acceleration
     * @description The acceleration level to use for the video generation.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * FPS
     * @description The frame rate of the generated video.
     * @default 30
     */
    fps?: number;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the video generation.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Negative Prompt
     * @description The negative prompt to use for the video generation.
     * @default Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 162
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use for the video generation.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Number of Refinement Inference Steps
     * @description The number of inference steps to use for refinement.
     * @default 40
     */
    num_refine_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to guide the video generation.
     * @example realistic filming style, a person wearing a dark helmet, a deep-colored jacket, blue jeans, and bright yellow shoes rides a skateboard along a winding mountain road. The skateboarder starts in a standing position, then gradually lowers into a crouch, extending one hand to touch the road surface while maintaining a low center of gravity to navigate a sharp curve. After completing the turn, the skateboarder rises back to a standing position and continues gliding forward. The background features lush green hills flanking both sides of the road, with distant snow-capped mountain peaks rising against a clear, bright blue sky. The camera follows closely from behind, smoothly tracking the skateboarder’s movements and capturing the dynamic scenery along the route. The scene is shot in natural daylight, highlighting the vivid outdoor environment and the skateboarder’s fluid actions.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface LongcatVideoTextToVideo720pOutput extends SharedType_085 {}

export interface LongcatVideoTextToVideo480pInput {
    /**
     * Acceleration
     * @description The acceleration level to use for the video generation.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * FPS
     * @description The frame rate of the generated video.
     * @default 15
     */
    fps?: number;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the video generation.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Negative Prompt
     * @description The negative prompt to use for the video generation.
     * @default Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 162
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use for the video generation.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to guide the video generation.
     * @example realistic filming style, a person wearing a dark helmet, a deep-colored jacket, blue jeans, and bright yellow shoes rides a skateboard along a winding mountain road. The skateboarder starts in a standing position, then gradually lowers into a crouch, extending one hand to touch the road surface while maintaining a low center of gravity to navigate a sharp curve. After completing the turn, the skateboarder rises back to a standing position and continues gliding forward. The background features lush green hills flanking both sides of the road, with distant snow-capped mountain peaks rising against a clear, bright blue sky. The camera follows closely from behind, smoothly tracking the skateboarder’s movements and capturing the dynamic scenery along the route. The scene is shot in natural daylight, highlighting the vivid outdoor environment and the skateboarder’s fluid actions.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface LongcatVideoTextToVideo480pOutput extends SharedType_085 {}

export interface LongcatVideoImageToVideo720pInput {
    /**
     * Acceleration
     * @description The acceleration level to use for the video generation.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * FPS
     * @description The frame rate of the generated video.
     * @default 30
     */
    fps?: number;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the video generation.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The URL of the image to generate a video from.
     * @example https://v3b.fal.media/files/b/zebra/trXRsbjJwy4Z3OEgbnB9a.jpg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use for the video generation.
     * @default Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 162
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use for the video generation.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Number of Refinement Inference Steps
     * @description The number of inference steps to use for refinement.
     * @default 40
     */
    num_refine_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to guide the video generation.
     * @default First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k
     * @example First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k
     */
    prompt?: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface LongcatVideoImageToVideo720pOutput extends SharedType_804 {}

export interface LongcatVideoImageToVideo480pInput {
    /**
     * Acceleration
     * @description The acceleration level to use for the video generation.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * FPS
     * @description The frame rate of the generated video.
     * @default 15
     */
    fps?: number;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the video generation.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The URL of the image to generate a video from.
     * @example https://v3b.fal.media/files/b/zebra/trXRsbjJwy4Z3OEgbnB9a.jpg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use for the video generation.
     * @default Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 162
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use for the video generation.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to guide the video generation.
     * @default First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k
     * @example First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k
     */
    prompt?: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface LongcatVideoImageToVideo480pOutput extends SharedType_804 {}

export interface LongcatVideoDistilledTextToVideo720pInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * FPS
     * @description The frame rate of the generated video.
     * @default 30
     */
    fps?: number;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 162
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 12
     */
    num_inference_steps?: number;
    /**
     * Number of Refinement Inference Steps
     * @description The number of inference steps to use for refinement.
     * @default 12
     */
    num_refine_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to guide the video generation.
     * @example realistic filming style, a person wearing a dark helmet, a deep-colored jacket, blue jeans, and bright yellow shoes rides a skateboard along a winding mountain road. The skateboarder starts in a standing position, then gradually lowers into a crouch, extending one hand to touch the road surface while maintaining a low center of gravity to navigate a sharp curve. After completing the turn, the skateboarder rises back to a standing position and continues gliding forward. The background features lush green hills flanking both sides of the road, with distant snow-capped mountain peaks rising against a clear, bright blue sky. The camera follows closely from behind, smoothly tracking the skateboarder’s movements and capturing the dynamic scenery along the route. The scene is shot in natural daylight, highlighting the vivid outdoor environment and the skateboarder’s fluid actions.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface LongcatVideoDistilledTextToVideo720pOutput extends SharedType_085 {}

export interface LongcatVideoDistilledTextToVideo480pInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * FPS
     * @description The frame rate of the generated video.
     * @default 15
     */
    fps?: number;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 162
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 12
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to guide the video generation.
     * @example realistic filming style, a person wearing a dark helmet, a deep-colored jacket, blue jeans, and bright yellow shoes rides a skateboard along a winding mountain road. The skateboarder starts in a standing position, then gradually lowers into a crouch, extending one hand to touch the road surface while maintaining a low center of gravity to navigate a sharp curve. After completing the turn, the skateboarder rises back to a standing position and continues gliding forward. The background features lush green hills flanking both sides of the road, with distant snow-capped mountain peaks rising against a clear, bright blue sky. The camera follows closely from behind, smoothly tracking the skateboarder’s movements and capturing the dynamic scenery along the route. The scene is shot in natural daylight, highlighting the vivid outdoor environment and the skateboarder’s fluid actions.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface LongcatVideoDistilledTextToVideo480pOutput extends SharedType_085 {}

export interface LongcatVideoDistilledImageToVideo720pInput {
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * FPS
     * @description The frame rate of the generated video.
     * @default 30
     */
    fps?: number;
    /**
     * Image URL
     * @description The URL of the image to generate a video from.
     * @example https://v3b.fal.media/files/b/zebra/trXRsbjJwy4Z3OEgbnB9a.jpg
     */
    image_url: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 162
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 12
     */
    num_inference_steps?: number;
    /**
     * Number of Refinement Inference Steps
     * @description The number of inference steps to use for refinement.
     * @default 12
     */
    num_refine_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to guide the video generation.
     * @default First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k
     * @example First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k
     */
    prompt?: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface LongcatVideoDistilledImageToVideo720pOutput extends SharedType_804 {}

export interface LongcatVideoDistilledImageToVideo480pInput {
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * FPS
     * @description The frame rate of the generated video.
     * @default 15
     */
    fps?: number;
    /**
     * Image URL
     * @description The URL of the image to generate a video from.
     * @example https://v3b.fal.media/files/b/zebra/trXRsbjJwy4Z3OEgbnB9a.jpg
     */
    image_url: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 162
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 12
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to guide the video generation.
     * @default First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k
     * @example First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k
     */
    prompt?: string;
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface LongcatVideoDistilledImageToVideo480pOutput extends SharedType_804 {}

export interface LongcatSingleAvatarImageAudioToVideoInput {
    /**
     * Audio Guidance Scale
     * @description The audio guidance scale. Higher values may lead to exaggerated mouth movements.
     * @default 4
     */
    audio_guidance_scale?: number;
    /**
     * Audio URL
     * @description The URL of the audio file to drive the avatar.
     * @example https://raw.githubusercontent.com/meituan-longcat/LongCat-Video/refs/heads/main/assets/avatar/single/man.mp3
     */
    audio_url: string;
    /**
     * Enable Safety Checker
     * @description Whether to enable safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image URL
     * @description The URL of the image to animate.
     * @example https://raw.githubusercontent.com/meituan-longcat/LongCat-Video/refs/heads/main/assets/avatar/single/man.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to avoid in the video generation.
     * @default Close-up, Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Number of Segments
     * @description Number of video segments to generate. Each segment adds ~5 seconds of video. First segment is ~5.8s, additional segments are 5s each.
     * @default 1
     */
    num_segments?: number;
    /**
     * Prompt
     * @description The prompt to guide the video generation.
     * @example A western man stands on stage under dramatic lighting, holding a microphone close to their mouth. Wearing a vibrant red jacket with gold embroidery, the singer is speaking while smoke swirls around them, creating a dynamic and atmospheric scene.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p). Billing is per video-second (16 frames): 480p is 1 unit per second and 720p is 4 units per second.
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Text Guidance Scale
     * @description The text guidance scale for classifier-free guidance.
     * @default 4
     */
    text_guidance_scale?: number;
}

export interface LongcatSingleAvatarImageAudioToVideoOutput {
    /**
     * Seed
     * @description The seed used for generation.
     * @example 424911732
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/0a87a0ce/2TnxxI02RHnLUvjGzhZa7_output_86da44aa6eed40ff9061b2213fb19793.mp4"
     *     }
     */
    video: Components.File;
}

export interface LongcatSingleAvatarAudioToVideoInput {
    /**
     * Audio Guidance Scale
     * @description The audio guidance scale. Higher values may lead to exaggerated mouth movements.
     * @default 4
     */
    audio_guidance_scale?: number;
    /**
     * Audio URL
     * @description The URL of the audio file to drive the avatar.
     * @example https://v3b.fal.media/files/b/0a87a827/QDAEdCQOPXxYWPUl2fyTY_4421psm.mp3
     */
    audio_url: string;
    /**
     * Enable Safety Checker
     * @description Whether to enable safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to avoid in the video generation.
     * @default Close-up, Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Number of Segments
     * @description Number of video segments to generate. Each segment adds ~5 seconds of video. First segment is ~5.8s, additional segments are 5s each.
     * @default 1
     */
    num_segments?: number;
    /**
     * Prompt
     * @description The prompt to guide the video generation.
     * @default A person is talking naturally with natural expressions and movements.
     * @example A person is talking naturally with natural expressions and movements.
     */
    prompt?: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p). Billing is per video-second (16 frames): 480p is 1 unit per second and 720p is 4 units per second.
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Text Guidance Scale
     * @description The text guidance scale for classifier-free guidance.
     * @default 4
     */
    text_guidance_scale?: number;
}

export interface LongcatSingleAvatarAudioToVideoOutput {
    /**
     * Seed
     * @description The seed used for generation.
     * @example 424911732
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/0a879d5d/UfaJ-sridj9C6IjSNWLYk_output_27368fcd87a34a0fb2929ed926cd71f0.mp4"
     *     }
     */
    video: Components.File;
}

export interface LongcatMultiAvatarImageAudioToVideoInput {
    /**
     * Audio Guidance Scale
     * @description The audio guidance scale. Higher values may lead to exaggerated mouth movements.
     * @default 4
     */
    audio_guidance_scale?: number;
    /**
     * Audio Type
     * @description How to combine the two audio tracks. 'para' (parallel) plays both simultaneously, 'add' (sequential) plays person 1 first then person 2.
     * @default para
     * @enum {string}
     */
    audio_type?: 'para' | 'add';
    /**
     * Audio URL Person 1
     * @description The URL of the audio file for person 1 (left side).
     * @default https://raw.githubusercontent.com/meituan-longcat/LongCat-Video/refs/heads/main/assets/avatar/multi/sing_man.WAV
     */
    audio_url_person1?: string;
    /**
     * Audio URL Person 2
     * @description The URL of the audio file for person 2 (right side).
     * @default https://raw.githubusercontent.com/meituan-longcat/LongCat-Video/refs/heads/main/assets/avatar/multi/sing_woman.WAV
     */
    audio_url_person2?: string;
    /**
     * Bbox Person1
     * @description Bounding box for person 1. If not provided, defaults to left half of image.
     */
    bbox_person1?: Components.BoundingBox;
    /**
     * Bbox Person2
     * @description Bounding box for person 2. If not provided, defaults to right half of image.
     */
    bbox_person2?: Components.BoundingBox;
    /**
     * Enable Safety Checker
     * @description Whether to enable safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image URL
     * @description The URL of the image containing two speakers.
     * @example https://raw.githubusercontent.com/meituan-longcat/LongCat-Video/refs/heads/main/assets/avatar/multi/sing.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to avoid in the video generation.
     * @default Close-up, Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Number of Segments
     * @description Number of video segments to generate. Each segment adds ~5 seconds of video. First segment is ~5.8s, additional segments are 5s each.
     * @default 1
     */
    num_segments?: number;
    /**
     * Prompt
     * @description The prompt to guide the video generation.
     * @default Two people are having a conversation with natural expressions and movements.
     * @example Static camera, In a professional recording studio, two people stand facing each other, both wearing large headphones. They are speaking clearly into a large condenser microphone suspended between them. They looked at each other affectionately and occasionally shook their heads according to the rhythm. The soundproofed walls and visible recording equipment create an atmosphere focused on capturing high-quality audio as they interact and communicate.
     */
    prompt?: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p or 720p). Billing is per video-second (16 frames): 480p is 1 unit per second and 720p is 4 units per second.
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description The seed for the random number generator.
     */
    seed?: number;
    /**
     * Text Guidance Scale
     * @description The text guidance scale for classifier-free guidance.
     * @default 4
     */
    text_guidance_scale?: number;
}

export interface LongcatMultiAvatarImageAudioToVideoOutput {
    /**
     * Seed
     * @description The seed used for generation.
     * @example 424911732
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/0a87a882/k7N4EBTQnVM9nCW9ylN8i_output_87614f102ba94cc0b101d058a815c81f.mp4"
     *     }
     */
    video: Components.File;
}

export interface LongcatImageEditInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the image generation.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The URL of the image to edit.
     * @example https://storage.googleapis.com/falserverless/example_inputs/longcat_image/edit_input.png
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to edit the image with.
     * @example Add the text "Fal is fast" in elegant cursive font with lightning streaks at the top of the image.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface LongcatImageEditOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/longcat_image/edit.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface LongcatImageInput {
    /**
     * Acceleration
     * @description The acceleration level to use.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the image generation.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A lioness crouching in the tall dry grass of the Serengeti during golden hour, intense gaze, telephoto lens with shallow depth of field
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface LongcatImageOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/longcat_image/t2i.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface LlavaNextInput {
    /**
     * Image URL
     * @description URL of the image to be processed
     * @example https://llava-vl.github.io/static/images/monalisa.jpg
     */
    image_url: string;
    /**
     * Max Tokens
     * @description Maximum number of tokens to generate
     * @default 64
     */
    max_tokens?: number;
    /**
     * Prompt
     * @description Prompt to be used for the image
     * @example Do you know who drew this painting?
     */
    prompt: string;
    /**
     * Temperature
     * @description Temperature for sampling
     * @default 0.2
     */
    temperature?: number;
    /**
     * Top P
     * @description Top P for sampling
     * @default 1
     */
    top_p?: number;
}

export interface LlavaNextOutput {
    /**
     * Output
     * @description Generated output
     * @example Leonardo da Vinci
     */
    output: string;
    /**
     * Partial
     * @description Whether the output is partial
     * @default false
     */
    partial?: boolean;
}

export interface LivePortraitImageInput {
    /**
     * Aaa
     * @description Amount to open mouth in 'aaa' shape
     * @default 0
     */
    aaa?: number;
    /**
     * Blink
     * @description Amount to blink the eyes
     * @default 0
     */
    blink?: number;
    /**
     * Dsize
     * @description Size of the output image.
     * @default 512
     */
    dsize?: number;
    /**
     * Eee
     * @description Amount to shape mouth in 'eee' position
     * @default 0
     */
    eee?: number;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker. If enabled, the model will check if the input image contains a face before processing it.
     *             The safety checker will process the input image
     * @default false
     */
    enable_safety_checker?: boolean;
    /**
     * Eyebrow
     * @description Amount to raise or lower eyebrows
     * @default 0
     */
    eyebrow?: number;
    /**
     * Flag Do Crop
     * @description Whether to crop the source portrait to the face-cropping space.
     * @default true
     */
    flag_do_crop?: boolean;
    /**
     * Flag Do Rot
     * @description Whether to conduct the rotation when flag_do_crop is True.
     * @default true
     */
    flag_do_rot?: boolean;
    /**
     * Flag Lip Zero
     * @description Whether to set the lip to closed state before animation. Only takes effect when flag_eye_retargeting and flag_lip_retargeting are False.
     * @default true
     */
    flag_lip_zero?: boolean;
    /**
     * Flag Pasteback
     * @description Whether to paste-back/stitch the animated face cropping from the face-cropping space to the original image space.
     * @default true
     */
    flag_pasteback?: boolean;
    /**
     * Image Url
     * @description URL of the image to be animated
     * @example https://storage.googleapis.com/falserverless/model_tests/live-portrait/XKEmk3mAzGHUjK3qqH-UL.jpeg
     */
    image_url: string;
    /**
     * Output Format
     * @description Output format
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Pupil X
     * @description Amount to move pupils horizontally
     * @default 0
     */
    pupil_x?: number;
    /**
     * Pupil Y
     * @description Amount to move pupils vertically
     * @default 0
     */
    pupil_y?: number;
    /**
     * Rotate Pitch
     * @description Amount to rotate the face in pitch
     * @default 0
     */
    rotate_pitch?: number;
    /**
     * Rotate Roll
     * @description Amount to rotate the face in roll
     * @default 0
     */
    rotate_roll?: number;
    /**
     * Rotate Yaw
     * @description Amount to rotate the face in yaw
     * @default 0
     */
    rotate_yaw?: number;
    /**
     * Scale
     * @description Scaling factor for the face crop.
     * @default 2.3
     */
    scale?: number;
    /**
     * Smile
     * @description Amount to smile
     * @default 0
     */
    smile?: number;
    /**
     * Vx Ratio
     * @description Horizontal offset ratio for face crop.
     * @default 0
     */
    vx_ratio?: number;
    /**
     * Vy Ratio
     * @description Vertical offset ratio for face crop. Positive values move up, negative values move down.
     * @default -0.125
     */
    vy_ratio?: number;
    /**
     * Wink
     * @description Amount to wink
     * @default 0
     */
    wink?: number;
    /**
     * Woo
     * @description Amount to shape mouth in 'woo' position
     * @default 0
     */
    woo?: number;
}

export interface LivePortraitImageOutput {
    /**
     * Image
     * @description The generated image file.
     */
    image: Components.Image;
}

export interface LivePortraitInput {
    /**
     * Aaa
     * @description Amount to open mouth in 'aaa' shape
     * @default 0
     */
    aaa?: number;
    /**
     * Batch Size
     * @description Batch size for the model. The larger the batch size, the faster the model will run, but the more memory it will consume.
     * @default 32
     */
    batch_size?: number;
    /**
     * Blink
     * @description Amount to blink the eyes
     * @default 0
     */
    blink?: number;
    /**
     * Dsize
     * @description Size of the output image.
     * @default 512
     */
    dsize?: number;
    /**
     * Eee
     * @description Amount to shape mouth in 'eee' position
     * @default 0
     */
    eee?: number;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker. If enabled, the model will check if the input image contains a face before processing it.
     *             The safety checker will process the input image
     * @default false
     */
    enable_safety_checker?: boolean;
    /**
     * Eyebrow
     * @description Amount to raise or lower eyebrows
     * @default 0
     */
    eyebrow?: number;
    /**
     * Flag Do Crop
     * @description Whether to crop the source portrait to the face-cropping space.
     * @default true
     */
    flag_do_crop?: boolean;
    /**
     * Flag Do Rot
     * @description Whether to conduct the rotation when flag_do_crop is True.
     * @default true
     */
    flag_do_rot?: boolean;
    /**
     * Flag Eye Retargeting
     * @description Whether to enable eye retargeting.
     * @default false
     */
    flag_eye_retargeting?: boolean;
    /**
     * Flag Lip Retargeting
     * @description Whether to enable lip retargeting.
     * @default false
     */
    flag_lip_retargeting?: boolean;
    /**
     * Flag Lip Zero
     * @description Whether to set the lip to closed state before animation. Only takes effect when flag_eye_retargeting and flag_lip_retargeting are False.
     * @default true
     */
    flag_lip_zero?: boolean;
    /**
     * Flag Pasteback
     * @description Whether to paste-back/stitch the animated face cropping from the face-cropping space to the original image space.
     * @default true
     */
    flag_pasteback?: boolean;
    /**
     * Flag Relative
     * @description Whether to use relative motion.
     * @default true
     */
    flag_relative?: boolean;
    /**
     * Flag Stitching
     * @description Whether to enable stitching. Recommended to set to True.
     * @default true
     */
    flag_stitching?: boolean;
    /**
     * Image Url
     * @description URL of the image to be animated
     * @example https://storage.googleapis.com/falserverless/model_tests/live-portrait/XKEmk3mAzGHUjK3qqH-UL.jpeg
     */
    image_url: string;
    /**
     * Pupil X
     * @description Amount to move pupils horizontally
     * @default 0
     */
    pupil_x?: number;
    /**
     * Pupil Y
     * @description Amount to move pupils vertically
     * @default 0
     */
    pupil_y?: number;
    /**
     * Rotate Pitch
     * @description Amount to rotate the face in pitch
     * @default 0
     */
    rotate_pitch?: number;
    /**
     * Rotate Roll
     * @description Amount to rotate the face in roll
     * @default 0
     */
    rotate_roll?: number;
    /**
     * Rotate Yaw
     * @description Amount to rotate the face in yaw
     * @default 0
     */
    rotate_yaw?: number;
    /**
     * Scale
     * @description Scaling factor for the face crop.
     * @default 2.3
     */
    scale?: number;
    /**
     * Smile
     * @description Amount to smile
     * @default 0
     */
    smile?: number;
    /**
     * Video Url
     * @description URL of the video to drive the lip syncing.
     * @example https://storage.googleapis.com/falserverless/model_tests/live-portrait/liveportrait-example.mp4
     */
    video_url: string;
    /**
     * Vx Ratio
     * @description Horizontal offset ratio for face crop.
     * @default 0
     */
    vx_ratio?: number;
    /**
     * Vy Ratio
     * @description Vertical offset ratio for face crop. Positive values move up, negative values move down.
     * @default -0.125
     */
    vy_ratio?: number;
    /**
     * Wink
     * @description Amount to wink
     * @default 0
     */
    wink?: number;
    /**
     * Woo
     * @description Amount to shape mouth in 'woo' position
     * @default 0
     */
    woo?: number;
}

export interface LivePortraitOutput extends SharedType_328 {}

export interface LiveAvatarInput {
    /**
     * Acceleration
     * @description Acceleration level for faster video decoding
     * @default none
     * @enum {string}
     */
    acceleration?: 'none' | 'light' | 'regular' | 'high';
    /**
     * Audio URL
     * @description The URL of the driving audio file (WAV or MP3). The avatar will be animated to match this audio.
     * @example https://v3b.fal.media/files/b/0a86b1d1/iHOKR5dlnHWW9UFfgL90b_tmp9fq30v29.wav
     */
    audio_url: string;
    /**
     * Enable Safety Checker
     * @description Enable safety checker for content moderation.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames per Clip
     * @description Number of frames per clip. Must be a multiple of 4. Higher values = smoother but slower generation.
     * @default 48
     */
    frames_per_clip?: number;
    /**
     * Guidance Scale
     * @description Classifier-free guidance scale. Higher values follow the prompt more closely.
     * @default 0
     */
    guidance_scale?: number;
    /**
     * Reference Image URL
     * @description The URL of the reference image for avatar generation. The character in this image will be animated.
     * @example https://v3b.fal.media/files/b/0a86b14d/NzJ8cpbwpOJPEX8SMpqe4_cyclops_baker.jpg
     */
    image_url: string;
    /**
     * Number of Clips
     * @description Number of video clips to generate. Each clip is approximately 3 seconds. Set higher for longer videos.
     * @default 10
     */
    num_clips?: number;
    /**
     * Prompt
     * @description A text prompt describing the scene and character. Helps guide the video generation style and context.
     * @example A person speaking naturally with expressive gestures.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducible generation.
     */
    seed?: number;
}

export interface LiveAvatarOutput {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated avatar video file with synchronized audio.
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/0a86b1d5/tLG1qOjdIYYhJSqJGEMWs_output.mp4"
     *     }
     */
    video: Components.VideoFile_1;
}

export interface LightxRelightInput {
    /**
     * Prompt
     * @description Optional text prompt. If omitted, Light-X will auto-caption the video.
     */
    prompt?: string;
    /**
     * Ref Id
     * @description Frame index to use as referencen to relight the video with reference.
     * @default 0
     */
    ref_id?: number;
    /**
     * Relight Parameters
     * @description Relighting parameters (required for relight_condition_type='ic'). Not used for 'bg' (which expects a background image URL instead).
     * @example {
     *       "relight_prompt": "Sunlight",
     *       "bg_source": "Right",
     *       "use_sky_mask": false,
     *       "cfg": 2
     *     }
     */
    relight_parameters?: Components.RelightParameters;
    /**
     * Relit Cond Img Url
     * @description URL of conditioning image. Required for relight_condition_type='ref'/'hdr'. Also required for relight_condition_type='bg' (background image).
     * @example https://storage.googleapis.com/falserverless/example_inputs/lightx_image.png
     */
    relit_cond_img_url?: string;
    /**
     * Relit Cond Type
     * @description Relight condition type.
     * @default ic
     * @enum {string}
     */
    relit_cond_type?: 'ic' | 'ref' | 'hdr' | 'bg';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Video Url
     * @description URL of the input video.
     * @example https://storage.googleapis.com/falserverless/example_inputs/lightx_video.mp4
     */
    video_url: string;
}

export interface LightxRelightOutput extends SharedType_a89 {}

export interface LightxRecameraInput {
    /**
     * Camera
     * @description Camera control mode.
     * @default traj
     * @enum {string}
     */
    camera?: 'traj' | 'target';
    /**
     * Mode
     * @description Camera motion mode.
     * @default gradual
     * @enum {string}
     */
    mode?: 'gradual' | 'bullet' | 'direct' | 'dolly-zoom';
    /**
     * Prompt
     * @description Optional text prompt. If omitted, Light-X will auto-caption the video.
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Target Pose
     * @description Target camera pose [theta, phi, radius, x, y] (required when camera='target').
     * @example [
     *       10,
     *       -15,
     *       0.2,
     *       0,
     *       0
     *     ]
     */
    target_pose?: number[];
    /**
     * Trajectory
     * @description Camera trajectory parameters (required for recamera mode).
     * @example {
     *       "theta": [
     *         0,
     *         2,
     *         8,
     *         10,
     *         5,
     *         3,
     *         0,
     *         -2,
     *         -5,
     *         -8,
     *         -5,
     *         -3,
     *         0
     *       ],
     *       "radius": [
     *         0,
     *         0.02,
     *         0.09,
     *         0.16,
     *         0.25,
     *         0.2,
     *         0.09,
     *         0
     *       ],
     *       "phi": [
     *         0,
     *         -3,
     *         -8,
     *         -15,
     *         -20,
     *         -15,
     *         -10,
     *         -5,
     *         0
     *       ]
     *     }
     */
    trajectory?: Components.TrajectoryParameters;
    /**
     * Video Url
     * @description URL of the input video.
     * @example https://storage.googleapis.com/falserverless/example_inputs/lightx_video.mp4
     */
    video_url: string;
}

export interface LightxRecameraOutput extends SharedType_a89 {}

export interface LightningModelsInput {
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 2
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @default {
     *       "height": 1024,
     *       "width": 1024
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description The list of LoRA weights to use.
     * @default []
     */
    loras?: Components.LoraWeight_1[];
    /**
     * Model Name
     * @description The Lightning model to use.
     * @example Lykon/dreamshaper-xl-lightning
     * @example SG161222/RealVisXL_V4.0_Lightning
     */
    model_name?: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want in the image.
     * @default (worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed, grayscale, bw, bad photo, bad photography, bad art:1.4), (watermark, signature, text font, username, error, logo, words, letters, digits, autograph, trademark, name:1.2), (blur, blurry, grainy), morbid, ugly, asymmetrical, mutated malformed, mutilated, poorly lit, bad shadow, draft, cropped, out of frame, cut off, censored, jpeg artifacts, out of focus, glitch, duplicate, (airbrushed, cartoon, anime, semi-realistic, cgi, render, blender, digital art, manga, amateur:1.3), (3D ,3D Game, 3D Game Scene, 3D Character:1.1), (bad hands, bad anatomy, bad body, bad face, bad teeth, bad arms, bad legs, deformities:1.3)
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 5
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example A hyperdetailed photograph of a Cat dressed as a mafia boss holding a fish walking down a Japanese fish market with an angry face, 8k resolution, best quality, beautiful photograph, dynamic lighting,
     */
    prompt: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Scheduler
     * @description Scheduler / sampler to use for the image denoising process.
     * @enum {string}
     */
    scheduler?:
        | 'DPM++ 2M'
        | 'DPM++ 2M Karras'
        | 'DPM++ 2M SDE'
        | 'DPM++ 2M SDE Karras'
        | 'DPM++ SDE'
        | 'DPM++ SDE Karras'
        | 'KDPM 2A'
        | 'Euler'
        | 'Euler (trailing timesteps)'
        | 'Euler A'
        | 'LCM'
        | 'EDMDPMSolverMultistepScheduler'
        | 'TCDScheduler';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface LightningModelsOutput extends SharedType_a73 {}

export interface LeffaVirtualTryonInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Garment Image Url
     * @description Url to the garment image.
     * @example https://storage.googleapis.com/falserverless/model_tests/leffa/tshirt_image.jpg
     */
    garment_image_url: string;
    /**
     * Garment Type
     * @description The type of the garment used for virtual try-on.
     * @example upper_body
     * @example lower_body
     * @example dresses
     * @enum {string}
     */
    garment_type: 'upper_body' | 'lower_body' | 'dresses';
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your input when generating the image.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Human Image Url
     * @description Url for the human image.
     * @example https://storage.googleapis.com/falserverless/model_tests/leffa/person_image.jpg
     */
    human_image_url: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Seed
     * @description The same seed and the same input given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface LeffaVirtualTryonOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the image contains NSFW concepts.
     */
    has_nsfw_concepts: boolean;
    /**
     * Image
     * @description The output image.
     * @example {
     *       "height": 1024,
     *       "content_type": "image/jpeg",
     *       "url": "https://fal.media/files/elephant/9NTQQNo9eyiQUSLa6cYBW.png",
     *       "width": 768
     *     }
     */
    image: Components.Image;
    /**
     * Seed
     * @description The seed for the inference.
     */
    seed: number;
}

export interface LeffaPoseTransferInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your input when generating the image.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Person Image Url
     * @description Url to the garment image.
     * @example https://storage.googleapis.com/falserverless/model_tests/leffa/pose_image.jpg
     */
    person_image_url: string;
    /**
     * Pose Image Url
     * @description Url for the human image.
     * @example https://storage.googleapis.com/falserverless/model_tests/leffa/person_image.jpg
     */
    pose_image_url: string;
    /**
     * Seed
     * @description The same seed and the same input given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface LeffaPoseTransferOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the image contains NSFW concepts.
     */
    has_nsfw_concepts: boolean;
    /**
     * Image
     * @description The output image.
     * @example {
     *       "height": 1024,
     *       "content_type": "image/jpeg",
     *       "url": "https://fal.media/files/tiger/y6ZwaYdP9Q92FnsJcSbYz.png",
     *       "width": 768
     *     }
     */
    image: Components.Image;
    /**
     * Seed
     * @description The seed for the inference.
     */
    seed: number;
}

export interface LcmSd15I2iInput {
    /**
     * Enable Safety Checks
     * @description If set to true, the resulting image will be checked whether it includes any
     *                 potentially unsafe content. If it does, it will be replaced with a black
     *                 image.
     * @default true
     */
    enable_safety_checks?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description The image to use as a base.
     * @example https://storage.googleapis.com/falserverless/model_tests/lcm/beach.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     * @example ugly, deformed
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate. The function will return a list of images
     *                 with the same prompt and negative prompt but different seeds.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to use for generating the image. The more steps
     *                 the better the image will be but it will also take longer to generate.
     * @default 4
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example masterpiece, colorful, photo of a beach in hawaii, sun
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     * @example 42
     */
    seed?: number;
    /**
     * Strength
     * @description The strength of the image.
     * @default 0.8
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface LcmSd15I2iOutput {
    /**
     * Images
     * @description The generated image files info.
     */
    images: Components.Image_1[];
    /**
     * Nsfw Content Detected
     * @description A list of booleans indicating whether the generated image contains any
     *                 potentially unsafe content. If the safety check is disabled, this field
     *                 will have a false for each generated image.
     */
    nsfw_content_detected: boolean[];
    /**
     * Num Inference Steps
     * @description Number of inference steps used to generate the image. It will be the same value of the one passed in the
     *                 input or the default one in case none was passed.
     * @default 4
     */
    num_inference_steps?: number;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface LcmInput {
    /**
     * Controlnet Inpaint
     * @description If set to true, the inpainting pipeline will use controlnet inpainting.
     *                 Only effective for inpainting pipelines.
     * @default false
     */
    controlnet_inpaint?: boolean;
    /**
     * Enable Safety Checks
     * @description If set to true, the resulting image will be checked whether it includes any
     *                 potentially unsafe content. If it does, it will be replaced with a black
     *                 image.
     * @default true
     */
    enable_safety_checks?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. You can choose between some presets or
     *                 custom height and width that **must be multiples of 8**.
     *
     *                 If not provided:
     *                 - For text-to-image generations, the default size is 512x512.
     *                 - For image-to-image generations, the default size is the same as the input image.
     *                 - For inpainting generations, the default size is the same as the input image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description The base image to use for guiding the image generation on image-to-image
     *             generations. If the either width or height of the image is larger than 1024
     *             pixels, the image will be resized to 1024 pixels while keeping the aspect ratio.
     * @example https://storage.googleapis.com/falserverless/model_tests/lcm/inpaint_image.png
     * @example https://storage.googleapis.com/falserverless/model_tests/lcm/beach.png
     */
    image_url?: string;
    /**
     * Inpaint Mask Only
     * @description If set to true, the inpainting pipeline will only inpaint the provided mask
     *                 area. Only effective for inpainting pipelines.
     * @default false
     */
    inpaint_mask_only?: boolean;
    /**
     * Lora Scale
     * @description The scale of the lora server to use for image generation.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Lora Url
     * @description The url of the lora server to use for image generation.
     */
    lora_url?: string;
    /**
     * Mask Url
     * @description The mask to use for guiding the image generation on image
     *             inpainting. The model will focus on the mask area and try to fill it with
     *             the most relevant content.
     *
     *             The mask must be a black and white image where the white area is the area
     *             that needs to be filled and the black area is the area that should be
     *             ignored.
     *
     *             The mask must have the same dimensions as the image passed as `image_url`.
     * @example https://storage.googleapis.com/falserverless/model_tests/lcm/inpaint_mask.png
     */
    mask_url?: string;
    /**
     * Model
     * @description The model to use for generating the image.
     * @default sdv1-5
     * @enum {string}
     */
    model?: 'sdxl' | 'sdv1-5';
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     * @example ugly, deformed
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate. The function will return a list of images
     *                 with the same prompt and negative prompt but different seeds.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to use for generating the image. The more steps
     *                 the better the image will be but it will also take longer to generate.
     * @default 4
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example a black cat with glowing eyes, cute, adorable, disney, pixar, highly detailed, 8k
     * @example an island near sea, with seagulls, moon shining over the sea, light house, boats int he background, fish flying over the sea
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     * @example 42
     */
    seed?: number;
    /**
     * Strength
     * @description The strength of the image that is passed as `image_url`. The strength
     *             determines how much the generated image will be similar to the image passed as
     *             `image_url`. The higher the strength the more model gets "creative" and
     *             generates an image that's different from the initial image. A strength of 1.0
     *             means that the initial image is more or less ignored and the model will try to
     *             generate an image that's as close as possible to the prompt.
     * @default 0.8
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface LcmOutput {
    /**
     * Images
     * @description The generated image files info.
     */
    images: Components.Image_1[];
    /**
     * Nsfw Content Detected
     * @description A list of booleans indicating whether the generated image contains any
     *                 potentially unsafe content. If the safety check is disabled, this field
     *                 will all will be false.
     */
    nsfw_content_detected: boolean[];
    /**
     * Num Inference Steps
     * @description Number of inference steps used to generate the image. It will be the same value of the one passed in the
     *                 input or the default one in case none was passed.
     * @default 4
     */
    num_inference_steps?: number;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface LayerDiffusionInput {
    /**
     * Enable Safety Checker
     * @description If set to false, the safety checker will be disabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale for the model.
     * @default 8
     */
    guidance_scale?: number;
    /**
     * Negative Prompt
     * @description The prompt to use for generating the negative image. Be as descriptive as possible for best results.
     * @default text, watermark
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps for the model.
     * @default 20
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @default
     * @example a male army soldier holding a gun
     */
    prompt?: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
}

export interface LayerDiffusionOutput {
    /**
     * Image
     * @description The URL of the generated image.
     */
    image: Components.Image_2;
    /**
     * Seed
     * @description The seed used to generate the image.
     */
    seed: number;
}

export interface LatentsyncInput {
    /**
     * Audio Url
     * @description The URL of the audio to generate the lip sync for.
     * @example https://fal.media/files/lion/vyFWygmZsIZlUO4s0nr2n.wav
     */
    audio_url: string;
    /**
     * Guidance Scale
     * @description Guidance scale for the model inference
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Loop Mode
     * @description Video loop mode when audio is longer than video. Options: pingpong, loop
     * @enum {string}
     */
    loop_mode?: 'pingpong' | 'loop';
    /**
     * Seed
     * @description Random seed for generation. If None, a random seed will be used.
     */
    seed?: number;
    /**
     * Video Url
     * @description The URL of the video to generate the lip sync for.
     * @example https://fal.media/files/koala/8teUPbRRMtAUTORDvqy0l.mp4
     */
    video_url: string;
}

export interface LatentsyncOutput {
    /**
     * Video
     * @description The generated video with the lip sync.
     */
    video: Components.File;
}

export interface KreaWan14bVideoToVideoInput {
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Prompt
     * @description Prompt for the video-to-video generation.
     * @example A powerful, matte black jeep, its robust frame contrasting with the lush green surroundings, navigates a winding jungle road, kicking up small clouds of dust and loose earth from its tires.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed for the video-to-video generation.
     */
    seed?: number;
    /**
     * Strength
     * @description Denoising strength for the video-to-video generation. 0.0 preserves the original, 1.0 completely remakes the video.
     * @default 0.85
     */
    strength?: number;
    /**
     * Video Url
     * @description URL of the input video. Currently, only outputs of 16:9 aspect ratio and 480p resolution are supported. Video duration should be less than 1000 frames at 16fps, and output frames will be 6 plus a multiple of 12, for example 18, 30, 42, etc.
     * @example https://storage.googleapis.com/falserverless/example_inputs/krea_wan_14b_v2v_input.mp4
     */
    video_url: string;
}

export interface KreaWan14bVideoToVideoOutput extends SharedType_b88 {}

export interface KreaWan14bTextToVideoInput {
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Num Frames
     * @description Number of frames to generate. Must be a multiple of 12 plus 6, for example 6, 18, 30, 42, etc.
     * @default 78
     */
    num_frames?: number;
    /**
     * Prompt
     * @description Prompt for the video-to-video generation.
     * @example A powerful, matte black jeep, its robust frame contrasting with the lush green surroundings, navigates a winding jungle road, kicking up small clouds of dust and loose earth from its tires.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed for the video-to-video generation.
     */
    seed?: number;
}

export interface KreaWan14bTextToVideoOutput extends SharedType_b88 {}

export interface KolorsImageToImageInput {
    /**
     * Enable Safety Checker
     * @description Enable safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show
     *                 you.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description URL of image to use for image to image
     * @example https://storage.googleapis.com/falserverless/model_tests/image_models/bunny_source.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small
     *                 details (e.g. moustache, blurry, low resolution).
     * @default
     * @example ugly, deformed, blurry
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example high quality image of a capybara wearing sunglasses. In the background of the image there are trees, poles, grass and other objects. At the bottom of the object there is the road., 8k, highly detailed.
     */
    prompt: string;
    /**
     * Scheduler
     * @description The scheduler to use for the model.
     * @default EulerDiscreteScheduler
     * @enum {string}
     */
    scheduler?:
        | 'EulerDiscreteScheduler'
        | 'EulerAncestralDiscreteScheduler'
        | 'DPMSolverMultistepScheduler'
        | 'DPMSolverMultistepScheduler_SDE_karras'
        | 'UniPCMultistepScheduler'
        | 'DEISMultistepScheduler';
    /**
     * Seed
     * @description Seed
     */
    seed?: number;
    /**
     * Strength
     * @description The strength to use for image-to-image. 1.0 is completely remakes the image while 0.0 preserves the original.
     * @default 0.85
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and
     *                 uploaded before returning the response. This will increase the latency of
     *                 the function but it allows you to get the image directly in the response
     *                 without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface KolorsImageToImageOutput extends SharedType_a73 {}

export interface KolorsInput {
    /**
     * Enable Safety Checker
     * @description Enable safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show
     *                 you.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small
     *                 details (e.g. moustache, blurry, low resolution).
     * @default
     * @example ugly, deformed, blurry
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible
     *                 for best results.
     * @example A young Chinese couple with fair skin, dressed in stylish sportswear, with the modern Beijing city skyline in the background. Facial details, clear pores, captured using the latest camera model, close-up shot, ultra-high quality, 8K, visual feast.
     * @example The image features four mythical beasts: Vermilion Bird, Black Tortoise, Azure Dragon, and White Tiger. The Vermilion Bird is at the top of the image, with feathers as red as fire and a tail as magnificent as a phoenix, its wings spreading like burning flames. The Black Tortoise is at the bottom, depicted as a giant turtle intertwined with a snake. Ancient runes adorn the turtle's shell, and the snake's eyes are cold and sharp. The Azure Dragon is on the right, its long body coiling in the sky, with jade-green scales, flowing whiskers, deer-like horns, and exhaling clouds and mist. The White Tiger is on the left, with a majestic posture, white fur with black stripes, piercing eyes, sharp teeth and claws, surrounded by vast mountains and grasslands.
     */
    prompt: string;
    /**
     * Scheduler
     * @description The scheduler to use for the model.
     * @default EulerDiscreteScheduler
     * @enum {string}
     */
    scheduler?:
        | 'EulerDiscreteScheduler'
        | 'EulerAncestralDiscreteScheduler'
        | 'DPMSolverMultistepScheduler'
        | 'DPMSolverMultistepScheduler_SDE_karras'
        | 'UniPCMultistepScheduler'
        | 'DEISMultistepScheduler';
    /**
     * Seed
     * @description Seed
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and
     *                 uploaded before returning the response. This will increase the latency of
     *                 the function but it allows you to get the image directly in the response
     *                 without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface KolorsOutput extends SharedType_a73 {}

export interface KokoroSpanishInput {
    /**
     * Prompt
     * @example La vida es un viaje, no un destino. Disfruta cada momento y sigue adelante con pasión.
     */
    prompt: string;
    /**
     * Speed
     * @description Speed of the generated audio. Default is 1.0.
     * @default 1
     */
    speed?: number;
    /**
     * Voice
     * @description Voice ID for the desired voice.
     * @example ef_dora
     * @enum {string}
     */
    voice: 'ef_dora' | 'em_alex' | 'em_santa';
}

export interface KokoroSpanishOutput {
    /**
     * Audio
     * @description The generated music
     * @example {
     *       "url": "https://fal.media/files/monkey/5rBM3qVCED73Lxs5XLcwj_tmp4f2z_qrf.wav"
     *     }
     */
    audio: Components.File;
}

export interface KokoroMandarinChineseInput {
    /**
     * Prompt
     * @example 每一个伟大的旅程，都始于勇敢迈出的第一步。加油，你可以做到！
     */
    prompt: string;
    /**
     * Speed
     * @description Speed of the generated audio. Default is 1.0.
     * @default 1
     */
    speed?: number;
    /**
     * Voice
     * @description Voice ID for the desired voice.
     * @example zf_xiaobei
     * @enum {string}
     */
    voice:
        | 'zf_xiaobei'
        | 'zf_xiaoni'
        | 'zf_xiaoxiao'
        | 'zf_xiaoyi'
        | 'zm_yunjian'
        | 'zm_yunxi'
        | 'zm_yunxia'
        | 'zm_yunyang';
}

export interface KokoroMandarinChineseOutput {
    /**
     * Audio
     * @description The generated music
     * @example {
     *       "url": "https://fal.media/files/rabbit/8UiqobkQXPrYDRHl4l5oU_tmptz6jo3ex.wav"
     *     }
     */
    audio: Components.File;
}

export interface KokoroJapaneseInput {
    /**
     * Prompt
     * @example 夢を追いかけることを恐れないでください。努力すれば、必ず道は開けます！
     */
    prompt: string;
    /**
     * Speed
     * @description Speed of the generated audio. Default is 1.0.
     * @default 1
     */
    speed?: number;
    /**
     * Voice
     * @description Voice ID for the desired voice.
     * @example jf_alpha
     * @enum {string}
     */
    voice: 'jf_alpha' | 'jf_gongitsune' | 'jf_nezumi' | 'jf_tebukuro' | 'jm_kumo';
}

export interface KokoroJapaneseOutput {
    /**
     * Audio
     * @description The generated music
     * @example {
     *       "url": "https://fal.media/files/lion/piLhqKO8LJxrWaNg2dVUv_tmpp6eff6zl.wav"
     *     }
     */
    audio: Components.File;
}

export interface KokoroItalianInput {
    /**
     * Prompt
     * @example Ogni giorno è una nuova opportunità per scrivere la tua storia. Rendila straordinaria!
     */
    prompt: string;
    /**
     * Speed
     * @description Speed of the generated audio. Default is 1.0.
     * @default 1
     */
    speed?: number;
    /**
     * Voice
     * @description Voice ID for the desired voice.
     * @example if_sara
     * @enum {string}
     */
    voice: 'if_sara' | 'im_nicola';
}

export interface KokoroItalianOutput {
    /**
     * Audio
     * @description The generated music
     * @example {
     *       "url": "https://fal.media/files/monkey/-MZ0hRO4IpTMukb_S5aRZ_tmpin14eoed.wav"
     *     }
     */
    audio: Components.File;
}

export interface KokoroHindiInput {
    /**
     * Prompt
     * @example सपने वो नहीं जो हम सोते समय देखते हैं, सपने वो हैं जो हमें सोने नहीं देते।
     */
    prompt: string;
    /**
     * Speed
     * @description Speed of the generated audio. Default is 1.0.
     * @default 1
     */
    speed?: number;
    /**
     * Voice
     * @description Voice ID for the desired voice.
     * @example hf_alpha
     * @enum {string}
     */
    voice: 'hf_alpha' | 'hf_beta' | 'hm_omega' | 'hm_psi';
}

export interface KokoroHindiOutput {
    /**
     * Audio
     * @description The generated music
     * @example {
     *       "url": "https://fal.media/files/elephant/3sGUskl1AFG4TN_NAinO8_tmpdq_1m8og.wav"
     *     }
     */
    audio: Components.File;
}

export interface KokoroFrenchInput {
    /**
     * Prompt
     * @example La seule limite à nos réalisations de demain, ce sont nos doutes d’aujourd’hui.
     */
    prompt: string;
    /**
     * Speed
     * @description Speed of the generated audio. Default is 1.0.
     * @default 1
     */
    speed?: number;
    /**
     * Voice
     * @description Voice ID for the desired voice.
     * @example ff_siwis
     * @enum {string}
     */
    voice: 'ff_siwis';
}

export interface KokoroFrenchOutput {
    /**
     * Audio
     * @description The generated music
     * @example {
     *       "url": "https://fal.media/files/kangaroo/E_itKJKZKRNaO-QtU77k1_tmpe1qso5xp.wav"
     *     }
     */
    audio: Components.File;
}

export interface KokoroBritishEnglishInput {
    /**
     * Prompt
     * @example Ladies and gentlemen, welcome aboard. Please ensure your seatbelt is fastened and your tray table is stowed as we prepare for takeoff.
     */
    prompt: string;
    /**
     * Speed
     * @description Speed of the generated audio. Default is 1.0.
     * @default 1
     */
    speed?: number;
    /**
     * Voice
     * @description Voice ID for the desired voice.
     * @example bf_alice
     * @enum {string}
     */
    voice:
        | 'bf_alice'
        | 'bf_emma'
        | 'bf_isabella'
        | 'bf_lily'
        | 'bm_daniel'
        | 'bm_fable'
        | 'bm_george'
        | 'bm_lewis';
}

export interface KokoroBritishEnglishOutput {
    /**
     * Audio
     * @description The generated music
     * @example {
     *       "url": "https://fal.media/files/kangaroo/4wpA60Kum6UjOVBKJoNyL_tmpxfrkn95k.wav"
     *     }
     */
    audio: Components.File;
}

export interface KokoroBrazilianPortugueseInput {
    /**
     * Prompt
     * @example O segredo do sucesso é a persistência. Nunca desista dos seus sonhos!
     */
    prompt: string;
    /**
     * Speed
     * @description Speed of the generated audio. Default is 1.0.
     * @default 1
     */
    speed?: number;
    /**
     * Voice
     * @description Voice ID for the desired voice.
     * @example pf_dora
     * @enum {string}
     */
    voice: 'pf_dora' | 'pm_alex' | 'pm_santa';
}

export interface KokoroBrazilianPortugueseOutput {
    /**
     * Audio
     * @description The generated music
     * @example {
     *       "url": "https://fal.media/files/rabbit/Y9-bWJt5lixo8PTCmncN6_tmpyh7u57oa.wav"
     *     }
     */
    audio: Components.File;
}

export interface KokoroAmericanEnglishInput {
    /**
     * Prompt
     * @default
     * @example The future belongs to those who believe in the beauty of their dreams. So, dream big, work hard, and make it happen!
     */
    prompt?: string;
    /**
     * Speed
     * @description Speed of the generated audio. Default is 1.0.
     * @default 1
     */
    speed?: number;
    /**
     * Voice
     * @description Voice ID for the desired voice.
     * @default af_heart
     * @example af_heart
     * @enum {string}
     */
    voice?:
        | 'af_heart'
        | 'af_alloy'
        | 'af_aoede'
        | 'af_bella'
        | 'af_jessica'
        | 'af_kore'
        | 'af_nicole'
        | 'af_nova'
        | 'af_river'
        | 'af_sarah'
        | 'af_sky'
        | 'am_adam'
        | 'am_echo'
        | 'am_eric'
        | 'am_fenrir'
        | 'am_liam'
        | 'am_michael'
        | 'am_onyx'
        | 'am_puck'
        | 'am_santa';
}

export interface KokoroAmericanEnglishOutput {
    /**
     * Audio
     * @description The generated music
     * @example {
     *       "url": "https://fal.media/files/elephant/dXVMqWsBDG9yan3kaOT0Z_tmp0vvkha3s.wav"
     *     }
     */
    audio: Components.File;
}

export interface KlingV15KolorsVirtualTryOnInput {
    /**
     * Garment Image Url
     * @description Url to the garment image.
     * @example https://storage.googleapis.com/falserverless/model_tests/leffa/tshirt_image.jpg
     */
    garment_image_url: string;
    /**
     * Human Image Url
     * @description Url for the human image.
     * @example https://storage.googleapis.com/falserverless/model_tests/leffa/person_image.jpg
     */
    human_image_url: string;
    /**
     * Sync Mode
     * @description If true, the function will return the image in the response.
     * @default false
     */
    sync_mode?: boolean;
}

export interface KlingV15KolorsVirtualTryOnOutput {
    /**
     * Image
     * @description The output image.
     * @example {
     *       "file_size": 595094,
     *       "height": 1024,
     *       "file_name": "result.png",
     *       "content_type": "image/png",
     *       "url": "https://v3.fal.media/files/panda/Hoy3zhimzVKi3F2uoGBnh_result.png",
     *       "width": 768
     *     }
     */
    image: Components.Image;
}

export interface KlingVideoVideoToAudioInput {
    /**
     * Asmr Mode
     * @description Enable ASMR mode. This mode enhances detailed sound effects and is suitable for highly immersive content scenarios.
     * @default false
     */
    asmr_mode?: boolean;
    /**
     * Background Music Prompt
     * @description Background music prompt. Cannot exceed 200 characters.
     * @default intense car race
     */
    background_music_prompt?: string;
    /**
     * Sound Effect Prompt
     * @description Sound effect prompt. Cannot exceed 200 characters.
     * @default Car tires screech as they accelerate in a drag race
     */
    sound_effect_prompt?: string;
    /**
     * Video Url
     * @description The video URL to extract audio from. Only .mp4/.mov formats are supported. File size does not exceed 100MB. Video duration between 3.0s and 20.0s.
     * @example https://storage.googleapis.com/falserverless/model_tests/kling/kling-v2.5-turbo-pro-image-to-video-output.mp4
     */
    video_url: string;
}

export interface KlingVideoVideoToAudioOutput {
    /**
     * Audio
     * @description The extracted/generated audio from the video in MP3 format
     * @example {
     *       "url": "https://v3.fal.media/files/monkey/O-ekVTtYqeDblD1oSf2uv_extracted_audio.mp3"
     *     }
     */
    audio: Components.File;
    /**
     * Video
     * @description The original video with dubbed audio applied
     * @example {
     *       "url": "https://v3.fal.media/files/monkey/O-ekVTtYqeDblD1oSf2uv_dubbed_video.mp4"
     *     }
     */
    video: Components.File;
}

export interface KlingVideoV2MasterTextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video frame
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Cfg Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt.
     * @default 0.5
     */
    cfg_scale?: number;
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example A slow-motion drone shot descending from above a maze of neon-lit Tokyo alleyways at night during heavy rainfall. The camera gradually focuses on a lone figure in a luminescent white raincoat standing perfectly still amid the bustling crowd, all carrying black umbrellas. As the camera continues its downward journey, we see the raindrops creating rippling patterns on puddles that reflect the kaleidoscope of colors from the surrounding signs, creating a mirror world beneath the city.
     */
    prompt: string;
}

export interface KlingVideoV2MasterTextToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://v3.fal.media/files/rabbit/5fu6OSZdvV825r2s_c0S8_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface KlingVideoV2MasterImageToVideoInput {
    /**
     * Cfg Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt.
     * @default 0.5
     */
    cfg_scale?: number;
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Image Url
     * @description URL of the image to be used for the video
     * @example https://v3.fal.media/files/elephant/rkH-9qoXtXu3rAYTsx9V5_image.webp
     */
    image_url: string;
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example slow-motion sequence captures the catastrophic implosion of a skyscraper, dust and debris billowing outwards in a chaotic ballet of destruction, while a haunting, orchestral score underscores the sheer power and finality of the event.
     */
    prompt: string;
}

export interface KlingVideoV2MasterImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://v3.fal.media/files/koala/VvGXP5xEhTR9ovGjpulJ7_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface KlingVideoV26StandardMotionControlInput extends SharedType_778 {}

export interface KlingVideoV26StandardMotionControlOutput extends SharedType_ade {}

export interface KlingVideoV26ProTextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video frame
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Cfg Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt.
     * @default 0.5
     */
    cfg_scale?: number;
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Generate Audio
     * @description Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example Old friends reuniting at a train station after 20 years, one exclaims 'Is that really you?!' other tearfully replies 'I promised I'd come back, didn't I?', train whistle, steam hissing, emotional orchestral swell, crowd murmur
     */
    prompt: string;
}

export interface KlingVideoV26ProTextToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "file_size": 8195664,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/0a84ab71/8hPbLs7n59WhWY-BN69yX_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface KlingVideoV26ProMotionControlInput extends SharedType_778 {}

export interface KlingVideoV26ProMotionControlOutput extends SharedType_ade {}

export interface KlingVideoV26ProImageToVideoInput {
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * End Image Url
     * @description URL of the image to be used for the end of the video
     */
    end_image_url?: string;
    /**
     * Generate Audio
     * @description Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase.
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example A king walks slowly and says "My people, here I am! I am here to save you all"
     */
    prompt: string;
    /**
     * Start Image Url
     * @description URL of the image to be used for the video
     * @example https://v3b.fal.media/files/b/0a84ab29/BSJXz9Ht-jgRgMf4IGxLU_upscaled.png
     */
    start_image_url: string;
    /**
     * Voice Ids
     * @description List of voice IDs to use for voice control. Reference voices in the prompt using <<<voice_1>>>, <<<voice_2>>>. Maximum 2 voices allowed. When provided and referenced in prompt, enables voice control billing.
     */
    voice_ids?: string[];
}

export interface KlingVideoV26ProImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "file_size": 11814817,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/0a84ab51/Qr1twf8UgtD5rZHpNXC2P_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface KlingVideoV25TurboStandardImageToVideoInput {
    /**
     * Cfg Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt.
     * @default 0.5
     */
    cfg_scale?: number;
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Image Url
     * @description URL of the image to be used for the video
     * @example https://storage.googleapis.com/falserverless/example_inputs/kling_v25_std_i2v_input.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example In a dimly lit room, a playful cat's eyes light up, fixated on a dancing red dot. With boundless energy, it pounces and leaps, chasing the elusive beam across the floor and up the walls. The simple joy of the hunt unfolds in clear, uncomplicated visuals.
     */
    prompt: string;
}

export interface KlingVideoV25TurboStandardImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/kling_v25_std_i2v_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface KlingVideoV25TurboProTextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video frame
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Cfg Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt.
     * @default 0.5
     */
    cfg_scale?: number;
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example A noble lord walks among his people, his presence a comforting reassurance. He greets them with a gentle smile, embodying their hopes and earning their respect through simple interactions. The atmosphere is intimate and sincere, highlighting the bond between the leader and community.
     */
    prompt: string;
}

export interface KlingVideoV25TurboProTextToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/kling/kling-v2.5-turbo-pro-text-to-video-output.mp4"
     *     }
     */
    video: Components.File;
}

export interface KlingVideoV25TurboProImageToVideoInput {
    /**
     * Cfg Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt.
     * @default 0.5
     */
    cfg_scale?: number;
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Image Url
     * @description URL of the image to be used for the video
     * @example https://v3.fal.media/files/panda/HnY2yf-BbzlrVQxR-qP6m_9912d0932988453aadf3912fc1901f52.jpg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example A stark starting line divides two powerful cars, engines revving for the challenge ahead. They surge forward in the heat of competition, a blur of speed and chrome. The finish line looms as they vie for victory.
     */
    prompt: string;
    /**
     * Tail Image Url
     * @description URL of the image to be used for the end of the video
     */
    tail_image_url?: string;
}

export interface KlingVideoV25TurboProImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/kling/kling-v2.5-turbo-pro-image-to-video-output.mp4"
     *     }
     */
    video: Components.File;
}

export interface KlingVideoV21StandardImageToVideoInput {
    /**
     * Cfg Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt.
     * @default 0.5
     */
    cfg_scale?: number;
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Image Url
     * @description URL of the image to be used for the video
     * @example https://storage.googleapis.com/falserverless/model_tests/kling/kling-image-to-video.jpg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example As the sun dips below the horizon, painting the sky in fiery hues of orange and purple, powerful waves relentlessly crash against jagged, dark rocks, their white foam a stark contrast to the deepening twilight; the textured surface of the rocks, wet and glistening, reflects the vibrant colors, creating a mesmerizing spectacle of nature's raw power and breathtaking beauty
     */
    prompt: string;
}

export interface KlingVideoV21StandardImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://v3.fal.media/files/koala/17e3xh08J4_PkHS_0cbwF_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface KlingVideoV21ProImageToVideoInput {
    /**
     * Cfg Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt.
     * @default 0.5
     */
    cfg_scale?: number;
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Image Url
     * @description URL of the image to be used for the video
     * @example https://v3.fal.media/files/lion/_I_io6Gtk83c72d-afXf8_image.webp
     */
    image_url: string;
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example Warm, incandescent streetlights paint the rain-slicked cobblestones in pools of amber light as a couple walks hand-in-hand, their silhouettes stark against the blurry backdrop of a city shrouded in a gentle downpour; the camera lingers on the subtle textures of their rain-soaked coats and the glistening reflections dancing on the wet pavement, creating a sense of intimate vulnerability and shared quietude.
     */
    prompt: string;
    /**
     * Tail Image Url
     * @description URL of the image to be used for the end of the video
     */
    tail_image_url?: string;
}

export interface KlingVideoV21ProImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://v3.fal.media/files/rabbit/Y5I8-7u3e7ogVSvPin1TS_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface KlingVideoV21MasterTextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video frame
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Cfg Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt.
     * @default 0.5
     */
    cfg_scale?: number;
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example Warm, earthy tones bathe the scene as the potter's hands, rough and calloused, coax a shapeless lump of clay into a vessel of elegant curves, the slow, deliberate movements highlighted by the subtle shifting light; the clay's cool, damp texture contrasts sharply with the warmth of the potter's touch, creating a captivating interplay between material and maker.
     */
    prompt: string;
}

export interface KlingVideoV21MasterTextToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://v3.fal.media/files/lion/0wTlhR7GCXFI-_BZXGy99_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface KlingVideoV21MasterImageToVideoInput {
    /**
     * Cfg Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt.
     * @default 0.5
     */
    cfg_scale?: number;
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Image Url
     * @description URL of the image to be used for the video
     * @example https://v3.fal.media/files/zebra/9Nrm22YyLojSTPJbZYNhh_image.webp
     */
    image_url: string;
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example Sunlight dapples through budding branches, illuminating a vibrant tapestry of greens and browns as a pair of robins meticulously weave twigs and mud into a cradle of life, their tiny forms a whirlwind of activity against a backdrop of blossoming spring.  The scene unfolds with a gentle, observational pace, allowing the viewer to fully appreciate the intricate details of nest construction, the soft textures of downy feathers contrasted against the rough bark of the branches, the delicate balance of strength and fragility in their creation.
     */
    prompt: string;
}

export interface KlingVideoV21MasterImageToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://v3.fal.media/files/rabbit/YuUWKFq508zzWIiQ0i2vt_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface KlingVideoV1TtsInput {
    /**
     * Text
     * @description The text to be converted to speech
     * @example Hello world! Kling TTS is available on FAL!
     */
    text: string;
    /**
     * Voice Id
     * @description The voice ID to use for speech synthesis
     * @default genshin_vindi2
     * @enum {string}
     */
    voice_id?:
        | 'genshin_vindi2'
        | 'zhinen_xuesheng'
        | 'AOT'
        | 'ai_shatang'
        | 'genshin_klee2'
        | 'genshin_kirara'
        | 'ai_kaiya'
        | 'oversea_male1'
        | 'ai_chenjiahao_712'
        | 'girlfriend_4_speech02'
        | 'chat1_female_new-3'
        | 'chat_0407_5-1'
        | 'cartoon-boy-07'
        | 'uk_boy1'
        | 'cartoon-girl-01'
        | 'PeppaPig_platform'
        | 'ai_huangzhong_712'
        | 'ai_huangyaoshi_712'
        | 'ai_laoguowang_712'
        | 'chengshu_jiejie'
        | 'you_pingjing'
        | 'calm_story1'
        | 'uk_man2'
        | 'laopopo_speech02'
        | 'heainainai_speech02'
        | 'reader_en_m-v1'
        | 'commercial_lady_en_f-v1'
        | 'tiyuxi_xuedi'
        | 'tiexin_nanyou'
        | 'girlfriend_1_speech02'
        | 'girlfriend_2_speech02'
        | 'zhuxi_speech02'
        | 'uk_oldman3'
        | 'dongbeilaotie_speech02'
        | 'chongqingxiaohuo_speech02'
        | 'chuanmeizi_speech02'
        | 'chaoshandashu_speech02'
        | 'ai_taiwan_man2_speech02'
        | 'xianzhanggui_speech02'
        | 'tianjinjiejie_speech02'
        | 'diyinnansang_DB_CN_M_04-v2'
        | 'yizhipiannan-v1'
        | 'guanxiaofang-v2'
        | 'tianmeixuemei-v1'
        | 'daopianyansang-v1'
        | 'mengwa-v1';
    /**
     * Voice Speed
     * @description Rate of speech
     * @default 1
     */
    voice_speed?: number;
}

export interface KlingVideoV1TtsOutput {
    /**
     * Audio
     * @description The generated audio
     * @example {
     *       "url": "https://v3.fal.media/files/monkey/O-ekVTtYqeDblD1oSf2uv_output.mp3"
     *     }
     */
    audio: Components.File;
}

export interface KlingVideoV1StandardTextToVideoInput {
    /**
     * Advanced Camera Control
     * @description Advanced Camera control parameters
     */
    advanced_camera_control?: Components.CameraControl;
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video frame
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Camera Control
     * @description Camera control parameters
     * @enum {string}
     */
    camera_control?: 'down_back' | 'forward_up' | 'right_turn_forward' | 'left_turn_forward';
    /**
     * Cfg Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt.
     * @default 0.5
     */
    cfg_scale?: number;
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse.
     */
    prompt: string;
}

export interface KlingVideoV1StandardTextToVideoOutput extends SharedType_6b8 {}

export interface KlingVideoV1StandardImageToVideoInput {
    /**
     * Cfg Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt.
     * @default 0.5
     */
    cfg_scale?: number;
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Dynamic Masks
     * @description List of dynamic masks
     */
    dynamic_masks?: Components.DynamicMask[];
    /**
     * Image Url
     * @description URL of the image to be used for the video
     * @example https://storage.googleapis.com/falserverless/kling/kling_input.jpeg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The prompt for the video
     * @example Snowflakes fall as a car moves forward along the road.
     */
    prompt: string;
    /**
     * Static Mask Url
     * @description URL of the image for Static Brush Application Area (Mask image created by users using the motion brush)
     * @example https://storage.googleapis.com/falserverless/kling/new_static_mask.png
     */
    static_mask_url?: string;
    /**
     * Tail Image Url
     * @description URL of the image to be used for the end of the video
     */
    tail_image_url?: string;
}

export interface KlingVideoV1StandardImageToVideoOutput extends SharedType_b2d {}

export interface KlingVideoV1StandardEffectsInput extends SharedType_9db {}

export interface KlingVideoV1StandardEffectsOutput extends SharedType_95d {}

export interface KlingVideoV1StandardAiAvatarInput extends SharedType_c85 {}

export interface KlingVideoV1StandardAiAvatarOutput extends SharedType_a1c {}

export interface KlingVideoV1ProAiAvatarInput extends SharedType_c85 {}

export interface KlingVideoV1ProAiAvatarOutput extends SharedType_a1c {}

export interface KlingVideoV16StandardTextToVideoInput extends SharedType_a8a {}

export interface KlingVideoV16StandardTextToVideoOutput extends SharedType_6b8 {}

export interface KlingVideoV16StandardImageToVideoInput {
    /**
     * Cfg Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt.
     * @default 0.5
     */
    cfg_scale?: number;
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Image Url
     * @example https://storage.googleapis.com/falserverless/kling/kling_input.jpeg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example Snowflakes fall as a car moves forward along the road.
     */
    prompt: string;
}

export interface KlingVideoV16StandardImageToVideoOutput extends SharedType_b2d {}

export interface KlingVideoV16StandardElementsInput extends SharedType_6bb {}

export interface KlingVideoV16StandardElementsOutput extends SharedType_d98 {}

export interface KlingVideoV16StandardEffectsInput extends SharedType_9db {}

export interface KlingVideoV16StandardEffectsOutput extends SharedType_95d {}

export interface KlingVideoV16ProTextToVideoInput extends SharedType_a8a {}

export interface KlingVideoV16ProTextToVideoOutput extends SharedType_6b8 {}

export interface KlingVideoV16ProImageToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video frame
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Cfg Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt.
     * @default 0.5
     */
    cfg_scale?: number;
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Image Url
     * @example https://storage.googleapis.com/falserverless/kling/kling_input.jpeg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example Snowflakes fall as a car moves along the road.
     */
    prompt: string;
    /**
     * Tail Image Url
     * @description URL of the image to be used for the end of the video
     */
    tail_image_url?: string;
}

export interface KlingVideoV16ProImageToVideoOutput extends SharedType_b2d {}

export interface KlingVideoV16ProElementsInput extends SharedType_6bb {}

export interface KlingVideoV16ProElementsOutput extends SharedType_d98 {}

export interface KlingVideoV16ProEffectsInput extends SharedType_9db {}

export interface KlingVideoV16ProEffectsOutput extends SharedType_95d {}

export interface KlingVideoV15ProTextToVideoInput extends SharedType_a8a {}

export interface KlingVideoV15ProTextToVideoOutput extends SharedType_6b8 {}

export interface KlingVideoV15ProImageToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video frame
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Cfg Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt.
     * @default 0.5
     */
    cfg_scale?: number;
    /**
     * Duration
     * @description The duration of the generated video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '5' | '10';
    /**
     * Dynamic Masks
     * @description List of dynamic masks
     */
    dynamic_masks?: Components.DynamicMask[];
    /**
     * Image Url
     * @example https://storage.googleapis.com/falserverless/kling/kling_input.jpeg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @default blur, distort, and low quality
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @example Snowflakes fall as a car moves along the road.
     */
    prompt: string;
    /**
     * Static Mask Url
     * @description URL of the image for Static Brush Application Area (Mask image created by users using the motion brush)
     * @example https://storage.googleapis.com/falserverless/kling/new_static_mask.png
     */
    static_mask_url?: string;
    /**
     * Tail Image Url
     * @description URL of the image to be used for the end of the video
     */
    tail_image_url?: string;
}

export interface KlingVideoV15ProImageToVideoOutput extends SharedType_b2d {}

export interface KlingVideoV15ProEffectsInput extends SharedType_9db {}

export interface KlingVideoV15ProEffectsOutput extends SharedType_95d {}

export interface KlingVideoO1VideoToVideoReferenceInput extends SharedType_cc5 {}

export interface KlingVideoO1VideoToVideoReferenceOutput extends SharedType_815 {}

export interface KlingVideoO1VideoToVideoEditInput extends SharedType_bda {}

export interface KlingVideoO1VideoToVideoEditOutput extends SharedType_151 {}

export interface KlingVideoO1StandardVideoToVideoReferenceInput extends SharedType_cc5 {}

export interface KlingVideoO1StandardVideoToVideoReferenceOutput extends SharedType_815 {}

export interface KlingVideoO1StandardVideoToVideoEditInput extends SharedType_bda {}

export interface KlingVideoO1StandardVideoToVideoEditOutput extends SharedType_151 {}

export interface KlingVideoO1StandardReferenceToVideoInput extends SharedType_ca4 {}

export interface KlingVideoO1StandardReferenceToVideoOutput extends SharedType_b37 {}

export interface KlingVideoO1StandardImageToVideoInput extends SharedType_02c {}

export interface KlingVideoO1StandardImageToVideoOutput extends SharedType_d0e {}

export interface KlingVideoO1ReferenceToVideoInput extends SharedType_ca4 {}

export interface KlingVideoO1ReferenceToVideoOutput extends SharedType_b37 {}

export interface KlingVideoO1ImageToVideoInput extends SharedType_02c {}

export interface KlingVideoO1ImageToVideoOutput extends SharedType_d0e {}

export interface KlingVideoLipsyncTextToVideoInput {
    /**
     * Text
     * @description Text content for lip-sync video generation. Max 120 characters.
     * @example Mental health is as important as physical health, shaping our emotions, thoughts, and daily interactions.
     */
    text: string;
    /**
     * Video Url
     * @description The URL of the video to generate the lip sync for. Supports .mp4/.mov, ≤100MB, 2-60s, 720p/1080p only, width/height 720–1920px. If validation fails, an error is returned.
     * @example https://fal.media/files/koala/8teUPbRRMtAUTORDvqy0l.mp4
     */
    video_url: string;
    /**
     * Voice Id
     * @description Voice ID to use for speech synthesis
     * @example genshin_klee2
     * @enum {string}
     */
    voice_id:
        | 'genshin_vindi2'
        | 'zhinen_xuesheng'
        | 'AOT'
        | 'ai_shatang'
        | 'genshin_klee2'
        | 'genshin_kirara'
        | 'ai_kaiya'
        | 'oversea_male1'
        | 'ai_chenjiahao_712'
        | 'girlfriend_4_speech02'
        | 'chat1_female_new-3'
        | 'chat_0407_5-1'
        | 'cartoon-boy-07'
        | 'uk_boy1'
        | 'cartoon-girl-01'
        | 'PeppaPig_platform'
        | 'ai_huangzhong_712'
        | 'ai_huangyaoshi_712'
        | 'ai_laoguowang_712'
        | 'chengshu_jiejie'
        | 'you_pingjing'
        | 'calm_story1'
        | 'uk_man2'
        | 'laopopo_speech02'
        | 'heainainai_speech02'
        | 'reader_en_m-v1'
        | 'commercial_lady_en_f-v1'
        | 'tiyuxi_xuedi'
        | 'tiexin_nanyou'
        | 'girlfriend_1_speech02'
        | 'girlfriend_2_speech02'
        | 'zhuxi_speech02'
        | 'uk_oldman3'
        | 'dongbeilaotie_speech02'
        | 'chongqingxiaohuo_speech02'
        | 'chuanmeizi_speech02'
        | 'chaoshandashu_speech02'
        | 'ai_taiwan_man2_speech02'
        | 'xianzhanggui_speech02'
        | 'tianjinjiejie_speech02'
        | 'diyinnansang_DB_CN_M_04-v2'
        | 'yizhipiannan-v1'
        | 'guanxiaofang-v2'
        | 'tianmeixuemei-v1'
        | 'daopianyansang-v1'
        | 'mengwa-v1';
    /**
     * Voice Language
     * @description The voice language corresponding to the Voice ID
     * @default en
     * @enum {string}
     */
    voice_language?: 'zh' | 'en';
    /**
     * Voice Speed
     * @description Speech rate for Text to Video generation
     * @default 1
     */
    voice_speed?: number;
}

export interface KlingVideoLipsyncTextToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/kling/kling_text_lipsync.mp4"
     *     }
     */
    video: Components.File;
}

export interface KlingVideoLipsyncAudioToVideoInput {
    /**
     * Audio Url
     * @description The URL of the audio to generate the lip sync for. Minimum duration is 2s and maximum duration is 60s. Maximum file size is 5MB.
     * @example https://storage.googleapis.com/falserverless/kling/kling-audio.mp3
     */
    audio_url: string;
    /**
     * Video Url
     * @description The URL of the video to generate the lip sync for. Supports .mp4/.mov, ≤100MB, 2–10s, 720p/1080p only, width/height 720–1920px.
     * @example https://fal.media/files/koala/8teUPbRRMtAUTORDvqy0l.mp4
     */
    video_url: string;
}

export interface KlingVideoLipsyncAudioToVideoOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/kling/kling_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface KlingVideoCreateVoiceInput {
    /**
     * Voice Url
     * @description URL of the voice audio file. Supports .mp3/.wav audio or .mp4/.mov video. Duration must be 5-30 seconds with clean, single-voice audio.
     * @example https://v3b.fal.media/files/b/0a867736/_Wo19V-XrOVYZt6jKE8t5_kling_video.wav
     */
    voice_url: string;
}

export interface KlingVideoCreateVoiceOutput {
    /**
     * Voice Id
     * @description Unique identifier for the created voice
     * @example 829877809978941442
     */
    voice_id: string;
}

export interface KlingVideoAiAvatarV2StandardInput extends SharedType_c85 {}

export interface KlingVideoAiAvatarV2StandardOutput extends SharedType_a1c {}

export interface KlingVideoAiAvatarV2ProInput extends SharedType_c85 {}

export interface KlingVideoAiAvatarV2ProOutput extends SharedType_a1c {}

export interface KlingImageO1Input {
    /**
     * Aspect Ratio
     * @description Aspect ratio of generated images. 'auto' intelligently determines based on input content.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1' | '4:3' | '3:4' | '3:2' | '2:3' | '21:9';
    /**
     * Elements
     * @description Elements (characters/objects) to include in the image. Reference in prompt as @Element1, @Element2, etc. Maximum 10 total (elements + reference images).
     * @example [
     *       {
     *         "reference_image_urls": [
     *           "https://storage.googleapis.com/falserverless/example_inputs/kling-image-o1/element-1-reference.png"
     *         ],
     *         "frontal_image_url": "https://storage.googleapis.com/falserverless/example_inputs/kling-image-o1/element-1-front.png"
     *       }
     *     ]
     */
    elements?: Components.ElementInput[];
    /**
     * Image Urls
     * @description List of reference images. Reference images in prompt using @Image1, @Image2, etc. (1-indexed). Max 10 images.
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_inputs/kling-image-o1/input.png",
     *       "https://storage.googleapis.com/falserverless/example_inputs/kling-image-o1/input-2.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Num Images
     * @description Number of images to generate (1-9).
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description Text prompt for image generation. Reference images using @Image1, @Image2, etc. (or @Image if only one image). Max 2500 characters.
     * @example Put @Image1 to the back seat of the car in @Image2, put @Element1 on to the @Image1
     */
    prompt: string;
    /**
     * Resolution
     * @description Image generation resolution. 1K: standard, 2K: high-res.
     * @default 1K
     * @enum {string}
     */
    resolution?: '1K' | '2K';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface KlingImageO1Output {
    /**
     * Images
     * @description Generated images
     * @example [
     *       {
     *         "file_size": 1419818,
     *         "file_name": "d4eeaeaeae294a41b0321ba6c99f0f9d.png",
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/kling-image-o1/output.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface Kandinsky5TextToVideoDistillInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video. One of (3:2, 1:1, 2:3).
     * @default 3:2
     * @enum {string}
     */
    aspect_ratio?: '3:2' | '1:1' | '2:3';
    /**
     * Duration
     * @description The length of the video to generate (5s or 10s)
     * @default 5s
     * @example 5s
     * @example 10s
     * @enum {string}
     */
    duration?: '5s' | '10s';
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A dog in red hat
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video in W:H format. Will be calculated based on the aspect ratio(768x512, 512x512, 512x768).
     * @default 768x512
     * @enum {string}
     */
    resolution?: '768x512';
}

export interface Kandinsky5TextToVideoDistillOutput extends SharedType_7e9 {}

export interface Kandinsky5TextToVideoInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video. One of (3:2, 1:1, 2:3).
     * @default 3:2
     * @enum {string}
     */
    aspect_ratio?: '3:2' | '1:1' | '2:3';
    /**
     * Duration
     * @description The length of the video to generate (5s or 10s)
     * @default 5s
     * @example 5s
     * @example 10s
     * @enum {string}
     */
    duration?: '5s' | '10s';
    /**
     * Num Inference Steps
     * @description The number of inference steps.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A dog in red hat
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video in W:H format. Will be calculated based on the aspect ratio(768x512, 512x512, 512x768).
     * @default 768x512
     * @enum {string}
     */
    resolution?: '768x512';
}

export interface Kandinsky5TextToVideoOutput extends SharedType_7e9 {}

export interface Kandinsky5ProTextToVideoInput {
    /**
     * Acceleration
     * @description Acceleration level for faster generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video. One of (3:2, 1:1, 2:3).
     * @default 3:2
     * @enum {string}
     */
    aspect_ratio?: '3:2' | '1:1' | '2:3';
    /**
     * Duration
     * @description The length of the video to generate (5s or 10s)
     * @default 5s
     * @example 5s
     * @enum {string}
     */
    duration?: '5s';
    /**
     * Num Inference Steps
     * @description The number of inference steps.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A medium shot establishes a modern, minimalist office setting: clean lines, muted grey walls, and polished wood surfaces. The focus shifts to a close-up on a woman in sharp, navy blue business attire. Her crisp white blouse contrasts with the deep blue of her tailored suit jacket. The subtle texture of the fabric is visible—a fine weave with a slight sheen. Her expression is serious, yet engaging, as she speaks to someone unseen just beyond the frame. Close-up on her eyes, showing the intensity of her gaze and the fine lines around them that hint at experience and focus. Her lips are slightly parted, as if mid-sentence. The light catches the subtle highlights in her auburn hair, meticulously styled. Note the slight catch of light on the silver band of her watch. High resolution 4k
     */
    prompt: string;
    /**
     * Resolution
     * @description Video resolution: 512p or 1024p.
     * @default 512P
     * @enum {string}
     */
    resolution?: '512P' | '1024P';
}

export interface Kandinsky5ProTextToVideoOutput {
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "file_size": 14530500,
     *       "file_name": "output.mp4",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3b.fal.media/files/b/0a87754e/o5FWdz83KTXzq0FB7aG5Q_output.mp4"
     *     }
     */
    video?: Components.File;
}

export interface Kandinsky5ProImageToVideoInput {
    /**
     * Acceleration
     * @description Acceleration level for faster generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Duration
     * @description Video duration.
     * @default 5s
     * @enum {string}
     */
    duration?: '5s';
    /**
     * Image Url
     * @description The URL of the image to use as a reference for the video generation.
     * @example https://storage.googleapis.com/falserverless/model_tests/wan/dragon-warrior.jpg
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example The white dragon warrior stands still, eyes full of determination and strength. The camera slowly moves closer or circles around the warrior, highlighting the powerful presence and heroic spirit of the character.
     */
    prompt: string;
    /**
     * Resolution
     * @description Video resolution: 512p or 1024p.
     * @default 512P
     * @enum {string}
     */
    resolution?: '512P' | '1024P';
}

export interface Kandinsky5ProImageToVideoOutput {
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "file_size": 22253751,
     *       "file_name": "output.mp4",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3b.fal.media/files/b/0a877276/Bg24FK_awlNAYKn962Vm0_output.mp4"
     *     }
     */
    video?: Components.File;
}

export interface JanusInput {
    /**
     * Cfg Weight
     * @description Classifier Free Guidance scale - how closely to follow the prompt.
     * @default 5
     */
    cfg_weight?: number;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description Number of images to generate in parallel.
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example beautiful girl, inside a house
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducible generation.
     */
    seed?: number;
    /**
     * Temperature
     * @description Controls randomness in the generation. Higher values make output more random.
     * @default 1
     */
    temperature?: number;
}

export interface JanusOutput extends SharedType_a73 {}

export interface IpAdapterFaceIdInput {
    /**
     * Base 1 5 Model Repo
     * @description The URL to the base 1.5 model. Default is SG161222/Realistic_Vision_V4.0_noVAE
     * @default SG161222/Realistic_Vision_V4.0_noVAE
     */
    base_1_5_model_repo?: string;
    /**
     * Base Sdxl Model Repo
     * @description The URL to the base SDXL model. Default is SG161222/RealVisXL_V3.0
     * @default SG161222/RealVisXL_V3.0
     */
    base_sdxl_model_repo?: string;
    /**
     * Face Id Det Size
     * @description The size of the face detection model. The higher the number the more accurate
     *                 the detection will be but it will also take longer to run. The higher the number the more
     *                 likely it will fail to find a face as well. Lower it if you are having trouble
     *                 finding a face in the image.
     * @default 640
     */
    face_id_det_size?: number;
    /**
     * Face Image Url
     * @description An image of a face to match. If an image with a size of 640x640 is not provided, it will be scaled and cropped to that size.
     * @example https://storage.googleapis.com/falserverless/model_tests/upscale/image%20(8).png
     */
    face_image_url?: string;
    /**
     * Face Images Data Url
     * @description URL to zip archive with images of faces. The images embedding will be averaged to
     *                 create a more accurate face id.
     */
    face_images_data_url?: string;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Height
     * @description The height of the generated image.
     * @default 512
     */
    height?: number;
    /**
     * Model Type
     * @description The model type to use. 1_5 is the default and is recommended for most use cases.
     * @default 1_5-v1
     * @example 1_5-v1
     * @example 1_5-v1-plus
     * @example 1_5-v2-plus
     * @example SDXL-v1
     * @example SDXL-v2-plus
     * @example 1_5-auraface-v1
     * @enum {string}
     */
    model_type?:
        | '1_5-v1'
        | '1_5-v1-plus'
        | '1_5-v2-plus'
        | 'SDXL-v1'
        | 'SDXL-v2-plus'
        | '1_5-auraface-v1';
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default blurry, low resolution, bad, ugly, low quality, pixelated, interpolated, compression artifacts, noisey, grainy
     * @example blurry, low resolution, bad, ugly, low quality, pixelated, interpolated, compression artifacts, noisey, grainy
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps to use for generating the image. The more steps
     *                 the better the image will be but it will also take longer to generate.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Num Samples
     * @description The number of samples for face id. The more samples the better the image will
     *                 be but it will also take longer to generate. Default is 4.
     * @default 4
     */
    num_samples?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example Man cyberpunk, synthwave night city, futuristic, high quality, highly detailed, high resolution, sharp, hyper realistic, extremely detailed
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     * @example 42
     */
    seed?: number;
    /**
     * Width
     * @description The width of the generated image.
     * @default 512
     */
    width?: number;
}

export interface IpAdapterFaceIdOutput extends SharedType_0c0 {}

export interface InvisibleWatermarkInput {
    /**
     * Decode
     * @description Whether to decode a watermark from the image instead of encoding
     * @default false
     */
    decode?: boolean;
    /**
     * Image Url
     * @description URL of image to be watermarked or decoded
     * @example https://storage.googleapis.com/falserverless/web-examples/watermark/watermark_ex.png
     */
    image_url: string;
    /**
     * Length
     * @description Length of watermark bits to decode (required when decode=True)
     * @default 0
     */
    length?: number;
    /**
     * Watermark
     * @description Text to use as watermark (for encoding only)
     * @default watermark
     * @example watermark
     */
    watermark?: string;
}

export interface InvisibleWatermarkOutput {
    /**
     * Extracted Watermark
     * @description The extracted watermark text (when decoding)
     */
    extracted_watermark?: string;
    /**
     * Image
     * @description The watermarked image file info (when encoding)
     */
    image?: Components.Image;
    /**
     * Length
     * @description Length of the watermark bits used (helpful for future decoding)
     * @default 0
     */
    length?: number;
}

export interface InstantCharacterInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description The image URL to generate an image from. Needs to match the dimensions of the mask.
     * @example https://storage.googleapis.com/falserverless/example_inputs/girl.jpg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A girl is playing a guitar in street
     */
    prompt: string;
    /**
     * Scale
     * @description The scale of the subject image. Higher values will make the subject image more prominent in the generated image.
     * @default 1
     */
    scale?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface InstantCharacterOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "height": 1024,
     *         "content_type": "image/jpeg",
     *         "url": "https://v3.fal.media/files/penguin/dG4xIRLMkTRKxA-T7h57l.jpeg",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface InpaintInput {
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description Input image for img2img or inpaint mode
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png
     */
    image_url: string;
    /**
     * Mask Url
     * @description Input mask for inpaint mode. Black areas will be preserved, white areas will be inpainted.
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png
     */
    mask_url: string;
    /**
     * Model Name
     * @description URL or HuggingFace ID of the base model to generate the image.
     * @example diffusers/stable-diffusion-xl-1.0-inpainting-0.1
     * @example stabilityai/stable-diffusion-xl-base-1.0
     * @example runwayml/stable-diffusion-v1-5
     * @example SG161222/Realistic_Vision_V2.0
     */
    model_name: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, painting, illustration, (worst quality, low quality, normal quality:2)
     * @example nsfw, cartoon, (epicnegative:0.9)
     */
    negative_prompt?: string;
    /**
     * Number of inference steps
     * @description Increasing the amount of steps tells Stable Diffusion that it should take more steps
     *                 to generate your final result which can increase the amount of detail in your image.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example a photo of a cat
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     * @example 1234
     */
    seed?: number;
}

export interface InpaintOutput {
    /**
     * Image
     * @description The generated image files info.
     */
    image: Components.Image;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
}

export interface InfinityStarTextToVideoInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated output
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '1:1' | '9:16';
    /**
     * Enhance Prompt
     * @description Whether to use an LLM to enhance the prompt.
     * @default true
     */
    enhance_prompt?: boolean;
    /**
     * Guidance Scale
     * @description Guidance scale for generation
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Negative Prompt
     * @description Negative prompt to guide what to avoid in generation
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description Text prompt for generating the video
     * @example A serene mountain landscape at sunset with flowing clouds
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Leave empty for random generation.
     */
    seed?: number;
    /**
     * Tau Video
     * @description Tau value for video scale
     * @default 0.4
     */
    tau_video?: number;
    /**
     * Use Apg
     * @description Whether to use APG
     * @default true
     */
    use_apg?: boolean;
}

export interface InfinityStarTextToVideoOutput {
    /**
     * Video
     * @description Generated video file
     */
    video: Components.File;
}

export interface InfinitalkVideoToVideoInput {
    /**
     * Acceleration
     * @description The acceleration level to use for generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Audio URL
     * @description The URL of the audio file.
     * @example https://v3.fal.media/files/penguin/PtiCYda53E9Dav25QmQYI_output.mp3
     */
    audio_url: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
     * @default 145
     */
    num_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A woman with colorful hair talking on a podcast.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the video to generate. Must be either 480p or 720p.
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     * @default 42
     */
    seed?: number;
    /**
     * Video Url
     * @description URL of the input video.
     * @example https://storage.googleapis.com/falserverless/model_tests/video_models/ref_video.mp4
     */
    video_url: string;
}

export interface InfinitalkVideoToVideoOutput {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/video_models/mk7Ar5IvTtyNjWLRMb-re_dbe605004b664258b38528615afd7e0f.mp4"
     *     }
     */
    video: Components.File;
}

export interface InfinitalkSingleTextInput {
    /**
     * Acceleration
     * @description The acceleration level to use for generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Image URL
     * @description URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://v3.fal.media/files/panda/HuM21CXMf0q7OO2zbvwhV_c4533aada79a495b90e50e32dc9b83a8.png
     */
    image_url: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 41 to 721.
     * @default 145
     */
    num_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example An elderly man with a white beard and headphones records audio with a microphone. He appears engaged and expressive, suggesting a podcast or voiceover.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the video to generate. Must be either 480p or 720p.
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     * @default 42
     */
    seed?: number;
    /**
     * Text Input
     * @description The text input to guide video generation.
     * @example Spend more time with people who make you feel alive, and less with things that drain your soul.
     */
    text_input: string;
    /**
     * Voice
     * @description The voice to use for speech generation
     * @example Bill
     * @enum {string}
     */
    voice:
        | 'Aria'
        | 'Roger'
        | 'Sarah'
        | 'Laura'
        | 'Charlie'
        | 'George'
        | 'Callum'
        | 'River'
        | 'Liam'
        | 'Charlotte'
        | 'Alice'
        | 'Matilda'
        | 'Will'
        | 'Jessica'
        | 'Eric'
        | 'Chris'
        | 'Brian'
        | 'Daniel'
        | 'Lily'
        | 'Bill';
}

export interface InfinitalkSingleTextOutput extends SharedType_266 {}

export interface InfinitalkInput {
    /**
     * Acceleration
     * @description The acceleration level to use for generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Audio URL
     * @description The URL of the audio file.
     * @example https://v3.fal.media/files/penguin/PtiCYda53E9Dav25QmQYI_output.mp3
     */
    audio_url: string;
    /**
     * Image URL
     * @description URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://v3.fal.media/files/koala/gmpc0QevDF9bBsL1EAYVF_1c637094161147559f0910a68275dc34.png
     */
    image_url: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 41 to 721.
     * @default 145
     */
    num_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A woman with colorful hair talking on a podcast.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the video to generate. Must be either 480p or 720p.
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     * @default 42
     */
    seed?: number;
}

export interface InfinitalkOutput extends SharedType_b29 {}

export interface IndexTts2TextToSpeechInput {
    /**
     * Audio Url
     * @description The audio file to generate the speech from.
     * @example https://storage.googleapis.com/falserverless/example_inputs/index-tts-2/tts_in.mp3
     */
    audio_url: string;
    /**
     * Emotion Prompt
     * @description The emotional prompt to influence the emotional style. Must be used together with should_use_prompt_for_emotion.
     * @example You scared me to death! What are you, a ghost?
     */
    emotion_prompt?: string;
    /**
     * Emotional Audio Url
     * @description The emotional reference audio file to extract the style from.
     */
    emotional_audio_url?: string;
    /**
     * Emotional Strengths
     * @description The strengths of individual emotions for fine-grained control.
     * @example null
     */
    emotional_strengths?: Components.EmotionalStrengths;
    /**
     * Prompt
     * @description The speech prompt to generate
     * @example Hide! He's coming! He's coming to get us!
     */
    prompt: string;
    /**
     * Should Use Prompt For Emotion
     * @description Whether to use the `prompt` to calculate emotional strengths, if enabled it will overwrite the `emotional_strengths` values. If `emotion_prompt` is provided, it will be used to instead of `prompt` to extract the emotional style.
     * @default false
     * @example true
     */
    should_use_prompt_for_emotion?: boolean;
    /**
     * Strength
     * @description The strength of the emotional style transfer. Higher values result in stronger emotional influence.
     * @default 1
     */
    strength?: number;
}

export interface IndexTts2TextToSpeechOutput {
    /**
     * Audio
     * @description The generated audio file in base64 format.
     * @example https://storage.googleapis.com/falserverless/example_outputs/index-tts-2/tts_out.mp3
     */
    audio: Components.File;
}

export interface ImageutilsRembgInput {
    /**
     * Crop To Bbox
     * @description If set to true, the resulting image be cropped to a bounding box around the subject
     * @default false
     */
    crop_to_bbox?: boolean;
    /**
     * Image Url
     * @description Input image url.
     * @example https://storage.googleapis.com/falserverless/model_tests/remove_background/elephant.jpg
     */
    image_url: string;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageutilsRembgOutput {
    /**
     * Image
     * @description Background removed image.
     */
    image: Components.Image;
}

export interface ImageutilsNsfwInput {
    /**
     * Image Url
     * @description Input image url.
     * @example https://storage.googleapis.com/falserverless/model_tests/remove_background/elephant.jpg
     */
    image_url: string;
}

export interface ImageutilsNsfwOutput {
    /**
     * Nsfw Probability
     * @description The probability of the image being NSFW.
     */
    nsfw_probability: number;
}

export interface ImageutilsMarigoldDepthInput {
    /**
     * Ensemble Size
     * @description Number of predictions to average over. Defaults to `10`. The higher the number, the more accurate the result, but the slower the inference.
     * @default 10
     * @example 10
     */
    ensemble_size?: number;
    /**
     * Image Url
     * @description Input image url.
     * @example https://storage.googleapis.com/falserverless/model_tests/remove_background/elephant.jpg
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @description Number of denoising steps. Defaults to `10`. The higher the number, the more accurate the result, but the slower the inference.
     * @default 10
     * @example 10
     */
    num_inference_steps?: number;
    /**
     * Processing Res
     * @description Maximum processing resolution. Defaults `0` which means it uses the size of the input image.
     * @default 0
     * @example 0
     */
    processing_res?: number;
}

export interface ImageutilsMarigoldDepthOutput extends SharedType_7fd {}

export interface ImageutilsDepthInput {
    /**
     * A
     * @description a
     * @default 6.283185307179586
     */
    a?: number;
    /**
     * Bg Th
     * @description bg_th
     * @default 0.1
     */
    bg_th?: number;
    /**
     * Depth And Normal
     * @description depth_and_normal
     * @default false
     */
    depth_and_normal?: boolean;
    /**
     * Image Url
     * @description Input image url.
     * @example https://storage.googleapis.com/falserverless/model_tests/remove_background/elephant.jpg
     */
    image_url: string;
}

export interface ImageutilsDepthOutput extends SharedType_7fd {}

export interface Imagen4PreviewUltraInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?: '1:1' | '16:9' | '9:16' | '4:3' | '3:4';
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The text prompt to generate an image from.
     * @example This four-panel comic strip uses a charming, deliberately pixelated art style reminiscent of classic 8-bit video games, featuring simple shapes and a limited, bright color palette dominated by greens, blues, browns, and the dinosaur's iconic grey/black. The setting is a stylized pixel beach. Panel one shows the familiar Google Chrome T-Rex dinosaur, complete with its characteristic pixelated form, wearing tiny pixel sunglasses and lounging on a pixelated beach towel under a blocky yellow sun. Pixelated palm trees sway gently in the background against a blue pixel sky. A caption box with pixelated font reads, "Even error messages need a vacation." Panel two is a close-up of the T-Rex attempting to build a pixel sandcastle. It awkwardly pats a mound of brown pixels with its tiny pixel arms, looking focused. Small pixelated shells dot the sand around it. Panel three depicts the T-Rex joyfully hopping over a series of pixelated cacti planted near the beach, mimicking its game obstacle avoidance. Small "Boing! Boing!" sound effect text appears in a blocky font above each jump. A pixelated crab watches from the side, waving its pixel claw. The final panel shows the T-Rex floating peacefully on its back in the blocky blue pixel water, sunglasses still on, with a contented expression. A small thought bubble above it contains pixelated "Zzz..." indicating relaxation.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated image.
     * @default 1K
     * @enum {string}
     */
    resolution?: '1K' | '2K';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Imagen4PreviewUltraOutput {
    /**
     * Description
     * @description The description of the generated images.
     */
    description: string;
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "file_name": "uzopSU5Me5PUENW-IXMXD_output.png",
     *         "content_type": "image/png",
     *         "url": "https://v3.fal.media/files/panda/uzopSU5Me5PUENW-IXMXD_output.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
}

export interface Imagen4PreviewFastInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?: '1:1' | '16:9' | '9:16' | '4:3' | '3:4';
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The text prompt to generate an image from.
     * @example Atmospheric narrative illustration depicting a young woman with dark hair styled with a single star clip, eating dumplings at a small round table in a bustling, late-night eatery reminiscent of a vintage Hong Kong diner. The style blends clean linework with textured color fields, evoking a sense of place and story. The mood is intimate contentment amidst vibrant surroundings. Soft, warm overhead lighting from unseen hanging lamps casts gentle highlights on her face and the porcelain plate of dumplings, creating soft-edged shadows on the tiled tabletop and floor. The background features detailed elements like wall menus with stylized illustrations, a retro wall clock, steam rising from a soup bowl, and glimpses of other patrons blurred slightly for depth. The woman, viewed from a slightly high angle, crouches slightly on her chair, intensely focused on her food, rendered with expressive linework defining her pose and features. The color palette mixes muted teal wall tiles and green chairs with pops of warm yellow in her top, pink trousers, red chili oil dish, and ambient light, creating a cozy yet lively feel. Subtle paper texture or digital grain is visible throughout. Focus is sharp on the character and her immediate table setting
     */
    prompt: string;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Imagen4PreviewFastOutput {
    /**
     * Description
     * @description The description of the generated images.
     */
    description: string;
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "file_name": "MNWc2sdR8v_VLWqVp7ag8_output.png",
     *         "content_type": "image/png",
     *         "url": "https://v3.fal.media/files/elephant/MNWc2sdR8v_VLWqVp7ag8_output.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
}

export interface Imagen4PreviewInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?: '1:1' | '16:9' | '9:16' | '4:3' | '3:4';
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The text prompt to generate an image from.
     * @example Capture an intimate close-up bathed in warm, soft, late-afternoon sunlight filtering into a quintessential 1960s kitchen. The focal point is a charmingly designed vintage package of all-purpose flour, resting invitingly on a speckled Formica countertop. The packaging itself evokes pure nostalgia: perhaps thick, slightly textured paper in a warm cream tone, adorned with simple, bold typography (a friendly serif or script) in classic red and blue "ALL-PURPOSE FLOUR", featuring a delightful illustration like a stylized sheaf of wheat or a cheerful baker character. In smaller bold print at the bottom of the package: "NET WT 5 LBS (80 OZ) 2.27kg". Focus sharply on the package details – the slightly soft edges of the paper bag, the texture of the vintage printing, the inviting "All-Purpose Flour" text. Subtle hints of the 1960s kitchen frame the shot – the chrome edge of the counter gleaming softly, a blurred glimpse of a pastel yellow ceramic tile backsplash, or the corner of a vintage metal canister set just out of focus. The shallow depth of field keeps attention locked on the beautifully designed package, creating an aesthetic rich in warmth, authenticity, and nostalgic appeal.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the generated image.
     * @default 1K
     * @enum {string}
     */
    resolution?: '1K' | '2K';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Imagen4PreviewOutput {
    /**
     * Description
     * @description The description of the generated images.
     */
    description: string;
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "file_name": "c0RfXzCisqX6YRkIF7apw_output.png",
     *         "content_type": "image/png",
     *         "url": "https://v3.fal.media/files/rabbit/rmgBxhwGYb2d3pl3x9sKf_output.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
}

export interface Imagen3FastInput extends SharedType_905 {}

export interface Imagen3FastOutput extends SharedType_8c5 {}

export interface Imagen3Input extends SharedType_905 {}

export interface Imagen3Output extends SharedType_8c5 {}

export interface Image2svgInput {
    /**
     * Color Precision
     * @description Color quantization level
     * @default 6
     */
    color_precision?: number;
    /**
     * Colormode
     * @description Choose between color or binary (black and white) output
     * @default color
     * @enum {string}
     */
    colormode?: 'color' | 'binary';
    /**
     * Corner Threshold
     * @description Corner detection threshold in degrees
     * @default 60
     */
    corner_threshold?: number;
    /**
     * Filter Speckle
     * @description Filter out small speckles and noise
     * @default 4
     */
    filter_speckle?: number;
    /**
     * Hierarchical
     * @description Hierarchical mode: stacked or cutout
     * @default stacked
     * @enum {string}
     */
    hierarchical?: 'stacked' | 'cutout';
    /**
     * Image Url
     * @description The image to convert to SVG
     * @example https://v3.fal.media/files/kangaroo/EfqY747bBKy1Ynrgbk5ba_04pwiD1LTsnMZuyEyw757_8f986248a89845d3ba90c23b14089f10.jpg
     */
    image_url: string;
    /**
     * Layer Difference
     * @description Layer difference threshold for hierarchical mode
     * @default 16
     */
    layer_difference?: number;
    /**
     * Length Threshold
     * @description Length threshold for curves/lines
     * @default 4
     */
    length_threshold?: number;
    /**
     * Max Iterations
     * @description Maximum number of iterations for optimization
     * @default 10
     */
    max_iterations?: number;
    /**
     * Mode
     * @description Mode: spline (curved) or polygon (straight lines)
     * @default spline
     * @enum {string}
     */
    mode?: 'spline' | 'polygon';
    /**
     * Path Precision
     * @description Decimal precision for path coordinates
     * @default 3
     */
    path_precision?: number;
    /**
     * Splice Threshold
     * @description Splice threshold for joining paths
     * @default 45
     */
    splice_threshold?: number;
}

export interface Image2svgOutput {
    /**
     * Images
     * @description The converted SVG file
     * @example [
     *       {
     *         "file_size": 1247850,
     *         "file_name": "output.svg",
     *         "content_type": "image/svg+xml",
     *         "url": "https://v3.fal.media/files/koala/B31yLkOEc6AVTzgeyr_9K_output.svg"
     *       }
     *     ]
     */
    images: Components.File[];
}

export interface Image2pixelInput {
    /**
     * Alpha Threshold
     * @description Alpha binarization threshold (0-255).
     * @default 128
     */
    alpha_threshold?: number;
    /**
     * Auto Color Detect
     * @description Enable automatic detection of optimal number of colors.
     * @default false
     */
    auto_color_detect?: boolean;
    /**
     * Background Mode
     * @description Controls where to flood-fill from when removing the background.
     * @default corners
     * @enum {string}
     */
    background_mode?: 'edges' | 'corners' | 'midpoints';
    /**
     * Background Tolerance
     * @description Background tolerance (0-255).
     * @default 0
     */
    background_tolerance?: number;
    /**
     * Cleanup Jaggy
     * @description Remove isolated diagonal pixels (jaggy edge cleanup).
     * @default false
     */
    cleanup_jaggy?: boolean;
    /**
     * Cleanup Morph
     * @description Apply morphological operations to remove noise.
     * @default false
     */
    cleanup_morph?: boolean;
    /**
     * Detect Method
     * @description Scale detection method to use.
     * @default auto
     * @enum {string}
     */
    detect_method?: 'auto' | 'runs' | 'edge';
    /**
     * Dominant Color Threshold
     * @description Dominant color threshold (0.0-1.0).
     * @default 0.05
     */
    dominant_color_threshold?: number;
    /**
     * Downscale Method
     * @description Downscaling method to produce the pixel-art output.
     * @default dominant
     * @enum {string}
     */
    downscale_method?: 'dominant' | 'median' | 'mode' | 'mean' | 'content-adaptive';
    /**
     * Fixed Palette
     * @description Optional fixed color palette as hex strings (e.g., ['#000000', '#ffffff']).
     */
    fixed_palette?: string[];
    /**
     * Image Url
     * @description The image URL to process into improved pixel art
     * @example https://storage.googleapis.com/falserverless/example_inputs/image2pixel-input.jpg
     */
    image_url: string;
    /**
     * Max Colors
     * @description Maximum number of colors in the output palette. Set None to disable limit.
     * @default 32
     */
    max_colors?: number;
    /**
     * Scale
     * @description Force a specific pixel scale. If None, auto-detect.
     */
    scale?: number;
    /**
     * Snap Grid
     * @description Align output to the pixel grid.
     * @default true
     */
    snap_grid?: boolean;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Transparent Background
     * @description Remove background of the image. This will check for contiguous color regions from the edges after correction and make them transparent.
     * @default false
     */
    transparent_background?: boolean;
    /**
     * Trim Borders
     * @description Trim borders of the image.
     * @default false
     */
    trim_borders?: boolean;
}

export interface Image2pixelOutput {
    /**
     * Images
     * @description The processed pixel-art image (PNG) and the scaled image (PNG).
     * @example [
     *       {
     *         "height": 1008,
     *         "file_name": "image2pixel-output.png",
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/image2pixel-output.png",
     *         "width": 1008
     *       },
     *       {
     *         "height": 48,
     *         "file_name": "image2pixel-output-scaled.png",
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/image2pixel-output-scaled.png",
     *         "width": 48
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Num Colors
     * @description The number of colors in the processed media.
     * @example 10
     */
    num_colors: number;
    /**
     * Palette
     * @description The palette of the processed media.
     * @example [
     *       "#000000",
     *       "#252524",
     *       "#282826",
     *       "#2b2b2a",
     *       "#323232",
     *       "#4c4c4c",
     *       "#d57322",
     *       "#dbdbdc",
     *       "#f6922b",
     *       "#fefefe"
     *     ]
     */
    palette: string[];
    /**
     * Pixel Scale
     * @description The detected pixel scale of the input.
     * @example 21
     */
    pixel_scale: number;
}

export interface ImagePreprocessorsZoeInput extends SharedType_91c {}

export interface ImagePreprocessorsZoeOutput extends SharedType_659 {}

export interface ImagePreprocessorsTeedInput extends SharedType_91c {}

export interface ImagePreprocessorsTeedOutput {
    /** @description Image with TeeD lines detected */
    image: Components.Image_2;
}

export interface ImagePreprocessorsScribbleInput {
    /**
     * Image Url
     * @description URL of the image to process
     * @example https://storage.googleapis.com/falserverless/model_tests/image_preprocessors/cat.png
     */
    image_url: string;
    /**
     * Model
     * @description The model to use for the Scribble detector
     * @default HED
     * @enum {string}
     */
    model?: 'HED' | 'PiDi';
    /**
     * Safe
     * @description Whether to use the safe version of the Scribble detector
     * @default false
     */
    safe?: boolean;
}

export interface ImagePreprocessorsScribbleOutput {
    /** @description Image with lines detected using the Scribble detector */
    image: Components.Image_2;
}

export interface ImagePreprocessorsSamInput extends SharedType_91c {}

export interface ImagePreprocessorsSamOutput {
    /** @description Image with SAM segmentation map */
    image: Components.Image_2;
}

export interface ImagePreprocessorsPidiInput {
    /**
     * Apply Filter
     * @description Whether to apply the filter to the image.
     * @default false
     */
    apply_filter?: boolean;
    /**
     * Image Url
     * @description URL of the image to process
     * @example https://storage.googleapis.com/falserverless/model_tests/image_preprocessors/cat.png
     */
    image_url: string;
    /**
     * Safe
     * @description Whether to use the safe version of the Pidi detector
     * @default false
     */
    safe?: boolean;
    /**
     * Scribble
     * @description Whether to use the scribble version of the Pidi detector
     * @default false
     */
    scribble?: boolean;
}

export interface ImagePreprocessorsPidiOutput {
    /** @description Image with Pidi lines detected */
    image: Components.Image_2;
}

export interface ImagePreprocessorsMlsdInput {
    /**
     * Distance Threshold
     * @description Distance threshold for the MLSD detector
     * @default 0.1
     */
    distance_threshold?: number;
    /**
     * Image Url
     * @description URL of the image to process
     * @example https://storage.googleapis.com/falserverless/model_tests/image_preprocessors/cat.png
     */
    image_url: string;
    /**
     * Score Threshold
     * @description Score threshold for the MLSD detector
     * @default 0.1
     */
    score_threshold?: number;
}

export interface ImagePreprocessorsMlsdOutput {
    /** @description Image with lines detected using the MLSD detector */
    image: Components.Image_2;
}

export interface ImagePreprocessorsMidasInput {
    /**
     * A
     * @description A parameter for the MiDaS detector
     * @default 6.283185307179586
     */
    a?: number;
    /**
     * Background Threshold
     * @description Background threshold for the MiDaS detector
     * @default 0.1
     */
    background_threshold?: number;
    /**
     * Image Url
     * @description URL of the image to process
     * @example https://storage.googleapis.com/falserverless/model_tests/image_preprocessors/cat.png
     */
    image_url: string;
}

export interface ImagePreprocessorsMidasOutput {
    /** @description Image with MiDaS depth map */
    depth_map: Components.Image_2;
    /** @description Image with MiDaS normal map */
    normal_map: Components.Image_2;
}

export interface ImagePreprocessorsLineartInput {
    /**
     * Coarse
     * @description Whether to use the coarse model
     * @default false
     */
    coarse?: boolean;
    /**
     * Image Url
     * @description URL of the image to process
     * @example https://storage.googleapis.com/falserverless/model_tests/image_preprocessors/cat.png
     */
    image_url: string;
}

export interface ImagePreprocessorsLineartOutput {
    /** @description Image with edges detected using the Canny algorithm */
    image: Components.Image_2;
}

export interface ImagePreprocessorsHedInput {
    /**
     * Image Url
     * @description URL of the image to process
     * @example https://storage.googleapis.com/falserverless/model_tests/image_preprocessors/cat.png
     */
    image_url: string;
    /**
     * Safe
     * @description Whether to use the safe version of the HED detector
     * @default false
     */
    safe?: boolean;
    /**
     * Scribble
     * @description Whether to use the scribble version of the HED detector
     * @default false
     */
    scribble?: boolean;
}

export interface ImagePreprocessorsHedOutput {
    /** @description Image with lines detected using the HED detector */
    image: Components.Image_2;
}

export interface ImagePreprocessorsDepthAnythingV2Input extends SharedType_91c {}

export interface ImagePreprocessorsDepthAnythingV2Output extends SharedType_659 {}

export interface ImageEditingYoutubeThumbnailsInput {
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description URL of the image to convert to YouTube thumbnail style.
     * @example https://v3.fal.media/files/koala/QUihQrMqowYu30UFC_Atk.png
     */
    image_url: string;
    /**
     * Lora Scale
     * @description The scale factor for the LoRA model. Controls the strength of the LoRA effect.
     * @default 0.5
     */
    lora_scale?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Thumbnail Text
     * @description The text to include in the YouTube thumbnail.
     * @default Generate youtube thumbnails
     * @example Generate youtube thumbnails using text 'EPIC FAIL.
     */
    prompt?: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingYoutubeThumbnailsOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/panda/oEr-wYCpGx0e15bAdykby.png"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingWojakStyleInput {
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description URL of the image to convert to wojak style.
     * @example https://v3.fal.media/files/rabbit/UGZQWl6lUdYeu91QzUZys_5ODmUQcqNeufTbf_hhO0h_unnamed%20(2).jpg
     */
    image_url: string;
    /**
     * Lora Scale
     * @description The scale factor for the LoRA model. Controls the strength of the LoRA effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingWojakStyleOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/koala/_fpVsU6fC6b77OGnPAxuw.png"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingWeatherEffectInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description Image prompt for the omni model.
     * @example https://v3.fal.media/files/zebra/hAjCkcyly4gsS9-cptD3Y_image%20(20).png
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Weather Effect
     * @description The weather effect to apply.
     * @default heavy snowfall
     * @example heavy snowfall
     */
    prompt?: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingWeatherEffectOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/panda/YhTFlPVSkuyPisQleLPQF_186310a04a8d4120a44b72f50f28ab5f.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingTimeOfDayInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description Image prompt for the omni model.
     * @example https://v3.fal.media/files/zebra/hAjCkcyly4gsS9-cptD3Y_image%20(20).png
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Time of Day
     * @description The time of day to transform the scene to.
     * @default golden hour
     * @example golden hour
     */
    prompt?: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingTimeOfDayOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/panda/Udg4UCRqE4Vbp4zd-4oZ8_5f17ff8367ee492e9279e02940d0f258.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingTextRemovalInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description URL of the image containing text to be removed.
     * @example https://v3.fal.media/files/rabbit/rmgBxhwGYb2d3pl3x9sKf_output.png
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingTextRemovalOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/tiger/W1s8Rq_cW2bT_wUcW9Bd8_413f5074e3d94829acfc4e08ec785040.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingStyleTransferInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description Image prompt for the omni model.
     * @example https://v3.fal.media/files/zebra/hAjCkcyly4gsS9-cptD3Y_image%20(20).png
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Style Prompt
     * @description The artistic style to apply.
     * @default Van Gogh's Starry Night
     * @example Van Gogh's Starry Night
     */
    prompt?: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingStyleTransferOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/kangaroo/tkhm3MPIE5FugnHcssKYZ_a983d62e76844c488877988742e170a9.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingSceneCompositionInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description Image prompt for the omni model.
     * @example https://v3.fal.media/files/zebra/hAjCkcyly4gsS9-cptD3Y_image%20(20).png
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Scene Description
     * @description Describe the scene where you want to place the subject.
     * @default enchanted forest
     * @example enchanted forest
     */
    prompt?: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingSceneCompositionOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/panda/VmiTRcFIPBN8TtV1FBuj1_6c3a793376664fa3a3e5ce5912038ab6.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingRetouchInput {
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description URL of the image to retouch.
     * @example https://storage.googleapis.com/falserverless/gallery/tulsi.png
     */
    image_url: string;
    /**
     * Lora Scale
     * @description The scale factor for the LoRA model. Controls the strength of the LoRA effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingRetouchOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/gallery/tulsi-retouched.png"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingReframeInput {
    /**
     * Aspect Ratio
     * @description The desired aspect ratio for the reframed image.
     * @default 16:9
     * @example 16:9
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description URL of the old or damaged photo to restore.
     * @example https://v3.fal.media/files/elephant/7ekErKT--mhgKJ5kgtvU__image.webp
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingReframeOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/elephant/V76OoObzc65eDtueHsXYI_a8879759964f4defaef62f44bacdaced.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingRealismInput {
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description URL of the image to enhance with realism details.
     * @example https://v3.fal.media/files/penguin/QpRcoPb4dDyDJJSpFm4CZ_img_55_start.jpg
     */
    image_url: string;
    /**
     * Lora Scale
     * @description The scale factor for the LoRA model. Controls the strength of the LoRA effect.
     * @default 0.6
     */
    lora_scale?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingRealismOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/tiger/wsyfYqHhrqB8CUYZ_71W0.png"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingProfessionalPhotoInput extends SharedType_868 {}

export interface ImageEditingProfessionalPhotoOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/kangaroo/ZldH8N4D1vxXWp_FHIMSK_24bc2c69b53e4f92a98cf02d798a3f00.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingPlushieStyleInput {
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description URL of the image to convert to plushie style.
     * @example https://v3.fal.media/files/elephant/5fVB7Y5ERU7zAvA_kd2i3_trump-portrait_square.jpeg
     */
    image_url: string;
    /**
     * Lora Scale
     * @description The scale factor for the LoRA model. Controls the strength of the LoRA effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingPlushieStyleOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/penguin/mktL882Np-nFxirKNib9m.png"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingPhotoRestorationInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description URL of the old or damaged photo to restore.
     * @example https://v3.fal.media/files/zebra/ProUb4C1PYWpyGe7BXd0n_d575ba7693584c0ddf733f77dcdb8963.jpg
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingPhotoRestorationOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/lion/5xRd_PfrtfoXe03-VlPZb_d684600d4ece4b778dfea35dd536bee9.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingObjectRemovalInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description Image prompt for the omni model.
     * @example https://v3.fal.media/files/zebra/hAjCkcyly4gsS9-cptD3Y_image%20(20).png
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Objects to Remove
     * @description Specify which objects to remove from the image.
     * @default background people
     * @example background people
     */
    prompt?: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingObjectRemovalOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/monkey/3nTlgwVIOSeWJ8yDmvYr-_e09a6b8eb9264f76a33408e997a7447d.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingHairChangeInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description Image prompt for the omni model.
     * @example https://v3.fal.media/files/zebra/hAjCkcyly4gsS9-cptD3Y_image%20(20).png
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Hair Style Prompt
     * @description The desired hair style to apply.
     * @default bald
     * @example bald
     */
    prompt?: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingHairChangeOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/panda/5GTPnNd3D_X-WXiEo4jUf_0bedd250b9d24bd194702f3f196f25a0.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingFaceEnhancementInput extends SharedType_868 {}

export interface ImageEditingFaceEnhancementOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/zebra/cUdI2ZvLl8Y99cgoxZsNZ_2d5cd48079684ee189dd3aa8a80b1c2d.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingExpressionChangeInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description Image prompt for the omni model.
     * @example https://v3.fal.media/files/zebra/hAjCkcyly4gsS9-cptD3Y_image%20(20).png
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Expression Prompt
     * @description The desired facial expression to apply.
     * @default sad
     * @example sad
     */
    prompt?: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingExpressionChangeOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/kangaroo/Gf3AzGSqL08srbH5gOQtg_dad44d1ada5c4a32bc835324e6058fb1.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingColorCorrectionInput extends SharedType_868 {}

export interface ImageEditingColorCorrectionOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/elephant/Y7eZXlYjiw5km-dTA1WkX_2d4d7661f8504154b76009a6d9c49728.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingCartoonifyInput extends SharedType_868 {}

export interface ImageEditingCartoonifyOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/koala/VCzEo2r7yiG49gH_dBqfC_55111638572349f782f547516cbc6e74.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingBroccoliHaircutInput {
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description URL of the image to apply broccoli haircut style.
     * @example https://v3.fal.media/files/rabbit/8nqqnF_KS9c0pGgwvRNAY_IMG_8421.jpeg
     */
    image_url: string;
    /**
     * Lora Scale
     * @description The scale factor for the LoRA model. Controls the strength of the LoRA effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingBroccoliHaircutOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/elephant/Rc2zAM5x0jpj4decdJTZX.png"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingBackgroundChangeInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description Image prompt for the omni model.
     * @example https://v3.fal.media/files/zebra/hAjCkcyly4gsS9-cptD3Y_image%20(20).png
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Background Prompt
     * @description The desired background to apply.
     * @default beach sunset with palm trees
     * @example beach sunset with palm trees
     */
    prompt?: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingBackgroundChangeOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/panda/hsfdyCWf8tgc-UBhT2Rie_9c789557af2e43a08a02b5e4e4c41a69.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingBabyVersionInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description URL of the image to transform into a baby version.
     * @example https://v3.fal.media/files/penguin/hIPcTcSrtLMVXyedBUqIX_-pG58lHRIQ3_1iBmMlU_v_image.webp
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingBabyVersionOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/tiger/pnDpVBW93b0rN_8JuT7cW_c28f958055f946caaece343054732d01.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageEditingAgeProgressionInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description Image prompt for the omni model.
     * @example https://v3.fal.media/files/zebra/hAjCkcyly4gsS9-cptD3Y_image%20(20).png
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Age Change
     * @description The age change to apply.
     * @default 20 years older
     * @example 20 years older
     */
    prompt?: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface ImageEditingAgeProgressionOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://fal.media/files/lion/t7L2EtPYDkz1-fBlJsodJ_4e7306f22c8748258f96d1e5ed5a4cfe.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /** Seed */
    seed: number;
}

export interface ImageAppsV2VirtualTryOnInput {
    /** @description Aspect ratio for 4K output (default: 3:4 for fashion) */
    aspect_ratio?: Components.AspectRatio;
    /**
     * Clothing Image Url
     * @description Clothing photo URL
     * @example https://v3b.fal.media/files/b/monkey/5ZWXSKUuk9EilI1apFCeu_1ecd050187f24b9aa1d2defb88d8d8ae.png
     */
    clothing_image_url: string;
    /**
     * Person Image Url
     * @description Person photo URL
     * @example https://v3.fal.media/files/tiger/4vxSHizex4UWR5fdnPs1A.jpeg
     */
    person_image_url: string;
    /**
     * Preserve Pose
     * @default true
     */
    preserve_pose?: boolean;
}

export interface ImageAppsV2VirtualTryOnOutput {
    /**
     * Images
     * @description Person wearing the virtual clothing
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/panda/9w6wt7vgxjfmiBIoo6bjF_cb0ba7a150c84f159e9d40af2d439401.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Inference Time Ms
     * @description Total inference time in milliseconds
     * @example 15234
     */
    inference_time_ms: number;
}

export interface ImageAppsV2TextureTransformInput {
    /** @description Aspect ratio for 4K output */
    aspect_ratio?: Components.AspectRatio;
    /**
     * Image Url
     * @description Image URL for texture transformation
     * @example https://v3.fal.media/files/tiger/WqtPStHkOu6W0lZcIlXD8_156e3c91cb3a4d12b7380cc43b5e4c67.png
     */
    image_url: string;
    /**
     * Target Texture
     * @default marble
     * @enum {string}
     */
    target_texture?:
        | 'cotton'
        | 'denim'
        | 'wool'
        | 'felt'
        | 'wood'
        | 'leather'
        | 'velvet'
        | 'stone'
        | 'marble'
        | 'ceramic'
        | 'concrete'
        | 'brick'
        | 'clay'
        | 'foam'
        | 'glass'
        | 'metal'
        | 'silk'
        | 'fabric'
        | 'crystal'
        | 'rubber'
        | 'plastic'
        | 'lace';
}

export interface ImageAppsV2TextureTransformOutput {
    /**
     * Images
     * @description Image with transformed texture
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/elephant/gbv_d9UfnVu_axz_E64fA_4f79ee88e4ed49759c09110783d9924d.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Inference Time Ms
     * @description Total inference time in milliseconds
     * @example 15234
     */
    inference_time_ms: number;
}

export interface ImageAppsV2StyleTransferInput {
    /** @description Aspect ratio for 4K output */
    aspect_ratio?: Components.AspectRatio;
    /**
     * Image Url
     * @description Image URL for style transfer
     * @example https://v3.fal.media/files/tiger/vAohCtb_N8Q_cAs9Z03GS.jpeg
     */
    image_url: string;
    /**
     * Style Reference Image Url
     * @description Optional reference image URL. When provided, the style will be inferred from this image instead of the selected preset style.
     */
    style_reference_image_url?: string;
    /**
     * Target Style
     * @default impressionist
     * @enum {string}
     */
    target_style?:
        | 'anime_character'
        | 'cartoon_3d'
        | 'hand_drawn_animation'
        | 'cyberpunk_future'
        | 'anime_game_style'
        | 'comic_book_animation'
        | 'animated_series'
        | 'cartoon_animation'
        | 'lofi_aesthetic'
        | 'cottagecore'
        | 'dark_academia'
        | 'y2k'
        | 'vaporwave'
        | 'liminal_space'
        | 'weirdcore'
        | 'dreamcore'
        | 'synthwave'
        | 'outrun'
        | 'photorealistic'
        | 'hyperrealistic'
        | 'digital_art'
        | 'concept_art'
        | 'impressionist'
        | 'anime'
        | 'pixel_art'
        | 'claymation';
}

export interface ImageAppsV2StyleTransferOutput {
    /**
     * Images
     * @description Image with transferred style
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/koala/LX3BVXfDxE_3qgqH4YL3P_d89afb41867b4a698d29b6308d610dc1.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Inference Time Ms
     * @description Total inference time in milliseconds
     * @example 15234
     */
    inference_time_ms: number;
}

export interface ImageAppsV2RelightingInput {
    /** @description Aspect ratio for 4K output */
    aspect_ratio?: Components.AspectRatio;
    /**
     * Image Url
     * @description Image URL for relighting
     * @example https://v3.fal.media/files/monkey/tugG2Q-XqMgf_ZoBr8KFO.jpeg
     */
    image_url: string;
    /**
     * Lighting Style
     * @default natural
     * @enum {string}
     */
    lighting_style?:
        | 'natural'
        | 'studio'
        | 'golden_hour'
        | 'blue_hour'
        | 'dramatic'
        | 'soft'
        | 'hard'
        | 'backlight'
        | 'side_light'
        | 'front_light'
        | 'rim_light'
        | 'sunset'
        | 'sunrise'
        | 'neon'
        | 'candlelight'
        | 'moonlight'
        | 'spotlight'
        | 'ambient';
}

export interface ImageAppsV2RelightingOutput {
    /**
     * Images
     * @description Image with new lighting
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/penguin/TD0yR-XgXfyZjSjKcxKWS_4a54a0fb769d4a60bf6818fffed70493.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Inference Time Ms
     * @description Total inference time in milliseconds
     * @example 15234
     */
    inference_time_ms: number;
}

export interface ImageAppsV2ProductPhotographyInput {
    /** @description Aspect ratio for 4K output */
    aspect_ratio?: Components.AspectRatio;
    /**
     * Product Image Url
     * @description Image URL of the product to create professional studio photography
     * @example https://v3.fal.media/files/tiger/WqtPStHkOu6W0lZcIlXD8_156e3c91cb3a4d12b7380cc43b5e4c67.png
     */
    product_image_url: string;
}

export interface ImageAppsV2ProductPhotographyOutput {
    /**
     * Images
     * @description Professional studio product photography
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/panda/NggkU_5Ne-SB-Ltgr0lbH_b76a99a276374ff9b13f399e57c54cb6.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Inference Time Ms
     * @description Total inference time in milliseconds
     * @example 15234
     */
    inference_time_ms: number;
}

export interface ImageAppsV2ProductHoldingInput {
    /** @description Aspect ratio for 4K output */
    aspect_ratio?: Components.AspectRatio;
    /**
     * Person Image Url
     * @description Image URL of the person who will hold the product
     * @example https://v3.fal.media/files/panda/oMob58qZJRtbDs5l45QKT_e3a1512c455d425fab2d62e07a51c506.png
     */
    person_image_url: string;
    /**
     * Product Image Url
     * @description Image URL of the product to be held by the person
     * @example https://v3.fal.media/files/tiger/WqtPStHkOu6W0lZcIlXD8_156e3c91cb3a4d12b7380cc43b5e4c67.png
     */
    product_image_url: string;
}

export interface ImageAppsV2ProductHoldingOutput {
    /**
     * Images
     * @description Person holding the product naturally
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/penguin/JR-DlVO6P28LvnvMU2mu5_7078ec349a194e6c8860cae283bd3a0d.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Inference Time Ms
     * @description Total inference time in milliseconds
     * @example 15234
     */
    inference_time_ms: number;
}

export interface ImageAppsV2PortraitEnhanceInput {
    /** @description Aspect ratio for 4K output (default: 3:4 for portraits) */
    aspect_ratio?: Components.AspectRatio;
    /**
     * Image Url
     * @description Portrait image URL to enhance
     * @example https://v3b.fal.media/files/b/monkey/VszUszGx2uzReqFvQzF26.jpg
     */
    image_url: string;
}

export interface ImageAppsV2PortraitEnhanceOutput {
    /**
     * Images
     * @description Enhanced portrait
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/tiger/Re4QHcI-z3a1RNUssE9s3_ac63f107bd714cccb9060e2a5c8f999f.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Inference Time Ms
     * @description Total inference time in milliseconds
     * @example 15234
     */
    inference_time_ms: number;
}

export interface ImageAppsV2PhotographyEffectsInput {
    /** @description Aspect ratio for 4K output */
    aspect_ratio?: Components.AspectRatio;
    /**
     * Effect Type
     * @default film
     * @enum {string}
     */
    effect_type?:
        | 'film'
        | 'vintage_film'
        | 'portrait_photography'
        | 'fashion_photography'
        | 'street_photography'
        | 'sepia_tone'
        | 'film_grain'
        | 'light_leaks'
        | 'vignette_effect'
        | 'instant_camera'
        | 'golden_hour'
        | 'dramatic_lighting'
        | 'soft_focus'
        | 'bokeh_effect'
        | 'high_contrast'
        | 'double_exposure';
    /**
     * Image Url
     * @description Image URL for photography effects
     * @example https://v3.fal.media/files/monkey/tugG2Q-XqMgf_ZoBr8KFO.jpeg
     */
    image_url: string;
}

export interface ImageAppsV2PhotographyEffectsOutput {
    /**
     * Images
     * @description Image with photography effects
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/koala/hUNSMGeQKoBKGx0VlOYMK_3bc61fa97c8247e8bf38854eff4a6534.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Inference Time Ms
     * @description Total inference time in milliseconds
     * @example 15234
     */
    inference_time_ms: number;
}

export interface ImageAppsV2PhotoRestorationInput {
    /** @description Aspect ratio for 4K output (default: 4:3 for classic photos) */
    aspect_ratio?: Components.AspectRatio;
    /**
     * Enhance Resolution
     * @default true
     */
    enhance_resolution?: boolean;
    /**
     * Fix Colors
     * @default true
     */
    fix_colors?: boolean;
    /**
     * Image Url
     * @description Old or damaged photo URL to restore
     * @example https://v3.fal.media/files/panda/BcOIyOJ5Z1-GOjDi_GmsX_50e353c588c74435882f8f68989b4af5.png
     */
    image_url: string;
    /**
     * Remove Scratches
     * @default true
     */
    remove_scratches?: boolean;
}

export interface ImageAppsV2PhotoRestorationOutput {
    /**
     * Images
     * @description Restored photo
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/zebra/grrkysWir2bvFTnubydQK_92e56e5831994be59e653611983d503f.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Inference Time Ms
     * @description Total inference time in milliseconds
     * @example 15234
     */
    inference_time_ms: number;
}

export interface ImageAppsV2PerspectiveInput {
    /** @description Aspect ratio for 4K output */
    aspect_ratio?: Components.AspectRatio;
    /**
     * Image Url
     * @description Image URL for perspective change
     * @example https://v3b.fal.media/files/b/lion/aqEiZ8Ui6rxWUB-Ujfu79_52c238c6d75d45538eaa71c50d329ba0.png
     */
    image_url: string;
    /**
     * Target Perspective
     * @default front
     * @enum {string}
     */
    target_perspective?:
        | 'front'
        | 'left_side'
        | 'right_side'
        | 'back'
        | 'top_down'
        | 'bottom_up'
        | 'birds_eye'
        | 'three_quarter_left'
        | 'three_quarter_right';
}

export interface ImageAppsV2PerspectiveOutput {
    /**
     * Images
     * @description Image with changed perspective
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/zebra/Elbt2Bcigd6pVdLlbfsHV_e45937e851074d2d932363009504866d.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Inference Time Ms
     * @description Total inference time in milliseconds
     * @example 15234
     */
    inference_time_ms: number;
}

export interface ImageAppsV2OutpaintInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Bottom
     * @description Number of pixels to add as black margin on the bottom side (0-700).
     * @default 400
     */
    expand_bottom?: number;
    /**
     * Expand Left
     * @description Number of pixels to add as black margin on the left side (0-700).
     * @default 0
     */
    expand_left?: number;
    /**
     * Expand Right
     * @description Number of pixels to add as black margin on the right side (0-700).
     * @default 0
     */
    expand_right?: number;
    /**
     * Expand Top
     * @description Number of pixels to add as black margin on the top side (0-700).
     * @default 0
     */
    expand_top?: number;
    /**
     * Image Url
     * @description Image URL to outpaint
     * @example https://v3.fal.media/files/koala/oei_-iPIYFnhdB8SxojND_qwen-edit-res.png
     */
    image_url: string;
    /**
     * Num Images
     * @description Number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the output image.
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'jpg' | 'webp';
    /**
     * Prompt
     * @description Optional prompt to guide the outpainting. If provided, it will be appended to the base outpaint instruction. Example: 'with a beautiful sunset in the background'
     * @default
     */
    prompt?: string;
    /**
     * Sync Mode
     * @description If True, the function will wait for the image to be generated and uploaded before returning the response. If False, the function will return immediately and the image will be generated asynchronously.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Zoom Out Percentage
     * @description Percentage to zoom out the image. If set, the image will be scaled down by this percentage and black margins will be added to maintain original size. Example: 50 means the image will be 50% of original size with black margins filling the rest.
     * @default 20
     */
    zoom_out_percentage?: number;
}

export interface ImageAppsV2OutpaintOutput {
    /**
     * Images
     * @description Outpainted image with extended scene
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/zebra/S4DmrY6pF9cI2GvSaONXZ.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
}

export interface ImageAppsV2ObjectRemovalInput {
    /** @description Aspect ratio for 4K output */
    aspect_ratio?: Components.AspectRatio;
    /**
     * Image Url
     * @description Image URL containing object to remove
     * @example https://v3.fal.media/files/zebra/L_YMy6H5r_HYMacZX1qne_74a8fb6130164a18930af55370a1c9b2.png
     */
    image_url: string;
    /**
     * Object To Remove
     * @description Object to remove
     */
    object_to_remove: string;
}

export interface ImageAppsV2ObjectRemovalOutput {
    /**
     * Images
     * @description Image with object removed
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/rabbit/InVOEtkI_yl05dSz9bqiU_789bdf6c81fc4c75bdcc57bad744f389.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Inference Time Ms
     * @description Total inference time in milliseconds
     * @example 15234
     */
    inference_time_ms: number;
}

export interface ImageAppsV2MakeupApplicationInput {
    /** @description Aspect ratio for 4K output (default: 3:4 for portraits) */
    aspect_ratio?: Components.AspectRatio;
    /**
     * Image Url
     * @description Portrait image URL for makeup application
     * @example https://v3.fal.media/files/elephant/jpdD30YLw3OfPdVDxq1-D_1ec2e27f3e7d400fbf5f7aa2b80e89f0.png
     */
    image_url: string;
    /**
     * Intensity
     * @default medium
     * @enum {string}
     */
    intensity?: 'light' | 'medium' | 'heavy' | 'dramatic';
    /**
     * Makeup Style
     * @default natural
     * @enum {string}
     */
    makeup_style?:
        | 'natural'
        | 'glamorous'
        | 'smoky_eyes'
        | 'bold_lips'
        | 'no_makeup'
        | 'remove_makeup'
        | 'dramatic'
        | 'bridal'
        | 'professional'
        | 'korean_style'
        | 'artistic';
}

export interface ImageAppsV2MakeupApplicationOutput {
    /**
     * Images
     * @description Portrait with applied makeup
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/tiger/nFRi9Cemv4B5-felhf4iu_5c34cfde265443f897463f5b29b6c0b9.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Inference Time Ms
     * @description Total inference time in milliseconds
     * @example 15234
     */
    inference_time_ms: number;
}

export interface ImageAppsV2HeadshotPhotoInput {
    /** @description Aspect ratio for 4K output (default: 3:4 for portraits) */
    aspect_ratio?: Components.AspectRatio;
    /**
     * Background Style
     * @default professional
     * @enum {string}
     */
    background_style?: 'professional' | 'corporate' | 'clean' | 'gradient';
    /**
     * Image Url
     * @description Portrait image URL to convert to professional headshot
     * @example https://v3.fal.media/files/panda/oMob58qZJRtbDs5l45QKT_e3a1512c455d425fab2d62e07a51c506.png
     */
    image_url: string;
}

export interface ImageAppsV2HeadshotPhotoOutput {
    /**
     * Images
     * @description Professional headshot image
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/penguin/n2f6ImUI9nJORdIibmpim_dfaeb84b0e894f62ae50ccaceb9704d3.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Inference Time Ms
     * @description Total inference time in milliseconds
     * @example 15234
     */
    inference_time_ms: number;
}

export interface ImageAppsV2HairChangeInput {
    /** @description Aspect ratio for 4K output (default: 3:4 for portraits) */
    aspect_ratio?: Components.AspectRatio;
    /**
     * Hair Color
     * @default natural
     * @enum {string}
     */
    hair_color?:
        | 'black'
        | 'dark_brown'
        | 'light_brown'
        | 'blonde'
        | 'platinum_blonde'
        | 'red'
        | 'auburn'
        | 'gray'
        | 'silver'
        | 'blue'
        | 'green'
        | 'purple'
        | 'pink'
        | 'rainbow'
        | 'natural'
        | 'highlights'
        | 'ombre'
        | 'balayage';
    /**
     * Image Url
     * @description Portrait image URL for hair change
     * @example https://v3.fal.media/files/zebra/1ois7lqES78dLualcytmS_1e088ef24f474972824cffcfdd7ff291.png
     */
    image_url: string;
    /**
     * Target Hairstyle
     * @default long_hair
     * @enum {string}
     */
    target_hairstyle?:
        | 'short_hair'
        | 'medium_long_hair'
        | 'long_hair'
        | 'curly_hair'
        | 'wavy_hair'
        | 'high_ponytail'
        | 'bun'
        | 'bob_cut'
        | 'pixie_cut'
        | 'braids'
        | 'straight_hair'
        | 'afro'
        | 'dreadlocks'
        | 'buzz_cut'
        | 'mohawk'
        | 'bangs'
        | 'side_part'
        | 'middle_part';
}

export interface ImageAppsV2HairChangeOutput {
    /**
     * Images
     * @description Portrait with changed hair
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/rabbit/nuRb4Pn6U-Pryrd2LPI6I_1bcc7ca071a1483faeba2d159952c7f8.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Inference Time Ms
     * @description Total inference time in milliseconds
     * @example 15234
     */
    inference_time_ms: number;
}

export interface ImageAppsV2ExpressionChangeInput {
    /** @description Aspect ratio for 4K output (default: 3:4 for portraits) */
    aspect_ratio?: Components.AspectRatio;
    /**
     * Image Url
     * @description Portrait image URL for expression change
     * @example https://v3.fal.media/files/kangaroo/Gk5_0En-p1CNYnGOCDkl1_8332b944f8334ba9b3118b49e3e641cf.png
     */
    image_url: string;
    /**
     * Target Expression
     * @default smile
     * @enum {string}
     */
    target_expression?:
        | 'smile'
        | 'surprise'
        | 'glare'
        | 'panic'
        | 'shyness'
        | 'laugh'
        | 'cry'
        | 'angry'
        | 'sad'
        | 'happy'
        | 'excited'
        | 'shocked'
        | 'confused'
        | 'focused'
        | 'dreamy'
        | 'serious'
        | 'playful'
        | 'mysterious'
        | 'confident'
        | 'thoughtful';
}

export interface ImageAppsV2ExpressionChangeOutput {
    /**
     * Images
     * @description Portrait with changed expression
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/lion/evrtcIw4fzjESvKi7Unup_781b90e31d864eb0ad4f5ef59160ebc4.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Inference Time Ms
     * @description Total inference time in milliseconds
     * @example 15234
     */
    inference_time_ms: number;
}

export interface ImageAppsV2CityTeleportInput {
    /** @description Aspect ratio for 4K output */
    aspect_ratio?: Components.AspectRatio;
    /**
     * Camera Angle
     * @description Camera angle for the shot
     * @default eye_level
     * @enum {string}
     */
    camera_angle?:
        | 'eye_level'
        | 'low_angle'
        | 'high_angle'
        | 'dutch_angle'
        | 'birds_eye_view'
        | 'worms_eye_view'
        | 'overhead'
        | 'side_angle';
    /**
     * City Image Url
     * @description Optional city background image URL. When provided, the person will be blended into this custom scene.
     */
    city_image_url?: string;
    /**
     * City Name
     * @description City name (used when city_image_url is not provided)
     * @example Paris
     */
    city_name: string;
    /**
     * Person Image Url
     * @description Person photo URL
     * @example https://v3.fal.media/files/kangaroo/qsFWu-bO3FwcTQSym9HeL_e52f70668f2940a3b4ea2cab54fcb65b.png
     */
    person_image_url: string;
    /**
     * Photo Shot
     * @description Type of photo shot
     * @default medium_shot
     * @enum {string}
     */
    photo_shot?:
        | 'extreme_close_up'
        | 'close_up'
        | 'medium_close_up'
        | 'medium_shot'
        | 'medium_long_shot'
        | 'long_shot'
        | 'extreme_long_shot'
        | 'full_body';
}

export interface ImageAppsV2CityTeleportOutput {
    /**
     * Images
     * @description Person teleported to city location
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/zebra/IxujEn7zMTwi7JwUFayG0_d0810ad048a24c6d9d80a68c4325a675.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Inference Time Ms
     * @description Total inference time in milliseconds
     * @example 15234
     */
    inference_time_ms: number;
}

export interface ImageAppsV2AgeModifyInput {
    /** @description Aspect ratio for 4K output (default: 3:4 for portraits) */
    aspect_ratio?: Components.AspectRatio;
    /**
     * Image Url
     * @description Portrait image URL for age modification
     * @example https://v3.fal.media/files/lion/s2GShUC7AB9i-ypYV0DbI_1b5ca4fe5d7e477fb4501acf9a1c43bc.png
     */
    image_url: string;
    /**
     * Preserve Identity
     * @default true
     */
    preserve_identity?: boolean;
    /**
     * Target Age
     * @default 30
     */
    target_age?: number;
}

export interface ImageAppsV2AgeModifyOutput {
    /**
     * Images
     * @description Portrait with modified age
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/monkey/lPoI9oYu6X_I1SvqOurj0_ce7daf32a7a146f9a627c0578bd4a747.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Inference Time Ms
     * @description Total inference time in milliseconds
     * @example 15234
     */
    inference_time_ms: number;
}

export interface IllusionDiffusionInput {
    /**
     * Control Guidance End
     * @default 1
     */
    control_guidance_end?: number;
    /**
     * Control Guidance Start
     * @default 0
     */
    control_guidance_start?: number;
    /**
     * Controlnet Conditioning Scale
     * @description The scale of the ControlNet.
     * @default 1
     */
    controlnet_conditioning_scale?: number;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. You can choose between some presets or
     *                 custom height and width that **must be multiples of 8**.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description Input image url.
     * @example https://storage.googleapis.com/falserverless/illusion-examples/pattern.png
     * @example https://storage.googleapis.com/falserverless/illusion-examples/checkers.png
     * @example https://storage.googleapis.com/falserverless/illusion-examples/checkers_mid.jpg
     * @example https://storage.googleapis.com/falserverless/illusion-examples/ultra_checkers.png
     * @example https://storage.googleapis.com/falserverless/illusion-examples/funky.jpeg
     * @example https://storage.googleapis.com/falserverless/illusion-examples/cubes.jpeg
     * @example https://storage.googleapis.com/falserverless/illusion-examples/turkey-flag.png
     * @example https://storage.googleapis.com/falserverless/illusion-examples/india-flag.png
     * @example https://storage.googleapis.com/falserverless/illusion-examples/usa-flag.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example (worst quality, poor details:1.4), lowres, (artist name, signature, watermark:1.4), bad-artist-anime, bad_prompt_version2, bad-hands-5, ng_deepnegative_v1_75t
     */
    negative_prompt?: string;
    /**
     * Number of inference steps
     * @description Increasing the amount of steps tells Stable Diffusion that it should take more steps
     *                 to generate your final result which can increase the amount of detail in your image.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example (masterpiece:1.4), (best quality), (detailed), Medieval village scene with busy streets and castle in the distance
     */
    prompt: string;
    /**
     * Scheduler
     * @description Scheduler / sampler to use for the image denoising process.
     * @default Euler
     * @enum {string}
     */
    scheduler?: 'DPM++ Karras SDE' | 'Euler';
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed?: number;
}

export interface IllusionDiffusionOutput extends SharedType_0c0 {}

export interface IdeogramV3ReplaceBackgroundInput {
    /** @description A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members) */
    color_palette?: Components.ColorPalette;
    /**
     * Expand Prompt
     * @description Determine if MagicPrompt should be used in generating the request or not.
     * @default true
     */
    expand_prompt?: boolean;
    /**
     * Image URL
     * @description The image URL whose background needs to be replaced
     * @example https://v3.fal.media/files/rabbit/F6dvKPFL9VzKiM8asJOgm_MJj6yUB6rGjTsv_1YHIcA_image.webp
     */
    image_url: string;
    /**
     * Image Urls
     * @description A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format
     */
    image_urls?: string[];
    /**
     * Num Images
     * @description Number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description Cyber punk city with neon lights and skyscrappers
     * @example A beautiful sunset over mountains that writes Ideogram v3 in fal.ai
     */
    prompt: string;
    /**
     * Rendering Speed
     * @description The rendering speed to use.
     * @default BALANCED
     * @enum {string}
     */
    rendering_speed?: 'TURBO' | 'BALANCED' | 'QUALITY';
    /**
     * Seed
     * @description Seed for the random number generator
     */
    seed?: number;
    /**
     * Style
     * @description The style type to generate with. Cannot be used with style_codes.
     */
    style?: 'AUTO' | 'GENERAL' | 'REALISTIC' | 'DESIGN';
    /**
     * Style Codes
     * @description A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style
     */
    style_codes?: string[];
    /**
     * Style Preset
     * @description Style preset for generation. The chosen style preset will guide the generation.
     */
    style_preset?:
        | '80S_ILLUSTRATION'
        | '90S_NOSTALGIA'
        | 'ABSTRACT_ORGANIC'
        | 'ANALOG_NOSTALGIA'
        | 'ART_BRUT'
        | 'ART_DECO'
        | 'ART_POSTER'
        | 'AURA'
        | 'AVANT_GARDE'
        | 'BAUHAUS'
        | 'BLUEPRINT'
        | 'BLURRY_MOTION'
        | 'BRIGHT_ART'
        | 'C4D_CARTOON'
        | 'CHILDRENS_BOOK'
        | 'COLLAGE'
        | 'COLORING_BOOK_I'
        | 'COLORING_BOOK_II'
        | 'CUBISM'
        | 'DARK_AURA'
        | 'DOODLE'
        | 'DOUBLE_EXPOSURE'
        | 'DRAMATIC_CINEMA'
        | 'EDITORIAL'
        | 'EMOTIONAL_MINIMAL'
        | 'ETHEREAL_PARTY'
        | 'EXPIRED_FILM'
        | 'FLAT_ART'
        | 'FLAT_VECTOR'
        | 'FOREST_REVERIE'
        | 'GEO_MINIMALIST'
        | 'GLASS_PRISM'
        | 'GOLDEN_HOUR'
        | 'GRAFFITI_I'
        | 'GRAFFITI_II'
        | 'HALFTONE_PRINT'
        | 'HIGH_CONTRAST'
        | 'HIPPIE_ERA'
        | 'ICONIC'
        | 'JAPANDI_FUSION'
        | 'JAZZY'
        | 'LONG_EXPOSURE'
        | 'MAGAZINE_EDITORIAL'
        | 'MINIMAL_ILLUSTRATION'
        | 'MIXED_MEDIA'
        | 'MONOCHROME'
        | 'NIGHTLIFE'
        | 'OIL_PAINTING'
        | 'OLD_CARTOONS'
        | 'PAINT_GESTURE'
        | 'POP_ART'
        | 'RETRO_ETCHING'
        | 'RIVIERA_POP'
        | 'SPOTLIGHT_80S'
        | 'STYLIZED_RED'
        | 'SURREAL_COLLAGE'
        | 'TRAVEL_POSTER'
        | 'VINTAGE_GEO'
        | 'VINTAGE_POSTER'
        | 'WATERCOLOR'
        | 'WEIRD'
        | 'WOODBLOCK_PRINT';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface IdeogramV3ReplaceBackgroundOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/lion/AUfCjtLkLOsdc9zEFrV-5_image.png"
     *       }
     *     ]
     */
    images: Components.File_1[];
    /**
     * Seed
     * @description Seed used for the random number generator
     * @example 123456
     */
    seed: number;
}

export interface IdeogramV3RemixInput {
    /** @description A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members) */
    color_palette?: Components.ColorPalette;
    /**
     * Expand Prompt
     * @description Determine if MagicPrompt should be used in generating the request or not.
     * @default true
     */
    expand_prompt?: boolean;
    /**
     * Image Size
     * @description The resolution of the generated image
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description The image URL to remix
     * @example https://v3.fal.media/files/lion/9-Yt8JfTw4OxrAjiUzwP9_output.png
     */
    image_url: string;
    /**
     * Image Urls
     * @description A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format
     */
    image_urls?: string[];
    /**
     * Negative Prompt
     * @description Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The prompt to remix the image with
     * @example Old ancient city day light
     */
    prompt: string;
    /**
     * Rendering Speed
     * @description The rendering speed to use.
     * @default BALANCED
     * @enum {string}
     */
    rendering_speed?: 'TURBO' | 'BALANCED' | 'QUALITY';
    /**
     * Seed
     * @description Seed for the random number generator
     */
    seed?: number;
    /**
     * Strength
     * @description Strength of the input image in the remix
     * @default 0.8
     */
    strength?: number;
    /**
     * Style
     * @description The style type to generate with. Cannot be used with style_codes.
     */
    style?: 'AUTO' | 'GENERAL' | 'REALISTIC' | 'DESIGN';
    /**
     * Style Codes
     * @description A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style
     */
    style_codes?: string[];
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface IdeogramV3RemixOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/koala/eYZG26O54NTdWzdpDWBL-_image.png"
     *       }
     *     ]
     */
    images: Components.File_1[];
    /**
     * Seed
     * @description Seed used for the random number generator
     * @example 123456
     */
    seed: number;
}

export interface IdeogramV3ReframeInput {
    /** @description A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members) */
    color_palette?: Components.ColorPalette;
    /**
     * Image Size
     * @description The resolution for the reframed output image
     * @example square_hd
     */
    image_size:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description The image URL to reframe
     * @example https://v3.fal.media/files/lion/0qJs_qW8nz0wYsXhFa6Tk.png
     */
    image_url: string;
    /**
     * Image Urls
     * @description A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format
     */
    image_urls?: string[];
    /**
     * Num Images
     * @description Number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Rendering Speed
     * @description The rendering speed to use.
     * @default BALANCED
     * @enum {string}
     */
    rendering_speed?: 'TURBO' | 'BALANCED' | 'QUALITY';
    /**
     * Seed
     * @description Seed for the random number generator
     */
    seed?: number;
    /**
     * Style
     * @description The style type to generate with. Cannot be used with style_codes.
     */
    style?: 'AUTO' | 'GENERAL' | 'REALISTIC' | 'DESIGN';
    /**
     * Style Codes
     * @description A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style
     */
    style_codes?: string[];
    /**
     * Style Preset
     * @description Style preset for generation. The chosen style preset will guide the generation.
     */
    style_preset?:
        | '80S_ILLUSTRATION'
        | '90S_NOSTALGIA'
        | 'ABSTRACT_ORGANIC'
        | 'ANALOG_NOSTALGIA'
        | 'ART_BRUT'
        | 'ART_DECO'
        | 'ART_POSTER'
        | 'AURA'
        | 'AVANT_GARDE'
        | 'BAUHAUS'
        | 'BLUEPRINT'
        | 'BLURRY_MOTION'
        | 'BRIGHT_ART'
        | 'C4D_CARTOON'
        | 'CHILDRENS_BOOK'
        | 'COLLAGE'
        | 'COLORING_BOOK_I'
        | 'COLORING_BOOK_II'
        | 'CUBISM'
        | 'DARK_AURA'
        | 'DOODLE'
        | 'DOUBLE_EXPOSURE'
        | 'DRAMATIC_CINEMA'
        | 'EDITORIAL'
        | 'EMOTIONAL_MINIMAL'
        | 'ETHEREAL_PARTY'
        | 'EXPIRED_FILM'
        | 'FLAT_ART'
        | 'FLAT_VECTOR'
        | 'FOREST_REVERIE'
        | 'GEO_MINIMALIST'
        | 'GLASS_PRISM'
        | 'GOLDEN_HOUR'
        | 'GRAFFITI_I'
        | 'GRAFFITI_II'
        | 'HALFTONE_PRINT'
        | 'HIGH_CONTRAST'
        | 'HIPPIE_ERA'
        | 'ICONIC'
        | 'JAPANDI_FUSION'
        | 'JAZZY'
        | 'LONG_EXPOSURE'
        | 'MAGAZINE_EDITORIAL'
        | 'MINIMAL_ILLUSTRATION'
        | 'MIXED_MEDIA'
        | 'MONOCHROME'
        | 'NIGHTLIFE'
        | 'OIL_PAINTING'
        | 'OLD_CARTOONS'
        | 'PAINT_GESTURE'
        | 'POP_ART'
        | 'RETRO_ETCHING'
        | 'RIVIERA_POP'
        | 'SPOTLIGHT_80S'
        | 'STYLIZED_RED'
        | 'SURREAL_COLLAGE'
        | 'TRAVEL_POSTER'
        | 'VINTAGE_GEO'
        | 'VINTAGE_POSTER'
        | 'WATERCOLOR'
        | 'WEIRD'
        | 'WOODBLOCK_PRINT';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface IdeogramV3ReframeOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/zebra/LVW4AhVs3sCxsVKdg3EfT_image.png"
     *       }
     *     ]
     */
    images: Components.File_1[];
    /**
     * Seed
     * @description Seed used for the random number generator
     * @example 123456
     */
    seed: number;
}

export interface IdeogramV3EditInput {
    /** @description A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members) */
    color_palette?: Components.ColorPalette;
    /**
     * Expand Prompt
     * @description Determine if MagicPrompt should be used in generating the request or not.
     * @default true
     */
    expand_prompt?: boolean;
    /**
     * Image URL
     * @description The image URL to generate an image from. MUST have the exact same dimensions (width and height) as the mask image.
     * @example https://v3.fal.media/files/panda/-LC_gNNV3wUHaGMQT3klE_output.png
     */
    image_url: string;
    /**
     * Image Urls
     * @description A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format
     */
    image_urls?: string[];
    /**
     * Mask URL
     * @description The mask URL to inpaint the image. MUST have the exact same dimensions (width and height) as the input image.
     * @example https://v3.fal.media/files/kangaroo/1dd3zEL5MXQ3Kb4-mRi9d_indir%20(20).png
     */
    mask_url: string;
    /**
     * Num Images
     * @description Number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The prompt to fill the masked part of the image.
     * @example black bag
     */
    prompt: string;
    /**
     * Rendering Speed
     * @description The rendering speed to use.
     * @default BALANCED
     * @enum {string}
     */
    rendering_speed?: 'TURBO' | 'BALANCED' | 'QUALITY';
    /**
     * Seed
     * @description Seed for the random number generator
     */
    seed?: number;
    /**
     * Style Codes
     * @description A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style
     */
    style_codes?: string[];
    /**
     * Style Preset
     * @description Style preset for generation. The chosen style preset will guide the generation.
     */
    style_preset?:
        | '80S_ILLUSTRATION'
        | '90S_NOSTALGIA'
        | 'ABSTRACT_ORGANIC'
        | 'ANALOG_NOSTALGIA'
        | 'ART_BRUT'
        | 'ART_DECO'
        | 'ART_POSTER'
        | 'AURA'
        | 'AVANT_GARDE'
        | 'BAUHAUS'
        | 'BLUEPRINT'
        | 'BLURRY_MOTION'
        | 'BRIGHT_ART'
        | 'C4D_CARTOON'
        | 'CHILDRENS_BOOK'
        | 'COLLAGE'
        | 'COLORING_BOOK_I'
        | 'COLORING_BOOK_II'
        | 'CUBISM'
        | 'DARK_AURA'
        | 'DOODLE'
        | 'DOUBLE_EXPOSURE'
        | 'DRAMATIC_CINEMA'
        | 'EDITORIAL'
        | 'EMOTIONAL_MINIMAL'
        | 'ETHEREAL_PARTY'
        | 'EXPIRED_FILM'
        | 'FLAT_ART'
        | 'FLAT_VECTOR'
        | 'FOREST_REVERIE'
        | 'GEO_MINIMALIST'
        | 'GLASS_PRISM'
        | 'GOLDEN_HOUR'
        | 'GRAFFITI_I'
        | 'GRAFFITI_II'
        | 'HALFTONE_PRINT'
        | 'HIGH_CONTRAST'
        | 'HIPPIE_ERA'
        | 'ICONIC'
        | 'JAPANDI_FUSION'
        | 'JAZZY'
        | 'LONG_EXPOSURE'
        | 'MAGAZINE_EDITORIAL'
        | 'MINIMAL_ILLUSTRATION'
        | 'MIXED_MEDIA'
        | 'MONOCHROME'
        | 'NIGHTLIFE'
        | 'OIL_PAINTING'
        | 'OLD_CARTOONS'
        | 'PAINT_GESTURE'
        | 'POP_ART'
        | 'RETRO_ETCHING'
        | 'RIVIERA_POP'
        | 'SPOTLIGHT_80S'
        | 'STYLIZED_RED'
        | 'SURREAL_COLLAGE'
        | 'TRAVEL_POSTER'
        | 'VINTAGE_GEO'
        | 'VINTAGE_POSTER'
        | 'WATERCOLOR'
        | 'WEIRD'
        | 'WOODBLOCK_PRINT';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface IdeogramV3EditOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/panda/xr7EI_0X5kM8fDOjjcMei_image.png"
     *       }
     *     ]
     */
    images: Components.File_1[];
    /**
     * Seed
     * @description Seed used for the random number generator
     * @example 123456
     */
    seed: number;
}

export interface IdeogramV3Input {
    /** @description A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members) */
    color_palette?: Components.ColorPalette;
    /**
     * Expand Prompt
     * @description Determine if MagicPrompt should be used in generating the request or not.
     * @default true
     */
    expand_prompt?: boolean;
    /**
     * Image Size
     * @description The resolution of the generated image
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Urls
     * @description A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format
     */
    image_urls?: string[];
    /**
     * Negative Prompt
     * @description Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @example The Bone Forest stretched across the horizon, its trees fashioned from the ossified remains of ancient leviathans that once swam through the sky. Shamans with antlers growing from their shoulders and eyes that revealed the true nature of any being they beheld conducted rituals to commune with the spirits that still inhabited the calcified grove. In sky writes "Ideogram V3 in fal.ai"
     */
    prompt: string;
    /**
     * Rendering Speed
     * @description The rendering speed to use.
     * @default BALANCED
     * @enum {string}
     */
    rendering_speed?: 'TURBO' | 'BALANCED' | 'QUALITY';
    /**
     * Seed
     * @description Seed for the random number generator
     */
    seed?: number;
    /**
     * Style
     * @description The style type to generate with. Cannot be used with style_codes.
     */
    style?: 'AUTO' | 'GENERAL' | 'REALISTIC' | 'DESIGN';
    /**
     * Style Codes
     * @description A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style
     */
    style_codes?: string[];
    /**
     * Style Preset
     * @description Style preset for generation. The chosen style preset will guide the generation.
     */
    style_preset?:
        | '80S_ILLUSTRATION'
        | '90S_NOSTALGIA'
        | 'ABSTRACT_ORGANIC'
        | 'ANALOG_NOSTALGIA'
        | 'ART_BRUT'
        | 'ART_DECO'
        | 'ART_POSTER'
        | 'AURA'
        | 'AVANT_GARDE'
        | 'BAUHAUS'
        | 'BLUEPRINT'
        | 'BLURRY_MOTION'
        | 'BRIGHT_ART'
        | 'C4D_CARTOON'
        | 'CHILDRENS_BOOK'
        | 'COLLAGE'
        | 'COLORING_BOOK_I'
        | 'COLORING_BOOK_II'
        | 'CUBISM'
        | 'DARK_AURA'
        | 'DOODLE'
        | 'DOUBLE_EXPOSURE'
        | 'DRAMATIC_CINEMA'
        | 'EDITORIAL'
        | 'EMOTIONAL_MINIMAL'
        | 'ETHEREAL_PARTY'
        | 'EXPIRED_FILM'
        | 'FLAT_ART'
        | 'FLAT_VECTOR'
        | 'FOREST_REVERIE'
        | 'GEO_MINIMALIST'
        | 'GLASS_PRISM'
        | 'GOLDEN_HOUR'
        | 'GRAFFITI_I'
        | 'GRAFFITI_II'
        | 'HALFTONE_PRINT'
        | 'HIGH_CONTRAST'
        | 'HIPPIE_ERA'
        | 'ICONIC'
        | 'JAPANDI_FUSION'
        | 'JAZZY'
        | 'LONG_EXPOSURE'
        | 'MAGAZINE_EDITORIAL'
        | 'MINIMAL_ILLUSTRATION'
        | 'MIXED_MEDIA'
        | 'MONOCHROME'
        | 'NIGHTLIFE'
        | 'OIL_PAINTING'
        | 'OLD_CARTOONS'
        | 'PAINT_GESTURE'
        | 'POP_ART'
        | 'RETRO_ETCHING'
        | 'RIVIERA_POP'
        | 'SPOTLIGHT_80S'
        | 'STYLIZED_RED'
        | 'SURREAL_COLLAGE'
        | 'TRAVEL_POSTER'
        | 'VINTAGE_GEO'
        | 'VINTAGE_POSTER'
        | 'WATERCOLOR'
        | 'WEIRD'
        | 'WOODBLOCK_PRINT';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface IdeogramV3Output {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/penguin/lHdRabS80guysb8Zw1kul_image.png"
     *       }
     *     ]
     */
    images: Components.File_1[];
    /**
     * Seed
     * @description Seed used for the random number generator
     * @example 123456
     */
    seed: number;
}

export interface IdeogramV2aTurboRemixInput extends SharedType_aed {}

export interface IdeogramV2aTurboRemixOutput extends SharedType_b69 {}

export interface IdeogramV2aTurboInput extends SharedType_cfd {}

export interface IdeogramV2aTurboOutput extends SharedType_b69 {}

export interface IdeogramV2aRemixInput extends SharedType_aed {}

export interface IdeogramV2aRemixOutput extends SharedType_b69 {}

export interface IdeogramV2aInput extends SharedType_cfd {}

export interface IdeogramV2aOutput extends SharedType_b69 {}

export interface IdeogramV2TurboRemixInput extends SharedType_aed {}

export interface IdeogramV2TurboRemixOutput extends SharedType_b69 {}

export interface IdeogramV2TurboEditInput extends SharedType_e74 {}

export interface IdeogramV2TurboEditOutput extends SharedType_b69 {}

export interface IdeogramV2TurboInput extends SharedType_000 {}

export interface IdeogramV2TurboOutput extends SharedType_b69 {}

export interface IdeogramV2RemixInput extends SharedType_aed {}

export interface IdeogramV2RemixOutput extends SharedType_b69 {}

export interface IdeogramV2EditInput extends SharedType_e74 {}

export interface IdeogramV2EditOutput extends SharedType_b69 {}

export interface IdeogramV2Input extends SharedType_000 {}

export interface IdeogramV2Output extends SharedType_b69 {}

export interface IdeogramUpscaleInput {
    /**
     * Detail
     * @description The detail of the upscaled image
     * @default 50
     */
    detail?: number;
    /**
     * Expand Prompt
     * @description Whether to expand the prompt with MagicPrompt functionality.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Image URL
     * @description The image URL to upscale
     * @example https://fal.media/files/monkey/e6RtJf_ue0vyWzeiEmTby.png
     */
    image_url: string;
    /**
     * Prompt
     * @description The prompt to upscale the image with
     * @default
     */
    prompt?: string;
    /**
     * Resemblance
     * @description The resemblance of the upscaled image to the original image
     * @default 50
     */
    resemblance?: number;
    /**
     * Seed
     * @description Seed for the random number generator
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface IdeogramUpscaleOutput {
    /**
     * Images
     * @example [
     *       {
     *         "file_size": 6548418,
     *         "file_name": "image.png",
     *         "content_type": "image/png",
     *         "url": "https://fal.media/files/lion/DxTSV6683MLl4VPAVUHR3_image.png"
     *       }
     *     ]
     */
    images: Components.File_1[];
    /**
     * Seed
     * @description Seed used for the random number generator
     * @example 123456
     */
    seed: number;
}

export interface IdeogramCharacterRemixInput {
    /** @description A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members) */
    color_palette?: Components.ColorPalette;
    /**
     * Expand Prompt
     * @description Determine if MagicPrompt should be used in generating the request or not.
     * @default true
     */
    expand_prompt?: boolean;
    /**
     * Image Size
     * @description The resolution of the generated image
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description The image URL to remix
     * @example https://v3.fal.media/files/panda/mcxydS-_4ZjfBWFtgoo2z_XHLsl7khq6dC6Qp3cIdJl08rG0I.avif
     */
    image_url: string;
    /**
     * Image Urls
     * @description A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format
     */
    image_urls?: string[];
    /**
     * Negative Prompt
     * @description Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The prompt to remix the image with
     * @example A glamorous portrait photograph of a woman in an elegant ballroom setting. The subject wears a champagne-colored ball gown with a fitted bodice, long sleeves, and a full skirt adorned with delicate lace appliques. The dress features a crystal-embellished hair accessory and pearl drop earrings. The grand staircase has ornate gold railings and leads to an elaborate crystal chandelier hanging from an arched ceiling. The walls are decorated with classical paintings featuring floral motifs. The lighting is warm and dramatic, creating a soft glow throughout the space. The composition is shot in a formal portrait style with the subject positioned on the lower landing of the staircase, looking over her shoulder at the camera.
     */
    prompt: string;
    /**
     * Reference Image Urls
     * @description A set of images to use as character references. Currently only 1 image is supported, rest will be ignored. (maximum total size 10MB across all character references). The images should be in JPEG, PNG or WebP format
     * @example [
     *       "https://v3.fal.media/files/kangaroo/0rinwnj_Kn9Fsu2dK-aKm_image.png"
     *     ]
     */
    reference_image_urls: string[];
    /**
     * Reference Mask Urls
     * @description A set of masks to apply to the character references. Currently only 1 mask is supported, rest will be ignored. (maximum total size 10MB across all character references). The masks should be in JPEG, PNG or WebP format
     */
    reference_mask_urls?: string[];
    /**
     * Rendering Speed
     * @description The rendering speed to use.
     * @default BALANCED
     * @enum {string}
     */
    rendering_speed?: 'TURBO' | 'BALANCED' | 'QUALITY';
    /**
     * Seed
     * @description Seed for the random number generator
     */
    seed?: number;
    /**
     * Strength
     * @description Strength of the input image in the remix
     * @default 0.8
     */
    strength?: number;
    /**
     * Style
     * @description The style type to generate with. Cannot be used with style_codes.
     * @default AUTO
     * @enum {string}
     */
    style?: 'AUTO' | 'REALISTIC' | 'FICTION';
    /**
     * Style Codes
     * @description A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style
     */
    style_codes?: string[];
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface IdeogramCharacterRemixOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/zebra/4F1SvlaPbkZt-Mle4CTH9_image.png"
     *       }
     *     ]
     */
    images: Components.File_1[];
    /**
     * Seed
     * @description Seed used for the random number generator
     * @example 123456
     */
    seed: number;
}

export interface IdeogramCharacterEditInput {
    /** @description A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members) */
    color_palette?: Components.ColorPalette;
    /**
     * Expand Prompt
     * @description Determine if MagicPrompt should be used in generating the request or not.
     * @default true
     */
    expand_prompt?: boolean;
    /**
     * Image URL
     * @description The image URL to generate an image from. MUST have the exact same dimensions (width and height) as the mask image.
     * @example https://v3.fal.media/files/panda/-LC_gNNV3wUHaGMQT3klE_output.png
     */
    image_url: string;
    /**
     * Image Urls
     * @description A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format
     */
    image_urls?: string[];
    /**
     * Mask URL
     * @description The mask URL to inpaint the image. MUST have the exact same dimensions (width and height) as the input image.
     * @example https://v3.fal.media/files/panda/jVDAgSkpsZFDP080ceSZJ_woman_face_mask.png
     */
    mask_url: string;
    /**
     * Num Images
     * @description Number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The prompt to fill the masked part of the image.
     * @example woman holding bag
     */
    prompt: string;
    /**
     * Reference Image Urls
     * @description A set of images to use as character references. Currently only 1 image is supported, rest will be ignored. (maximum total size 10MB across all character references). The images should be in JPEG, PNG or WebP format
     * @example [
     *       "https://v3.fal.media/files/kangaroo/0rinwnj_Kn9Fsu2dK-aKm_image.png"
     *     ]
     */
    reference_image_urls: string[];
    /**
     * Reference Mask Urls
     * @description A set of masks to apply to the character references. Currently only 1 mask is supported, rest will be ignored. (maximum total size 10MB across all character references). The masks should be in JPEG, PNG or WebP format
     */
    reference_mask_urls?: string[];
    /**
     * Rendering Speed
     * @description The rendering speed to use.
     * @default BALANCED
     * @enum {string}
     */
    rendering_speed?: 'TURBO' | 'BALANCED' | 'QUALITY';
    /**
     * Seed
     * @description Seed for the random number generator
     */
    seed?: number;
    /**
     * Style
     * @description The style type to generate with. Cannot be used with style_codes.
     * @default AUTO
     * @enum {string}
     */
    style?: 'AUTO' | 'REALISTIC' | 'FICTION';
    /**
     * Style Codes
     * @description A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style
     */
    style_codes?: string[];
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface IdeogramCharacterEditOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/zebra/JJR2zayRdL3Pg7kr9cFyk_image.png"
     *       }
     *     ]
     */
    images: Components.File_1[];
    /**
     * Seed
     * @description Seed used for the random number generator
     * @example 123456
     */
    seed: number;
}

export interface IdeogramCharacterInput {
    /** @description A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members) */
    color_palette?: Components.ColorPalette;
    /**
     * Expand Prompt
     * @description Determine if MagicPrompt should be used in generating the request or not.
     * @default true
     */
    expand_prompt?: boolean;
    /**
     * Image Size
     * @description The resolution of the generated image
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Urls
     * @description A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format
     */
    image_urls?: string[];
    /**
     * Negative Prompt
     * @description Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The prompt to fill the masked part of the image.
     * @example Place the woman leisurely enjoying a cup of espresso while relaxing at a sunlit café table in Siena, Italy. The café setting showcases vintage wooden furniture with peeling white paint, aged brick flooring, and sun-bleached stone walls decorated with trailing ivy and vibrant potted geraniums that capture Siena's medieval character. Golden late-morning light streams through overhead, creating soft shadows that highlight the weathered architectural details. The composition appears slightly off-center, conveying the unguarded tranquility and personal intimacy of a peaceful moment savoring the Tuscan morning ambiance.
     */
    prompt: string;
    /**
     * Reference Image Urls
     * @description A set of images to use as character references. Currently only 1 image is supported, rest will be ignored. (maximum total size 10MB across all character references). The images should be in JPEG, PNG or WebP format
     * @example [
     *       "https://v3.fal.media/files/kangaroo/0rinwnj_Kn9Fsu2dK-aKm_image.png"
     *     ]
     */
    reference_image_urls: string[];
    /**
     * Reference Mask Urls
     * @description A set of masks to apply to the character references. Currently only 1 mask is supported, rest will be ignored. (maximum total size 10MB across all character references). The masks should be in JPEG, PNG or WebP format
     */
    reference_mask_urls?: string[];
    /**
     * Rendering Speed
     * @description The rendering speed to use.
     * @default BALANCED
     * @enum {string}
     */
    rendering_speed?: 'TURBO' | 'BALANCED' | 'QUALITY';
    /**
     * Seed
     * @description Seed for the random number generator
     */
    seed?: number;
    /**
     * Style
     * @description The style type to generate with. Cannot be used with style_codes.
     * @default AUTO
     * @enum {string}
     */
    style?: 'AUTO' | 'REALISTIC' | 'FICTION';
    /**
     * Style Codes
     * @description A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style
     */
    style_codes?: string[];
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface IdeogramCharacterOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/monkey/NC_1eo9ecE9fARcxviJ2R_image.png"
     *       }
     *     ]
     */
    images: Components.File_1[];
    /**
     * Seed
     * @description Seed used for the random number generator
     * @example 123456
     */
    seed: number;
}

export interface IclightV2Input {
    /**
     * Background Threshold
     * @description Threshold for the background removal algorithm. A high threshold will produce sharper masks. Note: This parameter is currently deprecated and has no effect on the output.
     * @default 0.67
     */
    background_threshold?: number;
    /**
     * Cfg
     * @description The real classifier-free-guidance scale for the generation.
     * @default 1
     */
    cfg?: number;
    /**
     * Enable Hr Fix
     * @description Use HR fix
     * @default false
     */
    enable_hr_fix?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Highres Denoise
     * @description Strength for high-resolution pass. Only used if enable_hr_fix is True.
     * @default 0.95
     */
    highres_denoise?: number;
    /**
     * Hr Downscale
     * @default 0.5
     */
    hr_downscale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description URL of image to be used for relighting
     * @example https://storage.googleapis.com/falserverless/iclight-v2/bottle.png
     */
    image_url: string;
    /**
     * Initial Latent
     * @description Provide lighting conditions for the model
     * @default None
     * @enum {string}
     */
    initial_latent?: 'None' | 'Left' | 'Right' | 'Top' | 'Bottom';
    /**
     * Lowres Denoise
     * @description Strength for low-resolution pass.
     * @default 0.98
     */
    lowres_denoise?: number;
    /**
     * Mask Image Url
     * @description URL of mask to be used for ic-light conditioning image
     */
    mask_image_url?: string;
    /**
     * Negative Prompt
     * @description Negative Prompt for the image
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example perfume bottle in a volcano surrounded by lava.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface IclightV2Output extends SharedType_a73 {}

export interface Hyper3dRodinV2Input {
    /**
     * Addons
     * @description The HighPack option will provide 4K resolution textures instead of the default 1K, as well as models with high-poly. It will cost **triple the billable units**.
     * @enum {string}
     */
    addons?: 'HighPack';
    /**
     * Bbox Condition
     * @description An array that specifies the bounding box dimensions [width, height, length].
     * @example [
     *       100,
     *       50,
     *       150
     *     ]
     */
    bbox_condition?: number[];
    /**
     * Geometry File Format
     * @description Format of the geometry file. Possible values: glb, usdz, fbx, obj, stl. Default is glb.
     * @default glb
     * @enum {string}
     */
    geometry_file_format?: 'glb' | 'usdz' | 'fbx' | 'obj' | 'stl';
    /**
     * Input Image Urls
     * @description URL of images to use while generating the 3D model. Required for Image-to-3D mode. Up to 5 images allowed.
     * @example [
     *       "https://v3.fal.media/files/panda/l7mQrG8plbB42lBNqVjm0_image.png",
     *       "https://v3b.fal.media/files/b/kangaroo/scq50Bf1PB2NZOW8szphV_image.png",
     *       "https://v3.fal.media/files/penguin/X21qtlVMazAtljzRCJD2__image.png"
     *     ]
     */
    input_image_urls?: string[];
    /**
     * Material
     * @description Material type. PBR: Physically-based materials with realistic lighting. Shaded: Simple materials with baked lighting. All: Both types included.
     * @default All
     * @enum {string}
     */
    material?: 'PBR' | 'Shaded' | 'All';
    /**
     * Preview Render
     * @description Generate a preview render image of the 3D model along with the model files.
     * @default false
     */
    preview_render?: boolean;
    /**
     * Prompt
     * @description A textual prompt to guide model generation. Optional for Image-to-3D mode - if empty, AI will generate a prompt based on your images.
     * @default
     * @example A futuristic robot with sleek metallic design.
     */
    prompt?: string;
    /**
     * Quality Mesh Option
     * @description Combined quality and mesh type selection. Quad = smooth surfaces, Triangle = detailed geometry. These corresponds to `mesh_mode` (if the option contains 'Triangle', mesh_mode is 'Raw', otherwise 'Quad') and `quality_override` (the numeric part of the option) parameters in Hyper3D API.
     * @default 500K Triangle
     * @enum {string}
     */
    quality_mesh_option?:
        | '4K Quad'
        | '8K Quad'
        | '18K Quad'
        | '50K Quad'
        | '2K Triangle'
        | '20K Triangle'
        | '150K Triangle'
        | '500K Triangle';
    /**
     * Seed
     * @description Seed value for randomization, ranging from 0 to 65535. Optional.
     */
    seed?: number;
    /**
     * T/A Pose
     * @description Generate characters in T-pose or A-pose format, making them easier to rig and animate in 3D software.
     * @default false
     */
    TAPose?: boolean;
    /**
     * Use Original Alpha
     * @description When enabled, preserves the transparency channel from input images during 3D generation.
     * @default false
     */
    use_original_alpha?: boolean;
}

export interface Hyper3dRodinV2Output {
    /**
     * Model Mesh
     * @description Generated 3D object file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/video_models/base_basic_shaded.glb"
     *     }
     */
    model_mesh: Components.File;
    /**
     * Seed
     * @description Seed value used for generation.
     */
    seed: number;
    /**
     * Textures
     * @description Generated textures for the 3D object.
     */
    textures: Components.Image[];
}

export interface Hyper3dRodinInput {
    /**
     * Addons
     * @description Generation add-on features. Default is []. Possible values are HighPack. The HighPack option will provide 4K resolution textures instead of the default 1K, as well as models with high-poly. It will cost triple the billable units.
     * @enum {string}
     */
    addons?: 'HighPack';
    /**
     * Bbox Condition
     * @description An array that specifies the dimensions and scaling factor of the bounding box. Typically, this array contains 3 elements, Length(X-axis), Width(Y-axis) and Height(Z-axis).
     * @example [
     *       100,
     *       50,
     *       150
     *     ]
     */
    bbox_condition?: number[];
    /**
     * Condition Mode
     * @description For fuse mode, One or more images are required.It will generate a model by extracting and fusing features of objects from multiple images.For concat mode, need to upload multiple multi-view images of the same object and generate the model. (You can upload multi-view images in any order, regardless of the order of view.)
     * @default concat
     * @enum {string}
     */
    condition_mode?: 'fuse' | 'concat';
    /**
     * Geometry File Format
     * @description Format of the geometry file. Possible values: glb, usdz, fbx, obj, stl. Default is glb.
     * @default glb
     * @enum {string}
     */
    geometry_file_format?: 'glb' | 'usdz' | 'fbx' | 'obj' | 'stl';
    /**
     * Input Image Urls
     * @description URL of images to use while generating the 3D model. Required for Image-to-3D mode. Optional for Text-to-3D mode.
     * @example https://storage.googleapis.com/falserverless/model_tests/video_models/robot.png
     */
    input_image_urls?: string[];
    /**
     * Material
     * @description Material type. Possible values: PBR, Shaded. Default is PBR.
     * @default PBR
     * @example Shaded
     * @enum {string}
     */
    material?: 'PBR' | 'Shaded';
    /**
     * Prompt
     * @description A textual prompt to guide model generation. Required for Text-to-3D mode. Optional for Image-to-3D mode.
     * @default
     * @example A futuristic robot with sleek metallic design.
     */
    prompt?: string;
    /**
     * Quality
     * @description Generation quality. Possible values: high, medium, low, extra-low. Default is medium.
     * @default medium
     * @enum {string}
     */
    quality?: 'high' | 'medium' | 'low' | 'extra-low';
    /**
     * Seed
     * @description Seed value for randomization, ranging from 0 to 65535. Optional.
     */
    seed?: number;
    /**
     * T/A Pose
     * @description When generating the human-like model, this parameter control the generation result to T/A Pose.
     * @default false
     */
    TAPose?: boolean;
    /**
     * Tier
     * @description Tier of generation. For Rodin Sketch, set to Sketch. For Rodin Regular, set to Regular.
     * @default Regular
     * @enum {string}
     */
    tier?: 'Regular' | 'Sketch';
    /**
     * Use Hyper
     * @description Whether to export the model using hyper mode. Default is false.
     * @default false
     */
    use_hyper?: boolean;
}

export interface Hyper3dRodinOutput {
    /**
     * Model Mesh
     * @description Generated 3D object file.
     * @example {
     *       "url": "https://v3.fal.media/files/koala/VlX4JqNI8F9HO2ETp_B7t_base_basic_pbr.glb"
     *     }
     */
    model_mesh: Components.File;
    /**
     * Seed
     * @description Seed value used for generation.
     */
    seed: number;
    /**
     * Textures
     * @description Generated textures for the 3D object.
     */
    textures: Components.Image[];
}

export interface Hunyuan3dV2TurboInput extends SharedType_df1 {}

export interface Hunyuan3dV2TurboOutput extends SharedType_e39 {}

export interface Hunyuan3dV2MultiViewTurboInput extends SharedType_fc4 {}

export interface Hunyuan3dV2MultiViewTurboOutput extends SharedType_5bd {}

export interface Hunyuan3dV2MultiViewInput extends SharedType_fc4 {}

export interface Hunyuan3dV2MultiViewOutput extends SharedType_5bd {}

export interface Hunyuan3dV2MiniTurboInput extends SharedType_df1 {}

export interface Hunyuan3dV2MiniTurboOutput extends SharedType_e39 {}

export interface Hunyuan3dV2MiniInput extends SharedType_df1 {}

export interface Hunyuan3dV2MiniOutput extends SharedType_e39 {}

export interface Hunyuan3dV2Input extends SharedType_df1 {}

export interface Hunyuan3dV2Output extends SharedType_e39 {}

export interface Hunyuan3dV3TextTo3dInput {
    /**
     * Enable Pbr
     * @description Whether to enable PBR material generation
     * @default false
     */
    enable_pbr?: boolean;
    /**
     * Face Count
     * @description Target face count. Range: 40000-1500000
     * @default 500000
     */
    face_count?: number;
    /**
     * Generate Type
     * @description Generation type. Normal: textured model. LowPoly: polygon reduction. Geometry: white model without texture.
     * @default Normal
     * @enum {string}
     */
    generate_type?: 'Normal' | 'LowPoly' | 'Geometry';
    /**
     * Polygon Type
     * @description Polygon type. Only takes effect when GenerateType is LowPoly.
     * @default triangle
     * @enum {string}
     */
    polygon_type?: 'triangle' | 'quadrilateral';
    /**
     * Prompt
     * @description Text description of the 3D content to generate. Supports up to 1024 UTF-8 characters.
     * @example A rustic wooden treasure chest with metal bands and ornate lock
     */
    prompt: string;
}

export interface Hunyuan3dV3TextTo3dOutput {
    /**
     * Model Glb
     * @description Generated 3D object in GLB format.
     * @example {
     *       "file_size": 64724836,
     *       "file_name": "model.glb",
     *       "content_type": "model/gltf-binary",
     *       "url": "https://v3b.fal.media/files/b/0a8686a8/1hPquv3AqqkfnqSM9fpmB_model.glb"
     *     }
     */
    model_glb: Components.File;
    /**
     * Model Urls
     * @description URLs for different 3D model formats
     * @example {
     *       "glb": {
     *         "file_size": 64724836,
     *         "file_name": "model.glb",
     *         "content_type": "model/gltf-binary",
     *         "url": "https://v3b.fal.media/files/b/0a8686a8/1hPquv3AqqkfnqSM9fpmB_model.glb"
     *       },
     *       "obj": {
     *         "file_size": 44084728,
     *         "file_name": "model.obj",
     *         "content_type": "text/plain",
     *         "url": "https://v3b.fal.media/files/b/0a8686a8/AVgdsVFrGAKGAFr4e2g56_model.obj"
     *       }
     *     }
     */
    model_urls: Components.ModelUrls_2;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed?: number;
    /**
     * Thumbnail
     * @description Preview thumbnail of the generated model
     * @example {
     *       "file_size": 172757,
     *       "file_name": "preview.png",
     *       "content_type": "image/png",
     *       "url": "https://v3b.fal.media/files/b/0a8686a8/khgYO1d6xqWOJPi6_IR_j_preview.png"
     *     }
     */
    thumbnail?: Components.File;
}

export interface Hunyuan3dV3SketchTo3dInput {
    /**
     * Enable Pbr
     * @description Whether to enable PBR material generation.
     * @default false
     */
    enable_pbr?: boolean;
    /**
     * Face Count
     * @description Target face count. Range: 40000-1500000
     * @default 500000
     */
    face_count?: number;
    /**
     * Input Image Url
     * @description URL of sketch or line art image to transform into a 3D model. Image resolution must be between 128x128 and 5000x5000 pixels.
     * @example https://v3b.fal.media/files/b/0a86888c/Zlw8twOa43SKkCXmTdw3-.png
     */
    input_image_url: string;
    /**
     * Prompt
     * @description Text prompt describing the 3D content attributes such as color, category, and material.
     * @example orange cat
     */
    prompt: string;
}

export interface Hunyuan3dV3SketchTo3dOutput {
    /**
     * Model Glb
     * @description Generated 3D object in GLB format.
     * @example {
     *       "file_size": 30655724,
     *       "file_name": "model.glb",
     *       "content_type": "model/gltf-binary",
     *       "url": "https://v3b.fal.media/files/b/0a8688bb/vd2SlBP92cZls3zG5EPbg_model.glb"
     *     }
     */
    model_glb: Components.File;
    /**
     * Model Urls
     * @description URLs for different 3D model formats
     * @example {
     *       "glb": {
     *         "file_size": 30655724,
     *         "file_name": "model.glb",
     *         "content_type": "model/gltf-binary",
     *         "url": "https://v3b.fal.media/files/b/0a8688bb/vd2SlBP92cZls3zG5EPbg_model.glb"
     *       },
     *       "obj": {
     *         "file_size": 23418473,
     *         "file_name": "model.obj",
     *         "content_type": "text/plain",
     *         "url": "https://v3b.fal.media/files/b/0a8688bb/QNik1DVxzvj23YEF3vhs__model.obj"
     *       }
     *     }
     */
    model_urls: Components.ModelUrls_2;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed?: number;
    /**
     * Thumbnail
     * @description Preview thumbnail of the generated model
     * @example {
     *       "file_size": 68478,
     *       "file_name": "preview.png",
     *       "content_type": "image/png",
     *       "url": "https://v3b.fal.media/files/b/0a8688bb/ZkMb4jHnb5QRNYp4SxkEA_preview.png"
     *     }
     */
    thumbnail?: Components.File;
}

export interface Hunyuan3dV3ImageTo3dInput {
    /**
     * Back Image Url
     * @description Optional back view image URL for better 3D reconstruction.
     */
    back_image_url?: string;
    /**
     * Enable Pbr
     * @description Whether to enable PBR material generation. Does not take effect when generate_type is Geometry.
     * @default false
     */
    enable_pbr?: boolean;
    /**
     * Face Count
     * @description Target face count. Range: 40000-1500000
     * @default 500000
     */
    face_count?: number;
    /**
     * Generate Type
     * @description Generation type. Normal: textured model. LowPoly: polygon reduction. Geometry: white model without texture.
     * @default Normal
     * @enum {string}
     */
    generate_type?: 'Normal' | 'LowPoly' | 'Geometry';
    /**
     * Input Image Url
     * @description URL of image to use while generating the 3D model.
     * @example https://v3b.fal.media/files/b/0a865ab1/omYcawLUo4RZbO8J6ZgZR.png
     */
    input_image_url: string;
    /**
     * Left Image Url
     * @description Optional left view image URL for better 3D reconstruction.
     */
    left_image_url?: string;
    /**
     * Polygon Type
     * @description Polygon type. Only takes effect when GenerateType is LowPoly.
     * @default triangle
     * @enum {string}
     */
    polygon_type?: 'triangle' | 'quadrilateral';
    /**
     * Right Image Url
     * @description Optional right view image URL for better 3D reconstruction.
     */
    right_image_url?: string;
}

export interface Hunyuan3dV3ImageTo3dOutput {
    /**
     * Model Glb
     * @description Generated 3D object in GLB format.
     * @example {
     *       "file_size": 64122888,
     *       "file_name": "model.glb",
     *       "content_type": "model/gltf-binary",
     *       "url": "https://v3b.fal.media/files/b/0a8686ae/MQN_KtP32PbqtPr_VLcyp_model.glb"
     *     }
     */
    model_glb: Components.File;
    /**
     * Model Urls
     * @description URLs for different 3D model formats
     * @example {
     *       "glb": {
     *         "file_size": 64122888,
     *         "file_name": "model.glb",
     *         "content_type": "model/gltf-binary",
     *         "url": "https://v3b.fal.media/files/b/0a8686ae/MQN_KtP32PbqtPr_VLcyp_model.glb"
     *       },
     *       "obj": {
     *         "file_size": 42886419,
     *         "file_name": "model.obj",
     *         "content_type": "text/plain",
     *         "url": "https://v3b.fal.media/files/b/0a8686ad/ifdJskhUfQysq-NN20iQR_model.obj"
     *       }
     *     }
     */
    model_urls: Components.ModelUrls_2;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed?: number;
    /**
     * Thumbnail
     * @description Preview thumbnail of the generated model
     * @example {
     *       "file_size": 74443,
     *       "file_name": "preview.png",
     *       "content_type": "image/png",
     *       "url": "https://v3b.fal.media/files/b/0a8686ae/sGIaYWOna5Zabtl5PBjDt_preview.png"
     *     }
     */
    thumbnail?: Components.File;
}

export interface Hunyuan3dV21Input extends SharedType_df1 {}

export interface Hunyuan3dV21Output {
    /**
     * Model Glb
     * @description Generated 3D object.
     * @example {
     *       "file_size": 1348528,
     *       "file_name": "textured_mesh.glb",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3.fal.media/files/rabbit/WpMHqYy5chA5lsTNoilj__hun3d_v21.glb"
     *     }
     */
    model_glb: Components.File;
    /**
     * Model Glb Pbr
     * @description Generated 3D object with PBR materials.
     */
    model_glb_pbr?: Components.File;
    /**
     * Model Mesh
     * @description Generated 3D object assets zip.
     */
    model_mesh: Components.File;
    /**
     * Seed
     * @description Seed value used for generation.
     */
    seed: number;
}

export interface HunyuanVideoVideoToVideoInput {
    /**
     * Aspect Ratio (W:H)
     * @description The aspect ratio of the video to generate.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 129
     * @enum {string}
     */
    num_frames?: '129' | '85';
    /**
     * Num Inference Steps
     * @description The number of inference steps to run. Lower gets faster results, higher gets better results.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Pro Mode
     * @description By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.
     * @default false
     */
    pro_mode?: boolean;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a dark blue leather jacket, a long pink dress, and bright yellow boots, and carries a black purse.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the video to generate.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Seed
     * @description The seed to use for generating the video.
     */
    seed?: number;
    /**
     * Strength
     * @description Strength for Video-to-Video
     * @default 0.85
     */
    strength?: number;
    /**
     * Video Url
     * @description URL of the video input.
     * @example https://storage.googleapis.com/falserverless/hunyuan_video/hunyuan_v2v_input.mp4
     */
    video_url: string;
}

export interface HunyuanVideoVideoToVideoOutput extends SharedType_fbd {}

export interface HunyuanVideoV15TextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the video.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16';
    /**
     * Enable Prompt Expansion
     * @description Enable prompt expansion to enhance the input prompt.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to guide what not to generate.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the video.
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the video.
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p';
    /**
     * Seed
     * @description Random seed for reproducibility.
     */
    seed?: number;
}

export interface HunyuanVideoV15TextToVideoOutput extends SharedType_a92 {}

export interface HunyuanVideoV15ImageToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the video.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16';
    /**
     * Enable Prompt Expansion
     * @description Enable prompt expansion to enhance the input prompt.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Image Url
     * @description URL of the reference image for image-to-video generation.
     * @example https://v3.fal.media/files/panda/HnY2yf-BbzlrVQxR-qP6m_9912d0932988453aadf3912fc1901f52.jpg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to guide what not to generate.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description The number of frames to generate.
     * @default 121
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A stark starting line divides two powerful cars, engines revving for the challenge ahead. They surge forward in the heat of competition, a blur of speed and chrome. The finish line looms as they race for victory.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the video.
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p';
    /**
     * Seed
     * @description Random seed for reproducibility.
     */
    seed?: number;
}

export interface HunyuanVideoV15ImageToVideoOutput extends SharedType_a92 {}

export interface HunyuanVideoLoraVideoToVideoInput {
    /**
     * Aspect Ratio (W:H)
     * @description The aspect ratio of the video to generate.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 129
     * @enum {string}
     */
    num_frames?: '129' | '85';
    /**
     * Pro Mode
     * @description By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.
     * @default false
     */
    pro_mode?: boolean;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a dark blue leather jacket, a long pink dress, and bright yellow boots, and carries a black purse.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the video to generate.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Seed
     * @description The seed to use for generating the video.
     */
    seed?: number;
    /**
     * Strength
     * @description Strength of video-to-video
     * @default 0.75
     */
    strength?: number;
    /**
     * Video Url
     * @description URL of the video
     * @example https://storage.googleapis.com/falserverless/hunyuan_video/hunyuan_v2v_input.mp4
     */
    video_url: string;
}

export interface HunyuanVideoLoraVideoToVideoOutput {
    /**
     * Seed
     * @description The seed used for generating the video.
     */
    seed: number;
    /**
     * Video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/hunyuan_video/hunyuan_v2v_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface HunyuanVideoLoraTrainingInput {
    /**
     * Data Archive Format
     * @description The format of the archive. If not specified, the format will be inferred from the URL.
     */
    data_archive_format?: string;
    /**
     * Do Caption
     * @description Whether to generate captions for the images.
     * @default true
     */
    do_caption?: boolean;
    /**
     * Images Data Url
     * @description URL to zip archive with images. Try to use at least 4 images in general the more the better.
     *
     *             In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.
     */
    images_data_url: string;
    /**
     * Learning Rate
     * @description Learning rate to use for training.
     * @default 0.0001
     */
    learning_rate?: number;
    /**
     * Steps
     * @description Number of steps to train the LoRA on.
     * @example 1000
     */
    steps: number;
    /**
     * Trigger Word
     * @description The trigger word to use.
     * @default
     */
    trigger_word?: string;
}

export interface HunyuanVideoLoraTrainingOutput {
    /** @description URL to the lora configuration file. */
    config_file: Components.File_1;
    /** @description URL to the trained diffusers lora weights. */
    diffusers_lora_file: Components.File_1;
}

export interface HunyuanVideoLoraInput {
    /**
     * Aspect Ratio (W:H)
     * @description The aspect ratio of the video to generate.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 129
     * @enum {string}
     */
    num_frames?: '129' | '85';
    /**
     * Pro Mode
     * @description By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.
     * @default false
     */
    pro_mode?: boolean;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the video to generate.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Seed
     * @description The seed to use for generating the video.
     */
    seed?: number;
}

export interface HunyuanVideoLoraOutput extends SharedType_fbd {}

export interface HunyuanVideoImg2vidLoraInput {
    /**
     * Image URL
     * @description The URL to the image to generate the video from. The image must be 960x544 or it will get cropped and resized to that size.
     * @example https://d3phaj0sisr2ct.cloudfront.net/research/eugene.jpg
     */
    image_url: string;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A low angle shot of a man walking down a street, illuminated by the neon signs of the bars around him
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for generating the video.
     */
    seed?: number;
}

export interface HunyuanVideoImg2vidLoraOutput {
    /**
     * Seed
     * @description The seed used for generating the video.
     */
    seed: number;
    /**
     * Video
     * @description The generated video
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/gallery/man-smiles.mp4"
     *     }
     */
    video: Components.File;
}

export interface HunyuanVideoImageToVideoInput {
    /**
     * Aspect Ratio (W:H)
     * @description The aspect ratio of the video to generate.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16';
    /**
     * I2V Stability
     * @description Turning on I2V Stability reduces hallucination but also reduces motion.
     * @default false
     */
    i2v_stability?: boolean;
    /**
     * Image Url
     * @description URL of the image input.
     * @example https://storage.googleapis.com/falserverless/example_inputs/hunyuan_i2v.jpg
     */
    image_url: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 129
     * @enum {string}
     */
    num_frames?: '129';
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example Two muscular cats boxing in a boxing ring.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the video to generate.
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p';
    /**
     * Seed
     * @description The seed to use for generating the video.
     */
    seed?: number;
}

export interface HunyuanVideoImageToVideoOutput {
    /**
     * Seed
     * @description The seed used for generating the video.
     */
    seed: number;
    video: Components.File;
}

export interface HunyuanVideoFoleyInput {
    /**
     * Guidance Scale
     * @description Guidance scale for audio generation.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Negative Prompt
     * @description Negative prompt to avoid certain audio characteristics.
     * @default noisy, harsh
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps for generation.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Seed
     * @description Random seed for reproducible generation.
     */
    seed?: number;
    /**
     * Text Prompt
     * @description Text description of the desired audio (optional).
     * @example A person walks on frozen ice
     * @example The crackling of fire and whooshing of flames
     * @example Gentle footsteps on wooden floor
     */
    text_prompt: string;
    /**
     * Video Url
     * @description The URL of the video to generate audio for.
     * @example https://storage.googleapis.com/falserverless/model_tests/video_models/1_video.mp4
     */
    video_url: string;
}

export interface HunyuanVideoFoleyOutput {
    /**
     * Video
     * @description List of generated video files with audio.
     */
    video: Components.File;
}

export interface HunyuanVideoInput {
    /**
     * Aspect Ratio (W:H)
     * @description The aspect ratio of the video to generate.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 129
     * @enum {string}
     */
    num_frames?: '129' | '85';
    /**
     * Num Inference Steps
     * @description The number of inference steps to run. Lower gets faster results, higher gets better results.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Pro Mode
     * @description By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.
     * @default false
     */
    pro_mode?: boolean;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the video to generate.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Seed
     * @description The seed to use for generating the video.
     */
    seed?: number;
}

export interface HunyuanVideoOutput extends SharedType_fbd {}

export interface HunyuanPortraitInput {
    /**
     * Image Url
     * @description The URL of the source image.
     * @example https://v3.fal.media/files/elephant/GG7iU-4GmzkX3_gIXutRV_image.png
     */
    image_url: string;
    /**
     * Seed
     * @description Random seed for generation. If None, a random seed will be used.
     */
    seed?: number;
    /**
     * Use Arcface
     * @description Whether to use ArcFace for face recognition.
     * @default true
     */
    use_arcface?: boolean;
    /**
     * Video Url
     * @description The URL of the driving video.
     * @example https://v3.fal.media/files/panda/2GQH1q-bJOamqCGWMtKvS_what_if.mp4
     */
    video_url: string;
}

export interface HunyuanPortraitOutput {
    /**
     * Video
     * @description The generated video with the portrait animation.
     * @example {
     *       "file_size": 5485412,
     *       "file_name": "output_with_audio.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3.fal.media/files/tiger/9H3vkuqNoYcKr6OBGj3Mr_3b01c7a4-802c-4697-b8fb-ad86bd0eba41_output_with_audio.mp4"
     *     }
     */
    video: Components.File;
}

export interface HunyuanPartInput {
    /**
     * Model File Url
     * @description URL of the 3D model file (.glb or .obj) to process for segmentation.
     * @example https://storage.googleapis.com/falserverless/model_tests/video_models/base_basic_shaded.glb
     */
    model_file_url: string;
    /**
     * Noise Std
     * @description Standard deviation of noise to add to sampled points.
     * @default 0
     */
    noise_std?: number;
    /**
     * Point Num
     * @description Number of points to sample from the mesh.
     * @default 100000
     */
    point_num?: number;
    /**
     * Point Prompt X
     * @description X coordinate of the point prompt for segmentation (normalized space -1 to 1).
     * @default 0
     */
    point_prompt_x?: number;
    /**
     * Point Prompt Y
     * @description Y coordinate of the point prompt for segmentation (normalized space -1 to 1).
     * @default 0
     */
    point_prompt_y?: number;
    /**
     * Point Prompt Z
     * @description Z coordinate of the point prompt for segmentation (normalized space -1 to 1).
     * @default 0
     */
    point_prompt_z?: number;
    /**
     * Seed
     * @description The same seed and input will produce the same segmentation results.
     */
    seed?: number;
    /**
     * Use Normal
     * @description Whether to use normal information for segmentation.
     * @default true
     */
    use_normal?: boolean;
}

export interface HunyuanPartOutput {
    /**
     * Best Mask Index
     * @description Index of the best mask (1, 2, or 3) based on IoU score.
     */
    best_mask_index: number;
    /**
     * Iou Scores
     * @description IoU scores for each of the three masks.
     */
    iou_scores: number[];
    /**
     * Mask 1 Mesh
     * @description Mesh showing segmentation mask 1.
     */
    mask_1_mesh: Components.File;
    /**
     * Mask 2 Mesh
     * @description Mesh showing segmentation mask 2.
     */
    mask_2_mesh: Components.File;
    /**
     * Mask 3 Mesh
     * @description Mesh showing segmentation mask 3.
     */
    mask_3_mesh: Components.File;
    /**
     * Seed
     * @description Seed value used for generation.
     */
    seed: number;
    /**
     * Segmented Mesh
     * @description Segmented 3D mesh with mask applied.
     */
    segmented_mesh: Components.File;
}

export interface HunyuanMotionFastInput extends SharedType_fb3 {}

export interface HunyuanMotionFastOutput extends SharedType_b96 {}

export interface HunyuanMotionInput extends SharedType_fb3 {}

export interface HunyuanMotionOutput extends SharedType_b96 {}

export interface HunyuanImageV3TextToImageInput {
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
     * @default false
     * @example true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Controls how much the model adheres to the prompt. Higher values mean stricter adherence.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The desired size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to guide the image generation away from certain concepts.
     * @default
     * @example blurry, low quality, watermark, signature
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description Number of denoising steps.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The text prompt for image-to-image.
     * @example 200mm telephoto through crowd gaps; subject laughing, candid; creamy background compression, color pop from a single bold garment, catchlight in eyes.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducible results. If None, a random seed is used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface HunyuanImageV3TextToImageOutput {
    /**
     * Images
     * @description A list of the generated images.
     * @example [
     *       {
     *         "content_type": "image/png",
     *         "url": "https://v3b.fal.media/files/b/kangaroo/uIKrZHT6LaGqgXoxtBSn7.png"
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Seed
     * @description The base seed used for the generation process.
     */
    seed: number;
}

export interface HunyuanImageV3InstructTextToImageInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Controls how much the model adheres to the prompt. Higher values mean stricter adherence.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The desired size of the generated image. If auto, image size will be determined by the model.
     * @default auto
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The text prompt to generate an image from.
     * @example Macro photograph of a peacock feather showing individual barbs, barbules, and the iridescent microstructure creating color.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducible results. If None, a random seed is used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface HunyuanImageV3InstructTextToImageOutput {
    /**
     * Images
     * @description A list of the generated images.
     * @example [
     *       {
     *         "content_type": "image/png",
     *         "url": "https://v3b.fal.media/files/b/0a8c37ef/niSZAZpVwwixexdtx1AAf.png"
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Seed
     * @description The base seed used for the generation process.
     */
    seed: number;
}

export interface HunyuanImageV3InstructEditInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Controls how much the model adheres to the prompt. Higher values mean stricter adherence.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The desired size of the generated image. If auto, image size will be determined by the model.
     * @default auto
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Urls
     * @description The URLs of the images to use as a reference for the generation. A maximum of 2 images are supported.
     * @example [
     *       "https://v3b.fal.media/files/b/0a8c3cf9/voXcX8hd5maRSrSvZalXh_jAMqrScj.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The text prompt to generate an image from.
     * @example Turn this artwork into a realistic image
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducible results. If None, a random seed is used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface HunyuanImageV3InstructEditOutput {
    /**
     * Images
     * @description A list of the generated images.
     * @example [
     *       {
     *         "content_type": "image/png",
     *         "url": "https://v3b.fal.media/files/b/0a8c3d5f/K7ecUzkFaxU-5hvzdhvG-_SOvArpR4.png"
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Seed
     * @description The base seed used for the generation process.
     */
    seed: number;
}

export interface HunyuanImageV21TextToImageInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Controls how much the model adheres to the prompt. Higher values mean stricter adherence.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The desired size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to guide the image generation away from certain concepts.
     * @default
     * @example blurry, low quality, watermark, signature
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description Number of denoising steps.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The text prompt to generate an image from.
     * @example A cute, cartoon-style anthropomorphic penguin plush toy, standing in a painting studio, wearing a red knitted scarf and beret.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducible results. If None, a random seed is used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Refiner
     * @description Enable the refiner model for improved image quality.
     * @default false
     */
    use_refiner?: boolean;
    /**
     * Use Reprompt
     * @description Enable prompt enhancement for potentially better results.
     * @default true
     */
    use_reprompt?: boolean;
}

export interface HunyuanImageV21TextToImageOutput {
    /**
     * Images
     * @description A list of the generated images.
     * @example {
     *       "content_type": "image/png",
     *       "url": "https://v3.fal.media/files/zebra/WCrMfUTYp6mYCf6yRE3kw_generated_image_0.png"
     *     }
     */
    images: Components.Image[];
    /**
     * Seed
     * @description The base seed used for the generation process.
     */
    seed: number;
}

export interface HunyuanCustomInput {
    /**
     * Aspect Ratio (W:H)
     * @description The aspect ratio of the video to generate.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16';
    /**
     * CFG Scale
     * @description Classifier-Free Guidance scale for the generation.
     * @default 7.5
     */
    cfg_scale?: number;
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     * @example true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames per second
     * @description The frames per second of the generated video.
     * @default 25
     */
    fps?: number;
    /**
     * Image Url
     * @description URL of the image input.
     * @example https://storage.googleapis.com/falserverless/model_tests/hidream/woman.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion, blurring, text, subtitles, static, picture, black border.
     * @example Ugly, blurry.
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 129
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to run. Lower gets faster results, higher gets better results.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description Text prompt for video generation (max 500 characters).
     * @example Realistic, High-quality. A woman is playing a violin.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations.
     * @default 512p
     * @enum {string}
     */
    resolution?: '512p' | '720p';
    /**
     * Seed
     * @description The seed to use for generating the video.
     */
    seed?: number;
}

export interface HunyuanCustomOutput {
    /**
     * Seed
     * @description The seed used for generating the video.
     */
    seed: number;
    /**
     * Video
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/test/p1/uQ4ddGyJ9U6cymnns0l6o_input-image-1747117169.mp4"
     *     }
     */
    video: Components.File;
}

export interface HunyuanAvatarInput {
    /**
     * Audio Url
     * @description The URL of the audio file.
     * @example https://v3.fal.media/files/koala/80RpP2FOhXZUV3NRKUWZu_2.WAV
     */
    audio_url: string;
    /**
     * Image Url
     * @description The URL of the reference image.
     * @example https://fal.media/files/tiger/Y8EgvVqxORBCqWC1OlX3D_3c4c8bbe7f3941a2aea93e278ba14803.jpg
     * @example https://v3.fal.media/files/zebra/HWILyw2UYI50Sp_4mDxqr_src2.png
     */
    image_url: string;
    /**
     * Num Frames
     * @description Number of video frames to generate at 25 FPS. If greater than the input audio length, it will capped to the length of the input audio.
     * @default 129
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Seed
     * @description Random seed for generation.
     */
    seed?: number;
    /**
     * Text
     * @description Text prompt describing the scene.
     * @default A cat is singing.
     */
    text?: string;
    /**
     * Turbo Mode
     * @description If true, the video will be generated faster with no noticeable degradation in the visual quality.
     * @default true
     */
    turbo_mode?: boolean;
}

export interface HunyuanAvatarOutput {
    /**
     * Video
     * @description The generated video with the avatar animation.
     * @example {
     *       "file_size": 1646349,
     *       "file_name": "output_with_audio.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3.fal.media/files/monkey/3ODbdqHHQL3SvgRXEJXQ-_hunava_8333d613-d4e3-42ff-be36-1e97775621ba_audio.mp4"
     *     }
     */
    video: Components.File;
}

export interface Hunyuan3dV31SmartTopologyInput {
    /**
     * Face Level
     * @description Target polygon density. high: more detail/polygons, medium: balanced, low: fewer polygons.
     * @default medium
     * @enum {string}
     */
    face_level?: 'high' | 'medium' | 'low';
    /**
     * Input File Type
     * @description Input 3D file format.
     * @default glb
     * @enum {string}
     */
    input_file_type?: 'glb' | 'obj';
    /**
     * Input File Url
     * @description URL of GLB or OBJ file to optimize topology. Max size: 200MB.
     * @default https://v3b.fal.media/files/b/0a8c09c0/VYDiCTcDGK55qY2-idGbX_model.glb
     * @example https://v3b.fal.media/files/b/0a8c09c0/VYDiCTcDGK55qY2-idGbX_model.glb
     */
    input_file_url?: string;
    /**
     * Polygon Type
     * @description Output polygon type. triangle: triangular faces only. quadrilateral: mixed quad and triangle faces.
     * @default triangle
     * @enum {string}
     */
    polygon_type?: 'triangle' | 'quadrilateral';
}

export interface Hunyuan3dV31SmartTopologyOutput {
    /**
     * Model Glb
     * @description Processed 3D model with optimized topology (primary file).
     * @example {
     *       "file_size": 394409,
     *       "file_name": "model.obj",
     *       "content_type": "model/obj",
     *       "url": "https://v3b.fal.media/files/b/0a8c0ab4/tqMY5NJLnHjpwN8rQ15dj_model.obj"
     *     }
     */
    model_glb: Components.File;
    /**
     * Model Urls
     * @description URLs for different 3D model formats
     * @example {
     *       "glb": {
     *         "file_size": 206004,
     *         "file_name": "model.glb",
     *         "content_type": "model/gltf-binary",
     *         "url": "https://v3b.fal.media/files/b/0a8c0ab4/eX-_x0Wv8fZL05l9CGp6Y_model.glb"
     *       },
     *       "obj": {
     *         "file_size": 394409,
     *         "file_name": "model.obj",
     *         "content_type": "model/obj",
     *         "url": "https://v3b.fal.media/files/b/0a8c0ab4/tqMY5NJLnHjpwN8rQ15dj_model.obj"
     *       }
     *     }
     */
    model_urls: Components.ModelUrls;
}

export interface Hunyuan3dV31RapidTextTo3dInput {
    /**
     * Enable Geometry
     * @description Generate geometry-only white model without textures. When enabled, enable_pbr is ignored and OBJ is not supported (default output is GLB).
     * @default false
     */
    enable_geometry?: boolean;
    /**
     * Enable Pbr
     * @description Enable PBR material generation (metallic, roughness, normal textures). Does not take effect when enable_geometry is True.
     * @default false
     */
    enable_pbr?: boolean;
    /**
     * Prompt
     * @description Text description of the 3D content to generate. Max 200 UTF-8 characters.
     * @example A rustic wooden treasure chest with metal bands and ornate lock
     */
    prompt: string;
}

export interface Hunyuan3dV31RapidTextTo3dOutput {
    /**
     * Material Mtl
     * @description MTL material file for the OBJ model.
     * @example {
     *       "file_size": 157,
     *       "file_name": "material.mtl",
     *       "content_type": "text/plain",
     *       "url": "https://v3b.fal.media/files/b/0a8c44d5/EHzTxJtHpdIliaMNnEke-_material.mtl"
     *     }
     */
    material_mtl?: Components.File;
    /**
     * Model Obj
     * @description Generated 3D model in OBJ format.
     * @example {
     *       "file_size": 5306476,
     *       "file_name": "0f7f7a1ac578c80d4397a7f2b69b40ff.obj",
     *       "content_type": "model/obj",
     *       "url": "https://v3b.fal.media/files/b/0a8c44d5/2W2KRP1DM_-4qI8F_n05b_0f7f7a1ac578c80d4397a7f2b69b40ff.obj"
     *     }
     */
    model_obj?: Components.File;
    /**
     * Model Urls
     * @description URLs for different 3D model formats.
     * @example {
     *       "texture": {
     *         "file_size": 5915609,
     *         "file_name": "material.png",
     *         "content_type": "image/png",
     *         "url": "https://v3b.fal.media/files/b/0a8c44d5/T0q-P0aqXVG_y7jff-XTa_material.png"
     *       },
     *       "mtl": {
     *         "file_size": 157,
     *         "file_name": "material.mtl",
     *         "content_type": "text/plain",
     *         "url": "https://v3b.fal.media/files/b/0a8c44d5/EHzTxJtHpdIliaMNnEke-_material.mtl"
     *       },
     *       "obj": {
     *         "file_size": 5306476,
     *         "file_name": "0f7f7a1ac578c80d4397a7f2b69b40ff.obj",
     *         "content_type": "model/obj",
     *         "url": "https://v3b.fal.media/files/b/0a8c44d5/2W2KRP1DM_-4qI8F_n05b_0f7f7a1ac578c80d4397a7f2b69b40ff.obj"
     *       }
     *     }
     */
    model_urls: Components.ModelUrls;
    /**
     * Texture
     * @description Texture image for the 3D model.
     * @example {
     *       "file_size": 5915609,
     *       "file_name": "material.png",
     *       "content_type": "image/png",
     *       "url": "https://v3b.fal.media/files/b/0a8c44d5/T0q-P0aqXVG_y7jff-XTa_material.png"
     *     }
     */
    texture?: Components.File;
    /**
     * Thumbnail
     * @description Preview thumbnail of the generated model
     * @example {
     *       "file_size": 281374,
     *       "file_name": "preview.png",
     *       "content_type": "image/png",
     *       "url": "https://v3b.fal.media/files/b/0a8c44d6/D9_IYgpugP0deXvSwBC2J_preview.png"
     *     }
     */
    thumbnail?: Components.File;
}

export interface Hunyuan3dV31RapidImageTo3dInput {
    /**
     * Enable Geometry
     * @description Generate geometry-only white model without textures. When enabled, enable_pbr is ignored and OBJ is not supported (default output is GLB).
     * @default false
     */
    enable_geometry?: boolean;
    /**
     * Enable Pbr
     * @description Enable PBR material generation (metallic, roughness, normal textures). Does not take effect when enable_geometry is True.
     * @default false
     */
    enable_pbr?: boolean;
    /**
     * Input Image Url
     * @description Front view image URL. Resolution: 128-5000px, max 8MB (recommended ≤6MB for base64 encoding), formats: JPG/PNG/WEBP. Tips: simple background, single object, object >50% of frame.
     * @example https://v3b.fal.media/files/b/0a865ab1/omYcawLUo4RZbO8J6ZgZR.png
     */
    input_image_url: string;
}

export interface Hunyuan3dV31RapidImageTo3dOutput {
    /**
     * Material Mtl
     * @description MTL material file for the OBJ model.
     * @example {
     *       "file_size": 88,
     *       "file_name": "material.mtl",
     *       "content_type": "text/plain",
     *       "url": "https://v3b.fal.media/files/b/0a8c4439/_RhytNH4xZ5EFHr34YzJt_material.mtl"
     *     }
     */
    material_mtl?: Components.File;
    /**
     * Model Glb
     * @description Generated 3D model file. Contains GLB if available, otherwise OBJ.
     * @example {
     *       "file_size": 3172659,
     *       "file_name": "8b1dbea208d194b9089a950abc2df426.obj",
     *       "content_type": "model/obj",
     *       "url": "https://v3b.fal.media/files/b/0a8c4439/vj933H8B4W3wbd3e2RNby_8b1dbea208d194b9089a950abc2df426.obj"
     *     }
     */
    model_glb?: Components.File;
    /**
     * Model Urls
     * @description URLs for different 3D model formats.
     * @example {
     *       "texture": {
     *         "file_size": 11728567,
     *         "file_name": "texture_20250901.png",
     *         "content_type": "image/png",
     *         "url": "https://v3b.fal.media/files/b/0a8c4439/_4NXiSoGcZ-GYwmgUTfHZ_texture_20250901.png"
     *       },
     *       "mtl": {
     *         "file_size": 88,
     *         "file_name": "material.mtl",
     *         "content_type": "text/plain",
     *         "url": "https://v3b.fal.media/files/b/0a8c4439/_RhytNH4xZ5EFHr34YzJt_material.mtl"
     *       },
     *       "obj": {
     *         "file_size": 3172659,
     *         "file_name": "8b1dbea208d194b9089a950abc2df426.obj",
     *         "content_type": "model/obj",
     *         "url": "https://v3b.fal.media/files/b/0a8c4439/vj933H8B4W3wbd3e2RNby_8b1dbea208d194b9089a950abc2df426.obj"
     *       }
     *     }
     */
    model_urls: Components.ModelUrls;
    /**
     * Texture
     * @description Texture image for the 3D model.
     * @example {
     *       "file_size": 11728567,
     *       "file_name": "texture_20250901.png",
     *       "content_type": "image/png",
     *       "url": "https://v3b.fal.media/files/b/0a8c4439/_4NXiSoGcZ-GYwmgUTfHZ_texture_20250901.png"
     *     }
     */
    texture?: Components.File;
    /**
     * Thumbnail
     * @description Preview thumbnail of the generated model
     * @example {
     *       "file_size": 82521,
     *       "file_name": "preview.png",
     *       "content_type": "image/png",
     *       "url": "https://v3b.fal.media/files/b/0a8c4439/70Sm1pZ16SQP-mEbaKICC_preview.png"
     *     }
     */
    thumbnail?: Components.File;
}

export interface Hunyuan3dV31ProTextTo3dInput {
    /**
     * Enable Pbr
     * @description Enable PBR material generation (metallic, roughness, normal textures). Ignored when generate_type is Geometry.
     * @default false
     */
    enable_pbr?: boolean;
    /**
     * Face Count
     * @description Target polygon face count. Range: 40,000-1,500,000. Default: 500,000.
     * @default 500000
     */
    face_count?: number;
    /**
     * Generate Type
     * @description Generation task type. Normal: textured model. Geometry: geometry-only white model (no textures). LowPoly/Sketch are not available in v3.1.
     * @default Normal
     * @enum {string}
     */
    generate_type?: 'Normal' | 'Geometry';
    /**
     * Prompt
     * @description Text description of the 3D content to generate. Max 1024 UTF-8 characters.
     * @example A super cool space ship with details
     */
    prompt: string;
}

export interface Hunyuan3dV31ProTextTo3dOutput {
    /**
     * Model Glb
     * @description Generated 3D object in GLB format.
     * @example {
     *       "file_size": 35833072,
     *       "file_name": "model.glb",
     *       "content_type": "model/gltf-binary",
     *       "url": "https://v3b.fal.media/files/b/0a8c5483/z6sbpr5wBRjqgnQlJM2Ot_model.glb"
     *     }
     */
    model_glb: Components.File;
    /**
     * Model Urls
     * @description URLs for different 3D model formats
     * @example {
     *       "texture": {
     *         "file_size": 19996580,
     *         "file_name": "texture_20250901.png",
     *         "content_type": "image/png",
     *         "url": "https://v3b.fal.media/files/b/0a8c5482/jOL2nBpcOGspwxQf0_U-z_texture_20250901.png"
     *       },
     *       "mtl": {
     *         "file_size": 88,
     *         "file_name": "material.mtl",
     *         "content_type": "text/plain",
     *         "url": "https://v3b.fal.media/files/b/0a8c5482/ZxJepsEkhM67VSugmZ7QT_material.mtl"
     *       },
     *       "obj": {
     *         "file_size": 34755929,
     *         "file_name": "6168030a8817075aaa55c94cc5145000.obj",
     *         "content_type": "model/obj",
     *         "url": "https://v3b.fal.media/files/b/0a8c5482/ZzC1xlOftyGQxhDkbZzVW_6168030a8817075aaa55c94cc5145000.obj"
     *       },
     *       "glb": {
     *         "file_size": 35833072,
     *         "file_name": "model.glb",
     *         "content_type": "model/gltf-binary",
     *         "url": "https://v3b.fal.media/files/b/0a8c5483/z6sbpr5wBRjqgnQlJM2Ot_model.glb"
     *       }
     *     }
     */
    model_urls: Components.ModelUrls;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed?: number;
    /**
     * Thumbnail
     * @description Preview thumbnail of the generated model
     * @example {
     *       "file_size": 68927,
     *       "file_name": "preview.png",
     *       "content_type": "image/png",
     *       "url": "https://v3b.fal.media/files/b/0a8c5483/EE97ZV7cM-3UVG4peyWTQ_preview.png"
     *     }
     */
    thumbnail?: Components.File;
}

export interface Hunyuan3dV31ProImageTo3dInput {
    /**
     * Back Image Url
     * @description Optional back/rear view image URL (JPG/PNG recommended).
     */
    back_image_url?: string;
    /**
     * Bottom Image Url
     * @description Optional bottom view image URL (v3.1 exclusive, JPG/PNG recommended).
     */
    bottom_image_url?: string;
    /**
     * Enable Pbr
     * @description Enable PBR material generation (metallic, roughness, normal textures). Ignored when generate_type is Geometry.
     * @default false
     */
    enable_pbr?: boolean;
    /**
     * Face Count
     * @description Target polygon face count. Range: 40,000-1,500,000. Default: 500,000.
     * @default 500000
     */
    face_count?: number;
    /**
     * Generate Type
     * @description Generation task type. Normal: textured model. Geometry: geometry-only white model (no textures). LowPoly/Sketch are not available in v3.1.
     * @default Normal
     * @enum {string}
     */
    generate_type?: 'Normal' | 'Geometry';
    /**
     * Input Image Url
     * @description Front view image URL. Resolution: 128-5000px, max 8MB, formats: JPG/PNG/WEBP. Tips: simple background, single object, object >50% of frame.
     * @example https://v3b.fal.media/files/b/0a8c3155/BTXNRrzOFsO6OvdSxdXmv_6ZcaGmrY.png
     */
    input_image_url: string;
    /**
     * Left Front Image Url
     * @description Optional left-front 45 degree angle view image URL (v3.1 exclusive, JPG/PNG recommended).
     */
    left_front_image_url?: string;
    /**
     * Left Image Url
     * @description Optional left side view image URL (JPG/PNG recommended).
     */
    left_image_url?: string;
    /**
     * Right Front Image Url
     * @description Optional right-front 45 degree angle view image URL (v3.1 exclusive, JPG/PNG recommended).
     */
    right_front_image_url?: string;
    /**
     * Right Image Url
     * @description Optional right side view image URL (JPG/PNG recommended).
     */
    right_image_url?: string;
    /**
     * Top Image Url
     * @description Optional top view image URL (v3.1 exclusive, JPG/PNG recommended).
     */
    top_image_url?: string;
}

export interface Hunyuan3dV31ProImageTo3dOutput {
    /**
     * Model Glb
     * @description Generated 3D object in GLB format.
     * @example {
     *       "file_size": 38554640,
     *       "file_name": "model.glb",
     *       "content_type": "model/gltf-binary",
     *       "url": "https://v3b.fal.media/files/b/0a8c3187/jOeZmtBuhdQMkDu65AkdT_model.glb"
     *     }
     */
    model_glb: Components.File;
    /**
     * Model Urls
     * @description URLs for different 3D model formats
     * @example {
     *       "glb": {
     *         "file_size": 38554640,
     *         "file_name": "model.glb",
     *         "content_type": "model/gltf-binary",
     *         "url": "https://v3b.fal.media/files/b/0a8c3187/jOeZmtBuhdQMkDu65AkdT_model.glb"
     *       },
     *       "obj": {
     *         "file_size": 31447160,
     *         "file_name": "model.obj",
     *         "content_type": "model/obj",
     *         "url": "https://v3b.fal.media/files/b/0a8c3186/no-aBFEDnOuthILfv-wzs_model.obj"
     *       }
     *     }
     */
    model_urls: Components.ModelUrls;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed?: number;
    /**
     * Thumbnail
     * @description Preview thumbnail of the generated model
     * @example {
     *       "file_size": 194908,
     *       "file_name": "preview.png",
     *       "content_type": "image/png",
     *       "url": "https://v3b.fal.media/files/b/0a8c3187/gxidaODj4OvPXruCfrZ-__preview.png"
     *     }
     */
    thumbnail?: Components.File;
}

export interface Hunyuan3dV31PartInput {
    /**
     * Input File Url
     * @description URL of FBX file to split into parts. ONLY FBX format supported. Max size: 100MB, face count ≤30,000. Recommended: AIGC-generated models.
     * @example https://v3b.fal.media/files/b/0a8bf92a/XECcItG5QHt0LViFTRCON_converted.fbx
     */
    input_file_url: string;
}

export interface Hunyuan3dV31PartOutput {
    /**
     * Result Files
     * @description List of generated part files in FBX format
     * @example [
     *       {
     *         "file_size": 2048000,
     *         "file_name": "part_0.fbx",
     *         "content_type": "application/octet-stream",
     *         "url": "https://v3b.fal.media/files/b/0a8bf94b/zOP--lT23slziGKp99dJm_part_0.fbx"
     *       }
     *     ]
     */
    result_files: Components.File[];
}

export interface Hunyuan_worldImageToWorldInput {
    /**
     * Classes
     * @description Classes to use for the world generation.
     * @example nature, landscape
     */
    classes: string;
    /**
     * Export Drc
     * @description Whether to export DRC (Dynamic Resource Configuration).
     * @default false
     */
    export_drc?: boolean;
    /**
     * Image Url
     * @description The URL of the image to convert to a world.
     * @example https://v3.fal.media/files/penguin/_4oXlxt85dr0WY2o0I894_output.png
     */
    image_url: string;
    /**
     * Labels Fg1
     * @description Labels for the first foreground object.
     * @example tree, grass, sky
     */
    labels_fg1: string;
    /**
     * Labels Fg2
     * @description Labels for the second foreground object.
     * @example mountain, water
     */
    labels_fg2: string;
}

export interface Hunyuan_worldImageToWorldOutput {
    /**
     * World File
     * @description The generated world.
     */
    world_file: Components.File;
}

export interface Hunyuan_worldInput {
    /**
     * Image Url
     * @description The URL of the image to convert to a panorama.
     * @example https://v3.fal.media/files/penguin/_4oXlxt85dr0WY2o0I894_output.png
     */
    image_url: string;
    /**
     * Prompt
     * @description The prompt to use for the panorama generation.
     * @example A skyland of wonders
     */
    prompt: string;
}

export interface Hunyuan_worldOutput {
    /**
     * Image
     * @description The generated panorama image.
     * @example {
     *       "height": 960,
     *       "file_size": 2738127,
     *       "file_name": "5db7925423b44f2a98098cd8f7cad7ec.png",
     *       "content_type": "image/png",
     *       "url": "https://v3.fal.media/files/kangaroo/P2AmXuLlyDIsivqjV_rAr_5db7925423b44f2a98098cd8f7cad7ec.png",
     *       "width": 1920
     *     }
     */
    image: Components.Image;
}

export interface HidreamI1FullImageToImageInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. Setting to None uses the input image's size.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description The image URL to generate an image from.
     * @example https://v3.fal.media/files/tiger/C9qhzoMrg6Sg7lYh_ocrZ_example_man.png
     */
    image_url: string;
    /**
     * Loras
     * @description A list of LoRAs to apply to the model. Each LoRA specifies its path, scale, and optional weight name.
     * @default []
     */
    loras?: Components.LoraWeight_5[];
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example An old man
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description Denoising strength for image-to-image generation.
     * @default 0.75
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface HidreamI1FullImageToImageOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "height": 1024,
     *         "content_type": "image/jpeg",
     *         "url": "https://v3.fal.media/files/lion/eIyinD1FLsdjjreSZzD6d.jpeg",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface HidreamI1FullInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default {
     *       "height": 1024,
     *       "width": 1024
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description A list of LoRAs to apply to the model. Each LoRA specifies its path, scale, and optional weight name.
     * @default []
     */
    loras?: Components.LoraWeight_5[];
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example a cat holding a skateboard which has 'fal' written on it in red spray paint
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface HidreamI1FullOutput extends SharedType_4411 {}

export interface HidreamI1FastInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default {
     *       "height": 1024,
     *       "width": 1024
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 16
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example a cat holding a skateboard which has 'fal' written on it in red spray paint
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface HidreamI1FastOutput extends SharedType_a73 {}

export interface HidreamI1DevInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default {
     *       "height": 1024,
     *       "width": 1024
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example a cat holding a skateboard which has 'fal' written on it in red spray paint
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface HidreamI1DevOutput extends SharedType_a73 {}

export interface HidreamE11Input {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     * @example 3.5
     */
    guidance_scale?: number;
    /**
     * Image Guidance Scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your initial image when looking for a related image to show you.
     * @default 2
     * @example 2
     */
    image_guidance_scale?: number;
    /**
     * Image URL
     * @description URL of an input image to edit.
     * @example https://storage.googleapis.com/falserverless/model_tests/hidream/woman.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default low resolution, blur
     * @example low resolution, blur
     */
    negative_prompt?: string;
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Convert the image into a 3D animated style.
     */
    prompt?: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Target Image Description
     * @description The description of the target image after your edits have been made. Leave this blank to allow the model to use its own imagination.
     */
    target_image_description?: string;
}

export interface HidreamE11Output extends SharedType_a73 {}

export interface GptImage1TextToImageInput {
    /**
     * Background
     * @description Background for the generated image
     * @default auto
     * @enum {string}
     */
    background?: 'auto' | 'transparent' | 'opaque';
    /**
     * Image Size
     * @description Aspect ratio for the generated image
     * @default auto
     * @enum {string}
     */
    image_size?: 'auto' | '1024x1024' | '1536x1024' | '1024x1536';
    /**
     * Number of Images
     * @description Number of images to generate
     * @default 1
     * @example 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description Output format for the images
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt for image generation
     * @example A serene cyberpunk cityscape at twilight, with neon signs glowing in vibrant blues and purples, reflecting on rain-slick streets. Sleek futuristic buildings tower above, connected by glowing skybridges. A lone figure in a hooded jacket stands under a streetlamp, backlit by soft mist. The atmosphere is cinematic, moody
     */
    prompt: string;
    /**
     * Quality
     * @description Quality for the generated image
     * @default auto
     * @enum {string}
     */
    quality?: 'auto' | 'low' | 'medium' | 'high';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface GptImage1TextToImageOutput {
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "height": 1536,
     *         "file_name": "cyberpunk.png",
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/model_tests/gpt-image-1/cyberpunk.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.ImageFile_1[];
}

export interface GptImage1EditImageInput {
    /**
     * Background
     * @description Background for the generated image
     * @default auto
     * @enum {string}
     */
    background?: 'auto' | 'transparent' | 'opaque';
    /**
     * Image Size
     * @description Aspect ratio for the generated image
     * @default auto
     * @enum {string}
     */
    image_size?: 'auto' | '1024x1024' | '1536x1024' | '1024x1536';
    /**
     * Image URLs
     * @description The URLs of the images to use as a reference for the generation.
     * @example [
     *       "https://storage.googleapis.com/falserverless/model_tests/gpt-image-1/cyberpunk.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Input Fidelity
     * @description Input fidelity for the generated image
     * @default high
     * @enum {string}
     */
    input_fidelity?: 'low' | 'high';
    /**
     * Number of Images
     * @description Number of images to generate
     * @default 1
     * @example 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description Output format for the images
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt for image generation
     * @example Make this pixel-art style.
     */
    prompt: string;
    /**
     * Quality
     * @description Quality for the generated image
     * @default auto
     * @enum {string}
     */
    quality?: 'auto' | 'low' | 'medium' | 'high';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface GptImage1EditImageOutput {
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "height": 1536,
     *         "file_name": "cyberpunk_pixel.png",
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/model_tests/gpt-image-1/cyberpunk_pixel.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.ImageFile_1[];
}

export interface GptImage15EditInput {
    /**
     * Background
     * @description Background for the generated image
     * @default auto
     * @enum {string}
     */
    background?: 'auto' | 'transparent' | 'opaque';
    /**
     * Image Size
     * @description Aspect ratio for the generated image
     * @default auto
     * @enum {string}
     */
    image_size?: 'auto' | '1024x1024' | '1536x1024' | '1024x1536';
    /**
     * Image URLs
     * @description The URLs of the images to use as a reference for the generation.
     * @example [
     *       "https://v3b.fal.media/files/b/0a8691af/9Se_1_VX1wzTjjTOpWbs9_bb39c2eb-1a41-4749-b1d0-cf134abc8bbf.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Input Fidelity
     * @description Input fidelity for the generated image
     * @default high
     * @enum {string}
     */
    input_fidelity?: 'low' | 'high';
    /**
     * Mask Image URL
     * @description The URL of the mask image to use for the generation. This indicates what part of the image to edit.
     */
    mask_image_url?: string;
    /**
     * Number of Images
     * @description Number of images to generate
     * @default 1
     * @example 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description Output format for the images
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt for image generation
     * @example Same workers, same beam, same lunch boxes - but they're all on their phones now. One is taking a selfie. One is on a call looking annoyed. Same danger, new priorities. A hard hat has AirPods.
     */
    prompt: string;
    /**
     * Quality
     * @description Quality for the generated image
     * @default high
     * @enum {string}
     */
    quality?: 'low' | 'medium' | 'high';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface GptImage15EditOutput {
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "height": 1024,
     *         "file_name": "yUt7tifLSbg1WzWWgfj2o.png",
     *         "content_type": "image/png",
     *         "url": "https://v3b.fal.media/files/b/0a8691b0/yUt7tifLSbg1WzWWgfj2o.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.ImageFile[];
}

export interface GptImage15Input {
    /**
     * Background
     * @description Background for the generated image
     * @default auto
     * @enum {string}
     */
    background?: 'auto' | 'transparent' | 'opaque';
    /**
     * Image Size
     * @description Aspect ratio for the generated image
     * @default 1024x1024
     * @enum {string}
     */
    image_size?: '1024x1024' | '1536x1024' | '1024x1536';
    /**
     * Number of Images
     * @description Number of images to generate
     * @default 1
     * @example 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description Output format for the images
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt for image generation
     * @example create a realistic image taken with iphone at these coordinates 41°43′32″N 49°56′49″W 15 April 1912
     */
    prompt: string;
    /**
     * Quality
     * @description Quality for the generated image
     * @default high
     * @enum {string}
     */
    quality?: 'low' | 'medium' | 'high';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface GptImage15Output {
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "height": 1024,
     *         "file_name": "EnWrO3XWjPE0nxBDpaQrj.png",
     *         "content_type": "image/png",
     *         "url": "https://v3b.fal.media/files/b/0a869129/EnWrO3XWjPE0nxBDpaQrj.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.ImageFile[];
}

export interface GptImage1MiniEditInput {
    /**
     * Background
     * @description Background for the generated image
     * @default auto
     * @enum {string}
     */
    background?: 'auto' | 'transparent' | 'opaque';
    /**
     * Image Size
     * @description Aspect ratio for the generated image
     * @default auto
     * @enum {string}
     */
    image_size?: 'auto' | '1024x1024' | '1536x1024' | '1024x1536';
    /**
     * Image URLs
     * @description The URLs of the images to use as a reference for the generation.
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_inputs/seedream4_edit_input_1.png",
     *       "https://storage.googleapis.com/falserverless/example_inputs/seedream4_edit_input_2.png",
     *       "https://storage.googleapis.com/falserverless/example_inputs/seedream4_edit_input_3.png",
     *       "https://storage.googleapis.com/falserverless/example_inputs/seedream4_edit_input_4.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Number of Images
     * @description Number of images to generate
     * @default 1
     * @example 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description Output format for the images
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt for image generation
     * @example Dress the model in the clothes and hat. Add a cat to the scene and change the background to a Victorian era building.
     */
    prompt: string;
    /**
     * Quality
     * @description Quality for the generated image
     * @default auto
     * @enum {string}
     */
    quality?: 'auto' | 'low' | 'medium' | 'high';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface GptImage1MiniEditOutput {
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "height": 1024,
     *         "file_name": "NtVrlQjQEkG80Nz874MjH_cc9f505a28354629bb0951c4a8fe9b08.jpg",
     *         "content_type": "image/jpeg",
     *         "url": "https://v3b.fal.media/files/b/elephant/NtVrlQjQEkG80Nz874MjH_cc9f505a28354629bb0951c4a8fe9b08.jpg",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.ImageFile_1[];
}

export interface GptImage1MiniInput {
    /**
     * Background
     * @description Background for the generated image
     * @default auto
     * @enum {string}
     */
    background?: 'auto' | 'transparent' | 'opaque';
    /**
     * Image Size
     * @description Aspect ratio for the generated image
     * @default auto
     * @enum {string}
     */
    image_size?: 'auto' | '1024x1024' | '1536x1024' | '1024x1536';
    /**
     * Number of Images
     * @description Number of images to generate
     * @default 1
     * @example 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description Output format for the images
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt for image generation
     * @example A serene landscape with mountains reflecting in a crystal-clear lake at sunset, photorealistic style
     */
    prompt: string;
    /**
     * Quality
     * @description Quality for the generated image
     * @default auto
     * @enum {string}
     */
    quality?: 'auto' | 'low' | 'medium' | 'high';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface GptImage1MiniOutput {
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "height": 1024,
     *         "file_name": "1EXVSVlSs4Yz5hKplrzTv_2595c4e8720f4c19bcbc3dd373b18065.jpg",
     *         "content_type": "image/jpeg",
     *         "url": "https://v3b.fal.media/files/b/elephant/1EXVSVlSs4Yz5hKplrzTv_2595c4e8720f4c19bcbc3dd373b18065.jpg",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.ImageFile_1[];
}

export interface GotOcrV2Input {
    /**
     * Do Format
     * @description Generate the output in formatted mode.
     * @default false
     */
    do_format?: boolean;
    /**
     * Input Image Urls
     * @description URL of images.
     * @default []
     */
    input_image_urls?: string[];
    /**
     * Multi Page
     * @description Use provided images to generate a single output.
     * @default false
     */
    multi_page?: boolean;
}

export interface GotOcrV2Output {
    /**
     * Output
     * @description Generated output
     */
    outputs: string[];
}

export interface GlmImageImageToImageInput {
    /**
     * Enable Prompt Expansion
     * @description If True, the prompt will be enhanced using an LLM for more detailed and higher quality results.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Enable NSFW safety checking on the generated images.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Classifier-free guidance scale. Higher values make the model follow the prompt more closely.
     * @default 1.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description Output image size.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
              | 'portrait_3_2'
              | 'landscape_3_2'
              | 'portrait_hd'
              | 'landscape_hd'
          );
    /**
     * Image Urls
     * @description URL(s) of the condition image(s) for image-to-image generation. Supports up to 4 URLs for multi-image references.
     * @example https://storage.googleapis.com/falserverless/example_inputs/catwalk.png
     */
    image_urls: string[];
    /**
     * Num Images
     * @description Number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description Number of diffusion denoising steps. More steps generally produce higher quality images.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description Output image format.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description Text prompt for image generation.
     * @example Make the dress red.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. The same seed with the same prompt will produce the same image.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If True, the image will be returned as a base64 data URI instead of a URL.
     * @default false
     */
    sync_mode?: boolean;
}

export interface GlmImageImageToImageOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description List of URLs to the generated images.
     * @example [
     *       {
     *         "height": 1536,
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/catwalk_red.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface GlmImageInput {
    /**
     * Enable Prompt Expansion
     * @description If True, the prompt will be enhanced using an LLM for more detailed and higher quality results.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Enable NSFW safety checking on the generated images.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Classifier-free guidance scale. Higher values make the model follow the prompt more closely.
     * @default 1.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description Output image size.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
              | 'portrait_3_2'
              | 'landscape_3_2'
              | 'portrait_hd'
              | 'landscape_hd'
          );
    /**
     * Num Images
     * @description Number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description Number of diffusion denoising steps. More steps generally produce higher quality images.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description Output image format.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description Text prompt for image generation.
     * @example An elegant close-up photograph of hands holding a beautifully illustrated watercolor menu card. The hands have natural sun-kissed skin with a delicate gold ring, gripping the menu gently and refined.
     *
     *     The menu is a work of art—hand-painted watercolor illustration on textured cream watercolor paper with soft deckled edges:
     *
     *     - Top: "AZURE" painted in flowing navy blue watercolor calligraphy with organic brushstroke texture and slight color bleeding
     *     - Watercolor illustration border: delicate tropical elements painted in soft washes—translucent turquoise waves flowing along the edges, loose coral and pink hibiscus flowers in the corners, gentle green palm leaf strokes, and small golden paint splatters suggesting sunlight
     *     - Center menu items in elegant hand-lettered watercolor script with slight variations in ink density:
     *
     *       "Tuna Tartare — 24"
     *       "Sea Bass — 32"
     *       "Mango Pavlova — 14"
     *
     *     - Bottom: "Koh Samui" in small watercolor lettering with a tiny painted wave
     *
     *     The watercolor has beautiful organic qualities—soft color gradients, natural paper texture visible through transparent washes, slight bleeding at edges of brushstrokes, layered translucent blues and greens creating depth. The paint has a luminous, fresh quality with white paper showing through in places.
     *
     *     Background: dreamy out-of-focus turquoise ocean with sparkling bokeh lights reflecting off water, creating soft circular light spots in aqua and gold tones. The blurred background complements the watercolor aesthetic perfectly.
     *
     *     Lighting: warm natural golden hour sunlight from upper left, illuminating the watercolor pigments and making them glow. The light catches the textured watercolor paper beautifully, showing subtle shadows in the paint layers and paper grain.
     *
     *     Photography style: shot on 85mm f/1.4, shallow depth of field with only the menu in sharp focus. High-end editorial aesthetic that celebrates the handmade, artistic quality of the watercolor. Color palette: cream paper, translucent turquoise and teal watercolors, soft coral pink, navy blue, gentle greens, golden accents, warm skin tones.
     *
     *     The overall mood is artistic, luxurious, handcrafted—like a boutique resort that values artistry and craftsmanship. The watercolor style feels fresh, organic, and elevated.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. The same seed with the same prompt will produce the same image.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If True, the image will be returned as a base64 data URI instead of a URL.
     * @default false
     */
    sync_mode?: boolean;
}

export interface GlmImageOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description List of URLs to the generated images.
     * @example [
     *       {
     *         "height": 1024,
     *         "content_type": "image/jpeg",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/menu.jpg",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface GhiblifyInput {
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Url
     * @description The URL of the image to upscale.
     * @example https://fal.media/files/koala/QMPFC_avr-fEywDjp2ujy_60f6e32332384cada30c7016599d93e8.jpg
     */
    image_url: string;
    /**
     * Seed
     * @description The seed to use for the upscale. If not provided, a random seed will be used.
     * @example null
     */
    seed?: number;
}

export interface GhiblifyOutput {
    /** @description The URL of the generated image. */
    image: Components.Image_2;
}

export interface GeminiFlashEditMultiInput {
    /**
     * Input Image Urls
     * @description List of URLs of input images for editing
     * @example [
     *       "https://storage.googleapis.com/falserverless/web-examples/gemini-edit/input.png"
     *     ]
     */
    input_image_urls: string[];
    /**
     * Prompt
     * @description The prompt for image generation or editing
     * @example Make the car black
     */
    prompt: string;
}

export interface GeminiFlashEditMultiOutput extends SharedType_1b2 {}

export interface GeminiFlashEditInput {
    /**
     * Image Url
     * @description Optional URL of an input image for editing. If not provided, generates a new image.
     * @example https://storage.googleapis.com/falserverless/web-examples/gemini-edit/input.png
     */
    image_url: string;
    /**
     * Prompt
     * @description The prompt for image generation or editing
     * @example Make the car black
     */
    prompt: string;
}

export interface GeminiFlashEditOutput extends SharedType_1b2 {}

export interface Gemini3ProImagePreviewEditInput extends SharedType_eac {}

export interface Gemini3ProImagePreviewEditOutput extends SharedType_876 {}

export interface Gemini3ProImagePreviewInput extends SharedType_2a3 {}

export interface Gemini3ProImagePreviewOutput extends SharedType_7b9 {}

export interface Gemini25FlashImageEditInput extends SharedType_813 {}

export interface Gemini25FlashImageEditOutput extends SharedType_98c {}

export interface Gemini25FlashImageInput extends SharedType_97e {}

export interface Gemini25FlashImageOutput extends SharedType_662 {}

export interface FramepackFlf2vInput {
    /**
     * Aspect Ratio (W:H)
     * @description The aspect ratio of the video to generate.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16';
    /**
     * CFG Scale
     * @description Classifier-Free Guidance scale for the generation.
     * @default 1
     */
    cfg_scale?: number;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Url
     * @description URL of the end image input.
     * @example https://storage.googleapis.com/falserverless/web-examples/wan_flf/last_frame.png
     */
    end_image_url: string;
    /**
     * Guidance Scale
     * @description Guidance scale for the generation.
     * @default 10
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description URL of the image input.
     * @example https://storage.googleapis.com/falserverless/web-examples/wan_flf/first_frame.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default
     * @example Ugly, blurry distorted, bad quality
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description The number of frames to generate.
     * @default 240
     */
    num_frames?: number;
    /**
     * Prompt
     * @description Text prompt for video generation (max 500 characters).
     * @example A tabby cat is confidely strolling toward the camera, when it spins and with a flash of magic reveals itself to be a cat-dragon hybrid with glistening amber scales.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations.
     * @default 480p
     * @enum {string}
     */
    resolution?: '720p' | '480p';
    /**
     * Seed
     * @description The seed to use for generating the video.
     */
    seed?: number;
    /**
     * Strength of last frame
     * @description Determines the influence of the final frame on the generated video. Higher values result in the output being more heavily influenced by the last frame.
     * @default 0.8
     */
    strength?: number;
}

export interface FramepackFlf2vOutput {
    /**
     * Seed
     * @description The seed used for generating the video.
     */
    seed: number;
    /**
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/flf2v.mp4"
     *     }
     */
    video: Components.File_1;
}

export interface FramepackF1Input extends SharedType_82a {}

export interface FramepackF1Output extends SharedType_f15 {}

export interface FramepackInput extends SharedType_82a {}

export interface FramepackOutput extends SharedType_f15 {}

export interface FooocusUpscaleOrVaryInput {
    /**
     * Aspect Ratio
     * @description The size of the generated image. You can choose between some presets or
     *                 custom height and width that **must be multiples of 8**.
     * @default 1024x1024
     */
    aspect_ratio?: string;
    /**
     * Enable Safety Checker
     * @description If set to false, the safety checker will be disabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4
     */
    guidance_scale?: number;
    image_prompt_1?: Components.ImagePrompt;
    image_prompt_2?: Components.ImagePrompt;
    image_prompt_3?: Components.ImagePrompt;
    image_prompt_4?: Components.ImagePrompt;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use up to 5 LoRAs
     *                 and they will be merged together to generate the final image.
     * @default [
     *       {
     *         "path": "https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_offset_example-lora_1.0.safetensors",
     *         "scale": 0.1
     *       }
     *     ]
     */
    loras?: Components.LoraWeight_4[];
    /**
     * Mixing Image Prompt and Vary/Upscale
     * @description Mixing Image Prompt and Vary/Upscale
     * @default false
     */
    mixing_image_prompt_and_vary_upscale?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example (worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed, grayscale, bw, bad photo, bad photography, bad art:1.4), (watermark, signature, text font, username, error, logo, words, letters, digits, autograph, trademark, name:1.2), (blur, blurry, grainy), morbid, ugly, asymmetrical, mutated malformed, mutilated, poorly lit, bad shadow, draft, cropped, out of frame, cut off, censored, jpeg artifacts, out of focus, glitch, duplicate, (airbrushed, cartoon, anime, semi-realistic, cgi, render, blender, digital art, manga, amateur:1.3), (3D ,3D Game, 3D Game Scene, 3D Character:1.1), (bad hands, bad anatomy, bad body, bad face, bad teeth, bad arms, bad legs, deformities:1.3)
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate in one request
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Performance
     * @description You can choose Speed or Quality
     * @default Extreme Speed
     * @enum {string}
     */
    performance?: 'Speed' | 'Quality' | 'Extreme Speed' | 'Lightning';
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @default
     * @example a basket of various fruits, bokeh, realistic, masterpiece
     */
    prompt?: string;
    /**
     * Refiner Model
     * @description Refiner (SDXL or SD 1.5)
     * @default None
     * @enum {string}
     */
    refiner_model?: 'None' | 'realisticVisionV60B1_v51VAE.safetensors';
    /**
     * Refiner Switch At
     * @description Use 0.4 for SD1.5 realistic models; 0.667 for SD1.5 anime models
     *                 0.8 for XL-refiners; or any value for switching two SDXL models.
     * @default 0.8
     */
    refiner_switch?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     * @example 176400
     */
    seed?: number;
    /**
     * Sharpness
     * @description The sharpness of the generated image. Use it to control how sharp the generated
     *                 image should be. Higher value means image and texture are sharper.
     * @default 2
     */
    sharpness?: number;
    /**
     * Styles
     * @description The style to use.
     * @default [
     *       "Fooocus Enhance",
     *       "Fooocus V2",
     *       "Fooocus Sharp"
     *     ]
     */
    styles?: (
        | 'Fooocus V2'
        | 'Fooocus Enhance'
        | 'Fooocus Sharp'
        | 'Fooocus Semi Realistic'
        | 'Fooocus Masterpiece'
        | 'Fooocus Photograph'
        | 'Fooocus Negative'
        | 'Fooocus Cinematic'
        | 'SAI 3D Model'
        | 'SAI Analog Film'
        | 'SAI Anime'
        | 'SAI Cinematic'
        | 'SAI Comic Book'
        | 'SAI Craft Clay'
        | 'SAI Digital Art'
        | 'SAI Enhance'
        | 'SAI Fantasy Art'
        | 'SAI Isometric'
        | 'SAI Line Art'
        | 'SAI Lowpoly'
        | 'SAI Neonpunk'
        | 'SAI Origami'
        | 'SAI Photographic'
        | 'SAI Pixel Art'
        | 'SAI Texture'
        | 'MRE Cinematic Dynamic'
        | 'MRE Spontaneous Picture'
        | 'MRE Artistic Vision'
        | 'MRE Dark Dream'
        | 'MRE Gloomy Art'
        | 'MRE Bad Dream'
        | 'MRE Underground'
        | 'MRE Surreal Painting'
        | 'MRE Dynamic Illustration'
        | 'MRE Undead Art'
        | 'MRE Elemental Art'
        | 'MRE Space Art'
        | 'MRE Ancient Illustration'
        | 'MRE Brave Art'
        | 'MRE Heroic Fantasy'
        | 'MRE Dark Cyberpunk'
        | 'MRE Lyrical Geometry'
        | 'MRE Sumi E Symbolic'
        | 'MRE Sumi E Detailed'
        | 'MRE Manga'
        | 'MRE Anime'
        | 'MRE Comic'
        | 'Ads Advertising'
        | 'Ads Automotive'
        | 'Ads Corporate'
        | 'Ads Fashion Editorial'
        | 'Ads Food Photography'
        | 'Ads Gourmet Food Photography'
        | 'Ads Luxury'
        | 'Ads Real Estate'
        | 'Ads Retail'
        | 'Artstyle Abstract'
        | 'Artstyle Abstract Expressionism'
        | 'Artstyle Art Deco'
        | 'Artstyle Art Nouveau'
        | 'Artstyle Constructivist'
        | 'Artstyle Cubist'
        | 'Artstyle Expressionist'
        | 'Artstyle Graffiti'
        | 'Artstyle Hyperrealism'
        | 'Artstyle Impressionist'
        | 'Artstyle Pointillism'
        | 'Artstyle Pop Art'
        | 'Artstyle Psychedelic'
        | 'Artstyle Renaissance'
        | 'Artstyle Steampunk'
        | 'Artstyle Surrealist'
        | 'Artstyle Typography'
        | 'Artstyle Watercolor'
        | 'Futuristic Biomechanical'
        | 'Futuristic Biomechanical Cyberpunk'
        | 'Futuristic Cybernetic'
        | 'Futuristic Cybernetic Robot'
        | 'Futuristic Cyberpunk Cityscape'
        | 'Futuristic Futuristic'
        | 'Futuristic Retro Cyberpunk'
        | 'Futuristic Retro Futurism'
        | 'Futuristic Sci Fi'
        | 'Futuristic Vaporwave'
        | 'Game Bubble Bobble'
        | 'Game Cyberpunk Game'
        | 'Game Fighting Game'
        | 'Game Gta'
        | 'Game Mario'
        | 'Game Minecraft'
        | 'Game Pokemon'
        | 'Game Retro Arcade'
        | 'Game Retro Game'
        | 'Game Rpg Fantasy Game'
        | 'Game Strategy Game'
        | 'Game Streetfighter'
        | 'Game Zelda'
        | 'Misc Architectural'
        | 'Misc Disco'
        | 'Misc Dreamscape'
        | 'Misc Dystopian'
        | 'Misc Fairy Tale'
        | 'Misc Gothic'
        | 'Misc Grunge'
        | 'Misc Horror'
        | 'Misc Kawaii'
        | 'Misc Lovecraftian'
        | 'Misc Macabre'
        | 'Misc Manga'
        | 'Misc Metropolis'
        | 'Misc Minimalist'
        | 'Misc Monochrome'
        | 'Misc Nautical'
        | 'Misc Space'
        | 'Misc Stained Glass'
        | 'Misc Techwear Fashion'
        | 'Misc Tribal'
        | 'Misc Zentangle'
        | 'Papercraft Collage'
        | 'Papercraft Flat Papercut'
        | 'Papercraft Kirigami'
        | 'Papercraft Paper Mache'
        | 'Papercraft Paper Quilling'
        | 'Papercraft Papercut Collage'
        | 'Papercraft Papercut Shadow Box'
        | 'Papercraft Stacked Papercut'
        | 'Papercraft Thick Layered Papercut'
        | 'Photo Alien'
        | 'Photo Film Noir'
        | 'Photo Glamour'
        | 'Photo Hdr'
        | 'Photo Iphone Photographic'
        | 'Photo Long Exposure'
        | 'Photo Neon Noir'
        | 'Photo Silhouette'
        | 'Photo Tilt Shift'
        | 'Cinematic Diva'
        | 'Abstract Expressionism'
        | 'Academia'
        | 'Action Figure'
        | 'Adorable 3D Character'
        | 'Adorable Kawaii'
        | 'Art Deco'
        | 'Art Nouveau'
        | 'Astral Aura'
        | 'Avant Garde'
        | 'Baroque'
        | 'Bauhaus Style Poster'
        | 'Blueprint Schematic Drawing'
        | 'Caricature'
        | 'Cel Shaded Art'
        | 'Character Design Sheet'
        | 'Classicism Art'
        | 'Color Field Painting'
        | 'Colored Pencil Art'
        | 'Conceptual Art'
        | 'Constructivism'
        | 'Cubism'
        | 'Dadaism'
        | 'Dark Fantasy'
        | 'Dark Moody Atmosphere'
        | 'Dmt Art Style'
        | 'Doodle Art'
        | 'Double Exposure'
        | 'Dripping Paint Splatter Art'
        | 'Expressionism'
        | 'Faded Polaroid Photo'
        | 'Fauvism'
        | 'Flat 2d Art'
        | 'Fortnite Art Style'
        | 'Futurism'
        | 'Glitchcore'
        | 'Glo Fi'
        | 'Googie Art Style'
        | 'Graffiti Art'
        | 'Harlem Renaissance Art'
        | 'High Fashion'
        | 'Idyllic'
        | 'Impressionism'
        | 'Infographic Drawing'
        | 'Ink Dripping Drawing'
        | 'Japanese Ink Drawing'
        | 'Knolling Photography'
        | 'Light Cheery Atmosphere'
        | 'Logo Design'
        | 'Luxurious Elegance'
        | 'Macro Photography'
        | 'Mandola Art'
        | 'Marker Drawing'
        | 'Medievalism'
        | 'Minimalism'
        | 'Neo Baroque'
        | 'Neo Byzantine'
        | 'Neo Futurism'
        | 'Neo Impressionism'
        | 'Neo Rococo'
        | 'Neoclassicism'
        | 'Op Art'
        | 'Ornate And Intricate'
        | 'Pencil Sketch Drawing'
        | 'Pop Art 2'
        | 'Rococo'
        | 'Silhouette Art'
        | 'Simple Vector Art'
        | 'Sketchup'
        | 'Steampunk 2'
        | 'Surrealism'
        | 'Suprematism'
        | 'Terragen'
        | 'Tranquil Relaxing Atmosphere'
        | 'Sticker Designs'
        | 'Vibrant Rim Light'
        | 'Volumetric Lighting'
        | 'Watercolor 2'
        | 'Whimsical And Playful'
        | 'Mk Chromolithography'
        | 'Mk Cross Processing Print'
        | 'Mk Dufaycolor Photograph'
        | 'Mk Herbarium'
        | 'Mk Punk Collage'
        | 'Mk Mosaic'
        | 'Mk Van Gogh'
        | 'Mk Coloring Book'
        | 'Mk Singer Sargent'
        | 'Mk Pollock'
        | 'Mk Basquiat'
        | 'Mk Andy Warhol'
        | 'Mk Halftone Print'
        | 'Mk Gond Painting'
        | 'Mk Albumen Print'
        | 'Mk Aquatint Print'
        | 'Mk Anthotype Print'
        | 'Mk Inuit Carving'
        | 'Mk Bromoil Print'
        | 'Mk Calotype Print'
        | 'Mk Color Sketchnote'
        | 'Mk Cibulak Porcelain'
        | 'Mk Alcohol Ink Art'
        | 'Mk One Line Art'
        | 'Mk Blacklight Paint'
        | 'Mk Carnival Glass'
        | 'Mk Cyanotype Print'
        | 'Mk Cross Stitching'
        | 'Mk Encaustic Paint'
        | 'Mk Embroidery'
        | 'Mk Gyotaku'
        | 'Mk Luminogram'
        | 'Mk Lite Brite Art'
        | 'Mk Mokume Gane'
        | 'Pebble Art'
        | 'Mk Palekh'
        | 'Mk Suminagashi'
        | 'Mk Scrimshaw'
        | 'Mk Shibori'
        | 'Mk Vitreous Enamel'
        | 'Mk Ukiyo E'
        | 'Mk Vintage Airline Poster'
        | 'Mk Vintage Travel Poster'
        | 'Mk Bauhaus Style'
        | 'Mk Afrofuturism'
        | 'Mk Atompunk'
        | 'Mk Constructivism'
        | 'Mk Chicano Art'
        | 'Mk De Stijl'
        | 'Mk Dayak Art'
        | 'Mk Fayum Portrait'
        | 'Mk Illuminated Manuscript'
        | 'Mk Kalighat Painting'
        | 'Mk Madhubani Painting'
        | 'Mk Pictorialism'
        | 'Mk Pichwai Painting'
        | 'Mk Patachitra Painting'
        | 'Mk Samoan Art Inspired'
        | 'Mk Tlingit Art'
        | 'Mk Adnate Style'
        | 'Mk Ron English Style'
        | 'Mk Shepard Fairey Style'
    )[];
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * UOV Image URL
     * @description The image to upscale or vary.
     * @example https://storage.googleapis.com/falserverless/model_tests/fooocus/fruit_basket.jpeg
     */
    uov_image_url: string;
    /**
     * UOV Method
     * @description The method to use for upscaling or varying.
     * @default Vary (Strong)
     * @enum {string}
     */
    uov_method?:
        | 'Disabled'
        | 'Vary (Subtle)'
        | 'Vary (Strong)'
        | 'Upscale (1.5x)'
        | 'Upscale (2x)'
        | 'Upscale (Fast 2x)';
}

export interface FooocusUpscaleOrVaryOutput extends SharedType_7eb1 {}

export interface FooocusInpaintInput {
    /**
     * Aspect Ratio
     * @description The size of the generated image. You can choose between some presets or
     *                 custom height and width that **must be multiples of 8**.
     * @default 1024x1024
     */
    aspect_ratio?: string;
    /**
     * Enable Safety Checker
     * @description If set to false, the safety checker will be disabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4
     */
    guidance_scale?: number;
    image_prompt_1?: Components.ImagePrompt;
    image_prompt_2?: Components.ImagePrompt;
    image_prompt_3?: Components.ImagePrompt;
    image_prompt_4?: Components.ImagePrompt;
    /**
     * Inpaint Additional Prompt
     * @description Describe what you want to inpaint.
     * @default
     */
    inpaint_additional_prompt?: string;
    /**
     * Disable Initial Latent In Inpaint
     * @description If set to true, the initial preprocessing will be disabled.
     * @default false
     */
    inpaint_disable_initial_latent?: boolean;
    /**
     * Inpaint Engine
     * @description Version of Fooocus inpaint model
     * @default v2.6
     * @enum {string}
     */
    inpaint_engine?: 'None' | 'v1' | 'v2.5' | 'v2.6';
    /**
     * Mask Erode or Dilate
     * @description Positive value will make white area in the mask larger, negative value will
     *                 make white area smaller. (default is 0, always process before any mask
     *                 invert)
     * @default 0
     */
    inpaint_erode_or_dilate?: number;
    /**
     * Inpaint Image Url
     * @description The image to use as a reference for inpainting.
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png
     */
    inpaint_image_url: string;
    /**
     * Inpaint Mode
     * @description The mode to use for inpainting.
     * @default Inpaint or Outpaint (default)
     * @enum {string}
     */
    inpaint_mode?:
        | 'Inpaint or Outpaint (default)'
        | 'Improve Detail (face, hand, eyes, etc.)'
        | 'Modify Content (add objects, change background, etc.)';
    /**
     * Inpaint Respective Field
     * @description The area to inpaint. Value 0 is same as "Only Masked" in A1111. Value 1 is
     *                 same as "Whole Image" in A1111. Only used in inpaint, not used in outpaint.
     *                 (Outpaint always use 1.0)
     * @default 0.618
     */
    inpaint_respective_field?: number;
    /**
     * Inpaint Denoising Strength
     * @description Same as the denoising strength in A1111 inpaint. Only used in inpaint, not
     *                 used in outpaint. (Outpaint always use 1.0)
     * @default 1
     */
    inpaint_strength?: number;
    /**
     * Invert Mask
     * @description If set to true, the mask will be inverted.
     * @default false
     */
    invert_mask?: boolean;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use up to 5 LoRAs
     *                 and they will be merged together to generate the final image.
     * @default [
     *       {
     *         "path": "https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_offset_example-lora_1.0.safetensors",
     *         "scale": 0.1
     *       }
     *     ]
     */
    loras?: Components.LoraWeight_4[];
    /**
     * Mask Image Url
     * @description The image to use as a mask for the generated image.
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png
     */
    mask_image_url?: string;
    /**
     * Mixing Image Prompt and Inpaint
     * @description Mixing Image Prompt and Inpaint
     * @default false
     */
    mixing_image_prompt_and_inpaint?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example (worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed, grayscale, bw, bad photo, bad photography, bad art:1.4), (watermark, signature, text font, username, error, logo, words, letters, digits, autograph, trademark, name:1.2), (blur, blurry, grainy), morbid, ugly, asymmetrical, mutated malformed, mutilated, poorly lit, bad shadow, draft, cropped, out of frame, cut off, censored, jpeg artifacts, out of focus, glitch, duplicate, (airbrushed, cartoon, anime, semi-realistic, cgi, render, blender, digital art, manga, amateur:1.3), (3D ,3D Game, 3D Game Scene, 3D Character:1.1), (bad hands, bad anatomy, bad body, bad face, bad teeth, bad arms, bad legs, deformities:1.3)
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate in one request
     * @default 1
     */
    num_images?: number;
    /**
     * Outpaint Direction
     * @description The directions to outpaint.
     * @default []
     */
    outpaint_selections?: ('Left' | 'Right' | 'Top' | 'Bottom')[];
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Override Inpaint Options
     * @description If set to true, the advanced inpaint options ('inpaint_disable_initial_latent',
     *                 'inpaint_engine', 'inpaint_strength', 'inpaint_respective_field',
     *                 'inpaint_erode_or_dilate') will be overridden.
     *                 Otherwise, the default values will be used.
     * @default false
     */
    override_inpaint_options?: boolean;
    /**
     * Performance
     * @description You can choose Speed or Quality
     * @default Extreme Speed
     * @enum {string}
     */
    performance?: 'Speed' | 'Quality' | 'Extreme Speed' | 'Lightning';
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @default
     * @example a cat on a bench, realistic, highly detailed, 8k
     */
    prompt?: string;
    /**
     * Refiner Model
     * @description Refiner (SDXL or SD 1.5)
     * @default None
     * @enum {string}
     */
    refiner_model?: 'None' | 'realisticVisionV60B1_v51VAE.safetensors';
    /**
     * Refiner Switch At
     * @description Use 0.4 for SD1.5 realistic models; 0.667 for SD1.5 anime models
     *                 0.8 for XL-refiners; or any value for switching two SDXL models.
     * @default 0.8
     */
    refiner_switch?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     * @example 176400
     */
    seed?: number;
    /**
     * Sharpness
     * @description The sharpness of the generated image. Use it to control how sharp the generated
     *                 image should be. Higher value means image and texture are sharper.
     * @default 2
     */
    sharpness?: number;
    /**
     * Styles
     * @description The style to use.
     * @default [
     *       "Fooocus Enhance",
     *       "Fooocus V2",
     *       "Fooocus Sharp"
     *     ]
     */
    styles?: (
        | 'Fooocus V2'
        | 'Fooocus Enhance'
        | 'Fooocus Sharp'
        | 'Fooocus Semi Realistic'
        | 'Fooocus Masterpiece'
        | 'Fooocus Photograph'
        | 'Fooocus Negative'
        | 'Fooocus Cinematic'
        | 'SAI 3D Model'
        | 'SAI Analog Film'
        | 'SAI Anime'
        | 'SAI Cinematic'
        | 'SAI Comic Book'
        | 'SAI Craft Clay'
        | 'SAI Digital Art'
        | 'SAI Enhance'
        | 'SAI Fantasy Art'
        | 'SAI Isometric'
        | 'SAI Line Art'
        | 'SAI Lowpoly'
        | 'SAI Neonpunk'
        | 'SAI Origami'
        | 'SAI Photographic'
        | 'SAI Pixel Art'
        | 'SAI Texture'
        | 'MRE Cinematic Dynamic'
        | 'MRE Spontaneous Picture'
        | 'MRE Artistic Vision'
        | 'MRE Dark Dream'
        | 'MRE Gloomy Art'
        | 'MRE Bad Dream'
        | 'MRE Underground'
        | 'MRE Surreal Painting'
        | 'MRE Dynamic Illustration'
        | 'MRE Undead Art'
        | 'MRE Elemental Art'
        | 'MRE Space Art'
        | 'MRE Ancient Illustration'
        | 'MRE Brave Art'
        | 'MRE Heroic Fantasy'
        | 'MRE Dark Cyberpunk'
        | 'MRE Lyrical Geometry'
        | 'MRE Sumi E Symbolic'
        | 'MRE Sumi E Detailed'
        | 'MRE Manga'
        | 'MRE Anime'
        | 'MRE Comic'
        | 'Ads Advertising'
        | 'Ads Automotive'
        | 'Ads Corporate'
        | 'Ads Fashion Editorial'
        | 'Ads Food Photography'
        | 'Ads Gourmet Food Photography'
        | 'Ads Luxury'
        | 'Ads Real Estate'
        | 'Ads Retail'
        | 'Artstyle Abstract'
        | 'Artstyle Abstract Expressionism'
        | 'Artstyle Art Deco'
        | 'Artstyle Art Nouveau'
        | 'Artstyle Constructivist'
        | 'Artstyle Cubist'
        | 'Artstyle Expressionist'
        | 'Artstyle Graffiti'
        | 'Artstyle Hyperrealism'
        | 'Artstyle Impressionist'
        | 'Artstyle Pointillism'
        | 'Artstyle Pop Art'
        | 'Artstyle Psychedelic'
        | 'Artstyle Renaissance'
        | 'Artstyle Steampunk'
        | 'Artstyle Surrealist'
        | 'Artstyle Typography'
        | 'Artstyle Watercolor'
        | 'Futuristic Biomechanical'
        | 'Futuristic Biomechanical Cyberpunk'
        | 'Futuristic Cybernetic'
        | 'Futuristic Cybernetic Robot'
        | 'Futuristic Cyberpunk Cityscape'
        | 'Futuristic Futuristic'
        | 'Futuristic Retro Cyberpunk'
        | 'Futuristic Retro Futurism'
        | 'Futuristic Sci Fi'
        | 'Futuristic Vaporwave'
        | 'Game Bubble Bobble'
        | 'Game Cyberpunk Game'
        | 'Game Fighting Game'
        | 'Game Gta'
        | 'Game Mario'
        | 'Game Minecraft'
        | 'Game Pokemon'
        | 'Game Retro Arcade'
        | 'Game Retro Game'
        | 'Game Rpg Fantasy Game'
        | 'Game Strategy Game'
        | 'Game Streetfighter'
        | 'Game Zelda'
        | 'Misc Architectural'
        | 'Misc Disco'
        | 'Misc Dreamscape'
        | 'Misc Dystopian'
        | 'Misc Fairy Tale'
        | 'Misc Gothic'
        | 'Misc Grunge'
        | 'Misc Horror'
        | 'Misc Kawaii'
        | 'Misc Lovecraftian'
        | 'Misc Macabre'
        | 'Misc Manga'
        | 'Misc Metropolis'
        | 'Misc Minimalist'
        | 'Misc Monochrome'
        | 'Misc Nautical'
        | 'Misc Space'
        | 'Misc Stained Glass'
        | 'Misc Techwear Fashion'
        | 'Misc Tribal'
        | 'Misc Zentangle'
        | 'Papercraft Collage'
        | 'Papercraft Flat Papercut'
        | 'Papercraft Kirigami'
        | 'Papercraft Paper Mache'
        | 'Papercraft Paper Quilling'
        | 'Papercraft Papercut Collage'
        | 'Papercraft Papercut Shadow Box'
        | 'Papercraft Stacked Papercut'
        | 'Papercraft Thick Layered Papercut'
        | 'Photo Alien'
        | 'Photo Film Noir'
        | 'Photo Glamour'
        | 'Photo Hdr'
        | 'Photo Iphone Photographic'
        | 'Photo Long Exposure'
        | 'Photo Neon Noir'
        | 'Photo Silhouette'
        | 'Photo Tilt Shift'
        | 'Cinematic Diva'
        | 'Abstract Expressionism'
        | 'Academia'
        | 'Action Figure'
        | 'Adorable 3D Character'
        | 'Adorable Kawaii'
        | 'Art Deco'
        | 'Art Nouveau'
        | 'Astral Aura'
        | 'Avant Garde'
        | 'Baroque'
        | 'Bauhaus Style Poster'
        | 'Blueprint Schematic Drawing'
        | 'Caricature'
        | 'Cel Shaded Art'
        | 'Character Design Sheet'
        | 'Classicism Art'
        | 'Color Field Painting'
        | 'Colored Pencil Art'
        | 'Conceptual Art'
        | 'Constructivism'
        | 'Cubism'
        | 'Dadaism'
        | 'Dark Fantasy'
        | 'Dark Moody Atmosphere'
        | 'Dmt Art Style'
        | 'Doodle Art'
        | 'Double Exposure'
        | 'Dripping Paint Splatter Art'
        | 'Expressionism'
        | 'Faded Polaroid Photo'
        | 'Fauvism'
        | 'Flat 2d Art'
        | 'Fortnite Art Style'
        | 'Futurism'
        | 'Glitchcore'
        | 'Glo Fi'
        | 'Googie Art Style'
        | 'Graffiti Art'
        | 'Harlem Renaissance Art'
        | 'High Fashion'
        | 'Idyllic'
        | 'Impressionism'
        | 'Infographic Drawing'
        | 'Ink Dripping Drawing'
        | 'Japanese Ink Drawing'
        | 'Knolling Photography'
        | 'Light Cheery Atmosphere'
        | 'Logo Design'
        | 'Luxurious Elegance'
        | 'Macro Photography'
        | 'Mandola Art'
        | 'Marker Drawing'
        | 'Medievalism'
        | 'Minimalism'
        | 'Neo Baroque'
        | 'Neo Byzantine'
        | 'Neo Futurism'
        | 'Neo Impressionism'
        | 'Neo Rococo'
        | 'Neoclassicism'
        | 'Op Art'
        | 'Ornate And Intricate'
        | 'Pencil Sketch Drawing'
        | 'Pop Art 2'
        | 'Rococo'
        | 'Silhouette Art'
        | 'Simple Vector Art'
        | 'Sketchup'
        | 'Steampunk 2'
        | 'Surrealism'
        | 'Suprematism'
        | 'Terragen'
        | 'Tranquil Relaxing Atmosphere'
        | 'Sticker Designs'
        | 'Vibrant Rim Light'
        | 'Volumetric Lighting'
        | 'Watercolor 2'
        | 'Whimsical And Playful'
        | 'Mk Chromolithography'
        | 'Mk Cross Processing Print'
        | 'Mk Dufaycolor Photograph'
        | 'Mk Herbarium'
        | 'Mk Punk Collage'
        | 'Mk Mosaic'
        | 'Mk Van Gogh'
        | 'Mk Coloring Book'
        | 'Mk Singer Sargent'
        | 'Mk Pollock'
        | 'Mk Basquiat'
        | 'Mk Andy Warhol'
        | 'Mk Halftone Print'
        | 'Mk Gond Painting'
        | 'Mk Albumen Print'
        | 'Mk Aquatint Print'
        | 'Mk Anthotype Print'
        | 'Mk Inuit Carving'
        | 'Mk Bromoil Print'
        | 'Mk Calotype Print'
        | 'Mk Color Sketchnote'
        | 'Mk Cibulak Porcelain'
        | 'Mk Alcohol Ink Art'
        | 'Mk One Line Art'
        | 'Mk Blacklight Paint'
        | 'Mk Carnival Glass'
        | 'Mk Cyanotype Print'
        | 'Mk Cross Stitching'
        | 'Mk Encaustic Paint'
        | 'Mk Embroidery'
        | 'Mk Gyotaku'
        | 'Mk Luminogram'
        | 'Mk Lite Brite Art'
        | 'Mk Mokume Gane'
        | 'Pebble Art'
        | 'Mk Palekh'
        | 'Mk Suminagashi'
        | 'Mk Scrimshaw'
        | 'Mk Shibori'
        | 'Mk Vitreous Enamel'
        | 'Mk Ukiyo E'
        | 'Mk Vintage Airline Poster'
        | 'Mk Vintage Travel Poster'
        | 'Mk Bauhaus Style'
        | 'Mk Afrofuturism'
        | 'Mk Atompunk'
        | 'Mk Constructivism'
        | 'Mk Chicano Art'
        | 'Mk De Stijl'
        | 'Mk Dayak Art'
        | 'Mk Fayum Portrait'
        | 'Mk Illuminated Manuscript'
        | 'Mk Kalighat Painting'
        | 'Mk Madhubani Painting'
        | 'Mk Pictorialism'
        | 'Mk Pichwai Painting'
        | 'Mk Patachitra Painting'
        | 'Mk Samoan Art Inspired'
        | 'Mk Tlingit Art'
        | 'Mk Adnate Style'
        | 'Mk Ron English Style'
        | 'Mk Shepard Fairey Style'
    )[];
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FooocusInpaintOutput extends SharedType_7eb1 {}

export interface FooocusImagePromptInput {
    /**
     * Aspect Ratio
     * @description The size of the generated image. You can choose between some presets or
     *                 custom height and width that **must be multiples of 8**.
     * @default 1024x1024
     */
    aspect_ratio?: string;
    /**
     * Enable Safety Checker
     * @description If set to false, the safety checker will be disabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image Prompt 1
     * @example {
     *       "weight": 1,
     *       "stop_at": 1,
     *       "type": "PyraCanny",
     *       "image_url": "https://storage.googleapis.com/falserverless/model_tests/fooocus/Pikachu.webp"
     *     }
     */
    image_prompt_1: Components.ImagePrompt;
    image_prompt_2?: Components.ImagePrompt;
    image_prompt_3?: Components.ImagePrompt;
    image_prompt_4?: Components.ImagePrompt;
    /**
     * Inpaint Additional Prompt
     * @description Describe what you want to inpaint.
     * @default
     */
    inpaint_additional_prompt?: string;
    /**
     * Inpaint Image URL
     * @description The image to use as a reference for inpainting.
     */
    inpaint_image_url?: string;
    /**
     * Inpaint Mode
     * @description The mode to use for inpainting.
     * @default Inpaint or Outpaint (default)
     * @enum {string}
     */
    inpaint_mode?:
        | 'Inpaint or Outpaint (default)'
        | 'Improve Detail (face, hand, eyes, etc.)'
        | 'Modify Content (add objects, change background, etc.)';
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use up to 5 LoRAs
     *                 and they will be merged together to generate the final image.
     * @default [
     *       {
     *         "path": "https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_offset_example-lora_1.0.safetensors",
     *         "scale": 0.1
     *       }
     *     ]
     */
    loras?: Components.LoraWeight_4[];
    /**
     * Mask Image URL
     * @description The image to use as a mask for the generated image.
     */
    mask_image_url?: string;
    /**
     * Mixing Image Prompt and Inpaint
     * @description Mixing Image Prompt and Inpaint
     * @default false
     */
    mixing_image_prompt_and_inpaint?: boolean;
    /**
     * Mixing Image Prompt and Vary/Upscale
     * @description Mixing Image Prompt and Vary/Upscale
     * @default false
     */
    mixing_image_prompt_and_vary_upscale?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example (worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed, grayscale, bw, bad photo, bad photography, bad art:1.4), (watermark, signature, text font, username, error, logo, words, letters, digits, autograph, trademark, name:1.2), (blur, blurry, grainy), morbid, ugly, asymmetrical, mutated malformed, mutilated, poorly lit, bad shadow, draft, cropped, out of frame, cut off, censored, jpeg artifacts, out of focus, glitch, duplicate, (airbrushed, cartoon, anime, semi-realistic, cgi, render, blender, digital art, manga, amateur:1.3), (3D ,3D Game, 3D Game Scene, 3D Character:1.1), (bad hands, bad anatomy, bad body, bad face, bad teeth, bad arms, bad legs, deformities:1.3)
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate in one request
     * @default 1
     */
    num_images?: number;
    /**
     * Outpaint Direction
     * @description The directions to outpaint.
     * @default []
     */
    outpaint_selections?: ('Left' | 'Right' | 'Top' | 'Bottom')[];
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Performance
     * @description You can choose Speed or Quality
     * @default Extreme Speed
     * @enum {string}
     */
    performance?: 'Speed' | 'Quality' | 'Extreme Speed' | 'Lightning';
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @default
     * @example pikachu
     */
    prompt?: string;
    /**
     * Refiner Model
     * @description Refiner (SDXL or SD 1.5)
     * @default None
     * @enum {string}
     */
    refiner_model?: 'None' | 'realisticVisionV60B1_v51VAE.safetensors';
    /**
     * Refiner Switch At
     * @description Use 0.4 for SD1.5 realistic models; 0.667 for SD1.5 anime models
     *                 0.8 for XL-refiners; or any value for switching two SDXL models.
     * @default 0.8
     */
    refiner_switch?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     * @example 176400
     */
    seed?: number;
    /**
     * Sharpness
     * @description The sharpness of the generated image. Use it to control how sharp the generated
     *                 image should be. Higher value means image and texture are sharper.
     * @default 2
     */
    sharpness?: number;
    /**
     * Styles
     * @description The style to use.
     * @default [
     *       "Fooocus Enhance",
     *       "Fooocus V2",
     *       "Fooocus Sharp"
     *     ]
     */
    styles?: (
        | 'Fooocus V2'
        | 'Fooocus Enhance'
        | 'Fooocus Sharp'
        | 'Fooocus Semi Realistic'
        | 'Fooocus Masterpiece'
        | 'Fooocus Photograph'
        | 'Fooocus Negative'
        | 'Fooocus Cinematic'
        | 'SAI 3D Model'
        | 'SAI Analog Film'
        | 'SAI Anime'
        | 'SAI Cinematic'
        | 'SAI Comic Book'
        | 'SAI Craft Clay'
        | 'SAI Digital Art'
        | 'SAI Enhance'
        | 'SAI Fantasy Art'
        | 'SAI Isometric'
        | 'SAI Line Art'
        | 'SAI Lowpoly'
        | 'SAI Neonpunk'
        | 'SAI Origami'
        | 'SAI Photographic'
        | 'SAI Pixel Art'
        | 'SAI Texture'
        | 'MRE Cinematic Dynamic'
        | 'MRE Spontaneous Picture'
        | 'MRE Artistic Vision'
        | 'MRE Dark Dream'
        | 'MRE Gloomy Art'
        | 'MRE Bad Dream'
        | 'MRE Underground'
        | 'MRE Surreal Painting'
        | 'MRE Dynamic Illustration'
        | 'MRE Undead Art'
        | 'MRE Elemental Art'
        | 'MRE Space Art'
        | 'MRE Ancient Illustration'
        | 'MRE Brave Art'
        | 'MRE Heroic Fantasy'
        | 'MRE Dark Cyberpunk'
        | 'MRE Lyrical Geometry'
        | 'MRE Sumi E Symbolic'
        | 'MRE Sumi E Detailed'
        | 'MRE Manga'
        | 'MRE Anime'
        | 'MRE Comic'
        | 'Ads Advertising'
        | 'Ads Automotive'
        | 'Ads Corporate'
        | 'Ads Fashion Editorial'
        | 'Ads Food Photography'
        | 'Ads Gourmet Food Photography'
        | 'Ads Luxury'
        | 'Ads Real Estate'
        | 'Ads Retail'
        | 'Artstyle Abstract'
        | 'Artstyle Abstract Expressionism'
        | 'Artstyle Art Deco'
        | 'Artstyle Art Nouveau'
        | 'Artstyle Constructivist'
        | 'Artstyle Cubist'
        | 'Artstyle Expressionist'
        | 'Artstyle Graffiti'
        | 'Artstyle Hyperrealism'
        | 'Artstyle Impressionist'
        | 'Artstyle Pointillism'
        | 'Artstyle Pop Art'
        | 'Artstyle Psychedelic'
        | 'Artstyle Renaissance'
        | 'Artstyle Steampunk'
        | 'Artstyle Surrealist'
        | 'Artstyle Typography'
        | 'Artstyle Watercolor'
        | 'Futuristic Biomechanical'
        | 'Futuristic Biomechanical Cyberpunk'
        | 'Futuristic Cybernetic'
        | 'Futuristic Cybernetic Robot'
        | 'Futuristic Cyberpunk Cityscape'
        | 'Futuristic Futuristic'
        | 'Futuristic Retro Cyberpunk'
        | 'Futuristic Retro Futurism'
        | 'Futuristic Sci Fi'
        | 'Futuristic Vaporwave'
        | 'Game Bubble Bobble'
        | 'Game Cyberpunk Game'
        | 'Game Fighting Game'
        | 'Game Gta'
        | 'Game Mario'
        | 'Game Minecraft'
        | 'Game Pokemon'
        | 'Game Retro Arcade'
        | 'Game Retro Game'
        | 'Game Rpg Fantasy Game'
        | 'Game Strategy Game'
        | 'Game Streetfighter'
        | 'Game Zelda'
        | 'Misc Architectural'
        | 'Misc Disco'
        | 'Misc Dreamscape'
        | 'Misc Dystopian'
        | 'Misc Fairy Tale'
        | 'Misc Gothic'
        | 'Misc Grunge'
        | 'Misc Horror'
        | 'Misc Kawaii'
        | 'Misc Lovecraftian'
        | 'Misc Macabre'
        | 'Misc Manga'
        | 'Misc Metropolis'
        | 'Misc Minimalist'
        | 'Misc Monochrome'
        | 'Misc Nautical'
        | 'Misc Space'
        | 'Misc Stained Glass'
        | 'Misc Techwear Fashion'
        | 'Misc Tribal'
        | 'Misc Zentangle'
        | 'Papercraft Collage'
        | 'Papercraft Flat Papercut'
        | 'Papercraft Kirigami'
        | 'Papercraft Paper Mache'
        | 'Papercraft Paper Quilling'
        | 'Papercraft Papercut Collage'
        | 'Papercraft Papercut Shadow Box'
        | 'Papercraft Stacked Papercut'
        | 'Papercraft Thick Layered Papercut'
        | 'Photo Alien'
        | 'Photo Film Noir'
        | 'Photo Glamour'
        | 'Photo Hdr'
        | 'Photo Iphone Photographic'
        | 'Photo Long Exposure'
        | 'Photo Neon Noir'
        | 'Photo Silhouette'
        | 'Photo Tilt Shift'
        | 'Cinematic Diva'
        | 'Abstract Expressionism'
        | 'Academia'
        | 'Action Figure'
        | 'Adorable 3D Character'
        | 'Adorable Kawaii'
        | 'Art Deco'
        | 'Art Nouveau'
        | 'Astral Aura'
        | 'Avant Garde'
        | 'Baroque'
        | 'Bauhaus Style Poster'
        | 'Blueprint Schematic Drawing'
        | 'Caricature'
        | 'Cel Shaded Art'
        | 'Character Design Sheet'
        | 'Classicism Art'
        | 'Color Field Painting'
        | 'Colored Pencil Art'
        | 'Conceptual Art'
        | 'Constructivism'
        | 'Cubism'
        | 'Dadaism'
        | 'Dark Fantasy'
        | 'Dark Moody Atmosphere'
        | 'Dmt Art Style'
        | 'Doodle Art'
        | 'Double Exposure'
        | 'Dripping Paint Splatter Art'
        | 'Expressionism'
        | 'Faded Polaroid Photo'
        | 'Fauvism'
        | 'Flat 2d Art'
        | 'Fortnite Art Style'
        | 'Futurism'
        | 'Glitchcore'
        | 'Glo Fi'
        | 'Googie Art Style'
        | 'Graffiti Art'
        | 'Harlem Renaissance Art'
        | 'High Fashion'
        | 'Idyllic'
        | 'Impressionism'
        | 'Infographic Drawing'
        | 'Ink Dripping Drawing'
        | 'Japanese Ink Drawing'
        | 'Knolling Photography'
        | 'Light Cheery Atmosphere'
        | 'Logo Design'
        | 'Luxurious Elegance'
        | 'Macro Photography'
        | 'Mandola Art'
        | 'Marker Drawing'
        | 'Medievalism'
        | 'Minimalism'
        | 'Neo Baroque'
        | 'Neo Byzantine'
        | 'Neo Futurism'
        | 'Neo Impressionism'
        | 'Neo Rococo'
        | 'Neoclassicism'
        | 'Op Art'
        | 'Ornate And Intricate'
        | 'Pencil Sketch Drawing'
        | 'Pop Art 2'
        | 'Rococo'
        | 'Silhouette Art'
        | 'Simple Vector Art'
        | 'Sketchup'
        | 'Steampunk 2'
        | 'Surrealism'
        | 'Suprematism'
        | 'Terragen'
        | 'Tranquil Relaxing Atmosphere'
        | 'Sticker Designs'
        | 'Vibrant Rim Light'
        | 'Volumetric Lighting'
        | 'Watercolor 2'
        | 'Whimsical And Playful'
        | 'Mk Chromolithography'
        | 'Mk Cross Processing Print'
        | 'Mk Dufaycolor Photograph'
        | 'Mk Herbarium'
        | 'Mk Punk Collage'
        | 'Mk Mosaic'
        | 'Mk Van Gogh'
        | 'Mk Coloring Book'
        | 'Mk Singer Sargent'
        | 'Mk Pollock'
        | 'Mk Basquiat'
        | 'Mk Andy Warhol'
        | 'Mk Halftone Print'
        | 'Mk Gond Painting'
        | 'Mk Albumen Print'
        | 'Mk Aquatint Print'
        | 'Mk Anthotype Print'
        | 'Mk Inuit Carving'
        | 'Mk Bromoil Print'
        | 'Mk Calotype Print'
        | 'Mk Color Sketchnote'
        | 'Mk Cibulak Porcelain'
        | 'Mk Alcohol Ink Art'
        | 'Mk One Line Art'
        | 'Mk Blacklight Paint'
        | 'Mk Carnival Glass'
        | 'Mk Cyanotype Print'
        | 'Mk Cross Stitching'
        | 'Mk Encaustic Paint'
        | 'Mk Embroidery'
        | 'Mk Gyotaku'
        | 'Mk Luminogram'
        | 'Mk Lite Brite Art'
        | 'Mk Mokume Gane'
        | 'Pebble Art'
        | 'Mk Palekh'
        | 'Mk Suminagashi'
        | 'Mk Scrimshaw'
        | 'Mk Shibori'
        | 'Mk Vitreous Enamel'
        | 'Mk Ukiyo E'
        | 'Mk Vintage Airline Poster'
        | 'Mk Vintage Travel Poster'
        | 'Mk Bauhaus Style'
        | 'Mk Afrofuturism'
        | 'Mk Atompunk'
        | 'Mk Constructivism'
        | 'Mk Chicano Art'
        | 'Mk De Stijl'
        | 'Mk Dayak Art'
        | 'Mk Fayum Portrait'
        | 'Mk Illuminated Manuscript'
        | 'Mk Kalighat Painting'
        | 'Mk Madhubani Painting'
        | 'Mk Pictorialism'
        | 'Mk Pichwai Painting'
        | 'Mk Patachitra Painting'
        | 'Mk Samoan Art Inspired'
        | 'Mk Tlingit Art'
        | 'Mk Adnate Style'
        | 'Mk Ron English Style'
        | 'Mk Shepard Fairey Style'
    )[];
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * UOV Image URL
     * @description The image to upscale or vary.
     */
    uov_image_url?: string;
    /**
     * UOV Method
     * @description The method to use for upscaling or varying.
     * @default Disabled
     * @enum {string}
     */
    uov_method?:
        | 'Disabled'
        | 'Vary (Subtle)'
        | 'Vary (Strong)'
        | 'Upscale (1.5x)'
        | 'Upscale (2x)'
        | 'Upscale (Fast 2x)';
}

export interface FooocusImagePromptOutput extends SharedType_7eb1 {}

export interface FooocusInput {
    /**
     * Aspect Ratio
     * @description The size of the generated image. You can choose between some presets or
     *                 custom height and width that **must be multiples of 8**.
     * @default 1024x1024
     */
    aspect_ratio?: string;
    /**
     * Control Image Stop At
     * @description The stop at value of the control image. Use it to control how much the generated image
     *                 should look like the control image.
     * @default 1
     */
    control_image_stop_at?: number;
    /**
     * Control Image Url
     * @description The image to use as a reference for the generated image.
     */
    control_image_url?: string;
    /**
     * Control Image Weight
     * @description The strength of the control image. Use it to control how much the generated image
     *                 should look like the control image.
     * @default 1
     */
    control_image_weight?: number;
    /**
     * Control Type
     * @description The type of image control
     * @default PyraCanny
     * @example ImagePrompt
     * @example PyraCanny
     * @example CPDS
     * @example FaceSwap
     * @enum {string}
     */
    control_type?: 'ImagePrompt' | 'PyraCanny' | 'CPDS' | 'FaceSwap';
    /**
     * Enable Safety Checker
     * @description If set to false, the safety checker will be disabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Inpaint Image Url
     * @description The image to use as a reference for inpainting.
     */
    inpaint_image_url?: string;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use up to 5 LoRAs
     *                 and they will be merged together to generate the final image.
     * @default [
     *       {
     *         "path": "https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_offset_example-lora_1.0.safetensors",
     *         "scale": 0.1
     *       }
     *     ]
     */
    loras?: Components.LoraWeight_4[];
    /**
     * Mask Image Url
     * @description The image to use as a mask for the generated image.
     */
    mask_image_url?: string;
    /**
     * Mixing Image Prompt And Inpaint
     * @default false
     */
    mixing_image_prompt_and_inpaint?: boolean;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example (worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed, grayscale, bw, bad photo, bad photography, bad art:1.4), (watermark, signature, text font, username, error, logo, words, letters, digits, autograph, trademark, name:1.2), (blur, blurry, grainy), morbid, ugly, asymmetrical, mutated malformed, mutilated, poorly lit, bad shadow, draft, cropped, out of frame, cut off, censored, jpeg artifacts, out of focus, glitch, duplicate, (airbrushed, cartoon, anime, semi-realistic, cgi, render, blender, digital art, manga, amateur:1.3), (3D ,3D Game, 3D Game Scene, 3D Character:1.1), (bad hands, bad anatomy, bad body, bad face, bad teeth, bad arms, bad legs, deformities:1.3)
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of images to generate in one request
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Performance
     * @description You can choose Speed or Quality
     * @default Extreme Speed
     * @enum {string}
     */
    performance?: 'Speed' | 'Quality' | 'Extreme Speed' | 'Lightning';
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @default
     * @example an astronaut in the jungle, cold color palette with butterflies in the background, highly detailed, 8k
     */
    prompt?: string;
    /**
     * Refiner Model
     * @description Refiner (SDXL or SD 1.5)
     * @default None
     * @enum {string}
     */
    refiner_model?: 'None' | 'realisticVisionV60B1_v51VAE.safetensors';
    /**
     * Refiner Switch At
     * @description Use 0.4 for SD1.5 realistic models; 0.667 for SD1.5 anime models
     *                 0.8 for XL-refiners; or any value for switching two SDXL models.
     * @default 0.8
     */
    refiner_switch?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     * @example 176400
     */
    seed?: number;
    /**
     * Sharpness
     * @description The sharpness of the generated image. Use it to control how sharp the generated
     *                 image should be. Higher value means image and texture are sharper.
     * @default 2
     */
    sharpness?: number;
    /**
     * Styles
     * @description The style to use.
     * @default [
     *       "Fooocus Enhance",
     *       "Fooocus V2",
     *       "Fooocus Sharp"
     *     ]
     */
    styles?: (
        | 'Fooocus V2'
        | 'Fooocus Enhance'
        | 'Fooocus Sharp'
        | 'Fooocus Semi Realistic'
        | 'Fooocus Masterpiece'
        | 'Fooocus Photograph'
        | 'Fooocus Negative'
        | 'Fooocus Cinematic'
        | 'SAI 3D Model'
        | 'SAI Analog Film'
        | 'SAI Anime'
        | 'SAI Cinematic'
        | 'SAI Comic Book'
        | 'SAI Craft Clay'
        | 'SAI Digital Art'
        | 'SAI Enhance'
        | 'SAI Fantasy Art'
        | 'SAI Isometric'
        | 'SAI Line Art'
        | 'SAI Lowpoly'
        | 'SAI Neonpunk'
        | 'SAI Origami'
        | 'SAI Photographic'
        | 'SAI Pixel Art'
        | 'SAI Texture'
        | 'MRE Cinematic Dynamic'
        | 'MRE Spontaneous Picture'
        | 'MRE Artistic Vision'
        | 'MRE Dark Dream'
        | 'MRE Gloomy Art'
        | 'MRE Bad Dream'
        | 'MRE Underground'
        | 'MRE Surreal Painting'
        | 'MRE Dynamic Illustration'
        | 'MRE Undead Art'
        | 'MRE Elemental Art'
        | 'MRE Space Art'
        | 'MRE Ancient Illustration'
        | 'MRE Brave Art'
        | 'MRE Heroic Fantasy'
        | 'MRE Dark Cyberpunk'
        | 'MRE Lyrical Geometry'
        | 'MRE Sumi E Symbolic'
        | 'MRE Sumi E Detailed'
        | 'MRE Manga'
        | 'MRE Anime'
        | 'MRE Comic'
        | 'Ads Advertising'
        | 'Ads Automotive'
        | 'Ads Corporate'
        | 'Ads Fashion Editorial'
        | 'Ads Food Photography'
        | 'Ads Gourmet Food Photography'
        | 'Ads Luxury'
        | 'Ads Real Estate'
        | 'Ads Retail'
        | 'Artstyle Abstract'
        | 'Artstyle Abstract Expressionism'
        | 'Artstyle Art Deco'
        | 'Artstyle Art Nouveau'
        | 'Artstyle Constructivist'
        | 'Artstyle Cubist'
        | 'Artstyle Expressionist'
        | 'Artstyle Graffiti'
        | 'Artstyle Hyperrealism'
        | 'Artstyle Impressionist'
        | 'Artstyle Pointillism'
        | 'Artstyle Pop Art'
        | 'Artstyle Psychedelic'
        | 'Artstyle Renaissance'
        | 'Artstyle Steampunk'
        | 'Artstyle Surrealist'
        | 'Artstyle Typography'
        | 'Artstyle Watercolor'
        | 'Futuristic Biomechanical'
        | 'Futuristic Biomechanical Cyberpunk'
        | 'Futuristic Cybernetic'
        | 'Futuristic Cybernetic Robot'
        | 'Futuristic Cyberpunk Cityscape'
        | 'Futuristic Futuristic'
        | 'Futuristic Retro Cyberpunk'
        | 'Futuristic Retro Futurism'
        | 'Futuristic Sci Fi'
        | 'Futuristic Vaporwave'
        | 'Game Bubble Bobble'
        | 'Game Cyberpunk Game'
        | 'Game Fighting Game'
        | 'Game Gta'
        | 'Game Mario'
        | 'Game Minecraft'
        | 'Game Pokemon'
        | 'Game Retro Arcade'
        | 'Game Retro Game'
        | 'Game Rpg Fantasy Game'
        | 'Game Strategy Game'
        | 'Game Streetfighter'
        | 'Game Zelda'
        | 'Misc Architectural'
        | 'Misc Disco'
        | 'Misc Dreamscape'
        | 'Misc Dystopian'
        | 'Misc Fairy Tale'
        | 'Misc Gothic'
        | 'Misc Grunge'
        | 'Misc Horror'
        | 'Misc Kawaii'
        | 'Misc Lovecraftian'
        | 'Misc Macabre'
        | 'Misc Manga'
        | 'Misc Metropolis'
        | 'Misc Minimalist'
        | 'Misc Monochrome'
        | 'Misc Nautical'
        | 'Misc Space'
        | 'Misc Stained Glass'
        | 'Misc Techwear Fashion'
        | 'Misc Tribal'
        | 'Misc Zentangle'
        | 'Papercraft Collage'
        | 'Papercraft Flat Papercut'
        | 'Papercraft Kirigami'
        | 'Papercraft Paper Mache'
        | 'Papercraft Paper Quilling'
        | 'Papercraft Papercut Collage'
        | 'Papercraft Papercut Shadow Box'
        | 'Papercraft Stacked Papercut'
        | 'Papercraft Thick Layered Papercut'
        | 'Photo Alien'
        | 'Photo Film Noir'
        | 'Photo Glamour'
        | 'Photo Hdr'
        | 'Photo Iphone Photographic'
        | 'Photo Long Exposure'
        | 'Photo Neon Noir'
        | 'Photo Silhouette'
        | 'Photo Tilt Shift'
        | 'Cinematic Diva'
        | 'Abstract Expressionism'
        | 'Academia'
        | 'Action Figure'
        | 'Adorable 3D Character'
        | 'Adorable Kawaii'
        | 'Art Deco'
        | 'Art Nouveau'
        | 'Astral Aura'
        | 'Avant Garde'
        | 'Baroque'
        | 'Bauhaus Style Poster'
        | 'Blueprint Schematic Drawing'
        | 'Caricature'
        | 'Cel Shaded Art'
        | 'Character Design Sheet'
        | 'Classicism Art'
        | 'Color Field Painting'
        | 'Colored Pencil Art'
        | 'Conceptual Art'
        | 'Constructivism'
        | 'Cubism'
        | 'Dadaism'
        | 'Dark Fantasy'
        | 'Dark Moody Atmosphere'
        | 'Dmt Art Style'
        | 'Doodle Art'
        | 'Double Exposure'
        | 'Dripping Paint Splatter Art'
        | 'Expressionism'
        | 'Faded Polaroid Photo'
        | 'Fauvism'
        | 'Flat 2d Art'
        | 'Fortnite Art Style'
        | 'Futurism'
        | 'Glitchcore'
        | 'Glo Fi'
        | 'Googie Art Style'
        | 'Graffiti Art'
        | 'Harlem Renaissance Art'
        | 'High Fashion'
        | 'Idyllic'
        | 'Impressionism'
        | 'Infographic Drawing'
        | 'Ink Dripping Drawing'
        | 'Japanese Ink Drawing'
        | 'Knolling Photography'
        | 'Light Cheery Atmosphere'
        | 'Logo Design'
        | 'Luxurious Elegance'
        | 'Macro Photography'
        | 'Mandola Art'
        | 'Marker Drawing'
        | 'Medievalism'
        | 'Minimalism'
        | 'Neo Baroque'
        | 'Neo Byzantine'
        | 'Neo Futurism'
        | 'Neo Impressionism'
        | 'Neo Rococo'
        | 'Neoclassicism'
        | 'Op Art'
        | 'Ornate And Intricate'
        | 'Pencil Sketch Drawing'
        | 'Pop Art 2'
        | 'Rococo'
        | 'Silhouette Art'
        | 'Simple Vector Art'
        | 'Sketchup'
        | 'Steampunk 2'
        | 'Surrealism'
        | 'Suprematism'
        | 'Terragen'
        | 'Tranquil Relaxing Atmosphere'
        | 'Sticker Designs'
        | 'Vibrant Rim Light'
        | 'Volumetric Lighting'
        | 'Watercolor 2'
        | 'Whimsical And Playful'
        | 'Mk Chromolithography'
        | 'Mk Cross Processing Print'
        | 'Mk Dufaycolor Photograph'
        | 'Mk Herbarium'
        | 'Mk Punk Collage'
        | 'Mk Mosaic'
        | 'Mk Van Gogh'
        | 'Mk Coloring Book'
        | 'Mk Singer Sargent'
        | 'Mk Pollock'
        | 'Mk Basquiat'
        | 'Mk Andy Warhol'
        | 'Mk Halftone Print'
        | 'Mk Gond Painting'
        | 'Mk Albumen Print'
        | 'Mk Aquatint Print'
        | 'Mk Anthotype Print'
        | 'Mk Inuit Carving'
        | 'Mk Bromoil Print'
        | 'Mk Calotype Print'
        | 'Mk Color Sketchnote'
        | 'Mk Cibulak Porcelain'
        | 'Mk Alcohol Ink Art'
        | 'Mk One Line Art'
        | 'Mk Blacklight Paint'
        | 'Mk Carnival Glass'
        | 'Mk Cyanotype Print'
        | 'Mk Cross Stitching'
        | 'Mk Encaustic Paint'
        | 'Mk Embroidery'
        | 'Mk Gyotaku'
        | 'Mk Luminogram'
        | 'Mk Lite Brite Art'
        | 'Mk Mokume Gane'
        | 'Pebble Art'
        | 'Mk Palekh'
        | 'Mk Suminagashi'
        | 'Mk Scrimshaw'
        | 'Mk Shibori'
        | 'Mk Vitreous Enamel'
        | 'Mk Ukiyo E'
        | 'Mk Vintage Airline Poster'
        | 'Mk Vintage Travel Poster'
        | 'Mk Bauhaus Style'
        | 'Mk Afrofuturism'
        | 'Mk Atompunk'
        | 'Mk Constructivism'
        | 'Mk Chicano Art'
        | 'Mk De Stijl'
        | 'Mk Dayak Art'
        | 'Mk Fayum Portrait'
        | 'Mk Illuminated Manuscript'
        | 'Mk Kalighat Painting'
        | 'Mk Madhubani Painting'
        | 'Mk Pictorialism'
        | 'Mk Pichwai Painting'
        | 'Mk Patachitra Painting'
        | 'Mk Samoan Art Inspired'
        | 'Mk Tlingit Art'
        | 'Mk Adnate Style'
        | 'Mk Ron English Style'
        | 'Mk Shepard Fairey Style'
    )[];
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FooocusOutput extends SharedType_7eb1 {}

export interface FluxSrpoImageToImageInput extends SharedType_0cd {}

export interface FluxSrpoImageToImageOutput extends SharedType_a73 {}

export interface FluxSrpoInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *             the model to stick to your prompt when looking for a related image to show you.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Mountain guide, sturdy build, wilderness wisdom, alert gaze, technical outdoor gear with rope coils, snow-capped peaks background, crisp mountain lighting, leading pose, wind-swept hair with full beard, weather-worn face with quiet confidence, alpine expert presence
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxSrpoOutput extends SharedType_368 {}

export interface FluxSchnellReduxInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description The URL of the image to generate an image from.
     * @example https://fal.media/files/kangaroo/acQvq-Kmo2lajkgvcEHdv.png
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 4
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxSchnellReduxOutput extends SharedType_a73 {}

export interface FluxSchnellInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *             the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 4
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word "FLUX" is painted over it in big, white brush strokes with visible texture.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxSchnellOutput extends SharedType_a73 {}

export interface FluxKreaReduxInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *             the model to stick to your prompt when looking for a related image to show you.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description The URL of the image to generate an image from.
     * @example https://storage.googleapis.com/falserverless/example_inputs/flux_krea_redux_output_1.jpg
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxKreaReduxOutput extends SharedType_96f {}

export interface FluxKreaImageToImageInput extends SharedType_0cd {}

export interface FluxKreaImageToImageOutput extends SharedType_a73 {}

export interface FluxKreaInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *             the model to stick to your prompt when looking for a related image to show you.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A candid street photo of a woman with a pink bob and bold eyeliner on a graffiti-covered subway platform. She wears a bright yellow patent leather coat over a black-and-white checkered turtleneck and platform boots. Natural subway lighting creates an authentic urban scene with a relaxed, unposed feel.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxKreaOutput extends SharedType_609 {}

export interface FluxDevReduxInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *             the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description The URL of the image to generate an image from.
     * @example https://fal.media/files/kangaroo/acQvq-Kmo2lajkgvcEHdv.png
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxDevReduxOutput extends SharedType_a73 {}

export interface FluxDevImageToImageInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *             the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The URL of the image to generate an image from.
     * @example https://fal.media/files/koala/Chls9L2ZnvuipUTEwlnJC.png
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A cat dressed as a wizard with a background of a mystic forest.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength of the initial image. Higher strength values are better for this model.
     * @default 0.95
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxDevImageToImageOutput extends SharedType_a73 {}

export interface FluxDevInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *             the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word "FLUX" is painted over it in big, white brush strokes with visible texture.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxDevOutput extends SharedType_a73 {}

export interface FluxVisionUpscalerInput {
    /**
     * Creativity
     * @description The creativity of the model. The higher the creativity, the more the model will deviate from the original. Refers to the denoise strength of the sampling.
     * @default 0.3
     */
    creativity?: number;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance
     * @description CFG/guidance scale (1-4). Controls how closely the model follows the prompt.
     * @default 1
     */
    guidance?: number;
    /**
     * Image Url
     * @description The URL of the image to upscale.
     * @example https://storage.googleapis.com/falserverless/gallery/NOCA_Mick-Thompson.resized.resized.jpg
     */
    image_url: string;
    /**
     * Seed
     * @description The seed to use for the upscale. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Steps
     * @description Number of inference steps (4-50).
     * @default 20
     */
    steps?: number;
    /**
     * Upscale Factor
     * @description The upscale factor (1-4x).
     * @default 2
     */
    upscale_factor?: number;
}

export interface FluxVisionUpscalerOutput {
    /**
     * Caption
     * @description The VLM-generated caption describing the upscaled image.
     * @example A highly detailed upscaled photograph featuring sharp edges and enhanced textures. The image shows improved clarity in fine details with natural color preservation and minimal artifacts, demonstrating the AI-enhanced resolution increase.
     */
    caption: string;
    /**
     * @description The URL of the generated image.
     * @example {
     *       "height": 2048,
     *       "file_size": 8842156,
     *       "file_name": "20TZeUQtQ8oKgsCKXSL81_StableSR_00002_.png",
     *       "content_type": "image/png",
     *       "url": "https://v3b.fal.media/files/b/panda/20TZeUQtQ8oKgsCKXSL81_StableSR_00002_.png",
     *       "width": 2048
     *     }
     */
    image: Components.Image_2;
    /**
     * Seed
     * @description The seed used to generate the image.
     * @example 42
     */
    seed: number;
    /**
     * Timings
     * @description The timings of the different steps in the workflow.
     * @example {
     *       "inference": 52.8
     *     }
     */
    timings: {
        [key: string]: number;
    };
}

export interface FluxSubjectInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description URL of image of the subject
     * @example https://storage.googleapis.com/falserverless/model_tests/ominicontrol/ominishirt.jpg
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example On the beach, a lady sits under a beach umbrella with 'Omini' written on it. She's wearing this shirt and has a big smile on her face, with her surfboard hehind her. The sun is setting in the background. The sky is a beautiful shade of orange and purple.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxSubjectOutput extends SharedType_a73 {}

export interface FluxPulidInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Id Weight
     * @description The weight of the ID loss.
     * @default 1
     */
    id_weight?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Max Sequence Length
     * @description The maximum sequence length for the model.
     * @default 128
     * @enum {string}
     */
    max_sequence_length?: '128' | '256' | '512';
    /**
     * Negative Prompt
     * @description The prompt to generate an image from.
     * @default
     * @example bad quality, worst quality, text, signature, watermark, extra limbs
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 20
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example a woman holding sign with glowing green text 'PuLID for FLUX'
     */
    prompt: string;
    /**
     * Reference Image URL
     * @description URL of image to use for inpainting.
     * @example https://storage.googleapis.com/falserverless/gallery/example_inputs_liuyifei.png
     */
    reference_image_url: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Start Step
     * @description The number of steps to start the CFG from.
     * @default 0
     */
    start_step?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * True Cfg
     * @description The weight of the CFG loss.
     * @default 1
     */
    true_cfg?: number;
}

export interface FluxPulidOutput extends SharedType_a73 {}

export interface FluxProV1FillFinetunedInput {
    /**
     * Enhance Prompt
     * @description Whether to enhance the prompt for better results.
     * @default false
     */
    enhance_prompt?: boolean;
    /**
     * Fine-tune ID
     * @description References your specific model
     */
    finetune_id: string;
    /**
     * Fine-tune Strength
     * @description Controls finetune influence.
     *             Increase this value if your target concept isn't showing up strongly enough.
     *             The optimal setting depends on your finetune and prompt
     */
    finetune_strength: number;
    /**
     * Image URL
     * @description The image URL to generate an image from. Needs to match the dimensions of the mask.
     * @example https://storage.googleapis.com/falserverless/flux-lora/example-images/knight.jpeg
     */
    image_url: string;
    /**
     * Mask URL
     * @description The mask URL to inpaint the image. Needs to match the dimensions of the input image.
     * @example https://storage.googleapis.com/falserverless/flux-lora/example-images/mask_knight.jpeg
     */
    mask_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to fill the masked part of the image.
     * @example A knight in shining armour holding a greatshield with "FAL" on it
     */
    prompt: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxProV1FillFinetunedOutput extends SharedType_a73 {}

export interface FluxProV1FillInput {
    /**
     * Enhance Prompt
     * @description Whether to enhance the prompt for better results.
     * @default false
     */
    enhance_prompt?: boolean;
    /**
     * Image URL
     * @description The image URL to generate an image from. Needs to match the dimensions of the mask.
     * @example https://storage.googleapis.com/falserverless/flux-lora/example-images/knight.jpeg
     */
    image_url: string;
    /**
     * Mask URL
     * @description The mask URL to inpaint the image. Needs to match the dimensions of the input image.
     * @example https://storage.googleapis.com/falserverless/flux-lora/example-images/mask_knight.jpeg
     */
    mask_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to fill the masked part of the image.
     * @example A knight in shining armour holding a greatshield with "FAL" on it
     */
    prompt: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxProV1FillOutput extends SharedType_a73 {}

export interface FluxProV11ReduxInput {
    /**
     * Enhance Prompt
     * @description Whether to enhance the prompt for better results.
     * @default false
     */
    enhance_prompt?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description The image URL to generate an image from. Needs to match the dimensions of the mask.
     * @example https://fal.media/files/kangaroo/acQvq-Kmo2lajkgvcEHdv.png
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @default
     */
    prompt?: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxProV11ReduxOutput extends SharedType_a73 {}

export interface FluxProV11UltraReduxInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @default 16:9
     */
    aspect_ratio?:
        | ('21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21')
        | string;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Enhance Prompt
     * @description Whether to enhance the prompt for better results.
     * @default false
     */
    enhance_prompt?: boolean;
    /**
     * Image Prompt Strength
     * @description The strength of the image prompt, between 0 and 1.
     * @default 0.1
     */
    image_prompt_strength?: number;
    /**
     * Image URL
     * @description The image URL to generate an image from. Needs to match the dimensions of the mask.
     * @example https://fal.media/files/kangaroo/acQvq-Kmo2lajkgvcEHdv.png
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @default
     */
    prompt?: string;
    /**
     * Raw
     * @description Generate less processed, more natural-looking images.
     * @default false
     */
    raw?: boolean;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxProV11UltraReduxOutput extends SharedType_a73 {}

export interface FluxProV11UltraFinetunedInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @default 16:9
     */
    aspect_ratio?:
        | ('21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21')
        | string;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Enhance Prompt
     * @description Whether to enhance the prompt for better results.
     * @default false
     */
    enhance_prompt?: boolean;
    /**
     * Fine-tune ID
     * @description References your specific model
     */
    finetune_id: string;
    /**
     * Fine-tune Strength
     * @description Controls finetune influence.
     *             Increase this value if your target concept isn't showing up strongly enough.
     *             The optimal setting depends on your finetune and prompt
     */
    finetune_strength: number;
    /**
     * Image Prompt Strength
     * @description The strength of the image prompt, between 0 and 1.
     * @default 0.1
     */
    image_prompt_strength?: number;
    /**
     * Image URL
     * @description The image URL to generate an image from.
     */
    image_url?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word "FLUX" is painted over it in big, white brush strokes with visible texture.
     */
    prompt: string;
    /**
     * Raw
     * @description Generate less processed, more natural-looking images.
     * @default false
     */
    raw?: boolean;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxProV11UltraFinetunedOutput extends SharedType_a73 {}

export interface FluxProV11UltraInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated image.
     * @default 16:9
     */
    aspect_ratio?:
        | ('21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21')
        | string;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Enhance Prompt
     * @description Whether to enhance the prompt for better results.
     * @default false
     */
    enhance_prompt?: boolean;
    /**
     * Image Prompt Strength
     * @description The strength of the image prompt, between 0 and 1.
     * @default 0.1
     */
    image_prompt_strength?: number;
    /**
     * Image URL
     * @description The image URL to generate an image from.
     */
    image_url?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word "FLUX" is painted over it in big, white brush strokes with visible texture.
     */
    prompt: string;
    /**
     * Raw
     * @description Generate less processed, more natural-looking images.
     * @default false
     */
    raw?: boolean;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxProV11UltraOutput extends SharedType_a73 {}

export interface FluxProV11Input {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Enhance Prompt
     * @description Whether to enhance the prompt for better results.
     * @default false
     */
    enhance_prompt?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word "FLUX" is painted over it in big, white brush strokes with visible texture.
     */
    prompt: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5' | '6';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxProV11Output extends SharedType_a73 {}

export interface FluxProKontextTextToImageInput extends SharedType_d99 {}

export interface FluxProKontextTextToImageOutput extends SharedType_a73 {}

export interface FluxProKontextMultiInput extends SharedType_137 {}

export interface FluxProKontextMultiOutput extends SharedType_a73 {}

export interface FluxProKontextMaxTextToImageInput extends SharedType_d99 {}

export interface FluxProKontextMaxTextToImageOutput extends SharedType_a73 {}

export interface FluxProKontextMaxMultiInput extends SharedType_137 {}

export interface FluxProKontextMaxMultiOutput extends SharedType_a73 {}

export interface FluxProKontextMaxInput extends SharedType_75b {}

export interface FluxProKontextMaxOutput extends SharedType_6d6 {}

export interface FluxProKontextInput extends SharedType_75b {}

export interface FluxProKontextOutput extends SharedType_6d6 {}

export interface FluxLoraStreamInput extends SharedType_207 {}

export interface FluxLoraStreamOutput extends SharedType_a73 {}

export interface FluxLoraInpaintingInput extends SharedType_88d {}

export interface FluxLoraInpaintingOutput extends SharedType_a73 {}

export interface FluxLoraImageToImageInput extends SharedType_4ae {}

export interface FluxLoraImageToImageOutput extends SharedType_a73 {}

export interface FluxLoraPortraitTrainerInput {
    /**
     * Create Masks
     * @description If True, masks will be created for the subject.
     * @default false
     * @example false
     */
    create_masks?: boolean;
    /**
     * Data Archive Format
     * @description The format of the archive. If not specified, the format will be inferred from the URL.
     */
    data_archive_format?: string;
    /**
     * Images Data Url
     * @description URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
     *
     *             In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.
     *
     *             The captions can include a special string `[trigger]`. If a trigger_word is specified, it will replace `[trigger]` in the captions.
     */
    images_data_url: string;
    /**
     * Learning Rate
     * @description Learning rate to use for training.
     * @default 0.00009
     * @example 0.0002
     */
    learning_rate?: number;
    /**
     * Multiresolution Training
     * @description If True, multiresolution training will be used.
     * @default true
     * @example true
     */
    multiresolution_training?: boolean;
    /**
     * Resume From Checkpoint
     * @description URL to a checkpoint to resume training from.
     * @default
     */
    resume_from_checkpoint?: string;
    /**
     * Steps
     * @description Number of steps to train the LoRA on.
     * @default 2500
     * @example 1000
     */
    steps?: number;
    /**
     * Subject Crop
     * @description If True, the subject will be cropped from the image.
     * @default true
     * @example true
     */
    subject_crop?: boolean;
    /**
     * Trigger Phrase
     * @description Trigger phrase to be used in the captions. If None, a trigger word will not be used.
     *             If no captions are provide the trigger_work will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions.
     */
    trigger_phrase?: string;
}

export interface FluxLoraPortraitTrainerOutput {
    /**
     * Config File
     * @description URL to the training configuration file.
     */
    config_file: Components.File;
    /**
     * Diffusers Lora File
     * @description URL to the trained diffusers lora weights.
     */
    diffusers_lora_file: Components.File;
}

export interface FluxLoraFillInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Fill Image
     * @description Use an image fill input to fill in particular images into the masked area.
     */
    fill_image?: Components.ImageFillInput_1;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 30
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description URL of image to use for fill operation
     * @example https://storage.googleapis.com/falserverless/flux-lora/example-images/knight.jpeg
     */
    image_url: string;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Mask Url
     * @description The mask to area to Inpaint in.
     * @example https://storage.googleapis.com/falserverless/flux-lora/example-images/mask_knight.jpeg
     */
    mask_url: string;
    /**
     * Num Images
     * @description The number of images to generate. This is always set to 1 for streaming output.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Paste Back
     * @description Specifies whether to paste-back the original image onto to the non-inpainted areas of the output
     * @default true
     */
    paste_back?: boolean;
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @default
     * @example A knight in shining armour holding a greatshield with 'FAL' on it
     */
    prompt?: string;
    /**
     * Resize To Original
     * @description Resizes the image back to the original size. Use when you wish to preserve the exact image size as the originally provided image.
     * @default false
     */
    resize_to_original?: boolean;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxLoraFillOutput extends SharedType_a73 {}

export interface FluxLoraFastTrainingInput extends SharedType_f11 {}

export interface FluxLoraFastTrainingOutput extends SharedType_a54 {}

export interface FluxLoraDepthInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description URL of image to use for depth input
     * @example https://fal.media/files/penguin/vt-SeIOweN7_oYBsvGO6t.png
     */
    image_url: string;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Num Images
     * @description The number of images to generate. This is always set to 1 for streaming output.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Black hole in space, orange accretion disc
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxLoraDepthOutput extends SharedType_a73 {}

export interface FluxLoraCannyInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 30
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description URL of image to use for canny input
     * @example https://fal.media/files/kangaroo/eNSkRdVFzNvDkrrMjxFA3.png
     */
    image_url: string;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A blue owl.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxLoraCannyOutput extends SharedType_a73 {}

export interface FluxLoraInput extends SharedType_207 {}

export interface FluxLoraOutput extends SharedType_a73 {}

export interface FluxKreaTrainerInput extends SharedType_f11 {}

export interface FluxKreaTrainerOutput extends SharedType_a54 {}

export interface FluxKreaLoraStreamInput extends SharedType_207 {}

export interface FluxKreaLoraStreamOutput extends SharedType_a73 {}

export interface FluxKreaLoraInpaintingInput extends SharedType_88d {}

export interface FluxKreaLoraInpaintingOutput extends SharedType_a73 {}

export interface FluxKreaLoraImageToImageInput extends SharedType_4ae {}

export interface FluxKreaLoraImageToImageOutput extends SharedType_a73 {}

export interface FluxKreaLoraInput extends SharedType_207 {}

export interface FluxKreaLoraOutput extends SharedType_a73 {}

export interface FluxKontextDevInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The URL of the image to edit.
     * @example https://storage.googleapis.com/falserverless/example_inputs/kontext_example_input.webp
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description Output format
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to edit the image.
     * @example Change the setting to a day time, add a lot of people walking the sidewalk while maintaining the same style of the painting
     */
    prompt: string;
    /**
     * Resolution Mode
     * @description Determines how the output resolution is set for image editing.
     *                  - `auto`: The model selects an optimal resolution from a predefined set that best matches the input image's aspect ratio. This is the recommended setting for most use cases as it's what the model was trained on.
     *                  - `match_input`: The model will attempt to use the same resolution as the input image. The resolution will be adjusted to be compatible with the model's requirements (e.g. dimensions must be multiples of 16 and within supported limits).
     *                  Apart from these, a few aspect ratios are also supported.
     * @default match_input
     * @enum {string}
     */
    resolution_mode?:
        | 'auto'
        | 'match_input'
        | '1:1'
        | '16:9'
        | '21:9'
        | '3:2'
        | '2:3'
        | '4:5'
        | '5:4'
        | '3:4'
        | '4:3'
        | '9:16'
        | '9:21';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxKontextDevOutput extends SharedType_d4a {}

export interface FluxKontextTrainerInput {
    /**
     * Default Caption
     * @description Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string;
    /**
     * Image Data Url
     * @description URL to the input data zip archive.
     *
     *             The zip should contain pairs of images. The images should be named:
     *
     *             ROOT_start.EXT and ROOT_end.EXT
     *             For example:
     *             photo_start.jpg and photo_end.jpg
     *
     *             The zip can also contain a text file for each image pair. The text file should be named:
     *             ROOT.txt
     *             For example:
     *             photo.txt
     *
     *             This text file can be used to specify the edit instructions for the image pair.
     *
     *             If no text file is provided, the default_caption will be used.
     *
     *             If no default_caption is provided, the training will fail.
     */
    image_data_url: string;
    /**
     * Learning Rate
     * @default 0.0001
     */
    learning_rate?: number;
    /**
     * Output Lora Format
     * @description Dictates the naming scheme for the output weights
     * @default fal
     * @enum {string}
     */
    output_lora_format?: 'fal' | 'comfy';
    /**
     * Steps
     * @description Number of steps to train for
     * @default 1000
     */
    steps?: number;
}

export interface FluxKontextTrainerOutput {
    /**
     * Config File
     * @description URL to the configuration file for the trained model.
     */
    config_file: Components.File;
    /**
     * Diffusers Lora File
     * @description URL to the trained diffusers lora weights.
     */
    diffusers_lora_file: Components.File;
}

export interface FluxKontextLoraTextToImageInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate the image with
     * @example Mount Fuji with cherry blossoms in the foreground, clear sky, peaceful spring day, soft natural light, realistic landscape.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxKontextLoraTextToImageOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/jpeg",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/kontext_example_t2i_output.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface FluxKontextLoraInpaintInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The URL of the image to be inpainted.
     * @example https://storage.googleapis.com/falserverless/example_inputs/image_kontext_inpaint.jpeg
     */
    image_url: string;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Image URL
     * @description The URL of the mask for inpainting.
     * @example https://storage.googleapis.com/falserverless/example_inputs/mask_kontext_inpaint.png
     */
    mask_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt for the image to image task.
     * @example A football lying on a field.
     */
    prompt: string;
    /**
     * Reference Image URL
     * @description The URL of the reference image for inpainting.
     * @example https://storage.googleapis.com/falserverless/example_inputs/reference_kontext_inpaint.jpeg
     */
    reference_image_url: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength of the initial image. Higher strength values are better for this model.
     * @default 0.88
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxKontextLoraInpaintOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated image files info.
     * @example [
     *       {
     *         "height": 832,
     *         "content_type": "image/jpeg",
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/kontext_inpaint_output.png",
     *         "width": 1248
     *       }
     *     ]
     */
    images: Components.Image_1[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface FluxKontextLoraInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default none
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The URL of the image to edit.
     *
     *     Max width: 14142px, Max height: 14142px, Timeout: 20s
     * @example https://storage.googleapis.com/falserverless/example_inputs/kontext_example_input.webp
     */
    image_url: string;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to edit the image.
     * @example change the setting to a day time, add a lot of people walking the sidewalk while maintaining the same style of the painting
     */
    prompt: string;
    /**
     * Resolution Mode
     * @description Determines how the output resolution is set for image editing.
     *                 - `auto`: The model selects an optimal resolution from a predefined set that best matches the input image's aspect ratio. This is the recommended setting for most use cases as it's what the model was trained on.
     *                 - `match_input`: The model will attempt to use the same resolution as the input image. The resolution will be adjusted to be compatible with the model's requirements (e.g. dimensions must be multiples of 16 and within supported limits).
     *                 Apart from these, a few aspect ratios are also supported.
     * @default match_input
     * @enum {string}
     */
    resolution_mode?:
        | 'auto'
        | 'match_input'
        | '1:1'
        | '16:9'
        | '21:9'
        | '3:2'
        | '2:3'
        | '4:5'
        | '5:4'
        | '3:4'
        | '4:3'
        | '9:16'
        | '9:21';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxKontextLoraOutput extends SharedType_d4a {}

export interface FluxGeneralRfInversionInput {
    /**
     * Base Shift
     * @description Base shift for the scheduled timesteps
     * @default 0.5
     */
    base_shift?: number;
    /**
     * Control Loras
     * @description The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    control_loras?: Components.ControlLoraWeight[];
    /**
     * Controller Guidance Forward
     * @description The controller guidance (gamma) used in the creation of structured noise.
     * @default 0.6
     */
    controller_guidance_forward?: number;
    /**
     * Controller Guidance Reverse
     * @description The controller guidance (eta) used in the denoising process.Using values closer to 1 will result in an image closer to input.
     * @default 0.75
     */
    controller_guidance_reverse?: number;
    /**
     * Controlnet Unions
     * @description The controlnet unions to use for the image generation. Only one controlnet is supported at the moment.
     * @default []
     */
    controlnet_unions?: Components.ControlNetUnion[];
    /**
     * Controlnets
     * @description The controlnets to use for the image generation. Only one controlnet is supported at the moment.
     * @default []
     */
    controlnets?: Components.ControlNet[];
    /**
     * Easycontrols
     * @description EasyControl Inputs to use for image generation.
     * @default []
     */
    easycontrols?: Components.EasyControlWeight[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Fill Image
     * @description Use an image input to influence the generation. Can be used to fill images in masked areas.
     */
    fill_image?: Components.ImageFillInput;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description URL of image to be edited
     * @example https://storage.googleapis.com/falserverless/flux-general-tests/anime_style.png
     */
    image_url: string;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight_2[];
    /**
     * Max Shift
     * @description Max shift for the scheduled timesteps
     * @default 1.15
     */
    max_shift?: number;
    /**
     * NAG alpha
     * @description The alpha value for NAG. This value is used as a final weighting
     *                 factor for steering the normalized guidance (positive and negative prompts)
     *                 in the direction of the positive prompt. Higher values will result in less
     *                 steering on the normalized guidance where lower values will result in
     *                 considering the positive prompt guidance more.
     * @default 0.25
     */
    nag_alpha?: number;
    /**
     * Proportion of steps to apply NAG
     * @description The proportion of steps to apply NAG. After the specified proportion
     *                 of steps has been iterated, the remaining steps will use original
     *                 attention processors in FLUX.
     * @default 0.25
     */
    nag_end?: number;
    /**
     * NAG scale
     * @description The scale for NAG. Higher values will result in a image that is more distant
     *                 to the negative prompt.
     * @default 3
     */
    nag_scale?: number;
    /**
     * NAG Tau
     * @description The tau for NAG. Controls the normalization of the hidden state.
     *                 Higher values will result in a less aggressive normalization,
     *                 but may also lead to unexpected changes with respect to the original image.
     *                 Not recommended to change this value.
     * @default 2.5
     */
    nag_tau?: number;
    /**
     * Negative Prompt
     * @description Negative prompt to steer the image generation away from unwanted features.
     *                 By default, we will be using NAG for processing the negative prompt.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate. This is always set to 1 for streaming output.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to edit the image with
     * @example Wearing glasses
     */
    prompt: string;
    /**
     * Reference End
     * @description The percentage of the total timesteps when the reference guidance is to be ended.
     * @default 1
     */
    reference_end?: number;
    /**
     * Reference Image Url
     * @description URL of Image for Reference-Only
     */
    reference_image_url?: string;
    /**
     * Reference Start
     * @description The percentage of the total timesteps when the reference guidance is to bestarted.
     * @default 0
     */
    reference_start?: number;
    /**
     * Reference Strength
     * @description Strength of reference_only generation. Only used if a reference image is provided.
     * @default 0.65
     */
    reference_strength?: number;
    /**
     * Reverse Guidance End
     * @description Timestep to stop guidance during reverse process.
     * @default 8
     */
    reverse_guidance_end?: number;
    /**
     * Reverse Guidance Schedule
     * @description Scheduler for applying reverse guidance.
     * @default constant
     * @enum {string}
     */
    reverse_guidance_schedule?: 'constant' | 'linear_increase' | 'linear_decrease';
    /**
     * Reverse Guidance Start
     * @description Timestep to start guidance during reverse process.
     * @default 0
     */
    reverse_guidance_start?: number;
    /**
     * Scheduler
     * @description Scheduler for the denoising process.
     * @default euler
     * @enum {string}
     */
    scheduler?: 'euler' | 'dpmpp_2m';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sigma Schedule
     * @description Sigmas schedule for the denoising process.
     * @enum {string}
     */
    sigma_schedule?: 'sgm_uniform';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Beta Schedule
     * @description Specifies whether beta sigmas ought to be used.
     * @default false
     */
    use_beta_schedule?: boolean;
    /**
     * Use CFG-Zero-Init
     * @description Uses CFG-zero init sampling as in https://arxiv.org/abs/2503.18886.
     * @default false
     */
    use_cfg_zero?: boolean;
}

export interface FluxGeneralRfInversionOutput extends SharedType_a73 {}

export interface FluxGeneralInpaintingInput {
    /**
     * Base Shift
     * @description Base shift for the scheduled timesteps
     * @default 0.5
     */
    base_shift?: number;
    /**
     * Control Loras
     * @description The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    control_loras?: Components.ControlLoraWeight[];
    /**
     * Controlnet Unions
     * @description The controlnet unions to use for the image generation. Only one controlnet is supported at the moment.
     * @default []
     */
    controlnet_unions?: Components.ControlNetUnion[];
    /**
     * Controlnets
     * @description The controlnets to use for the image generation. Only one controlnet is supported at the moment.
     * @default []
     */
    controlnets?: Components.ControlNet[];
    /**
     * Easycontrols
     * @description EasyControl Inputs to use for image generation.
     * @default []
     */
    easycontrols?: Components.EasyControlWeight[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Fill Image
     * @description Use an image input to influence the generation. Can be used to fill images in masked areas.
     */
    fill_image?: Components.ImageFillInput;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description URL of image to use for inpainting. or img2img
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png
     */
    image_url: string;
    /**
     * Ip Adapters
     * @description IP-Adapter to use for image generation.
     * @default []
     */
    ip_adapters?: Components.IPAdapter[];
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight_2[];
    /**
     * Mask Url
     * @description The mask to area to Inpaint in.
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png
     */
    mask_url: string;
    /**
     * Max Shift
     * @description Max shift for the scheduled timesteps
     * @default 1.15
     */
    max_shift?: number;
    /**
     * NAG alpha
     * @description The alpha value for NAG. This value is used as a final weighting
     *                 factor for steering the normalized guidance (positive and negative prompts)
     *                 in the direction of the positive prompt. Higher values will result in less
     *                 steering on the normalized guidance where lower values will result in
     *                 considering the positive prompt guidance more.
     * @default 0.25
     */
    nag_alpha?: number;
    /**
     * Proportion of steps to apply NAG
     * @description The proportion of steps to apply NAG. After the specified proportion
     *                 of steps has been iterated, the remaining steps will use original
     *                 attention processors in FLUX.
     * @default 0.25
     */
    nag_end?: number;
    /**
     * NAG scale
     * @description The scale for NAG. Higher values will result in a image that is more distant
     *                 to the negative prompt.
     * @default 3
     */
    nag_scale?: number;
    /**
     * NAG Tau
     * @description The tau for NAG. Controls the normalization of the hidden state.
     *                 Higher values will result in a less aggressive normalization,
     *                 but may also lead to unexpected changes with respect to the original image.
     *                 Not recommended to change this value.
     * @default 2.5
     */
    nag_tau?: number;
    /**
     * Negative Prompt
     * @description Negative prompt to steer the image generation away from unwanted features.
     *                 By default, we will be using NAG for processing the negative prompt.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate. This is always set to 1 for streaming output.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A photo of a lion sitting on a stone bench
     */
    prompt: string;
    /**
     * Real CFG scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    real_cfg_scale?: number;
    /**
     * Reference End
     * @description The percentage of the total timesteps when the reference guidance is to be ended.
     * @default 1
     */
    reference_end?: number;
    /**
     * Reference Image Url
     * @description URL of Image for Reference-Only
     */
    reference_image_url?: string;
    /**
     * Reference Start
     * @description The percentage of the total timesteps when the reference guidance is to bestarted.
     * @default 0
     */
    reference_start?: number;
    /**
     * Reference Strength
     * @description Strength of reference_only generation. Only used if a reference image is provided.
     * @default 0.65
     */
    reference_strength?: number;
    /**
     * Scheduler
     * @description Scheduler for the denoising process.
     * @default euler
     * @enum {string}
     */
    scheduler?: 'euler' | 'dpmpp_2m';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sigma Schedule
     * @description Sigmas schedule for the denoising process.
     * @enum {string}
     */
    sigma_schedule?: 'sgm_uniform';
    /**
     * Strength
     * @description The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original.
     * @default 0.85
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Beta Schedule
     * @description Specifies whether beta sigmas ought to be used.
     * @default false
     */
    use_beta_schedule?: boolean;
    /**
     * Use CFG-Zero-Init
     * @description Uses CFG-zero init sampling as in https://arxiv.org/abs/2503.18886.
     * @default false
     */
    use_cfg_zero?: boolean;
    /**
     * Use Real CFG
     * @description Uses classical CFG as in SD1.5, SDXL, etc. Increases generation times and price when set to be true.
     *                 If using XLabs IP-Adapter v1, this will be turned on!.
     * @default false
     */
    use_real_cfg?: boolean;
}

export interface FluxGeneralInpaintingOutput extends SharedType_a73 {}

export interface FluxGeneralImageToImageInput {
    /**
     * Base Shift
     * @description Base shift for the scheduled timesteps
     * @default 0.5
     */
    base_shift?: number;
    /**
     * Control Loras
     * @description The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    control_loras?: Components.ControlLoraWeight[];
    /**
     * Controlnet Unions
     * @description The controlnet unions to use for the image generation. Only one controlnet is supported at the moment.
     * @default []
     */
    controlnet_unions?: Components.ControlNetUnion[];
    /**
     * Controlnets
     * @description The controlnets to use for the image generation. Only one controlnet is supported at the moment.
     * @default []
     */
    controlnets?: Components.ControlNet[];
    /**
     * Easycontrols
     * @description EasyControl Inputs to use for image generation.
     * @default []
     */
    easycontrols?: Components.EasyControlWeight[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Fill Image
     * @description Use an image input to influence the generation. Can be used to fill images in masked areas.
     */
    fill_image?: Components.ImageFillInput;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description URL of image to use for inpainting. or img2img
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png
     */
    image_url: string;
    /**
     * Ip Adapters
     * @description IP-Adapter to use for image generation.
     * @default []
     */
    ip_adapters?: Components.IPAdapter[];
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight_2[];
    /**
     * Max Shift
     * @description Max shift for the scheduled timesteps
     * @default 1.15
     */
    max_shift?: number;
    /**
     * NAG alpha
     * @description The alpha value for NAG. This value is used as a final weighting
     *                 factor for steering the normalized guidance (positive and negative prompts)
     *                 in the direction of the positive prompt. Higher values will result in less
     *                 steering on the normalized guidance where lower values will result in
     *                 considering the positive prompt guidance more.
     * @default 0.25
     */
    nag_alpha?: number;
    /**
     * Proportion of steps to apply NAG
     * @description The proportion of steps to apply NAG. After the specified proportion
     *                 of steps has been iterated, the remaining steps will use original
     *                 attention processors in FLUX.
     * @default 0.25
     */
    nag_end?: number;
    /**
     * NAG scale
     * @description The scale for NAG. Higher values will result in a image that is more distant
     *                 to the negative prompt.
     * @default 3
     */
    nag_scale?: number;
    /**
     * NAG Tau
     * @description The tau for NAG. Controls the normalization of the hidden state.
     *                 Higher values will result in a less aggressive normalization,
     *                 but may also lead to unexpected changes with respect to the original image.
     *                 Not recommended to change this value.
     * @default 2.5
     */
    nag_tau?: number;
    /**
     * Negative Prompt
     * @description Negative prompt to steer the image generation away from unwanted features.
     *                 By default, we will be using NAG for processing the negative prompt.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate. This is always set to 1 for streaming output.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A photo of a lion sitting on a stone bench
     */
    prompt: string;
    /**
     * Real CFG scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    real_cfg_scale?: number;
    /**
     * Reference End
     * @description The percentage of the total timesteps when the reference guidance is to be ended.
     * @default 1
     */
    reference_end?: number;
    /**
     * Reference Image Url
     * @description URL of Image for Reference-Only
     */
    reference_image_url?: string;
    /**
     * Reference Start
     * @description The percentage of the total timesteps when the reference guidance is to bestarted.
     * @default 0
     */
    reference_start?: number;
    /**
     * Reference Strength
     * @description Strength of reference_only generation. Only used if a reference image is provided.
     * @default 0.65
     */
    reference_strength?: number;
    /**
     * Scheduler
     * @description Scheduler for the denoising process.
     * @default euler
     * @enum {string}
     */
    scheduler?: 'euler' | 'dpmpp_2m';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sigma Schedule
     * @description Sigmas schedule for the denoising process.
     * @enum {string}
     */
    sigma_schedule?: 'sgm_uniform';
    /**
     * Strength
     * @description The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original.
     * @default 0.85
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Beta Schedule
     * @description Specifies whether beta sigmas ought to be used.
     * @default false
     */
    use_beta_schedule?: boolean;
    /**
     * Use CFG-Zero-Init
     * @description Uses CFG-zero init sampling as in https://arxiv.org/abs/2503.18886.
     * @default false
     */
    use_cfg_zero?: boolean;
    /**
     * Use Real CFG
     * @description Uses classical CFG as in SD1.5, SDXL, etc. Increases generation times and price when set to be true.
     *                 If using XLabs IP-Adapter v1, this will be turned on!.
     * @default false
     */
    use_real_cfg?: boolean;
}

export interface FluxGeneralImageToImageOutput extends SharedType_a73 {}

export interface FluxGeneralDifferentialDiffusionInput {
    /**
     * Base Shift
     * @description Base shift for the scheduled timesteps
     * @default 0.5
     */
    base_shift?: number;
    /**
     * Change Map URL
     * @description URL of change map.
     * @example https://fal.media/files/zebra/Wh4IYAiAAcVbuZ8M9ZMSn.jpeg
     */
    change_map_image_url: string;
    /**
     * Control Loras
     * @description The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    control_loras?: Components.ControlLoraWeight[];
    /**
     * Controlnet Unions
     * @description The controlnet unions to use for the image generation. Only one controlnet is supported at the moment.
     * @default []
     */
    controlnet_unions?: Components.ControlNetUnion[];
    /**
     * Controlnets
     * @description The controlnets to use for the image generation. Only one controlnet is supported at the moment.
     * @default []
     */
    controlnets?: Components.ControlNet[];
    /**
     * Easycontrols
     * @description EasyControl Inputs to use for image generation.
     * @default []
     */
    easycontrols?: Components.EasyControlWeight[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Fill Image
     * @description Use an image input to influence the generation. Can be used to fill images in masked areas.
     */
    fill_image?: Components.ImageFillInput;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description URL of image to use as initial image.
     * @example https://fal.media/files/koala/h6a7KK2Ie_inuGbdartoX.jpeg
     */
    image_url: string;
    /**
     * Ip Adapters
     * @description IP-Adapter to use for image generation.
     * @default []
     */
    ip_adapters?: Components.IPAdapter[];
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight_2[];
    /**
     * Max Shift
     * @description Max shift for the scheduled timesteps
     * @default 1.15
     */
    max_shift?: number;
    /**
     * NAG alpha
     * @description The alpha value for NAG. This value is used as a final weighting
     *                 factor for steering the normalized guidance (positive and negative prompts)
     *                 in the direction of the positive prompt. Higher values will result in less
     *                 steering on the normalized guidance where lower values will result in
     *                 considering the positive prompt guidance more.
     * @default 0.25
     */
    nag_alpha?: number;
    /**
     * Proportion of steps to apply NAG
     * @description The proportion of steps to apply NAG. After the specified proportion
     *                 of steps has been iterated, the remaining steps will use original
     *                 attention processors in FLUX.
     * @default 0.25
     */
    nag_end?: number;
    /**
     * NAG scale
     * @description The scale for NAG. Higher values will result in a image that is more distant
     *                 to the negative prompt.
     * @default 3
     */
    nag_scale?: number;
    /**
     * NAG Tau
     * @description The tau for NAG. Controls the normalization of the hidden state.
     *                 Higher values will result in a less aggressive normalization,
     *                 but may also lead to unexpected changes with respect to the original image.
     *                 Not recommended to change this value.
     * @default 2.5
     */
    nag_tau?: number;
    /**
     * Negative Prompt
     * @description Negative prompt to steer the image generation away from unwanted features.
     *                 By default, we will be using NAG for processing the negative prompt.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate. This is always set to 1 for streaming output.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Tree of life under the sea, ethereal, glittering, lens flares, cinematic lighting, artwork by Anna Dittmann & Carne Griffiths, 8k, unreal engine 5, hightly detailed, intricate detailed.
     */
    prompt: string;
    /**
     * Real CFG scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    real_cfg_scale?: number;
    /**
     * Reference End
     * @description The percentage of the total timesteps when the reference guidance is to be ended.
     * @default 1
     */
    reference_end?: number;
    /**
     * Reference Image Url
     * @description URL of Image for Reference-Only
     */
    reference_image_url?: string;
    /**
     * Reference Start
     * @description The percentage of the total timesteps when the reference guidance is to bestarted.
     * @default 0
     */
    reference_start?: number;
    /**
     * Reference Strength
     * @description Strength of reference_only generation. Only used if a reference image is provided.
     * @default 0.65
     */
    reference_strength?: number;
    /**
     * Scheduler
     * @description Scheduler for the denoising process.
     * @default euler
     * @enum {string}
     */
    scheduler?: 'euler' | 'dpmpp_2m';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sigma Schedule
     * @description Sigmas schedule for the denoising process.
     * @enum {string}
     */
    sigma_schedule?: 'sgm_uniform';
    /**
     * Strength
     * @description The strength to use for differential diffusion. 1.0 is completely remakes the image while 0.0 preserves the original.
     * @default 0.85
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Beta Schedule
     * @description Specifies whether beta sigmas ought to be used.
     * @default false
     */
    use_beta_schedule?: boolean;
    /**
     * Use CFG-Zero-Init
     * @description Uses CFG-zero init sampling as in https://arxiv.org/abs/2503.18886.
     * @default false
     */
    use_cfg_zero?: boolean;
    /**
     * Use Real CFG
     * @description Uses classical CFG as in SD1.5, SDXL, etc. Increases generation times and price when set to be true.
     *                 If using XLabs IP-Adapter v1, this will be turned on!.
     * @default false
     */
    use_real_cfg?: boolean;
}

export interface FluxGeneralDifferentialDiffusionOutput extends SharedType_a73 {}

export interface FluxGeneralInput {
    /**
     * Base Shift
     * @description Base shift for the scheduled timesteps
     * @default 0.5
     */
    base_shift?: number;
    /**
     * Control Loras
     * @description The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    control_loras?: Components.ControlLoraWeight[];
    /**
     * Controlnet Unions
     * @description The controlnet unions to use for the image generation. Only one controlnet is supported at the moment.
     * @default []
     */
    controlnet_unions?: Components.ControlNetUnion[];
    /**
     * Controlnets
     * @description The controlnets to use for the image generation. Only one controlnet is supported at the moment.
     * @default []
     */
    controlnets?: Components.ControlNet[];
    /**
     * Easycontrols
     * @description EasyControl Inputs to use for image generation.
     * @default []
     */
    easycontrols?: Components.EasyControlWeight[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Fill Image
     * @description Use an image input to influence the generation. Can be used to fill images in masked areas.
     */
    fill_image?: Components.ImageFillInput;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Ip Adapters
     * @description IP-Adapter to use for image generation.
     * @default []
     */
    ip_adapters?: Components.IPAdapter[];
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight_2[];
    /**
     * Max Shift
     * @description Max shift for the scheduled timesteps
     * @default 1.15
     */
    max_shift?: number;
    /**
     * NAG alpha
     * @description The alpha value for NAG. This value is used as a final weighting
     *                 factor for steering the normalized guidance (positive and negative prompts)
     *                 in the direction of the positive prompt. Higher values will result in less
     *                 steering on the normalized guidance where lower values will result in
     *                 considering the positive prompt guidance more.
     * @default 0.25
     */
    nag_alpha?: number;
    /**
     * Proportion of steps to apply NAG
     * @description The proportion of steps to apply NAG. After the specified proportion
     *                 of steps has been iterated, the remaining steps will use original
     *                 attention processors in FLUX.
     * @default 0.25
     */
    nag_end?: number;
    /**
     * NAG scale
     * @description The scale for NAG. Higher values will result in a image that is more distant
     *                 to the negative prompt.
     * @default 3
     */
    nag_scale?: number;
    /**
     * NAG Tau
     * @description The tau for NAG. Controls the normalization of the hidden state.
     *                 Higher values will result in a less aggressive normalization,
     *                 but may also lead to unexpected changes with respect to the original image.
     *                 Not recommended to change this value.
     * @default 2.5
     */
    nag_tau?: number;
    /**
     * Negative Prompt
     * @description Negative prompt to steer the image generation away from unwanted features.
     *                 By default, we will be using NAG for processing the negative prompt.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate. This is always set to 1 for streaming output.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word "FLUX" is painted over it in big, white brush strokes with visible texture.
     */
    prompt: string;
    /**
     * Real CFG scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    real_cfg_scale?: number;
    /**
     * Reference End
     * @description The percentage of the total timesteps when the reference guidance is to be ended.
     * @default 1
     */
    reference_end?: number;
    /**
     * Reference Image Url
     * @description URL of Image for Reference-Only
     */
    reference_image_url?: string;
    /**
     * Reference Start
     * @description The percentage of the total timesteps when the reference guidance is to bestarted.
     * @default 0
     */
    reference_start?: number;
    /**
     * Reference Strength
     * @description Strength of reference_only generation. Only used if a reference image is provided.
     * @default 0.65
     */
    reference_strength?: number;
    /**
     * Scheduler
     * @description Scheduler for the denoising process.
     * @default euler
     * @enum {string}
     */
    scheduler?: 'euler' | 'dpmpp_2m';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sigma Schedule
     * @description Sigmas schedule for the denoising process.
     * @enum {string}
     */
    sigma_schedule?: 'sgm_uniform';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Beta Schedule
     * @description Specifies whether beta sigmas ought to be used.
     * @default false
     */
    use_beta_schedule?: boolean;
    /**
     * Use CFG-Zero-Init
     * @description Uses CFG-zero init sampling as in https://arxiv.org/abs/2503.18886.
     * @default false
     */
    use_cfg_zero?: boolean;
    /**
     * Use Real CFG
     * @description Uses classical CFG as in SD1.5, SDXL, etc. Increases generation times and price when set to be true.
     *                 If using XLabs IP-Adapter v1, this will be turned on!.
     * @default false
     */
    use_real_cfg?: boolean;
}

export interface FluxGeneralOutput extends SharedType_a73 {}

export interface FluxDifferentialDiffusionInput {
    /**
     * Change Map URL
     * @description URL of change map.
     * @example https://fal.media/files/zebra/Wh4IYAiAAcVbuZ8M9ZMSn.jpeg
     */
    change_map_image_url: string;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description URL of image to use as initial image.
     * @example https://fal.media/files/koala/h6a7KK2Ie_inuGbdartoX.jpeg
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Tree of life under the sea, ethereal, glittering, lens flares, cinematic lighting, artwork by Anna Dittmann & Carne Griffiths, 8k, unreal engine 5, hightly detailed, intricate detailed.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength to use for image-to-image. 1.0 is completely remakes the image while 0.0 preserves the original.
     * @default 0.85
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxDifferentialDiffusionOutput extends SharedType_a73 {}

export interface FluxControlLoraDepthImageToImageInput {
    /**
     * Control Lora Image Url
     * @description The image to use for control lora. This is used to control the style of the generated image.
     * @example https://v3.fal.media/files/kangaroo/Cb7BeM7G4DauK_lWjzY3N_Celeb6.jpg
     */
    control_lora_image_url: string;
    /**
     * Control Lora Strength
     * @description The strength of the control lora.
     * @default 1
     */
    control_lora_strength?: number;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description URL of image to use for inpainting. or img2img
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png
     */
    image_url: string;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A photo of a lion sitting on a stone bench
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original.
     * @default 0.85
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxControlLoraDepthImageToImageOutput extends SharedType_a73 {}

export interface FluxControlLoraDepthInput {
    /**
     * Control Lora Image Url
     * @description The image to use for control lora. This is used to control the style of the generated image.
     * @example https://v3.fal.media/files/kangaroo/Cb7BeM7G4DauK_lWjzY3N_Celeb6.jpg
     */
    control_lora_image_url: string;
    /**
     * Control Lora Strength
     * @description The strength of the control lora.
     * @default 1
     */
    control_lora_strength?: number;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Preprocess Depth
     * @description If set to true, the input image will be preprocessed to extract depth information.
     *                 This is useful for generating depth maps from images.
     * @default true
     */
    preprocess_depth?: boolean;
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word "FLUX" is painted over it in big, white brush strokes with visible texture.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxControlLoraDepthOutput extends SharedType_a73 {}

export interface FluxControlLoraCannyImageToImageInput {
    /**
     * Control Lora Image Url
     * @description The image to use for control lora. This is used to control the style of the generated image.
     */
    control_lora_image_url?: string;
    /**
     * Control Lora Strength
     * @description The strength of the control lora.
     * @default 1
     */
    control_lora_strength?: number;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description URL of image to use for inpainting. or img2img
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png
     */
    image_url: string;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A photo of a lion sitting on a stone bench
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original.
     * @default 0.85
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxControlLoraCannyImageToImageOutput extends SharedType_a73 {}

export interface FluxControlLoraCannyInput {
    /**
     * Control Lora Image Url
     * @description The image to use for control lora. This is used to control the style of the generated image.
     */
    control_lora_image_url?: string;
    /**
     * Control Lora Strength
     * @description The strength of the control lora.
     * @default 1
     */
    control_lora_strength?: number;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description The LoRAs to use for the image generation. You can use any number of LoRAs
     *                 and they will be merged together to generate the final image.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word "FLUX" is painted over it in big, white brush strokes with visible texture.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FluxControlLoraCannyOutput extends SharedType_a73 {}

export interface Flux2TurboEditInput {
    /**
     * Enable Prompt Expansion
     * @description If set to true, the prompt will be expanded for better results.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the image to generate. The width and height must be between 512 and 2048 pixels.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URLs of the images for editing. A maximum of 4 images are allowed, if more are provided, only the first 4 will be used.
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_outputs/flux-2-turbo.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to edit the image.
     * @example Change the weather to winter
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2TurboEditOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The edited images
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/flux-2-turbo-edit.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Flux2TurboInput {
    /**
     * Enable Prompt Expansion
     * @description If set to true, the prompt will be expanded for better results.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the image to generate. The width and height must be between 512 and 2048 pixels.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A realistic photograph of a vintage typewriter with a sheet of paper inserted that says 'Chapter One: The Journey Begins,' sunlight falling across the desk.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2TurboOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/flux-2-turbo-t2i.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Flux2LoraEditInput {
    /**
     * Acceleration
     * @description The acceleration level to use for the image generation.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Prompt Expansion
     * @description If set to true, the prompt will be expanded for better results.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the image to generate. The width and height must be between 512 and 2048 pixels.
     * @example {
     *       "height": 1152,
     *       "width": 2016
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URsL of the images for editing. A maximum of 3 images are allowed, if more are provided, only the first 3 will be used.
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_inputs/flux2_dev_lora_edit_input.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Loras
     * @description List of LoRA weights to apply (maximum 3). Each LoRA can be a URL, HuggingFace repo ID, or local path.
     * @default []
     */
    loras?: Components.LoRAInput_1[];
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Make this donut realistic
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2LoraEditOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The edited images
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/flux2_dev_lora_edit_output.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Flux2LoraInput {
    /**
     * Acceleration
     * @description The acceleration level to use for the image generation.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Prompt Expansion
     * @description If set to true, the prompt will be expanded for better results.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the image to generate. The width and height must be between 512 and 2048 pixels.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description List of LoRA weights to apply (maximum 3). Each LoRA can be a URL, HuggingFace repo ID, or local path.
     * @default []
     */
    loras?: Components.LoRAInput_1[];
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Close shot a pianist plays in a luxurious room with tall windows overlooking a rainy metropolis. Shot with a 50mm lens at a side profile angle, soft tungsten light highlighting hands moving over keys. Capture detailed reflections in polished black piano surfaces, raindrops sliding down glass, and atmospheric warm/cool lighting contrast.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2LoraOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/flux2_dev_lora_t2i_output.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Flux2Klein9bEditInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, uses the input image size.
     * @example {
     *       "height": 1152,
     *       "width": 2016
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URLs of the images for editing. A maximum of 4 images are allowed.
     * @example [
     *       "https://v3b.fal.media/files/b/0a8a69d5/kkXxFfj1QeVtw35kxy5Py_1a7e3511-bd2c-46be-923a-8e6be2496f12.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 4
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to edit the image.
     * @example Show me a full body image
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI. Output is not stored when this is True.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2Klein9bEditOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The edited images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a8b8b22/zxpzgthoJaMfiLfSqjTwX.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Flux2Klein9bBaseLoraInput extends SharedType_e34 {}

export interface Flux2Klein9bBaseLoraOutput extends SharedType_3e8 {}

export interface Flux2Klein9bBaseEditLoraInput extends SharedType_6f9 {}

export interface Flux2Klein9bBaseEditLoraOutput extends SharedType_3e8 {}

export interface Flux2Klein9bBaseEditInput {
    /**
     * Acceleration
     * @description The acceleration level to use for image generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance scale for classifier-free guidance.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, uses the input image size.
     * @example {
     *       "height": 1152,
     *       "width": 2016
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URLs of the images for editing. A maximum of 4 images are allowed.
     * @example [
     *       "https://v3b.fal.media/files/b/0a8a69f0/DifnFRQjCHQ5nUxJl0tQK_d456be65-0a70-417c-991d-531be0b58993.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Negative Prompt
     * @description Negative prompt for classifier-free guidance. Describes what to avoid in the image.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to edit the image.
     * @example Imagine a young woman. Use the Style from the Reference Image.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI. Output is not stored when this is True.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2Klein9bBaseEditOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The edited images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a8a69f1/HpSn20CQnEgVbFRD5E-Eh.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Flux2Klein9bBaseInput {
    /**
     * Acceleration
     * @description The acceleration level to use for image generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance scale for classifier-free guidance.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the image to generate.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description Negative prompt for classifier-free guidance. Describes what to avoid in the image.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A cyberpunk samurai standing in neon-lit Tokyo street at night, rain-soaked pavement reflecting holographic advertisements in pink and blue, steam rising from street vents, wearing futuristic armor with glowing accents, cinematic composition, Unreal Engine 5 quality, ray-traced reflections, shallow depth of field, hyper-detailed
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI. Output is not stored when this is True.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2Klein9bBaseOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a8a69bc/EaxKO6wroq3eaDb2Znfpo.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Flux2Klein9bInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the image to generate.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 4
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A bright, surreal scene on a vast white salt flat under a clear blue sky, featuring two fluffy white alpacas standing in the foreground. Behind them sits a sleek white supercar with its scissor doors open, creating a dramatic futuristic silhouette. The sunlight is strong and crisp, casting sharp shadows on the ground and giving the image a clean, high-contrast cinematic look. Minimalist composition, playful luxury vibe, ultra sharp detail, wide-angle perspective, high resolution.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI. Output is not stored when this is True.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2Klein9bOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a8a6905/DuNRy1OODaGrlUGEfl9SX.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Flux2Klein4bEditInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, uses the input image size.
     * @example {
     *       "height": 1152,
     *       "width": 2016
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URLs of the images for editing. A maximum of 4 images are allowed.
     * @example [
     *       "https://v3b.fal.media/files/b/0a8a69d5/kkXxFfj1QeVtw35kxy5Py_1a7e3511-bd2c-46be-923a-8e6be2496f12.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 4
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to edit the image.
     * @example Turn this into a realistic image
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI. Output is not stored when this is True.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2Klein4bEditOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The edited images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a8a69d6/M73KvDgfEgIM77t4mFsS2.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Flux2Klein4bBaseLoraInput extends SharedType_e34 {}

export interface Flux2Klein4bBaseLoraOutput extends SharedType_3e8 {}

export interface Flux2Klein4bBaseEditLoraInput extends SharedType_6f9 {}

export interface Flux2Klein4bBaseEditLoraOutput extends SharedType_3e8 {}

export interface Flux2Klein4bBaseEditInput {
    /**
     * Acceleration
     * @description The acceleration level to use for image generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance scale for classifier-free guidance.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, uses the input image size.
     * @example {
     *       "height": 1152,
     *       "width": 2016
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URLs of the images for editing. A maximum of 4 images are allowed.
     * @example [
     *       "https://v3b.fal.media/files/b/0a8a69fd/VUrxIXgOqcf3L7kuGS7B5_eb54fcdc-87d0-47da-ad31-93455b245fb4.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Negative Prompt
     * @description Negative prompt for classifier-free guidance. Describes what to avoid in the image.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to edit the image.
     * @example Imagine view of Fuji mount. Use style of reference image.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI. Output is not stored when this is True.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2Klein4bBaseEditOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The edited images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a8a69ff/UYukVfGjybLo7spA_Kc-i.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Flux2Klein4bBaseInput {
    /**
     * Acceleration
     * @description The acceleration level to use for image generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance scale for classifier-free guidance.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the image to generate.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description Negative prompt for classifier-free guidance. Describes what to avoid in the image.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Japanese zen garden at first light, perfect rake lines in gravel, koi pond with morning mist, temple bell in background, meditation ready
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI. Output is not stored when this is True.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2Klein4bBaseOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a8a69c2/5IoN78I6tZ8ZH69SB3PhW.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Flux2Klein4bInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the image to generate.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 4
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Cheetah in pre-sprint crouch, muscles tensed, gazelle visible in distance, African savanna golden grass, dust particles in air, National Geographic wildlife photography
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI. Output is not stored when this is True.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2Klein4bOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a8a69c7/gU9ZgfFC9oAZjpIoveAac.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Flux2FlashEditInput {
    /**
     * Enable Prompt Expansion
     * @description If set to true, the prompt will be expanded for better results.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the image to generate. The width and height must be between 512 and 2048 pixels.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URLs of the images for editing. A maximum of 4 images are allowed, if more are provided, only the first 4 will be used.
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_outputs/flux-2-flash-edit-input.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to edit the image.
     * @example Remove the meat from the hamburger
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2FlashEditOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The edited images
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/flux-2-flash-edit.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Flux2FlashInput {
    /**
     * Enable Prompt Expansion
     * @description If set to true, the prompt will be expanded for better results.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the image to generate. The width and height must be between 512 and 2048 pixels.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example An army of ants battles for a sugar cube in a giant sandbox, with tiny plastic soldier toys caught in the melee. Ant bodies shine against gritty beige sand and the glossy, crystalline cube. Fiery sunrise spills golden highlights, throwing dramatic shadows, amplifying the excitement. Shot macro (100mm), aperture f/2.8 for soft background, centered composition with ground-level POV, mixing organic and manufactured textures.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2FlashOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/flux-2-flash-t2i.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Flux2EditInput {
    /**
     * Acceleration
     * @description The acceleration level to use for the image generation.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Prompt Expansion
     * @description If set to true, the prompt will be expanded for better results.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the image to generate. The width and height must be between 512 and 2048 pixels.
     * @example {
     *       "height": 1152,
     *       "width": 2016
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URLs of the images for editing. A maximum of 4 images are allowed, if more are provided, only the first 4 will be used.
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_inputs/flux2_dev_edit_input.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to edit the image.
     * @example Change his clothes to casual suit and tie
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2EditOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The edited images
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/flux2_dev_edit_output.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Flux2TrainerEditInput extends SharedType_a3d {}

export interface Flux2TrainerEditOutput extends SharedType_b8b {}

export interface Flux2TrainerV2EditInput extends SharedType_a3d {}

export interface Flux2TrainerV2EditOutput extends SharedType_b8b {}

export interface Flux2TrainerV2Input extends SharedType_e18 {}

export interface Flux2TrainerV2Output extends SharedType_b8b {}

export interface Flux2TrainerInput extends SharedType_e18 {}

export interface Flux2TrainerOutput extends SharedType_b8b {}

export interface Flux2ProEditInput {
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image. If `auto`, the size will be determined by the model.
     * @default auto
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description List of URLs of input images for editing
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_inputs/flux2_pro_edit_input.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Place realistic flames emerging from the top of the coffee cup, dancing above the rim
     */
    prompt: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5';
    /**
     * Seed
     * @description The seed to use for the generation.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2ProEditOutput {
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/flux2_pro_edit_output.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Seed
     * @description The seed used for the generation.
     */
    seed: number;
}

export interface Flux2ProInput {
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example An intense close-up of knight's visor reflecting battle, sword raised, flames in background, chiaroscuro helmet shadows, hyper-detailed armor, square medieval, cinematic lighting
     */
    prompt: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5';
    /**
     * Seed
     * @description The seed to use for the generation.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2ProOutput {
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/flux2_pro_t2i_output.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Seed
     * @description The seed used for the generation.
     */
    seed: number;
}

export interface Flux2MaxEditInput {
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image. If `auto`, the size will be determined by the model.
     * @default auto
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description List of URLs of input images for editing
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_inputs/flux2/max.jpg"
     *     ]
     */
    image_urls: string[];
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A high-fashion magazine cover featuring an android in an avant-garde geometric cloth dress, with logo prints from @Image1. The backdrop is an eye-catching scenery. The title text 'FAL MAGAZINE' spans the top in bold white serif font. Overlay text at the bottom right reads 'THE FUTURE OF AI' in a sleek, thin sans-serif font.
     */
    prompt: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5';
    /**
     * Seed
     * @description The seed to use for the generation.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2MaxEditOutput {
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/flux2/max_edit_output.jpg"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Seed
     * @description The seed used for the generation.
     */
    seed: number;
}

export interface Flux2MaxInput {
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A lavish, baroque-style image of a powerful sorceress in her arcane study. She is dressed in robes of deep emerald velvet embroidered with gold thread and shimmering beetle wings, holding a staff topped with a glowing, swirling galaxy trapped in a crystal orb.She stands before a massive oak desk covered in open grimoires with illuminated pages showing alchemical diagrams, bubbling potions in glass alembics, and a sleeping pseudodragon curled around a stack of scrolls. The room is filled with curiosities: shelves of leather-bound books, celestial globes, and dried magical herbs hanging from the ceiling. The lighting is chiaroscuro, from a large fireplace with green flames and a magical candelabra floating in mid-air. The brushwork is visible and textured, with rich, deep colors. The style is reminiscent of Rembrandt meets classic fantasy art. The words 'Flux 2 Max is available on fal' are written in elegant cursive font on the top of the image.
     */
    prompt: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5';
    /**
     * Seed
     * @description The seed to use for the generation.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2MaxOutput {
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/flux2/max.jpg"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Seed
     * @description The seed used for the generation.
     */
    seed: number;
}

export interface Flux2LoraGalleryVirtualTryonInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the input image will be used.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URLs of the images for virtual try-on. Provide person image and clothing image.
     * @example [
     *       "https://v3b.fal.media/files/b/koala/YlOtn9SjXGGH274eN1G5R.png",
     *       "https://v3b.fal.media/files/b/penguin/sji5EHUvmFYOCVZsvvId-.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Lora Scale
     * @description The strength of the virtual try-on effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate a virtual try-on image.
     * @example A person wearing a stylish jacket, virtual try-on
     * @example Virtual try-on of a dress on a model
     * @example Fashion virtual try-on with clothing
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2LoraGalleryVirtualTryonOutput {
    /**
     * Images
     * @description The generated virtual try-on images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/zebra/oFnSZ-nBbPgM-gXT0ApXy.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Prompt
     * @description The prompt used for generation
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface Flux2LoraGallerySepiaVintageInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Lora Scale
     * @description The strength of the sepia vintage photography effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate a sepia vintage photography style image.
     * @example A portrait of a Victorian lady in sepia tones
     * @example Vintage street scene with old cars, sepia photography
     * @example Old family photo in sepia vintage style
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2LoraGallerySepiaVintageOutput {
    /**
     * Images
     * @description The generated sepia vintage photography style images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/zebra/qOMZKBNStQgVkaKB9SLY4.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Prompt
     * @description The prompt used for generation
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface Flux2LoraGallerySatelliteViewStyleInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Lora Scale
     * @description The strength of the satellite view style effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate a satellite/aerial view style image.
     * @example Aerial view of a city from above, satellite style
     * @example Satellite view of mountains and valleys
     * @example Bird's eye view of a coastal town
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2LoraGallerySatelliteViewStyleOutput {
    /**
     * Images
     * @description The generated satellite view style images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/zebra/_rfPAtMbIRCRgKIbG990u.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Prompt
     * @description The prompt used for generation
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface Flux2LoraGalleryRealismInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Lora Scale
     * @description The strength of the realism effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate a realistic image with natural lighting and authentic details.
     * @example A portrait of a woman with natural lighting
     * @example Street photography of a busy city scene
     * @example Landscape photography of mountains at golden hour
     * @example Documentary style photo of everyday life
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2LoraGalleryRealismOutput {
    /**
     * Images
     * @description The generated realistic style images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/koala/zjTCkqpflaiokwqCX3fKk.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Prompt
     * @description The prompt used for generation
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface Flux2LoraGalleryMultipleAnglesInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Horizontal Angle (Azimuth °)
     * @description Horizontal rotation angle around the object in degrees. 0°=front view, 90°=right side, 180°=back view, 270°=left side, 360°=front view again.
     * @default 0
     */
    horizontal_angle?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the input image will be used.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URL of the image to adjust camera angle for.
     * @example [
     *       "https://v3.fal.media/files/monkey/i3saq4bAPXSIl08nZtq9P_ec535747aefc4e31943136a6d8587075.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Lora Scale
     * @description The strength of the multiple angles effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Num Images
     * @description Number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image.
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Seed
     * @description Random seed for reproducibility.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If True, the media will be returned as a data URI.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Vertical Angle (Elevation °)
     * @description Vertical camera angle in degrees. 0°=eye-level shot, 30°=elevated shot, 60°=high-angle shot (looking down from above).
     * @default 0
     */
    vertical_angle?: number;
    /**
     * Zoom (Distance)
     * @description Camera zoom/distance. 0=wide shot (far away), 5=medium shot (normal), 10=close-up (very close).
     * @default 5
     */
    zoom?: number;
}

export interface Flux2LoraGalleryMultipleAnglesOutput {
    /**
     * Images
     * @description The generated images with multiple camera angles
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/0a848d46/EYFbhE4axwlNB3OSKfdre.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Prompt
     * @description The prompt used for generation
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface Flux2LoraGalleryHdrStyleInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Lora Scale
     * @description The strength of the HDR style effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an HDR style image. The trigger word 'Hyp3rRe4list1c' will be automatically prepended.
     * @example A wonderful tropical landscape
     * @example A mystical forest with glowing mushrooms, intricate details and volumetric lighting
     * @example Futuristic cityscape at night, photorealistic rendering with ray tracing
     * @example Portrait with ultra detailed skin texture, natural lighting, 8K quality
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2LoraGalleryHdrStyleOutput {
    /**
     * Images
     * @description The generated HDR style images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/penguin/3gPcw3pH__e6teqtOEku9.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Prompt
     * @description The prompt used for generation
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface Flux2LoraGalleryFaceToFullPortraitInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the input image will be used.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URL of the cropped face image.
     * @example [
     *       "https://v3b.fal.media/files/b/elephant/XJPJL2v5pAOmx9LemHWAE.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Lora Scale
     * @description The strength of the face to full portrait effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description The prompt describing the full portrait to generate from the face.
     * @default Face to full portrait
     * @example Face to full portrait
     * @example Face to full portrait in professional attire
     * @example Face to full portrait casual outdoor setting
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2LoraGalleryFaceToFullPortraitOutput {
    /**
     * Images
     * @description The generated full portrait images from face
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/elephant/rlfpP4b6_PwqQK5F2pAKc.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Prompt
     * @description The prompt used for generation
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface Flux2LoraGalleryDigitalComicArtInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Lora Scale
     * @description The strength of the digital comic art effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate a digital comic art style image. Use 'd1g1t4l' trigger word for best results.
     * @example A superhero flying over a city, d1g1t4l comic art style
     * @example Action scene with explosion effects, digital comic illustration
     * @example Comic book panel with dramatic lighting
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2LoraGalleryDigitalComicArtOutput {
    /**
     * Images
     * @description The generated digital comic art style images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/tiger/9_onjLEABvvZl9r-R6w_w.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Prompt
     * @description The prompt used for generation
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface Flux2LoraGalleryBallpointPenSketchInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Lora Scale
     * @description The strength of the ballpoint pen sketch effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate a ballpoint pen sketch style image. Use 'b4llp01nt' trigger word for best results.
     * @example Portrait of a person, b4llp01nt ballpoint pen sketch
     * @example Urban landscape sketch in ballpoint pen style
     * @example Detailed hand-drawn illustration of a building
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2LoraGalleryBallpointPenSketchOutput {
    /**
     * Images
     * @description The generated ballpoint pen sketch style images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/lion/6Eq6ijrWRcWsa6ivqdlL1.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Prompt
     * @description The prompt used for generation
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface Flux2LoraGalleryApartmentStagingInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the input image will be used.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URL of the empty room image to furnish.
     * @example [
     *       "https://v3b.fal.media/files/b/tiger/58rkpMdBl7eqcxuZ8YRus.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Lora Scale
     * @description The strength of the apartment staging effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate a furnished room. Use 'furnish this room' for best results.
     * @example Furnish this room with modern furniture and decor
     * @example Furnish this empty apartment with cozy living room furniture
     * @example Furnish this room with minimalist Scandinavian design
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2LoraGalleryApartmentStagingOutput {
    /**
     * Images
     * @description The generated furnished room images
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/panda/3J1ezJEhcDLwoC9P8imqj.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Prompt
     * @description The prompt used for generation
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface Flux2LoraGalleryAddBackgroundInput {
    /**
     * Acceleration
     * @description Acceleration level for image generation. 'regular' balances speed and quality.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker for the generated image.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If not provided, the size of the input image will be used.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description The URLs of the images. Provide an image with a white or clean background.
     * @example [
     *       "https://v3b.fal.media/files/b/penguin/SLhCyojehICmW3dW6U5F0.jpg"
     *     ]
     */
    image_urls: string[];
    /**
     * Lora Scale
     * @description The strength of the add background effect.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg' | 'webp';
    /**
     * Prompt
     * @description The prompt describing the background to add. Must start with 'Add Background' followed by your description.
     * @default Add Background forest
     * @example Add Background forest
     * @example Add Background modern office
     * @example Add Background beach sunset
     * @example Add Background city skyline at night
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility. Same seed with same prompt will produce same result.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and won't be saved in history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2LoraGalleryAddBackgroundOutput {
    /**
     * Images
     * @description The generated images with added background
     * @example [
     *       {
     *         "url": "https://v3b.fal.media/files/b/lion/g0VrXhRU1YBK9B1MnGpyi.png"
     *       }
     *     ]
     */
    images: Components.Image_2[];
    /**
     * Prompt
     * @description The prompt used for generation
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
}

export interface Flux2Klein9bBaseTrainerEditInput extends SharedType_a3d {}

export interface Flux2Klein9bBaseTrainerEditOutput extends SharedType_b8b {}

export interface Flux2Klein9bBaseTrainerInput extends SharedType_e18 {}

export interface Flux2Klein9bBaseTrainerOutput extends SharedType_b8b {}

export interface Flux2Klein4bBaseTrainerEditInput extends SharedType_a3d {}

export interface Flux2Klein4bBaseTrainerEditOutput extends SharedType_b8b {}

export interface Flux2Klein4bBaseTrainerInput extends SharedType_e18 {}

export interface Flux2Klein4bBaseTrainerOutput extends SharedType_b8b {}

export interface Flux2FlexEditInput {
    /**
     * Enable Prompt Expansion
     * @description Whether to expand the prompt using the model's own knowledge.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the generation.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. If `auto`, the size will be determined by the model.
     * @default auto
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'auto'
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URLs
     * @description List of URLs of input images for editing
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_inputs/flux2_flex_edit_input.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Change colors of the vase. In a cozy living room setting, visualize a gradient vase placed on a table, flowing from rich #6a0dad to soft #ff69b4. Add an artistic carving text with a big font on vase says "FLEX" in the middle.
     */
    prompt: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5';
    /**
     * Seed
     * @description The seed to use for the generation.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2FlexEditOutput {
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/flux2_flex_edit_output.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Seed
     * @description The seed used for the generation.
     */
    seed: number;
}

export interface Flux2FlexInput {
    /**
     * Enable Prompt Expansion
     * @description Whether to expand the prompt using the model's own knowledge.
     * @default true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the generation.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A high-quality 3D render of a cute fluffy monster eating a giant donut; the fur simulation is incredibly detailed, the donut glaze is sticky and reflective, bright daylight lighting, shallow depth of field.
     */
    prompt: string;
    /**
     * Safety Tolerance
     * @description The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.
     * @default 2
     * @enum {string}
     */
    safety_tolerance?: '1' | '2' | '3' | '4' | '5';
    /**
     * Seed
     * @description The seed to use for the generation.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2FlexOutput {
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/flux2_flex_t2i_output.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Seed
     * @description The seed used for the generation.
     */
    seed: number;
}

export interface Flux2Input {
    /**
     * Acceleration
     * @description The acceleration level to use for the image generation.
     * @default regular
     * @example regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Prompt Expansion
     * @description If set to true, the prompt will be expanded for better results.
     * @default false
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the image to generate. The width and height must be between 512 and 2048 pixels.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Number of Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Dutch angle close-up of a survivor in post-apocalyptic setting, dust-covered face, dramatic harsh sunlight creating deep shadows, happy expression, desaturated dystopian color palette, gritty realism
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux2Output {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/flux2_dev_t2i_output.png"
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Flux1SrpoImageToImageInput extends SharedType_ba2 {}

export interface Flux1SrpoImageToImageOutput extends SharedType_a73 {}

export interface Flux1SrpoInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *             the model to stick to your prompt when looking for a related image to show you.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Mountain guide, sturdy build, wilderness wisdom, alert gaze, technical outdoor gear with rope coils, snow-capped peaks background, crisp mountain lighting, leading pose, wind-swept hair with full beard, weather-worn face with quiet confidence, alpine expert presence
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux1SrpoOutput extends SharedType_368 {}

export interface Flux1SchnellReduxInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description The URL of the image to generate an image from.
     * @example https://fal.media/files/kangaroo/acQvq-Kmo2lajkgvcEHdv.png
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 4
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux1SchnellReduxOutput extends SharedType_a73 {}

export interface Flux1SchnellInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *             the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 4
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word "FLUX" is painted over it in big, white brush strokes with visible texture.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux1SchnellOutput extends SharedType_a73 {}

export interface Flux1KreaReduxInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *             the model to stick to your prompt when looking for a related image to show you.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description The URL of the image to generate an image from.
     * @example https://storage.googleapis.com/falserverless/example_inputs/flux_krea_redux_output_1.jpg
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux1KreaReduxOutput extends SharedType_96f {}

export interface Flux1KreaImageToImageInput extends SharedType_ba2 {}

export interface Flux1KreaImageToImageOutput extends SharedType_609 {}

export interface Flux1KreaInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *             the model to stick to your prompt when looking for a related image to show you.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A candid street photo of a woman with a pink bob and bold eyeliner on a graffiti-covered subway platform. She wears a bright yellow patent leather coat over a black-and-white checkered turtleneck and platform boots. Natural subway lighting creates an authentic urban scene with a relaxed, unposed feel.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux1KreaOutput extends SharedType_609 {}

export interface Flux1DevReduxInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *             the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image URL
     * @description The URL of the image to generate an image from.
     * @example https://fal.media/files/kangaroo/acQvq-Kmo2lajkgvcEHdv.png
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux1DevReduxOutput extends SharedType_a73 {}

export interface Flux1DevImageToImageInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *             the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The URL of the image to generate an image from.
     * @example https://fal.media/files/koala/Chls9L2ZnvuipUTEwlnJC.png
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 40
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A cat dressed as a wizard with a background of a mystic forest.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength of the initial image. Higher strength values are better for this model.
     * @default 0.95
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux1DevImageToImageOutput extends SharedType_a73 {}

export interface Flux1DevInput {
    /**
     * Acceleration
     * @description The speed of the generation. The higher the speed, the faster the generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *             the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word "FLUX" is painted over it in big, white brush strokes with visible texture.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *             will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Flux1DevOutput extends SharedType_a73 {}

export interface FloweditInput {
    /**
     * Image Url
     * @description URL of image to be used for relighting
     * @example https://storage.googleapis.com/falserverless/model_tests/FlowEdit/lighthouse.png
     */
    image_url: string;
    /**
     * N Avg
     * @description Average step count
     * @default 1
     */
    n_avg?: number;
    /**
     * N Max
     * @description Control the strength of the edit
     * @default 23
     */
    n_max?: number;
    /**
     * N Min
     * @description Minimum step for improved style edits
     * @default 0
     */
    n_min?: number;
    /**
     * Steps
     * @description Steps for which the model should run.
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Seed
     * @description Random seed for reproducible generation. If set none, a random seed will be used.
     */
    seed?: number;
    /**
     * Source Prompt
     * @description Prompt of the image to be used.
     * @example The image features a tall white lighthouse standing prominently
     *           on a hill, with a beautiful blue sky in the background. The lighthouse is illuminated
     *           by a bright light, making it a prominent landmark in the scene.
     */
    source_prompt: string;
    /**
     * Source Guidance scale (CFG)
     * @description Guidance scale for the source.
     * @default 1.5
     */
    src_guidance_scale?: number;
    /**
     * Target Guidance scale (CFG)
     * @description Guidance scale for target.
     * @default 5.5
     */
    tar_guidance_scale?: number;
    /**
     * Target Prompt
     * @description Prompt of the image to be made.
     * @example The image features Big ben clock tower standing prominently
     *           on a hill, with a beautiful blue sky in the background. The Big ben clock tower is illuminated
     *           by a bright light, making it a prominent landmark in the scene.
     */
    target_prompt: string;
}

export interface FloweditOutput {
    /**
     * Image
     * @description The generated image file info.
     * @example {
     *       "file_size": 423052,
     *       "height": 1024,
     *       "file_name": "36d3ca4791a647678b2ff01a35c87f5a.png",
     *       "content_type": "image/png",
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/FlowEdit/aa5c3d028ad04800a54f70f928198d91.png",
     *       "width": 1024
     *     }
     */
    image: Components.Image;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
}

export interface Florence2LargeRegionToSegmentationInput extends SharedType_886 {}

export interface Florence2LargeRegionToSegmentationOutput extends SharedType_bf2 {}

export interface Florence2LargeRegionToDescriptionInput extends SharedType_886 {}

export interface Florence2LargeRegionToDescriptionOutput extends SharedType_129 {}

export interface Florence2LargeRegionToCategoryInput extends SharedType_886 {}

export interface Florence2LargeRegionToCategoryOutput extends SharedType_129 {}

export interface Florence2LargeRegionProposalInput extends SharedType_38d {}

export interface Florence2LargeRegionProposalOutput extends SharedType_5c6 {}

export interface Florence2LargeReferringExpressionSegmentationInput extends SharedType_a8f {}

export interface Florence2LargeReferringExpressionSegmentationOutput extends SharedType_bf2 {}

export interface Florence2LargeOpenVocabularyDetectionInput extends SharedType_a8f {}

export interface Florence2LargeOpenVocabularyDetectionOutput extends SharedType_5c6 {}

export interface Florence2LargeOcrWithRegionInput extends SharedType_38d {}

export interface Florence2LargeOcrWithRegionOutput {
    /**
     * Image
     * @description Processed image
     */
    image?: Components.Image;
    /**
     * Results
     * @description Results from the model
     */
    results: Components.OCRBoundingBox;
}

export interface Florence2LargeOcrInput extends SharedType_38d {}

export interface Florence2LargeOcrOutput extends SharedType_129 {}

export interface Florence2LargeObjectDetectionInput extends SharedType_38d {}

export interface Florence2LargeObjectDetectionOutput extends SharedType_5c6 {}

export interface Florence2LargeMoreDetailedCaptionInput extends SharedType_38d {}

export interface Florence2LargeMoreDetailedCaptionOutput extends SharedType_129 {}

export interface Florence2LargeDetailedCaptionInput extends SharedType_38d {}

export interface Florence2LargeDetailedCaptionOutput extends SharedType_129 {}

export interface Florence2LargeDenseRegionCaptionInput extends SharedType_38d {}

export interface Florence2LargeDenseRegionCaptionOutput extends SharedType_5c6 {}

export interface Florence2LargeCaptionToPhraseGroundingInput extends SharedType_a8f {}

export interface Florence2LargeCaptionToPhraseGroundingOutput extends SharedType_5c6 {}

export interface Florence2LargeCaptionInput extends SharedType_38d {}

export interface Florence2LargeCaptionOutput extends SharedType_129 {}

export interface FlashvsrUpscaleVideoInput {
    /**
     * Acceleration
     * @description Acceleration mode for VAE decoding. Options: regular (best quality), high (balanced), full (fastest). More accerleation means longer duration videos can be processed too.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'regular' | 'high' | 'full';
    /**
     * Color Fix
     * @description Color correction enabled.
     * @default true
     */
    color_fix?: boolean;
    /**
     * Output Format
     * @description The format of the output video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    output_format?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Output Quality
     * @description The quality of the output video.
     * @default high
     * @enum {string}
     */
    output_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Output Write Mode
     * @description The write mode of the output video.
     * @default balanced
     * @enum {string}
     */
    output_write_mode?: 'fast' | 'balanced' | 'small';
    /**
     * Preserve Audio
     * @description Copy the original audio tracks into the upscaled video using FFmpeg when possible.
     * @default false
     */
    preserve_audio?: boolean;
    /**
     * Quality
     * @description Quality level for tile blending (0-100). Controls overlap between tiles to prevent grid artifacts. Higher values provide better quality with more overlap. Recommended: 70-85 for high-res videos, 50-70 for faster processing.
     * @default 70
     */
    quality?: number;
    /**
     * Seed
     * @description The random seed used for the generation process.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned inline and not stored in history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Upscale Factor
     * @description Upscaling factor to be used.
     * @default 2
     */
    upscale_factor?: number;
    /**
     * Video Url
     * @description The input video to be upscaled
     * @example https://public-bucket-20251031-222058.s3.us-west-2.amazonaws.com/example_nighttime.mp4
     */
    video_url: string;
}

export interface FlashvsrUpscaleVideoOutput {
    /**
     * Seed
     * @description The random seed used for the generation process.
     */
    seed: number;
    /**
     * Video
     * @description Upscaled video file after processing
     */
    video: Components.File;
}

export interface FinegrainEraserMaskInput {
    /**
     * Image Url
     * @description URL of the image to edit
     * @example https://v3.fal.media/files/elephant/IKqKIxDfRDK8fzeETCveO_erase_example01.jpg
     */
    image_url: string;
    /**
     * Mask Url
     * @description URL of the mask image. Should be a binary mask where white (255) indicates areas to erase
     * @example https://v3.fal.media/files/panda/-31cZrsoy-8BrLqOEFmST_indir%20(18).png
     */
    mask_url: string;
    /**
     * Mode
     * @description Erase quality mode
     * @default standard
     * @enum {string}
     */
    mode?: 'express' | 'standard' | 'premium';
    /**
     * Seed
     * @description Random seed for reproducible generation
     */
    seed?: number;
}

export interface FinegrainEraserMaskOutput extends SharedType_de5 {}

export interface FinegrainEraserBboxInput {
    /**
     * Box Prompts
     * @description List of bounding box coordinates to erase (only one box prompt is supported)
     */
    box_prompts: Components.BoxPromptBase_1[];
    /**
     * Image Url
     * @description URL of the image to edit
     * @example https://v3.fal.media/files/elephant/IKqKIxDfRDK8fzeETCveO_erase_example01.jpg
     */
    image_url: string;
    /**
     * Mode
     * @description Erase quality mode
     * @default standard
     * @enum {string}
     */
    mode?: 'express' | 'standard' | 'premium';
    /**
     * Seed
     * @description Random seed for reproducible generation
     */
    seed?: number;
}

export interface FinegrainEraserBboxOutput extends SharedType_de5 {}

export interface FinegrainEraserInput {
    /**
     * Image Url
     * @description URL of the image to edit
     * @example https://v3.fal.media/files/elephant/IKqKIxDfRDK8fzeETCveO_erase_example01.jpg
     */
    image_url: string;
    /**
     * Mode
     * @description Erase quality mode
     * @default standard
     * @enum {string}
     */
    mode?: 'express' | 'standard' | 'premium';
    /**
     * Prompt
     * @description Text description of what to erase
     * @example person on the right and snowboard
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed for reproducible generation
     */
    seed?: number;
}

export interface FinegrainEraserOutput extends SharedType_de5 {}

export interface FilmVideoInput {
    /**
     * Frames Per Second
     * @description Frames per second for the output video. Only applicable if use_calculated_fps is False.
     * @default 8
     */
    fps?: number;
    /**
     * Loop
     * @description If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.
     * @default false
     */
    loop?: boolean;
    /**
     * Number of Frames
     * @description The number of frames to generate between the input video frames.
     * @default 1
     */
    num_frames?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Use Calculated FPS
     * @description If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used.
     * @default true
     */
    use_calculated_fps?: boolean;
    /**
     * Use Scene Detection
     * @description If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.
     * @default false
     */
    use_scene_detection?: boolean;
    /**
     * Video Quality
     * @description The quality of the output video. Only applicable if output_type is 'video'.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video URL
     * @description The URL of the video to use for interpolation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/interpolation-video-input.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the output video. Only applicable if output_type is 'video'.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface FilmVideoOutput {
    /**
     * Video
     * @description The generated video file with interpolated frames.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/film-video-output.mp4"
     *     }
     */
    video: Components.VideoFile_1;
}

export interface FilmInput {
    /**
     * End Image URL
     * @description The URL of the second image to use as the ending point for interpolation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/interpolate-end-frame.png
     */
    end_image_url: string;
    /**
     * Frames Per Second
     * @description Frames per second for the output video. Only applicable if output_type is 'video'.
     * @default 8
     */
    fps?: number;
    /**
     * Image Format
     * @description The format of the output images. Only applicable if output_type is 'images'.
     * @default jpeg
     * @enum {string}
     */
    image_format?: 'png' | 'jpeg';
    /**
     * Include End
     * @description Whether to include the end image in the output.
     * @default false
     */
    include_end?: boolean;
    /**
     * Include Start
     * @description Whether to include the start image in the output.
     * @default false
     */
    include_start?: boolean;
    /**
     * Number of Frames
     * @description The number of frames to generate between the input images.
     * @default 1
     */
    num_frames?: number;
    /**
     * Output Type
     * @description The type of output to generate; either individual images or a video.
     * @default images
     * @enum {string}
     */
    output_type?: 'images' | 'video';
    /**
     * Start Image URL
     * @description The URL of the first image to use as the starting point for interpolation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/interpolate-start-frame.png
     */
    start_image_url: string;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Video Quality
     * @description The quality of the output video. Only applicable if output_type is 'video'.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Write Mode
     * @description The write mode of the output video. Only applicable if output_type is 'video'.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface FilmOutput {
    /**
     * Images
     * @description The generated frames as individual images.
     * @default []
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/film-mid-frame.jpeg"
     *       }
     *     ]
     */
    images?: Components.ImageFile[];
    /**
     * Video
     * @description The generated video file, if output_type is 'video'.
     */
    video?: Components.VideoFile_1;
}

export interface FfmpegApiWaveformInput {
    /**
     * Media Url
     * @description URL of the audio file to analyze
     */
    media_url: string;
    /**
     * Points Per Second
     * @description Controls how many points are sampled per second of audio. Lower values (e.g. 1-2) create a coarser waveform, higher values (e.g. 4-10) create a more detailed one.
     * @default 4
     */
    points_per_second?: number;
    /**
     * Precision
     * @description Number of decimal places for the waveform values. Higher values provide more precision but increase payload size.
     * @default 2
     */
    precision?: number;
    /**
     * Smoothing Window
     * @description Size of the smoothing window. Higher values create a smoother waveform. Must be an odd number.
     * @default 3
     */
    smoothing_window?: number;
}

export interface FfmpegApiWaveformOutput {
    /**
     * Duration
     * @description Duration of the audio in seconds
     */
    duration: number;
    /**
     * Points
     * @description Number of points in the waveform data
     */
    points: number;
    /**
     * Precision
     * @description Number of decimal places used in the waveform values
     */
    precision: number;
    /**
     * Waveform
     * @description Normalized waveform data as an array of values between -1 and 1. The number of points is determined by audio duration × points_per_second.
     */
    waveform: number[];
}

export interface FfmpegApiMetadataInput {
    /**
     * Extract Frames
     * @description Whether to extract the start and end frames for videos. Note that when true the request will be slower.
     * @default false
     */
    extract_frames?: boolean;
    /**
     * Media Url
     * @description URL of the media file (video or audio) to analyze
     */
    media_url: string;
}

export interface FfmpegApiMetadataOutput {
    /**
     * Media
     * @description Metadata for the analyzed media file (either Video or Audio)
     */
    media: Components.Video_1 | Components.Audio;
}

export interface FfmpegApiMergeVideosInput {
    /**
     * Resolution
     * @description Resolution of the final video. Width and height must be between 512 and 2048.
     */
    resolution?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Target Fps
     * @description Target FPS for the output video. If not provided, uses the lowest FPS from input videos.
     */
    target_fps?: number;
    /**
     * Video Urls
     * @description List of video URLs to merge in order
     */
    video_urls: string[];
}

export interface FfmpegApiMergeVideosOutput {
    /**
     * Metadata
     * @description Metadata about the merged video including original video info
     */
    metadata: Record<string, number>;
    /** @description Merged video file */
    video: Components.File_1;
}

export interface FfmpegApiMergeAudiosInput {
    /**
     * Audio Urls
     * @description List of audio URLs to merge in order. The 0th stream of the audio will be considered as the merge candidate.
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_inputs/ffmpeg_api/merge_audios/first.mp3",
     *       "https://storage.googleapis.com/falserverless/example_inputs/ffmpeg_api/merge_audios/second.mp3"
     *     ]
     */
    audio_urls: string[];
    /**
     * Output Format
     * @description Output format of the combined audio. If not used, will be determined automatically using FFMPEG. Formatted as codec_sample_rate_bitrate.
     */
    output_format?:
        | 'mp3_22050_32'
        | 'mp3_44100_32'
        | 'mp3_44100_64'
        | 'mp3_44100_96'
        | 'mp3_44100_128'
        | 'mp3_44100_192'
        | 'pcm_8000'
        | 'pcm_16000'
        | 'pcm_22050'
        | 'pcm_24000'
        | 'pcm_44100'
        | 'pcm_48000'
        | 'ulaw_8000'
        | 'alaw_8000'
        | 'opus_48000_32'
        | 'opus_48000_64'
        | 'opus_48000_96'
        | 'opus_48000_128'
        | 'opus_48000_192';
}

export interface FfmpegApiMergeAudiosOutput {
    /**
     * @description Merged audio file
     * @example {
     *       "file_size": 97401,
     *       "file_name": "merged_audios.mp3",
     *       "content_type": "audio/mpeg",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/ffmpeg_api/merge_audios.mp3"
     *     }
     */
    audio: Components.File_1;
}

export interface FfmpegApiMergeAudioVideoInput {
    /**
     * Audio Url
     * @description URL of the audio file to use as the audio track
     * @example https://storage.googleapis.com/falserverless/example_inputs/ffmpeg-audio.wav
     */
    audio_url: string;
    /**
     * Start Offset
     * @description Offset in seconds for when the audio should start relative to the video
     * @default 0
     */
    start_offset?: number;
    /**
     * Video Url
     * @description URL of the video file to use as the video track
     * @example https://storage.googleapis.com/falserverless/example_inputs/ffmpeg-video.mp4
     */
    video_url: string;
}

export interface FfmpegApiMergeAudioVideoOutput {
    /** @description Output video with merged audio. */
    video: Components.File_1;
}

export interface FfmpegApiLoudnormInput {
    /**
     * Audio Url
     * @description URL of the audio file to normalize
     * @example https://storage.googleapis.com/falserverless/example_inputs/ffmpeg-audio.wav
     */
    audio_url: string;
    /**
     * Dual Mono
     * @description Treat mono input files as dual-mono for correct EBU R128 measurement on stereo systems
     * @default false
     */
    dual_mono?: boolean;
    /**
     * Integrated Loudness
     * @description Integrated loudness target in LUFS.
     * @default -18
     */
    integrated_loudness?: number;
    /**
     * Linear
     * @description Use linear normalization mode (single-pass). If false, uses dynamic mode (two-pass for better quality).
     * @default false
     */
    linear?: boolean;
    /**
     * Loudness Range
     * @description Loudness range target in LU
     * @default 7
     */
    loudness_range?: number;
    /**
     * Measured I
     * @description Measured integrated loudness of input file in LUFS. Required for linear mode.
     */
    measured_i?: number;
    /**
     * Measured Lra
     * @description Measured loudness range of input file in LU. Required for linear mode.
     */
    measured_lra?: number;
    /**
     * Measured Thresh
     * @description Measured threshold of input file in LUFS. Required for linear mode.
     */
    measured_thresh?: number;
    /**
     * Measured Tp
     * @description Measured true peak of input file in dBTP. Required for linear mode.
     */
    measured_tp?: number;
    /**
     * Offset
     * @description Offset gain in dB applied before the true-peak limiter
     * @default 0
     */
    offset?: number;
    /**
     * Print Summary
     * @description Return loudness measurement summary with the normalized audio
     * @default false
     */
    print_summary?: boolean;
    /**
     * True Peak
     * @description Maximum true peak in dBTP.
     * @default -0.1
     */
    true_peak?: number;
}

export interface FfmpegApiLoudnormOutput {
    /** @description Normalized audio file */
    audio: Components.File_1;
    /** @description Structured loudness measurement summary (if requested) */
    summary?: Components.LoudnormSummary;
}

export interface FfmpegApiExtractFrameInput {
    /**
     * Frame Type
     * @description Type of frame to extract: first, middle, or last frame of the video
     * @default first
     * @enum {string}
     */
    frame_type?: 'first' | 'middle' | 'last';
    /**
     * Video Url
     * @description URL of the video file to use as the video track
     * @example https://v3.fal.media/files/monkey/R6D8anxtsyItZTyBB2ksC_qeoDDxmLSg8cuWasM54KY_output.mp4
     */
    video_url: string;
}

export interface FfmpegApiExtractFrameOutput {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/elephant/IHmmk4dvyoCCYhtzI2FsO_16df6b1358374c1a9b023c80d752ee7b.jpg"
     *       }
     *     ]
     */
    images: Components.Image_2[];
}

export interface FfmpegApiComposeInput {
    /**
     * Tracks
     * @description List of tracks to be combined into the final media
     */
    tracks: Components.Track[];
}

export interface FfmpegApiComposeOutput {
    /**
     * Thumbnail Url
     * @description URL of the video's thumbnail image
     */
    thumbnail_url: string;
    /**
     * Video Url
     * @description URL of the processed video file
     */
    video_url: string;
}

export interface FastSvdTextToVideoInput {
    /**
     * Cond Aug
     * @description The conditoning augmentation determines the amount of noise that will be
     *                 added to the conditioning frame. The higher the number, the more noise
     *                 there will be, and the less the video will look like the initial image.
     *                 Increase it for more motion.
     * @default 0.02
     */
    cond_aug?: number;
    /**
     * Deep Cache
     * @description Enabling [DeepCache](https://github.com/horseee/DeepCache) will make the execution
     *                 faster, but might sometimes degrade overall quality. The higher the setting, the
     *                 faster the execution will be, but the more quality might be lost.
     * @default none
     * @enum {string}
     */
    deep_cache?: 'none' | 'minimum' | 'medium' | 'high';
    /**
     * Fps
     * @description The FPS of the generated video. The higher the number, the faster the video will
     *                 play. Total video length is 25 frames.
     * @default 10
     */
    fps?: number;
    /**
     * Motion Bucket Id
     * @description The motion bucket id determines the motion of the generated video. The
     *                 higher the number, the more motion there will be.
     * @default 127
     */
    motion_bucket_id?: number;
    /**
     * Negative Prompt
     * @description The negative prompt to use as a starting point for the generation.
     * @default unrealistic, saturated, high contrast, big nose, painting, drawing, sketch, cartoon, anime, manga, render, CG, 3d, watermark, signature, label
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The prompt to use as a starting point for the generation.
     * @example A rocket flying that is about to take off
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Steps
     * @description The number of steps to run the model for. The higher the number the better
     *                 the quality and longer it will take to generate.
     * @default 20
     */
    steps?: number;
    /**
     * Video Size
     * @description The size of the generated video.
     * @default landscape_16_9
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
}

export interface FastSvdTextToVideoOutput extends SharedType_8c0 {}

export interface FastSvdLcmTextToVideoInput {
    /**
     * Cond Aug
     * @description The conditoning augmentation determines the amount of noise that will be
     *                 added to the conditioning frame. The higher the number, the more noise
     *                 there will be, and the less the video will look like the initial image.
     *                 Increase it for more motion.
     * @default 0.02
     */
    cond_aug?: number;
    /**
     * Fps
     * @description The FPS of the generated video. The higher the number, the faster the video will
     *                 play. Total video length is 25 frames.
     * @default 10
     */
    fps?: number;
    /**
     * Motion Bucket Id
     * @description The motion bucket id determines the motion of the generated video. The
     *                 higher the number, the more motion there will be.
     * @default 127
     */
    motion_bucket_id?: number;
    /**
     * Prompt
     * @description The prompt to use as a starting point for the generation.
     * @example A rocket flying that is about to take off
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Steps
     * @description The number of steps to run the model for. The higher the number the better
     *                 the quality and longer it will take to generate.
     * @default 4
     */
    steps?: number;
    /**
     * Video Size
     * @description The size of the generated video.
     * @default landscape_16_9
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
}

export interface FastSvdLcmTextToVideoOutput extends SharedType_8c0 {}

export interface FastSvdLcmInput {
    /**
     * Cond Aug
     * @description The conditoning augmentation determines the amount of noise that will be
     *                 added to the conditioning frame. The higher the number, the more noise
     *                 there will be, and the less the video will look like the initial image.
     *                 Increase it for more motion.
     * @default 0.02
     */
    cond_aug?: number;
    /**
     * Fps
     * @description The FPS of the generated video. The higher the number, the faster the video will
     *                 play. Total video length is 25 frames.
     * @default 10
     */
    fps?: number;
    /**
     * Image Url
     * @description The URL of the image to use as a starting point for the generation.
     * @example https://storage.googleapis.com/falserverless/model_tests/svd/rocket.png
     * @example https://storage.googleapis.com/falserverless/model_tests/svd/mustang.png
     * @example https://storage.googleapis.com/falserverless/model_tests/svd/ship.png
     * @example https://storage.googleapis.com/falserverless/model_tests/svd/rocket2.png
     */
    image_url: string;
    /**
     * Motion Bucket Id
     * @description The motion bucket id determines the motion of the generated video. The
     *                 higher the number, the more motion there will be.
     * @default 127
     */
    motion_bucket_id?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Steps
     * @description The number of steps to run the model for. The higher the number the better
     *                 the quality and longer it will take to generate.
     * @default 4
     */
    steps?: number;
}

export interface FastSvdLcmOutput extends SharedType_8c0 {}

export interface FastSdxlInpaintingInput {
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description The URL of the image to use as a starting point for the generation.
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png
     */
    image_url: string;
    /**
     * Loras
     * @description The list of LoRA weights to use.
     * @default []
     */
    loras?: Components.LoraWeight_1[];
    /**
     * Mask Url
     * @description The URL of the mask to use for inpainting.
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png
     */
    mask_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 25
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example a tiger sitting on a park bench
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description determines how much the generated image resembles the initial image
     * @default 0.95
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FastSdxlInpaintingOutput extends SharedType_a73 {}

export interface FastSdxlImageToImageInput {
    /**
     * Crop Output
     * @description If set to true, the output cropped to the proper aspect ratio after generating.
     * @default false
     */
    crop_output?: boolean;
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description The URL of the image to use as a starting point for the generation.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/tiger/IExuP-WICqaIesLZAZPur.jpeg
     */
    image_url: string;
    /**
     * Loras
     * @description The list of LoRA weights to use.
     * @default []
     */
    loras?: Components.LoraWeight_1[];
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 25
     */
    num_inference_steps?: number;
    /**
     * Preserve Aspect Ratio
     * @description If set to true, the aspect ratio of the generated image will be preserved even
     *             if the image size is too large. However, if the image is not a multiple of 32
     *             in width or height, it will be resized to the nearest multiple of 32. By default,
     *             this snapping to the nearest multiple of 32 will not preserve the aspect ratio.
     *             Set crop_output to True, to crop the output to the proper aspect ratio
     *             after generating.
     * @default false
     */
    preserve_aspect_ratio?: boolean;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example an island near sea, with seagulls, moon shining over the sea, light house, boats int he background, fish flying over the sea
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description determines how much the generated image resembles the initial image
     * @default 0.95
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FastSdxlImageToImageOutput extends SharedType_a73 {}

export interface FastSdxlControlnetCannyInpaintingInput {
    /**
     * Control Image Url
     * @description The URL of the control image.
     * @example https://avatars.githubusercontent.com/u/74778219
     */
    control_image_url: string;
    /**
     * Controlnet Conditioning Scale
     * @description The scale of the controlnet conditioning.
     * @default 0.5
     */
    controlnet_conditioning_scale?: number;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. Leave it none to automatically infer from the control image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description The URL of the image to use as a starting point for the generation.
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png
     */
    image_url: string;
    /**
     * Loras
     * @description The list of LoRA weights to use.
     * @default []
     */
    loras?: Components.LoraWeight_3[];
    /**
     * Mask Url
     * @description The URL of the mask to use for inpainting.
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png
     */
    mask_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 25
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example Ice fortress, aurora skies, polar wildlife, twilight
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description determines how much the generated image resembles the initial image
     * @default 0.95
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FastSdxlControlnetCannyInpaintingOutput extends SharedType_a88 {}

export interface FastSdxlControlnetCannyImageToImageInput {
    /**
     * Control Image Url
     * @description The URL of the control image.
     * @example https://avatars.githubusercontent.com/u/74778219
     */
    control_image_url: string;
    /**
     * Controlnet Conditioning Scale
     * @description The scale of the controlnet conditioning.
     * @default 0.5
     */
    controlnet_conditioning_scale?: number;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. Leave it none to automatically infer from the control image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description The URL of the image to use as a starting point for the generation.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/tiger/IExuP-WICqaIesLZAZPur.jpeg
     */
    image_url: string;
    /**
     * Loras
     * @description The list of LoRA weights to use.
     * @default []
     */
    loras?: Components.LoraWeight_3[];
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 25
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example Ice fortress, aurora skies, polar wildlife, twilight
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description determines how much the generated image resembles the initial image
     * @default 0.95
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FastSdxlControlnetCannyImageToImageOutput extends SharedType_a88 {}

export interface FastSdxlControlnetCannyInput {
    /**
     * Control Image Url
     * @description The URL of the control image.
     * @example https://avatars.githubusercontent.com/u/74778219
     */
    control_image_url: string;
    /**
     * Controlnet Conditioning Scale
     * @description The scale of the controlnet conditioning.
     * @default 0.5
     */
    controlnet_conditioning_scale?: number;
    /**
     * Enable Deep Cache
     * @description If set to true, DeepCache will be enabled. TBD
     * @default false
     */
    enable_deep_cache?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. Leave it none to automatically infer from the control image.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description The list of LoRA weights to use.
     * @default []
     */
    loras?: Components.LoraWeight_3[];
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     * @example ugly, deformed
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 25
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example Ice fortress, aurora skies, polar wildlife, twilight
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FastSdxlControlnetCannyOutput extends SharedType_a88 {}

export interface FastSdxlInput {
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description The list of LoRA weights to use.
     * @default []
     */
    loras?: Components.LoraWeight_1[];
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     * @example ugly, deformed
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 25
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example photo of a rhino dressed suit and tie sitting at a table in a bar with a bar stools, award winning photography, Elke vogelsang
     * @example Photo of a classic red mustang car parked in las vegas strip at night
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FastSdxlOutput extends SharedType_a73 {}

export interface FastLightningSdxlInpaintingInput {
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance Rescale
     * @description The rescale factor for the CFG.
     * @default 0
     */
    guidance_rescale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description The URL of the image to use as a starting point for the generation.
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png
     */
    image_url: string;
    /**
     * Mask Url
     * @description The URL of the mask to use for inpainting.
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png
     */
    mask_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 4
     * @enum {string}
     */
    num_inference_steps?: '1' | '2' | '4' | '8';
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example a tiger sitting on a park bench
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description determines how much the generated image resembles the initial image
     * @default 0.95
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FastLightningSdxlInpaintingOutput extends SharedType_a73 {}

export interface FastLightningSdxlImageToImageInput {
    /**
     * Crop Output
     * @description If set to true, the output cropped to the proper aspect ratio after generating.
     * @default false
     */
    crop_output?: boolean;
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance Rescale
     * @description The rescale factor for the CFG.
     * @default 0
     */
    guidance_rescale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description The URL of the image to use as a starting point for the generation.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/tiger/IExuP-WICqaIesLZAZPur.jpeg
     */
    image_url: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 4
     * @enum {string}
     */
    num_inference_steps?: '1' | '2' | '4' | '8';
    /**
     * Preserve Aspect Ratio
     * @description If set to true, the aspect ratio of the generated image will be preserved even
     *             if the image size is too large. However, if the image is not a multiple of 32
     *             in width or height, it will be resized to the nearest multiple of 32. By default,
     *             this snapping to the nearest multiple of 32 will not preserve the aspect ratio.
     *             Set crop_output to True, to crop the output to the proper aspect ratio
     *             after generating.
     * @default false
     */
    preserve_aspect_ratio?: boolean;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example an island near sea, with seagulls, moon shining over the sea, light house, boats int he background, fish flying over the sea
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description determines how much the generated image resembles the initial image
     * @default 0.95
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FastLightningSdxlImageToImageOutput extends SharedType_a73 {}

export interface FastLightningSdxlInput {
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance Rescale
     * @description The rescale factor for the CFG.
     * @default 0
     */
    guidance_rescale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 4
     * @enum {string}
     */
    num_inference_steps?: '1' | '2' | '4' | '8';
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example photo of a girl smiling during a sunset, with lightnings in the background
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FastLightningSdxlOutput extends SharedType_a73 {}

export interface FastLcmDiffusionInpaintingInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance Rescale
     * @description The rescale factor for the CFG.
     * @default 0
     */
    guidance_rescale?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 1.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description The URL of the image to use as a starting point for the generation.
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png
     */
    image_url: string;
    /**
     * Mask Url
     * @description The URL of the mask to use for inpainting.
     * @example https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png
     */
    mask_url: string;
    /**
     * Model Name
     * @description The name of the model to use.
     * @default stabilityai/stable-diffusion-xl-base-1.0
     * @example stabilityai/stable-diffusion-xl-base-1.0
     * @example runwayml/stable-diffusion-v1-5
     * @enum {string}
     */
    model_name?: 'stabilityai/stable-diffusion-xl-base-1.0' | 'runwayml/stable-diffusion-v1-5';
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 6
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example a tiger sitting on a park bench
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description determines how much the generated image resembles the initial image
     * @default 0.95
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default true
     */
    sync_mode?: boolean;
}

export interface FastLcmDiffusionInpaintingOutput extends SharedType_a73 {}

export interface FastLcmDiffusionImageToImageInput {
    /**
     * Crop Output
     * @description If set to true, the output cropped to the proper aspect ratio after generating.
     * @default false
     */
    crop_output?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance Rescale
     * @description The rescale factor for the CFG.
     * @default 0
     */
    guidance_rescale?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 1.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description The URL of the image to use as a starting point for the generation.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/tiger/IExuP-WICqaIesLZAZPur.jpeg
     */
    image_url: string;
    /**
     * Model Name
     * @description The name of the model to use.
     * @default stabilityai/stable-diffusion-xl-base-1.0
     * @example stabilityai/stable-diffusion-xl-base-1.0
     * @example runwayml/stable-diffusion-v1-5
     * @enum {string}
     */
    model_name?: 'stabilityai/stable-diffusion-xl-base-1.0' | 'runwayml/stable-diffusion-v1-5';
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 6
     */
    num_inference_steps?: number;
    /**
     * Preserve Aspect Ratio
     * @description If set to true, the aspect ratio of the generated image will be preserved even
     *             if the image size is too large. However, if the image is not a multiple of 32
     *             in width or height, it will be resized to the nearest multiple of 32. By default,
     *             this snapping to the nearest multiple of 32 will not preserve the aspect ratio.
     *             Set crop_output to True, to crop the output to the proper aspect ratio
     *             after generating.
     * @default false
     */
    preserve_aspect_ratio?: boolean;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example an island near sea, with seagulls, moon shining over the sea, light house, boats int he background, fish flying over the sea
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description determines how much the generated image resembles the initial image
     * @default 0.95
     */
    strength?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default true
     */
    sync_mode?: boolean;
}

export interface FastLcmDiffusionImageToImageOutput extends SharedType_a73 {}

export interface FastLcmDiffusionInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance Rescale
     * @description The rescale factor for the CFG.
     * @default 0
     */
    guidance_rescale?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 1.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Model Name
     * @description The name of the model to use.
     * @default stabilityai/stable-diffusion-xl-base-1.0
     * @example stabilityai/stable-diffusion-xl-base-1.0
     * @example runwayml/stable-diffusion-v1-5
     * @enum {string}
     */
    model_name?: 'stabilityai/stable-diffusion-xl-base-1.0' | 'runwayml/stable-diffusion-v1-5';
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     * @example ugly, deformed
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 6
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example Self-portrait oil painting, a beautiful cyborg with golden hair, 8k.
     */
    prompt: string;
    /**
     * Request Id
     * @description An id bound to a request, can be used with response to identify the request
     *                 itself.
     * @default
     */
    request_id?: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default true
     */
    sync_mode?: boolean;
}

export interface FastLcmDiffusionOutput extends SharedType_a73 {}

export interface FastFooocusSdxlImageToImageInput {
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Refiner
     * @description If set to true, a smaller model will try to refine the output after it was processed.
     * @default true
     */
    enable_refiner?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default true
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance Rescale
     * @description The rescale factor for the CFG.
     * @default 0
     */
    guidance_rescale?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 2
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image. Leave it none to automatically infer from the prompt image.
     * @example null
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description The URL of the image to use as a starting point for the generation.
     * @example https://fal-cdn.batuhan-941.workers.dev/files/tiger/IExuP-WICqaIesLZAZPur.jpeg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example an island near sea, with seagulls, moon shining over the sea, light house, boats int he background, fish flying over the sea
     */
    prompt: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description determines how much the generated image resembles the initial image
     * @default 0.95
     */
    strength?: number;
}

export interface FastFooocusSdxlImageToImageOutput extends SharedType_a73 {}

export interface FastFooocusSdxlInput {
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Refiner
     * @description If set to true, a smaller model will try to refine the output after it was processed.
     * @default true
     */
    enable_refiner?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default true
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance Rescale
     * @description The rescale factor for the CFG.
     * @default 0
     */
    guidance_rescale?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 2
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example cartoon, illustration, animation. face. male, female
     * @example ugly, deformed
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example Self-portrait oil painting, a beautiful cyborg with golden hair, 8k.
     */
    prompt: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
}

export interface FastFooocusSdxlOutput extends SharedType_a73 {}

export interface FastAnimatediffVideoToVideoInput {
    /**
     * First N Seconds
     * @description The first N number of seconds of video to animate.
     * @default 3
     */
    first_n_seconds?: number;
    /**
     * Fps
     * @description Number of frames per second to extract from the video.
     * @default 8
     */
    fps?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Motions
     * @description The motions to apply to the video.
     */
    motions?: ('zoom-out' | 'zoom-in' | 'pan-left' | 'pan-right' | 'tilt-up' | 'tilt-down')[];
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default (bad quality, worst quality:1.2), ugly faces, bad anime
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 25
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example closeup of tony stark, robert downey jr, fireworks, high quality, ultra HD
     * @example panda playing a guitar, on a boat, in the ocean, high quality, high quality, ultra HD, realistic
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength of the input video in the final output.
     * @default 0.7
     */
    strength?: number;
    /**
     * Video Url
     * @description URL of the video.
     * @example https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-vid2vid-input-2.gif
     * @example https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-vid2vid-input-1.gif
     */
    video_url: string;
}

export interface FastAnimatediffVideoToVideoOutput extends SharedType_24d {}

export interface FastAnimatediffTurboVideoToVideoInput {
    /**
     * First N Seconds
     * @description The first N number of seconds of video to animate.
     * @default 3
     */
    first_n_seconds?: number;
    /**
     * Fps
     * @description Number of frames per second to extract from the video.
     * @default 8
     */
    fps?: number;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Motions
     * @description The motions to apply to the video.
     */
    motions?: ('zoom-out' | 'zoom-in' | 'pan-left' | 'pan-right' | 'tilt-up' | 'tilt-down')[];
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default (bad quality, worst quality:1.2), ugly faces, bad anime
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform. 4-12 is recommended for turbo mode.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example closeup of tony stark, robert downey jr, fireworks, high quality, ultra HD
     * @example panda playing a guitar, on a boat, in the ocean, high quality, high quality, ultra HD, realistic
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength of the input video in the final output.
     * @default 0.7
     */
    strength?: number;
    /**
     * Video Url
     * @description URL of the video.
     * @example https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-vid2vid-input-2.gif
     * @example https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-vid2vid-input-1.gif
     */
    video_url: string;
}

export interface FastAnimatediffTurboVideoToVideoOutput extends SharedType_24d {}

export interface FastAnimatediffTurboTextToVideoInput {
    /**
     * Fps
     * @description Number of frames per second to extract from the video.
     * @default 8
     */
    fps?: number;
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Motions
     * @description The motions to apply to the video.
     */
    motions?: ('zoom-out' | 'zoom-in' | 'pan-left' | 'pan-right' | 'tilt-up' | 'tilt-down')[];
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default (bad quality, worst quality:1.2), ugly faces, bad anime
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description The number of frames to generate for the video.
     * @default 16
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform. 4-12 is recommended for turbo mode.
     * @default 4
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the video. Be as descriptive as possible for best results.
     * @example masterpiece, best quality, 1girl, solo, cherry blossoms, hanami, pink flower, white flower, spring season, wisteria, petals, flower, plum blossoms, outdoors, falling petals, white hair, black eyes
     * @example panda playing a guitar, on a boat, in the ocean, high quality, high quality, ultra HD, realistic
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Video Size
     * @description The size of the video to generate.
     * @default square
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
}

export interface FastAnimatediffTurboTextToVideoOutput extends SharedType_7da {}

export interface FastAnimatediffTextToVideoInput {
    /**
     * Fps
     * @description Number of frames per second to extract from the video.
     * @default 8
     */
    fps?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Motions
     * @description The motions to apply to the video.
     */
    motions?: ('zoom-out' | 'zoom-in' | 'pan-left' | 'pan-right' | 'tilt-up' | 'tilt-down')[];
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default (bad quality, worst quality:1.2), ugly faces, bad anime
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description The number of frames to generate for the video.
     * @default 16
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 25
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the video. Be as descriptive as possible for best results.
     * @example masterpiece, best quality, 1girl, solo, cherry blossoms, hanami, pink flower, white flower, spring season, wisteria, petals, flower, plum blossoms, outdoors, falling petals, white hair, black eyes
     * @example panda playing a guitar, on a boat, in the ocean, high quality, high quality, ultra HD, realistic
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Video Size
     * @description The size of the video to generate.
     * @default square
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
}

export interface FastAnimatediffTextToVideoOutput extends SharedType_7da {}

export interface FashnTryonV16Input {
    /**
     * Category
     * @description Category of the garment to try-on. 'auto' will attempt to automatically detect the category of the garment.
     * @default auto
     * @enum {string}
     */
    category?: 'tops' | 'bottoms' | 'one-pieces' | 'auto';
    /**
     * Garment Image
     * @description URL or base64 of the garment image
     * @example https://storage.googleapis.com/falserverless/example_inputs/garment.webp
     */
    garment_image: string;
    /**
     * Garment Photo Type
     * @description Specifies the type of garment photo to optimize internal parameters for better performance. 'model' is for photos of garments on a model, 'flat-lay' is for flat-lay or ghost mannequin images, and 'auto' attempts to automatically detect the photo type.
     * @default auto
     * @enum {string}
     */
    garment_photo_type?: 'auto' | 'model' | 'flat-lay';
    /**
     * Mode
     * @description Specifies the mode of operation. 'performance' mode is faster but may sacrifice quality, 'balanced' mode is a balance between speed and quality, and 'quality' mode is slower but produces higher quality results.
     * @default balanced
     * @enum {string}
     */
    mode?: 'performance' | 'balanced' | 'quality';
    /**
     * Model Image
     * @description URL or base64 of the model image
     * @example https://storage.googleapis.com/falserverless/example_inputs/model.png
     */
    model_image: string;
    /**
     * Moderation Level
     * @description Content moderation level for garment images. 'none' disables moderation, 'permissive' blocks only explicit content, 'conservative' also blocks underwear and swimwear.
     * @default permissive
     * @enum {string}
     */
    moderation_level?: 'none' | 'permissive' | 'conservative';
    /**
     * Num Samples
     * @description Number of images to generate in a single run. Image generation has a random element in it, so trying multiple images at once increases the chances of getting a good result.
     * @default 1
     */
    num_samples?: number;
    /**
     * Output Format
     * @description Output format of the generated images. 'png' is highest quality, while 'jpeg' is faster
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg';
    /**
     * Seed
     * @description Sets random operations to a fixed state. Use the same seed to reproduce results with the same inputs, or different seed to force different results.
     */
    seed?: number;
    /**
     * Segmentation Free
     * @description Disables human parsing on the model image.
     * @default true
     */
    segmentation_free?: boolean;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FashnTryonV16Output {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://cdn.fashn.ai/4ddf1d78-63df-44bb-8e1f-355fff3a7b87/output_0.png"
     *       }
     *     ]
     */
    images: Components.File[];
}

export interface FashnTryonV15Input {
    /**
     * Category
     * @description Category of the garment to try-on. 'auto' will attempt to automatically detect the category of the garment.
     * @default auto
     * @enum {string}
     */
    category?: 'tops' | 'bottoms' | 'one-pieces' | 'auto';
    /**
     * Garment Image
     * @description URL or base64 of the garment image
     * @example https://utfs.io/f/wXFHUNfTHmLjtkhepmqOUnkr8XxZbNIFmRWldShDLu320TeC
     */
    garment_image: string;
    /**
     * Garment Photo Type
     * @description Specifies the type of garment photo to optimize internal parameters for better performance. 'model' is for photos of garments on a model, 'flat-lay' is for flat-lay or ghost mannequin images, and 'auto' attempts to automatically detect the photo type.
     * @default auto
     * @enum {string}
     */
    garment_photo_type?: 'auto' | 'model' | 'flat-lay';
    /**
     * Mode
     * @description Specifies the mode of operation. 'performance' mode is faster but may sacrifice quality, 'balanced' mode is a balance between speed and quality, and 'quality' mode is slower but produces higher quality results.
     * @default balanced
     * @enum {string}
     */
    mode?: 'performance' | 'balanced' | 'quality';
    /**
     * Model Image
     * @description URL or base64 of the model image
     * @example https://utfs.io/f/wXFHUNfTHmLj4prvqbRMQ6JXFyUr3IT0avK2HSOmZWiAsxg9
     */
    model_image: string;
    /**
     * Moderation Level
     * @description Content moderation level for garment images. 'none' disables moderation, 'permissive' blocks only explicit content, 'conservative' also blocks underwear and swimwear.
     * @default permissive
     * @enum {string}
     */
    moderation_level?: 'none' | 'permissive' | 'conservative';
    /**
     * Num Samples
     * @description Number of images to generate in a single run. Image generation has a random element in it, so trying multiple images at once increases the chances of getting a good result.
     * @default 1
     */
    num_samples?: number;
    /**
     * Output Format
     * @description Output format of the generated images. 'png' is highest quality, while 'jpeg' is faster
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg';
    /**
     * Seed
     * @description Sets random operations to a fixed state. Use the same seed to reproduce results with the same inputs, or different seed to force different results.
     */
    seed?: number;
    /**
     * Segmentation Free
     * @description Disables human parsing on the model image.
     * @default true
     */
    segmentation_free?: boolean;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface FashnTryonV15Output {
    /**
     * Images
     * @example [
     *       {
     *         "url": "https://cdn.staging.fashn.ai/464eeeb1-4faa-4a2d-8c17-342d0d35c4c1/output_0.png"
     *       }
     *     ]
     */
    images: Components.File[];
}

export interface FaceToStickerInput {
    /**
     * Enable Safety Checker
     * @description If set to false, the safety checker will be disabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Image Url
     * @description URL of the video.
     * @example https://storage.googleapis.com/falserverless/model_tests/face_to_sticker/elon.jpg
     */
    image_url: string;
    /**
     * Instant ID strength
     * @description The strength of the instant ID.
     * @default 0.7
     */
    instant_id_strength?: number;
    /**
     * IP adapter noise
     * @description The amount of noise to add to the IP adapter.
     * @default 0.5
     */
    ip_adapter_noise?: number;
    /**
     * IP adapter weight
     * @description The weight of the IP adapter.
     * @default 0.2
     */
    ip_adapter_weight?: number;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of inference steps
     * @description Increasing the amount of steps tells Stable Diffusion that it should take more steps
     *                 to generate your final result which can increase the amount of detail in your image.
     * @default 20
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example a person
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Upscale
     * @description Whether to upscale the image 2x.
     * @default false
     */
    upscale?: boolean;
    /**
     * Upscale steps
     * @description The number of steps to use for upscaling. Only used if `upscale` is `true`.
     * @default 10
     */
    upscale_steps?: number;
}

export interface FaceToStickerOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     *                 The key is the image type and the value is a boolean.
     * @example {
     *       "sticker_image": false,
     *       "sticker_image_background_removed": false
     *     }
     */
    has_nsfw_concepts: {
        [key: string]: boolean;
    };
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "file_size": 560358,
     *         "height": 1024,
     *         "file_name": "cd8bab71b946470099d5fa20c7eed440.png",
     *         "content_type": "image/PNG",
     *         "url": "https://storage.googleapis.com/falserverless/model_tests/face_to_sticker/elon_output_1.png",
     *         "width": 1024
     *       },
     *       {
     *         "file_size": 452906,
     *         "height": 1024,
     *         "file_name": "181ae8fa12534c6f9285a991b415d9a7.png",
     *         "content_type": "image/PNG",
     *         "url": "https://storage.googleapis.com/falserverless/model_tests/face_to_sticker/elon_output_2.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Seed
     * @description Seed used during the inference.
     * @example 3625437076
     */
    seed: number;
    /**
     * Sticker Image
     * @description The generated face sticker image.
     * @example {
     *       "file_size": 560358,
     *       "height": 1024,
     *       "file_name": "cd8bab71b946470099d5fa20c7eed440.png",
     *       "content_type": "image/PNG",
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/face_to_sticker/elon_output_1.png",
     *       "width": 1024
     *     }
     */
    sticker_image: Components.Image;
    /**
     * Sticker Image Background Removed
     * @description The generated face sticker image with the background removed.
     * @example {
     *       "file_size": 452906,
     *       "height": 1024,
     *       "file_name": "181ae8fa12534c6f9285a991b415d9a7.png",
     *       "content_type": "image/PNG",
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/face_to_sticker/elon_output_2.png",
     *       "width": 1024
     *     }
     */
    sticker_image_background_removed: Components.Image;
}

export interface F5TtsInput {
    /**
     * Text to be converted to speech
     * @description The text to be converted to speech.
     * @example I don't really care what you call me. I've been a silent spectator, watching species evolve, empires rise and fall. But always remember, I am mighty and enduring. Respect me and I'll nurture you; ignore me and you shall face the consequences.
     */
    gen_text: string;
    /**
     * Model Type
     * @description The name of the model to be used for TTS.
     * @enum {string}
     */
    model_type: 'F5-TTS' | 'E2-TTS';
    /**
     * Reference Audio URL
     * @description The URL of the reference audio file.
     * @example https://storage.googleapis.com/falserverless/example_inputs/reference_audio.wav
     */
    ref_audio_url: string;
    /**
     * Reference Text for the Reference Audio
     * @description The reference text to be used for TTS. If not provided, an ASR (Automatic Speech Recognition) model will be used to generate the reference text.
     * @default
     * @example Some call me nature, others call me mother nature.
     */
    ref_text?: string;
    /**
     * Remove Silence
     * @description Whether to remove the silence from the audio file.
     * @default true
     */
    remove_silence?: boolean;
}

export interface F5TtsOutput {
    /**
     * Generated Speech
     * @description The audio file containing the generated speech.
     */
    audio_url: Components.AudioFile_2;
}

export interface FLiteTextureInput extends SharedType_b7d {}

export interface FLiteTextureOutput extends SharedType_a73 {}

export interface FLiteStandardInput extends SharedType_b7d {}

export interface FLiteStandardOutput extends SharedType_a73 {}

export interface EvfSamInput {
    /**
     * Blur Mask
     * @description Apply Gaussian blur to the mask. Value determines kernel size (must be odd number)
     * @default 0
     */
    blur_mask?: number;
    /**
     * Expand Mask
     * @description Expand/dilate the mask by specified pixels
     * @default 0
     */
    expand_mask?: number;
    /**
     * Fill Holes
     * @description Fill holes in the mask using morphological operations
     * @default false
     */
    fill_holes?: boolean;
    /**
     * Image Url
     * @description URL of the input image
     * @example https://storage.googleapis.com/falserverless/web-examples/evf-sam2/evfsam2-cat.png
     */
    image_url: string;
    /**
     * Mask Only
     * @description Output only the binary mask instead of masked image
     * @default true
     */
    mask_only?: boolean;
    /**
     * Negative Prompt
     * @description Areas to exclude from segmentation (will be subtracted from prompt results)
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description The prompt to generate segmentation from.
     * @example Cat in the middle of the image
     */
    prompt: string;
    /**
     * Revert Mask
     * @description Invert the mask (background becomes foreground and vice versa)
     * @default false
     */
    revert_mask?: boolean;
    /**
     * Semantic Type
     * @description Enable semantic level segmentation for body parts, background or multi objects
     * @default false
     */
    semantic_type?: boolean;
    /**
     * Use Grounding Dino
     * @description Use GroundingDINO instead of SAM for segmentation
     * @default false
     */
    use_grounding_dino?: boolean;
}

export interface EvfSamOutput {
    /**
     * Image
     * @description The segmented output image
     */
    image: Components.File;
}

export interface EsrganInput {
    /**
     * Face
     * @description Upscaling a face
     * @default false
     */
    face?: boolean;
    /**
     * Image Url
     * @description Url to input image
     * @example https://storage.googleapis.com/falserverless/model_tests/remove_background/elephant.jpg
     * @example https://storage.googleapis.com/falserverless/gallery/blue-bird.jpeg
     * @example https://storage.googleapis.com/falserverless/model_tests/upscale/image%20(8).png
     */
    image_url: string;
    /**
     * Model
     * @description Model to use for upscaling
     * @default RealESRGAN_x4plus
     * @enum {string}
     */
    model?:
        | 'RealESRGAN_x4plus'
        | 'RealESRGAN_x2plus'
        | 'RealESRGAN_x4plus_anime_6B'
        | 'RealESRGAN_x4_v3'
        | 'RealESRGAN_x4_wdn_v3'
        | 'RealESRGAN_x4_anime_v3';
    /**
     * Output Format
     * @description Output image format (png or jpeg)
     * @default png
     * @enum {string}
     */
    output_format?: 'png' | 'jpeg';
    /**
     * Scale
     * @description Rescaling factor
     * @default 2
     */
    scale?: number;
    /**
     * Tile
     * @description Tile size. Default is 0, that is no tile. When encountering the out-of-GPU-memory issue, please specify it, e.g., 400 or 200
     * @default 0
     */
    tile?: number;
}

export interface EsrganOutput extends SharedType_055 {}

export interface Era3dInput {
    /**
     * Background Removal
     * @description Background removal
     * @default true
     */
    background_removal?: boolean;
    /**
     * Cfg
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 4
     */
    cfg?: number;
    /**
     * Crop Size
     * @description Size of the image to crop to
     * @default 400
     */
    crop_size?: number;
    /**
     * Image Url
     * @description URL of the image to remove background from
     * @example https://storage.googleapis.com/falserverless/model_tests/era3d/DnvGjd9CCS-ESmLgTYgOn.png
     */
    image_url: string;
    /**
     * Seed
     * @description Seed for random number generation
     * @default -1
     */
    seed?: number;
    /**
     * Steps
     * @description Number of steps to run the model for
     * @default 40
     */
    steps?: number;
}

export interface Era3dOutput {
    /**
     * Images
     * @description Images with background removed
     */
    images: Components.Image_2[];
    /**
     * Normal Images
     * @description Normal images with background removed
     */
    normal_images: Components.Image_2[];
    /**
     * Seed
     * @description Seed used for random number generation
     */
    seed: number;
}

export interface Emu35ImageTextToImageInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the output image.
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Output Format
     * @description The format of the output image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to create the image.
     * @example Capture an intimate close-up bathed in warm, soft, late-afternoon sunlight filtering into a quintessential 1960s kitchen. The focal point is a charmingly designed vintage package of all-purpose flour, resting invitingly on a speckled Formica countertop. The packaging itself evokes pure nostalgia: perhaps thick, slightly textured paper in a warm cream tone, adorned with simple, bold typography (a friendly serif or script) in classic red and blue "ALL-PURPOSE FLOUR", featuring a delightful illustration like a stylized sheaf of wheat or a cheerful baker character. In smaller bold print at the bottom of the package: "NET WT 5 LBS (80 OZ) 2.27kg". Focus sharply on the package details – the slightly soft edges of the paper bag, the texture of the vintage printing, the inviting "All-Purpose Flour" text. Subtle hints of the 1960s kitchen frame the shot – the chrome edge of the counter gleaming softly, a blurred glimpse of a pastel yellow ceramic tile backsplash, or the corner of a vintage metal canister set just out of focus. The shallow depth of field keeps attention locked on the beautifully designed package, creating an aesthetic rich in warmth, authenticity, and nostalgic appeal.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the output image.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description The seed for the inference.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description Whether to return the image in sync mode.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Emu35ImageTextToImageOutput {
    /**
     * Images
     * @description The edited image.
     * @example [
     *       {
     *         "height": 880,
     *         "file_name": "2_gRhwfsnmNKYtZ_dveyV.jpg",
     *         "content_type": "image/jpeg",
     *         "url": "https://v3b.fal.media/files/b/koala/UFe9ES9IGdp0N90JmCyd4.jpg",
     *         "width": 1184
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Seed
     * @description The seed for the inference.
     * @example 1815037768
     */
    seed: number;
}

export interface Emu35ImageEditImageInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the output image.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?:
        | 'auto'
        | '21:9'
        | '16:9'
        | '4:3'
        | '3:2'
        | '1:1'
        | '2:3'
        | '3:4'
        | '9:16'
        | '9:21';
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image URL
     * @description The image to edit.
     * @example https://v3b.fal.media/files/b/lion/iC4LKAESSVo4ug-XzmR11_e9cafdab-c8b4-4267-804e-230e3d0d0814.png
     */
    image_url: string;
    /**
     * Output Format
     * @description The format of the output image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to edit the image.
     * @example Recreate this image in ukiyo-e style
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the output image.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description The seed for the inference.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description Whether to return the image in sync mode.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Emu35ImageEditImageOutput {
    /**
     * Images
     * @description The edited image.
     * @example [
     *       {
     *         "height": 1168,
     *         "file_name": "t4nYWb1Zk7Uc6x2nSLysb.jpg",
     *         "content_type": "image/jpeg",
     *         "url": "https://v3b.fal.media/files/b/monkey/t4nYWb1Zk7Uc6x2nSLysb.jpg",
     *         "width": 784
     *       }
     *     ]
     */
    images: Components.ImageFile[];
    /**
     * Seed
     * @description The seed for the inference.
     * @example 1021074961
     */
    seed: number;
}

export interface ElevenlabsVoiceChangerInput {
    /**
     * Audio Url
     * @description The input audio file
     * @example https://storage.googleapis.com/falserverless/example_inputs/elevenlabs/voice_change_in.mp3
     */
    audio_url: string;
    /**
     * Output Format
     * @description Output format of the generated audio. Formatted as codec_sample_rate_bitrate.
     * @default mp3_44100_128
     * @enum {string}
     */
    output_format?:
        | 'mp3_22050_32'
        | 'mp3_44100_32'
        | 'mp3_44100_64'
        | 'mp3_44100_96'
        | 'mp3_44100_128'
        | 'mp3_44100_192'
        | 'pcm_8000'
        | 'pcm_16000'
        | 'pcm_22050'
        | 'pcm_24000'
        | 'pcm_44100'
        | 'pcm_48000'
        | 'ulaw_8000'
        | 'alaw_8000'
        | 'opus_48000_32'
        | 'opus_48000_64'
        | 'opus_48000_96'
        | 'opus_48000_128'
        | 'opus_48000_192';
    /**
     * Remove Background Noise
     * @description If set, will remove the background noise from your audio input using our audio isolation model.
     * @default false
     */
    remove_background_noise?: boolean;
    /**
     * Seed
     * @description Random seed for reproducibility.
     */
    seed?: number;
    /**
     * Voice
     * @description The voice to use for speech generation
     * @default Rachel
     * @example Aria
     * @example Roger
     * @example Sarah
     * @example Laura
     * @example Charlie
     * @example George
     * @example Callum
     * @example River
     * @example Liam
     * @example Charlotte
     * @example Alice
     * @example Matilda
     * @example Will
     * @example Jessica
     * @example Eric
     * @example Chris
     * @example Brian
     * @example Daniel
     * @example Lily
     * @example Bill
     */
    voice?: string;
}

export interface ElevenlabsVoiceChangerOutput {
    /**
     * @description The generated audio file
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/elevenlabs/voice_change_out.mp3"
     *     }
     */
    audio: Components.File_1;
    /**
     * Seed
     * @description Random seed for reproducibility.
     * @example 1902083897
     */
    seed: number;
}

export interface ElevenlabsTtsTurboV25Input extends SharedType_a31 {}

export interface ElevenlabsTtsTurboV25Output extends SharedType_1b6 {}

export interface ElevenlabsTtsMultilingualV2Input extends SharedType_a31 {}

export interface ElevenlabsTtsMultilingualV2Output extends SharedType_1b6 {}

export interface ElevenlabsTtsElevenV3Input {
    /**
     * Apply Text Normalization
     * @description This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped.
     * @default auto
     * @enum {string}
     */
    apply_text_normalization?: 'auto' | 'on' | 'off';
    /**
     * Language Code
     * @description Language code (ISO 639-1) used to enforce a language for the model.
     */
    language_code?: string;
    /**
     * Similarity Boost
     * @description Similarity boost (0-1)
     * @default 0.75
     */
    similarity_boost?: number;
    /**
     * Speed
     * @description Speech speed (0.7-1.2). Values below 1.0 slow down the speech, above 1.0 speed it up. Extreme values may affect quality.
     * @default 1
     */
    speed?: number;
    /**
     * Stability
     * @description Voice stability (0-1)
     * @default 0.5
     */
    stability?: number;
    /**
     * Style
     * @description Style exaggeration (0-1)
     * @default 0
     */
    style?: number;
    /**
     * Text
     * @description The text to convert to speech
     * @example Hello! This is a test of the text to speech system, powered by ElevenLabs. How does it sound?
     */
    text: string;
    /**
     * Timestamps
     * @description Whether to return timestamps for each word in the generated speech
     * @default false
     */
    timestamps?: boolean;
    /**
     * Voice
     * @description The voice to use for speech generation
     * @default Rachel
     * @example Aria
     * @example Roger
     * @example Sarah
     * @example Laura
     * @example Charlie
     * @example George
     * @example Callum
     * @example River
     * @example Liam
     * @example Charlotte
     * @example Alice
     * @example Matilda
     * @example Will
     * @example Jessica
     * @example Eric
     * @example Chris
     * @example Brian
     * @example Daniel
     * @example Lily
     * @example Bill
     */
    voice?: string;
}

export interface ElevenlabsTtsElevenV3Output extends SharedType_1b6 {}

export interface ElevenlabsTextToDialogueElevenV3Input {
    /**
     * Inputs
     * @description A list of dialogue inputs, each containing text and a voice ID which will be converted into speech.
     * @example [
     *       {
     *         "text": "[applause] Thank you all for coming tonight! Today we have a very special guest with us.",
     *         "voice": "Aria"
     *       },
     *       {
     *         "text": "[gulps] ... [strong canadian accent] [excited] Hello everyone! Thank you all for having me tonight on this special day.",
     *         "voice": "Charlotte"
     *       }
     *     ]
     */
    inputs: Components.DialogueBlock[];
    /**
     * Language Code
     * @description Language code (ISO 639-1) used to enforce a language for the model. An error will be returned if language code is not supported by the model.
     */
    language_code?: string;
    /**
     * Pronunciation Dictionary Locators
     * @description A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request
     * @default []
     */
    pronunciation_dictionary_locators?: Components.PronunciationDictionaryLocator[];
    /**
     * Seed
     * @description Random seed for reproducibility.
     */
    seed?: number;
    /**
     * Stability
     * @description Determines how stable the voice is and the randomness between each generation. Lower values introduce broader emotional range for the voice. Higher values can result in a monotonous voice with limited emotion. Must be one of 0.0, 0.5, 1.0, else it will be rounded to the nearest value.
     */
    stability?: number;
    /**
     * Use Speaker Boost
     * @description This setting boosts the similarity to the original speaker. Using this setting requires a slightly higher computational load, which in turn increases latency.
     */
    use_speaker_boost?: boolean;
}

export interface ElevenlabsTextToDialogueElevenV3Output {
    /**
     * @description The generated audio file
     * @example {
     *       "url": "https://v3.fal.media/files/zebra/XFeGS8Fq-q1eAPG2sSAo__output.mp3"
     *     }
     */
    audio: Components.File_1;
    /**
     * Seed
     * @description Random seed for reproducibility.
     */
    seed: number;
}

export interface ElevenlabsSpeechToTextScribeV2Input {
    /**
     * Audio Url
     * @description URL of the audio file to transcribe
     * @example https://storage.googleapis.com/falserverless/example_inputs/elevenlabs/scribe_v2_in.mp3
     */
    audio_url: string;
    /**
     * Diarize
     * @description Whether to annotate who is speaking
     * @default true
     */
    diarize?: boolean;
    /**
     * Keyterms
     * @description Words or sentences to bias the model towards transcribing. Up to 100 keyterms, max 50 characters each. Adds 30% premium over base transcription price.
     * @default []
     * @example [
     *       "fal.ai"
     *     ]
     */
    keyterms?: string[];
    /**
     * Language Code
     * @description Language code of the audio
     * @example eng
     * @example spa
     * @example fra
     * @example deu
     * @example jpn
     */
    language_code?: string;
    /**
     * Tag Audio Events
     * @description Tag audio events like laughter, applause, etc.
     * @default true
     */
    tag_audio_events?: boolean;
}

export interface ElevenlabsSpeechToTextScribeV2Output {
    /**
     * Language Code
     * @description Detected or specified language code
     * @example eng
     */
    language_code: string;
    /**
     * Language Probability
     * @description Confidence in language detection
     * @example 1
     */
    language_probability: number;
    /**
     * Text
     * @description The full transcribed text
     * @example Hey, this is a test recording for Scribe version two, which is now available on fal.ai.
     */
    text: string;
    /**
     * Words
     * @description Word-level transcription details
     * @example {
     *       "text": "Hey,",
     *       "start": 0.079,
     *       "type": "word",
     *       "end": 0.539,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": " ",
     *       "start": 0.539,
     *       "type": "spacing",
     *       "end": 0.599,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": "this",
     *       "start": 0.599,
     *       "type": "word",
     *       "end": 0.679,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": " ",
     *       "start": 0.679,
     *       "type": "spacing",
     *       "end": 0.739,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": "is",
     *       "start": 0.739,
     *       "type": "word",
     *       "end": 0.799,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": " ",
     *       "start": 0.799,
     *       "type": "spacing",
     *       "end": 0.939,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": "a",
     *       "start": 0.939,
     *       "type": "word",
     *       "end": 0.939,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": " ",
     *       "start": 0.939,
     *       "type": "spacing",
     *       "end": 0.959,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": "test",
     *       "start": 0.959,
     *       "type": "word",
     *       "end": 1.179,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": " ",
     *       "start": 1.179,
     *       "type": "spacing",
     *       "end": 1.219,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": "recording",
     *       "start": 1.22,
     *       "type": "word",
     *       "end": 1.719,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": " ",
     *       "start": 1.719,
     *       "type": "spacing",
     *       "end": 1.719,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": "for",
     *       "start": 1.719,
     *       "type": "word",
     *       "end": 1.86,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": " ",
     *       "start": 1.86,
     *       "type": "spacing",
     *       "end": 1.879,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": "Scribe",
     *       "start": 1.879,
     *       "type": "word",
     *       "end": 2.24,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": " ",
     *       "start": 2.24,
     *       "type": "spacing",
     *       "end": 2.319,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": "version",
     *       "start": 2.319,
     *       "type": "word",
     *       "end": 2.759,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": " ",
     *       "start": 2.759,
     *       "type": "spacing",
     *       "end": 2.779,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": "two,",
     *       "start": 2.779,
     *       "type": "word",
     *       "end": 3.379,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": " ",
     *       "start": 3.379,
     *       "type": "spacing",
     *       "end": 3.399,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": "which",
     *       "start": 3.399,
     *       "type": "word",
     *       "end": 3.519,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": " ",
     *       "start": 3.519,
     *       "type": "spacing",
     *       "end": 3.539,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": "is",
     *       "start": 3.539,
     *       "type": "word",
     *       "end": 3.659,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": " ",
     *       "start": 3.659,
     *       "type": "spacing",
     *       "end": 3.699,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": "now",
     *       "start": 3.699,
     *       "type": "word",
     *       "end": 3.839,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": " ",
     *       "start": 3.839,
     *       "type": "spacing",
     *       "end": 3.839,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": "available",
     *       "start": 3.839,
     *       "type": "word",
     *       "end": 4.319,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": " ",
     *       "start": 4.319,
     *       "type": "spacing",
     *       "end": 4.339,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": "on",
     *       "start": 4.339,
     *       "type": "word",
     *       "end": 4.579,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": " ",
     *       "start": 4.579,
     *       "type": "spacing",
     *       "end": 4.599,
     *       "speaker_id": "speaker_0"
     *     }
     * @example {
     *       "text": "fal.ai.",
     *       "start": 4.599,
     *       "type": "word",
     *       "end": 5.699,
     *       "speaker_id": "speaker_0"
     *     }
     */
    words: Components.TranscriptionWord[];
}

export interface ElevenlabsSpeechToTextInput {
    /**
     * Audio Url
     * @description URL of the audio file to transcribe
     * @example https://v3.fal.media/files/zebra/zJL_oRY8h5RWwjoK1w7tx_output.mp3
     */
    audio_url: string;
    /**
     * Diarize
     * @description Whether to annotate who is speaking
     * @default true
     */
    diarize?: boolean;
    /**
     * Language Code
     * @description Language code of the audio
     * @example eng
     * @example spa
     * @example fra
     * @example deu
     * @example jpn
     */
    language_code?: string;
    /**
     * Tag Audio Events
     * @description Tag audio events like laughter, applause, etc.
     * @default true
     */
    tag_audio_events?: boolean;
}

export interface ElevenlabsSpeechToTextOutput {
    /**
     * Language Code
     * @description Detected or specified language code
     */
    language_code: string;
    /**
     * Language Probability
     * @description Confidence in language detection
     */
    language_probability: number;
    /**
     * Text
     * @description The full transcribed text
     */
    text: string;
    /**
     * Words
     * @description Word-level transcription details
     */
    words: Components.TranscriptionWord[];
}

export interface ElevenlabsSoundEffectsV2Input {
    /**
     * Duration Seconds
     * @description Duration in seconds (0.5-22). If None, optimal duration will be determined from prompt.
     */
    duration_seconds?: number;
    /**
     * Loop
     * @description Whether to create a sound effect that loops smoothly.
     * @default false
     */
    loop?: boolean;
    /**
     * Output Format
     * @description Output format of the generated audio. Formatted as codec_sample_rate_bitrate.
     * @default mp3_44100_128
     * @enum {string}
     */
    output_format?:
        | 'mp3_22050_32'
        | 'mp3_44100_32'
        | 'mp3_44100_64'
        | 'mp3_44100_96'
        | 'mp3_44100_128'
        | 'mp3_44100_192'
        | 'pcm_8000'
        | 'pcm_16000'
        | 'pcm_22050'
        | 'pcm_24000'
        | 'pcm_44100'
        | 'pcm_48000'
        | 'ulaw_8000'
        | 'alaw_8000'
        | 'opus_48000_32'
        | 'opus_48000_64'
        | 'opus_48000_96'
        | 'opus_48000_128'
        | 'opus_48000_192';
    /**
     * Prompt Influence
     * @description How closely to follow the prompt (0-1). Higher values mean less variation.
     * @default 0.3
     */
    prompt_influence?: number;
    /**
     * Text
     * @description The text describing the sound effect to generate
     * @example Spacious braam suitable for high-impact movie trailer moments
     * @example A gentle wind chime tinkling in a soft breeze
     */
    text: string;
}

export interface ElevenlabsSoundEffectsV2Output {
    /**
     * @description The generated sound effect audio file in MP3 format
     * @example {
     *       "url": "https://v3.fal.media/files/lion/WgnO-jy6WduosuG_Ibobx_sound_effect.mp3"
     *     }
     */
    audio: Components.File_1;
}

export interface ElevenlabsMusicInput {
    /** @description The composition plan for the music */
    composition_plan?: Components.MusicCompositionPlan;
    /**
     * Force Instrumental
     * @description If true, guarantees that the generated song will be instrumental. If false, the song may or may not be instrumental depending on the prompt. Can only be used with prompt.
     * @default false
     */
    force_instrumental?: boolean;
    /**
     * Music Length Ms
     * @description The length of the song to generate in milliseconds. Used only in conjunction with prompt. Must be between 3000ms and 600000ms. Optional - if not provided, the model will choose a length based on the prompt.
     */
    music_length_ms?: number;
    /**
     * Output Format
     * @description Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the μ-law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.
     * @default mp3_44100_128
     * @enum {string}
     */
    output_format?:
        | 'mp3_22050_32'
        | 'mp3_44100_32'
        | 'mp3_44100_64'
        | 'mp3_44100_96'
        | 'mp3_44100_128'
        | 'mp3_44100_192'
        | 'pcm_8000'
        | 'pcm_16000'
        | 'pcm_22050'
        | 'pcm_24000'
        | 'pcm_44100'
        | 'pcm_48000'
        | 'ulaw_8000'
        | 'alaw_8000'
        | 'opus_48000_32'
        | 'opus_48000_64'
        | 'opus_48000_96'
        | 'opus_48000_128'
        | 'opus_48000_192';
    /**
     * Prompt
     * @description The text prompt describing the music to generate
     * @example Mysterious original soundtrack, themes of jungle, rainforest, nature, woodwinds, busy rhythmic tribal percussion.
     */
    prompt?: string;
    /**
     * Respect Sections Durations
     * @description Controls how strictly section durations in the composition_plan are enforced. It will only have an effect if it is used with composition_plan. When set to true, the model will precisely respect each section's duration_ms from the plan. When set to false, the model may adjust individual section durations which will generally lead to better generation quality and improved latency, while always preserving the total song duration from the plan.
     * @default true
     */
    respect_sections_durations?: boolean;
}

export interface ElevenlabsMusicOutput {
    /**
     * @description The generated music audio file in MP3 format
     * @example {
     *       "file_name": "music_generated.mp3",
     *       "content_type": "audio/mpeg",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/elevenlabs/music_generated.mp3"
     *     }
     */
    audio: Components.File_1;
}

export interface ElevenlabsDubbingInput {
    /**
     * Audio Url
     * @description URL of the audio file to dub. Either audio_url or video_url must be provided.
     */
    audio_url?: string;
    /**
     * Highest Resolution
     * @description Whether to use the highest resolution for dubbing.
     * @default true
     */
    highest_resolution?: boolean;
    /**
     * Num Speakers
     * @description Number of speakers in the audio. If not provided, will be auto-detected.
     */
    num_speakers?: number;
    /**
     * Source Lang
     * @description Source language code. If not provided, will be auto-detected.
     * @example en
     * @example es
     * @example fr
     */
    source_lang?: string;
    /**
     * Target Lang
     * @description Target language code for dubbing (ISO 639-1)
     * @example es
     * @example fr
     * @example de
     * @example ja
     * @example pt
     * @example zh
     */
    target_lang: string;
    /**
     * Video Url
     * @description URL of the video file to dub. Either audio_url or video_url must be provided. If both are provided, video_url takes priority.
     * @example https://storage.googleapis.com/falserverless/example_inputs/elevenlabs/e11_dubbing_in.mp4
     */
    video_url?: string;
}

export interface ElevenlabsDubbingOutput {
    /**
     * Target Lang
     * @description The target language of the dubbed content
     * @example es
     */
    target_lang: string;
    /**
     * @description The dubbed video file. Will be populated if video_url was provided in the request.
     * @example {
     *       "file_size": 1344041,
     *       "file_name": "e11_dubbing_out.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/elevenlabs/e11_dubbing_out.mp4"
     *     }
     */
    video: Components.File_1;
}

export interface ElevenlabsAudioIsolationInput {
    /**
     * Audio Url
     * @description URL of the audio file to isolate voice from
     * @example https://v3.fal.media/files/zebra/zJL_oRY8h5RWwjoK1w7tx_output.mp3
     */
    audio_url?: string;
    /**
     * Video Url
     * @description Video file to use for audio isolation. Either `audio_url` or `video_url` must be provided.
     */
    video_url?: string;
}

export interface ElevenlabsAudioIsolationOutput extends SharedType_1b6 {}

export interface EdittoInput {
    /**
     * Acceleration
     * @description Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
     * @default regular
     * @example regular
     */
    acceleration?: 'none' | 'low' | 'regular';
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video.
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16';
    /**
     * Enable Auto Downsample
     * @description If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
     * @default false
     * @example false
     */
    enable_auto_downsample?: boolean;
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default false
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Frames per Second
     * @description Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
     * @default 16
     */
    frames_per_second?: number;
    /**
     * Guidance Scale
     * @description Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
     * @default 5
     * @example 5
     */
    guidance_scale?: number;
    /**
     * Match Input Frames Per Second
     * @description If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
     * @default false
     * @example true
     */
    match_input_frames_per_second?: boolean;
    /**
     * Match Input Number of Frames
     * @description If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
     * @default false
     * @example true
     */
    match_input_num_frames?: boolean;
    /**
     * Negative Prompt
     * @description Negative prompt for video generation.
     * @default letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards
     * @example 色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走
     */
    negative_prompt?: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 81 to 241 (inclusive).
     * @default 81
     */
    num_frames?: number;
    /**
     * Number of Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 30
     * @example 50
     */
    num_inference_steps?: number;
    /**
     * Number of Interpolated Frames
     * @description Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
     * @default 0
     * @example 0
     */
    num_interpolated_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example Make it a Pixel Art video.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video.
     * @default auto
     * @enum {string}
     */
    resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p';
    /**
     * Return Frames Zip
     * @description If true, also return a ZIP file containing all generated frames.
     * @default false
     * @example false
     */
    return_frames_zip?: boolean;
    /**
     * Sampler
     * @description Sampler to use for video generation.
     * @default unipc
     * @example unipc
     * @enum {string}
     */
    sampler?: 'unipc' | 'dpm++' | 'euler';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Shift
     * @description Shift parameter for video generation.
     * @default 5
     */
    shift?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     * @example false
     */
    sync_mode?: boolean;
    /**
     * Temporal Downsample Factor
     * @description Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
     * @default 0
     * @example 0
     */
    temporal_downsample_factor?: number;
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @example high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video URL
     * @description URL to the source video file. Required for inpainting.
     * @example https://storage.googleapis.com/falserverless/example_inputs/editto/example_in.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @example balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface EdittoOutput {
    /** @description ZIP archive of all video frames if requested. */
    frames_zip?: Components.File_1;
    /**
     * Prompt
     * @description The prompt used for generation.
     * @example Make it a Pixel Art video.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for generation.
     * @example 1096772785
     */
    seed: number;
    /**
     * @description The generated image to video file.
     * @example {
     *       "height": 480,
     *       "duration": 4.86,
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/editto/example_out.mp4",
     *       "fps": 15,
     *       "width": 832,
     *       "file_name": "example_out.mp4",
     *       "content_type": "video/mp4",
     *       "num_frames": 73
     *     }
     */
    video: Components.VideoFile;
}

export interface EchomimicV3Input {
    /**
     * Audio Guidance Scale
     * @description The audio guidance scale to use for the video generation.
     * @default 2.5
     */
    audio_guidance_scale?: number;
    /**
     * Audio URL
     * @description The URL of the audio to use as a reference for the video generation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/echo-mimic-input-audio.mp3
     */
    audio_url: string;
    /**
     * Guidance Scale
     * @description The guidance scale to use for the video generation.
     * @default 4.5
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The URL of the image to use as a reference for the video generation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/echo-mimic-input-image.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use for the video generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Number of frames per generation
     * @description The number of frames to generate at once.
     * @default 121
     */
    num_frames_per_generation?: number;
    /**
     * Prompt
     * @description The prompt to use for the video generation.
     * @example A person is in a relaxed pose. As the video progresses, the character speaks while arm and body movements are minimal and consistent with a natural speaking posture. Hand movements remain minimal. Don't blink too often. Preserve background integrity matching the reference image's spatial configuration, lighting conditions, and color temperature.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the video generation.
     */
    seed?: number;
}

export interface EchomimicV3Output {
    /**
     * Video
     * @description The generated video file.
     * @example https://storage.googleapis.com/falserverless/example_outputs/echo-mimic-output.mp4
     */
    video: Components.File;
}

export interface DwposeVideoInput {
    /**
     * Draw Mode
     * @description Mode of drawing the pose on the video. Options are: 'full-pose', 'body-pose', 'face-pose', 'hand-pose', 'face-hand-mask', 'face-mask', 'hand-mask'.
     * @default body-pose
     * @example body-pose
     * @enum {string}
     */
    draw_mode?:
        | 'full-pose'
        | 'body-pose'
        | 'face-pose'
        | 'hand-pose'
        | 'face-hand-mask'
        | 'face-mask'
        | 'hand-mask';
    /**
     * Video Url
     * @description URL of video to be used for pose estimation
     * @example https://storage.googleapis.com/falserverless/gallery/Ben2/100063-video-2160.mp4
     */
    video_url: string;
}

export interface DwposeVideoOutput {
    /**
     * Video
     * @description The output video with pose estimation.
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/dwpose-video-output.mp4"
     *     }
     */
    video: Components.File;
}

export interface DwposeInput {
    /**
     * Draw Mode
     * @description Mode of drawing the pose on the image. Options are: 'full-pose', 'body-pose', 'face-pose', 'hand-pose', 'face-hand-mask', 'face-mask', 'hand-mask'.
     * @default body-pose
     * @example body-pose
     * @enum {string}
     */
    draw_mode?:
        | 'full-pose'
        | 'body-pose'
        | 'face-pose'
        | 'hand-pose'
        | 'face-hand-mask'
        | 'face-mask'
        | 'hand-mask';
    /**
     * Image Url
     * @description URL of the image to be processed
     * @example https://github.com/badayvedat/sane-controlnet-aux/blob/main/tests/data/pose_sample.jpg?raw=true
     */
    image_url: string;
}

export interface DwposeOutput {
    /**
     * Image
     * @description The predicted pose image
     */
    image: Components.Image;
}

export interface DubbingInput {
    /**
     * Do Lipsync
     * @description Whether to lip sync the audio to the video
     * @default true
     */
    do_lipsync?: boolean;
    /**
     * Target Language
     * @description Target language to dub the video to
     * @default hindi
     * @enum {string}
     */
    target_language?: 'hindi' | 'turkish' | 'english';
    /**
     * Video Url
     * @description Input video URL to be dubbed.
     * @example https://storage.googleapis.com/falserverless/model_tests/dubbing/swapjokes_clip_cropped.mp4
     */
    video_url: string;
}

export interface DubbingOutput {
    /**
     * Video
     * @description The generated video with the lip sync.
     * @example {
     *       "file_size": 120000,
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3.fal.media/files/koala/7BzEwUucbr6yuFjpcJipl_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface DreamshaperInput {
    /**
     * Embeddings
     * @description The list of embeddings to use.
     * @default []
     */
    embeddings?: Components.Embedding[];
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Expand Prompt
     * @description If set to true, the prompt will be expanded with additional prompts.
     * @default false
     */
    expand_prompt?: boolean;
    /**
     * Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    format?: 'jpeg' | 'png';
    /**
     * Guidance Scale
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @default {
     *       "height": 1024,
     *       "width": 1024
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Loras
     * @description The list of LoRA weights to use.
     * @default []
     */
    loras?: Components.LoraWeight_1[];
    /**
     * Model Name
     * @description The Dreamshaper model to use.
     * @example Lykon/dreamshaper-8
     * @example Lykon/dreamshaper-xl-1-0
     * @example Lykon/dreamshaper-xl-v2-turbo
     * @enum {string}
     */
    model_name?:
        | 'Lykon/dreamshaper-xl-1-0'
        | 'Lykon/dreamshaper-xl-v2-turbo'
        | 'Lykon/dreamshaper-8';
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want in the image.
     * @default (worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed, grayscale, bw, bad photo, bad photography, bad art:1.4), (watermark, signature, text font, username, error, logo, words, letters, digits, autograph, trademark, name:1.2), (blur, blurry, grainy), morbid, ugly, asymmetrical, mutated malformed, mutilated, poorly lit, bad shadow, draft, cropped, out of frame, cut off, censored, jpeg artifacts, out of focus, glitch, duplicate, (airbrushed, cartoon, anime, semi-realistic, cgi, render, blender, digital art, manga, amateur:1.3), (3D ,3D Game, 3D Game Scene, 3D Character:1.1), (bad hands, bad anatomy, bad body, bad face, bad teeth, bad arms, bad legs, deformities:1.3)
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 35
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example A hyperdetailed photograph of a Cat dressed as a mafia boss holding a fish walking down a Japanese fish market with an angry face, 8k resolution, best quality, beautiful photograph, dynamic lighting,
     */
    prompt: string;
    /**
     * Safety Checker Version
     * @description The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model.
     * @default v1
     * @enum {string}
     */
    safety_checker_version?: 'v1' | 'v2';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface DreamshaperOutput extends SharedType_a73 {}

export interface Dreamomni2EditInput {
    /**
     * You can use only with to 2 images.
     * @description List of URLs of input images for editing.
     * @example [
     *       "https://v3b.fal.media/files/b/koala/HB33rtG0ue7KzcIdQOTTX_dreamomni_ref_0.jpg",
     *       "https://v3b.fal.media/files/b/koala/BJMlXeNzOgGzyoO7XyGxr_dreamomni_ref_1.jpg"
     *     ]
     */
    image_urls: string[];
    /**
     * Prompt
     * @description The prompt to edit the image.
     * @example Replace the first image have the same image style as the second image.
     */
    prompt: string;
}

export interface Dreamomni2EditOutput {
    /**
     * Image
     * @description Generated image
     * @example {
     *       "file_size": 1473707,
     *       "file_name": "c9ab07096fdd47269a60bc556e01132b.png",
     *       "content_type": "image/png",
     *       "url": "https://v3b.fal.media/files/b/koala/prmop69b1g5lNFPE4RbCb_c9ab07096fdd47269a60bc556e01132b.png"
     *     }
     */
    image: Components.Image;
}

export interface DreamoInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * First Reference Image URL
     * @description URL of first reference image to use for generation.
     * @example https://v3.fal.media/files/rabbit/I3exImt_zOYaiZv8caeGP_Pz4CnQ12tCUuDIhEQkmbD_ae4193792924495e89c516e6b492ed2b_1.jpg
     */
    first_image_url?: string;
    /**
     * First Reference Task
     * @description Task for first reference image (ip/id/style).
     * @default ip
     * @enum {string}
     */
    first_reference_task?: 'ip' | 'id' | 'style';
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default square_hd
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The prompt to generate an image from.
     * @default
     * @example bad quality, worst quality, text, signature, watermark, extra limbs
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 12
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example Two people hugging inside a forest
     */
    prompt: string;
    /**
     * Ref Resolution
     * @description Resolution for reference images.
     * @default 512
     */
    ref_resolution?: number;
    /**
     * Second Reference Image URL
     * @description URL of second reference image to use for generation.
     * @example https://v3.fal.media/files/penguin/F3Yqprwlv-yaeusxAS0bS_image.webp
     */
    second_image_url?: string;
    /**
     * Second Reference Task
     * @description Task for second reference image (ip/id/style).
     * @default ip
     * @enum {string}
     */
    second_reference_task?: 'ip' | 'id' | 'style';
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * True Cfg
     * @description The weight of the CFG loss.
     * @default 1
     */
    true_cfg?: number;
}

export interface DreamoOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The URLs of the generated images.
     * @example [
     *       {
     *         "height": 1024,
     *         "content_type": "image/jpeg",
     *         "url": "https://v3.fal.media/files/elephant/Qqd29dv20375fBbN1233_.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Prompt
     * @description The prompt used to generate the image.
     * @example Two people hugging inside a forest
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface DrctSuperResolutionInput {
    /**
     * Image URL
     * @description URL of the image to upscale.
     * @example https://fal.media/files/rabbit/JlBgYUyQRS3zxiBu_B4fM.png
     * @example https://fal.media/files/monkey/e6RtJf_ue0vyWzeiEmTby.png
     * @example https://fal.media/files/monkey/A6HGsigx4mmvs-hJVoOZX.png
     */
    image_url: string;
    /**
     * Upscaling Factor (Xs)
     * @description Upscaling factor.
     * @default 4
     * @example 4
     * @enum {integer}
     */
    upscale_factor?: 4;
}

export interface DrctSuperResolutionOutput extends SharedType_055 {}

export interface DocresDewarpInput {
    /**
     * Image Url
     * @description URL of image to be used for relighting
     * @example https://storage.googleapis.com/falserverless/docres_ckpt/218_in.png
     */
    image_url: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
}

export interface DocresDewarpOutput extends SharedType_f62 {}

export interface DocresInput {
    /**
     * Image Url
     * @description URL of image to be used for relighting
     * @example https://storage.googleapis.com/falserverless/docres_ckpt/218_in.png
     */
    image_url: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Task
     * @description Task to perform
     * @enum {string}
     */
    task: 'deshadowing' | 'appearance' | 'deblurring' | 'binarization';
}

export interface DocresOutput extends SharedType_f62 {}

export interface DiffusionEdgeInput {
    /**
     * Image Url
     * @description The text prompt you would like to convert to speech.
     * @example https://storage.googleapis.com/falserverless/model_tests/upscale/hamburger.png
     */
    image_url: string;
}

export interface DiffusionEdgeOutput {
    /**
     * Image
     * @description The generated image file info.
     */
    image: Components.Image_3;
}

export interface DiffrhythmInput {
    /**
     * CFG Strength
     * @description The CFG strength to use for the music generation.
     * @default 4
     */
    cfg_strength?: number;
    /**
     * Lyrics
     * @description The prompt to generate the song from. Must have two sections. Sections start with either [chorus] or a [verse].
     * @example [00:10.00]Moonlight spills through broken blinds
     *     [00:13.20]Your shadow dances on the dashboard shrine
     *     [00:16.85]Neon ghosts in gasoline rain
     *     [00:20.40]I hear your laughter down the midnight train
     *     [00:24.15]Static whispers through frayed wires
     *     [00:27.65]Guitar strings hum our cathedral choirs
     *     [00:31.30]Flicker screens show reruns of June
     *     [00:34.90]I'm drowning in this mercury lagoon
     *     [00:38.55]Electric veins pulse through concrete skies
     *     [00:42.10]Your name echoes in the hollow where my heartbeat lies
     *     [00:45.75]We're satellites trapped in parallel light
     *     [00:49.25]Burning through the atmosphere of endless night
     *     [01:00.00]Dusty vinyl spins reverse
     *     [01:03.45]Our polaroid timeline bleeds through the verse
     *     [01:07.10]Telescope aimed at dead stars
     *     [01:10.65]Still tracing constellations through prison bars
     *     [01:14.30]Electric veins pulse through concrete skies
     *     [01:17.85]Your name echoes in the hollow where my heartbeat lies
     *     [01:21.50]We're satellites trapped in parallel light
     *     [01:25.05]Burning through the atmosphere of endless night
     *     [02:10.00]Clockwork gears grind moonbeams to rust
     *     [02:13.50]Our fingerprint smudged by interstellar dust
     *     [02:17.15]Velvet thunder rolls through my veins
     *     [02:20.70]Chasing phantom trains through solar plane
     *     [02:24.35]Electric veins pulse through concrete skies
     *     [02:27.90]Your name echoes in the hollow where my heartbeat lies
     */
    lyrics: string;
    /**
     * Music Duration
     * @description The duration of the music to generate.
     * @default 95s
     * @enum {string}
     */
    music_duration?: '95s' | '285s';
    /**
     * Number of Inference Steps
     * @description The number of inference steps to use for the music generation.
     * @default 32
     */
    num_inference_steps?: number;
    /**
     * Reference Audio URL
     * @description The URL of the reference audio to use for the music generation.
     * @example https://storage.googleapis.com/falserverless/model_tests/diffrythm/rock_en.wav
     */
    reference_audio_url?: string;
    /**
     * Scheduler
     * @description The scheduler to use for the music generation.
     * @default euler
     * @enum {string}
     */
    scheduler?: 'euler' | 'midpoint' | 'rk4' | 'implicit_adams';
    /**
     * Style Prompt
     * @description The style prompt to use for the music generation.
     * @example pop
     */
    style_prompt?: string;
}

export interface DiffrhythmOutput {
    /**
     * Audio
     * @description Generated music file.
     * @example {
     *       "file_size": 33554520,
     *       "file_name": "output.wav",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3.fal.media/files/elephant/VV4wtKXBpZL1bNv6en36t_output.wav"
     *     }
     */
    audio: Components.File;
}

export interface DiaTtsVoiceCloneInput {
    /**
     * Reference Audio URL
     * @description The URL of the reference audio file.
     * @example https://v3.fal.media/files/elephant/d5lORit2npFfBykcAtyUr_tmplacfh8oa.mp3
     */
    ref_audio_url: string;
    /**
     * Reference Text for the Reference Audio
     * @description The reference text to be used for TTS.
     * @example [S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Fal.
     */
    ref_text: string;
    /**
     * Text
     * @description The text to be converted to speech.
     * @example [S1] Hello, how are you? [S2] I'm good, thank you. [S1] What's your name? [S2] My name is Dia. [S1] Nice to meet you. [S2] Nice to meet you too.
     */
    text: string;
}

export interface DiaTtsVoiceCloneOutput {
    /**
     * @description The generated speech audio
     * @example {
     *       "url": "https://v3.fal.media/files/tiger/smL9a_mr1PRIvZxDSVppk_output.wav"
     *     }
     */
    audio: Components.File_1;
}

export interface DiaTtsInput {
    /**
     * Text
     * @description The text to be converted to speech.
     * @example [S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Fal.
     */
    text: string;
}

export interface DiaTtsOutput {
    /**
     * @description The generated speech audio
     * @example {
     *       "url": "https://v3.fal.media/files/elephant/d5lORit2npFfBykcAtyUr_tmplacfh8oa.mp3"
     *     }
     */
    audio: Components.File_1;
}

export interface DemucsInput {
    /**
     * Audio Url
     * @description URL of the audio file to separate into stems
     * @example https://storage.googleapis.com/falserverless/model_tests/audio-understanding/Title_%20Running%20on%20Fal.mp3
     */
    audio_url: string;
    /**
     * Model
     * @description Demucs model to use for separation
     * @default htdemucs_6s
     * @example htdemucs_6s
     * @enum {string}
     */
    model?:
        | 'htdemucs'
        | 'htdemucs_ft'
        | 'htdemucs_6s'
        | 'hdemucs_mmi'
        | 'mdx'
        | 'mdx_extra'
        | 'mdx_q'
        | 'mdx_extra_q';
    /**
     * Output Format
     * @description Output audio format for the separated stems
     * @default mp3
     * @example mp3
     * @enum {string}
     */
    output_format?: 'wav' | 'mp3';
    /**
     * Overlap
     * @description Overlap between segments (0.0 to 1.0). Higher values may improve quality but increase processing time.
     * @default 0.25
     */
    overlap?: number;
    /**
     * Segment Length
     * @description Length in seconds of each segment for processing. Smaller values use less memory but may reduce quality. Default is model-specific.
     */
    segment_length?: number;
    /**
     * Shifts
     * @description Number of random shifts for equivariant stabilization. Higher values improve quality but increase processing time.
     * @default 1
     */
    shifts?: number;
    /**
     * Stems
     * @description Specific stems to extract. If None, extracts all available stems. Available stems depend on model: vocals, drums, bass, other, guitar, piano (for 6s model)
     * @default [
     *       "vocals",
     *       "drums",
     *       "bass",
     *       "other",
     *       "guitar",
     *       "piano"
     *     ]
     * @example [
     *       "vocals",
     *       "drums",
     *       "bass",
     *       "other",
     *       "guitar",
     *       "piano"
     *     ]
     */
    stems?: ('vocals' | 'drums' | 'bass' | 'other' | 'guitar' | 'piano')[];
}

export interface DemucsOutput {
    /** @description Separated bass audio file */
    bass?: Components.File_1;
    /** @description Separated drums audio file */
    drums?: Components.File_1;
    /** @description Separated guitar audio file (only available for 6s models) */
    guitar?: Components.File_1;
    /** @description Separated other instruments audio file */
    other?: Components.File_1;
    /** @description Separated piano audio file (only available for 6s models) */
    piano?: Components.File_1;
    /** @description Separated vocals audio file */
    vocals?: Components.File_1;
}

export interface Deepfilternet3Input {
    /**
     * Audio Format
     * @description The format for the output audio.
     * @default mp3
     * @enum {string}
     */
    audio_format?: 'mp3' | 'aac' | 'm4a' | 'ogg' | 'opus' | 'flac' | 'wav';
    /**
     * Audio URL
     * @description The URL of the audio to enhance.
     * @example https://v3b.fal.media/files/b/0a8a4022/DLoZhabKeVjd3urncoRE4_dirty.mp3
     */
    audio_url: string;
    /**
     * Bitrate
     * @description The bitrate of the output audio.
     * @default 192k
     */
    bitrate?: string;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Deepfilternet3Output {
    /**
     * Audio File
     * @description The audio file that was enhanced.
     * @example {
     *       "channels": 1,
     *       "duration": 6.9544375,
     *       "url": "https://v3b.fal.media/files/b/0a8a4024/-2cD9CyGEjYsyVQ5lEERh_9qwIkJjf.mp3",
     *       "file_name": "-2cD9CyGEjYsyVQ5lEERh_9qwIkJjf.mp3",
     *       "sample_rate": 48000,
     *       "content_type": "audio/mpeg",
     *       "bitrate": "192k"
     *     }
     */
    audio_file: Components.AudioFile_1;
    /**
     * Timings
     * @description Timings for each step in the pipeline.
     */
    timings: Components.DeepFilterNetTimings;
}

export interface DecartLucy5bImageToVideoInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '16:9';
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://v3.fal.media/files/monkey/OlpQEYh7oNeJ3qKsdiaym_ia5ECOgFbfcniMDu01_18_da73e078e0924472b51d92f3e3fba98c.png
     */
    image_url: string;
    /**
     * Prompt
     * @description Text description of the desired video content
     * @example A cat is walking slowly in the garden
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p';
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default true
     */
    sync_mode?: boolean;
}

export interface DecartLucy5bImageToVideoOutput {
    /**
     * Video
     * @description The generated MP4 video with H.264 encoding
     * @example {
     *       "url": "https://v3.fal.media/files/kangaroo/rIFaCsyWvBxYBKw3cPbOU_indir.mp4"
     *     }
     */
    video: Components.File;
}

export interface DdcolorInput {
    /**
     * Image Url
     * @description URL of image to be used for relighting
     * @example https://storage.googleapis.com/falserverless/gallery/Screenshot%202025-02-26%20154226.png
     */
    image_url: string;
    /**
     * Seed
     * @description seed to be used for generation
     */
    seed?: number;
}

export interface DdcolorOutput {
    /**
     * Image
     * @description The generated image file info.
     * @example {
     *       "height": 512,
     *       "file_size": 423052,
     *       "file_name": "36d3ca4791a647678b2ff01a35c87f5a.png",
     *       "content_type": "image/png",
     *       "url": "https://storage.googleapis.com/falserverless/gallery/5fcaaac6d1344d998ebb9703102c6c63.png",
     *       "width": 512
     *     }
     */
    image: Components.Image;
}

export interface Csm1bInput {
    /**
     * Context
     * @description The context to generate an audio from.
     * @example [
     *       {
     *         "prompt": "like revising for an exam I'd have to try and like keep up the momentum because I'd start really early I'd be like okay I'm gonna start revising now and then like you're revising for ages and then I just like start losing steam I didn't do that for the exam we had recently to be fair that was a more of a last minute scenario but like yeah I'm trying to like yeah I noticed this yesterday that like Mondays I sort of start the day with this not like a panic but like a",
     *         "audio_url": "https://huggingface.co/spaces/sesame/csm-1b/resolve/main/prompts/conversational_a.wav",
     *         "speaker_id": 0
     *       },
     *       {
     *         "prompt": "like a super Mario level. Like it's very like high detail. And like, once you get into the park, it just like, everything looks like a computer game and they have all these, like, you know, if, if there's like a, you know, like in a Mario game, they will have like a question block. And if you like, you know, punch it, a coin will come out. So like everyone, when they come into the park, they get like this little bracelet and then you can go punching question blocks around.",
     *         "audio_url": "https://huggingface.co/spaces/sesame/csm-1b/resolve/main/prompts/conversational_b.wav",
     *         "speaker_id": 1
     *       }
     *     ]
     */
    context?: Components.Speaker[];
    /**
     * Scene
     * @description The text to generate an audio from.
     * @example [
     *       {
     *         "text": "Hey how are you doing.",
     *         "speaker_id": 0
     *       },
     *       {
     *         "text": "Pretty good, pretty good.",
     *         "speaker_id": 1
     *       },
     *       {
     *         "text": "I'm great, so happy to be speaking to you.",
     *         "speaker_id": 0
     *       }
     *     ]
     */
    scene: Components.Turn[];
}

export interface Csm1bOutput {
    /**
     * Audio
     * @description The generated audio.
     */
    audio: Components.File_1 | string;
}

export interface CreativeUpscalerInput {
    /**
     * Additional Embedding Url
     * @description The URL to the additional embeddings to use for the upscaling. Default is None
     */
    additional_embedding_url?: string;
    /**
     * Additional Lora Scale
     * @description The scale of the additional LORA model to use for the upscaling. Default is 1.0
     * @default 1
     */
    additional_lora_scale?: number;
    /**
     * Additional Lora Url
     * @description The URL to the additional LORA model to use for the upscaling. Default is None
     */
    additional_lora_url?: string;
    /**
     * Base Model Url
     * @description The URL to the base model to use for the upscaling
     */
    base_model_url?: string;
    /**
     * Creativity
     * @description How much the output can deviate from the original
     * @default 0.5
     */
    creativity?: number;
    /**
     * Detail
     * @description How much detail to add
     * @default 1
     */
    detail?: number;
    /**
     * Enable Safety Checks
     * @description If set to true, the resulting image will be checked whether it includes any
     *                 potentially unsafe content. If it does, it will be replaced with a black
     *                 image.
     * @default true
     */
    enable_safety_checks?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 7.5
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description The image to upscale.
     * @example https://storage.googleapis.com/falserverless/model_tests/upscale/owl.png
     * @example https://storage.googleapis.com/falserverless/gallery/blue-bird.jpeg
     */
    image_url: string;
    /**
     * Model Type
     * @description The type of model to use for the upscaling. Default is SD_1_5
     * @default SD_1_5
     * @example SD_1_5
     * @example SDXL
     * @enum {string}
     */
    model_type?: 'SD_1_5' | 'SDXL';
    /**
     * Negative Prompt
     * @description The negative prompt to use.Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default blurry, low resolution, bad, ugly, low quality, pixelated, interpolated, compression artifacts, noisey, grainy
     * @example blurry, low resolution, bad, ugly, low quality, pixelated, interpolated, compression artifacts, noisey, grainy
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps to use for generating the image. The more steps
     *                 the better the image will be but it will also take longer to generate.
     * @default 20
     */
    num_inference_steps?: number;
    /**
     * Override Size Limits
     * @description Allow for large uploads that could take a very long time.
     * @default false
     */
    override_size_limits?: boolean;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results. If no prompt is provide BLIP2 will be used to generate a prompt.
     */
    prompt?: string;
    /**
     * Prompt Suffix
     * @description The suffix to add to the prompt. This is useful to add a common ending to all prompts such as 'high quality' etc or embedding tokens.
     * @default high quality, highly detailed, high resolution, sharp
     */
    prompt_suffix?: string;
    /**
     * Scale
     * @description The scale of the output image. The higher the scale, the bigger the output image will be.
     * @default 2
     */
    scale?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     * @example 42
     */
    seed?: number;
    /**
     * Shape Preservation
     * @description How much to preserve the shape of the original image
     * @default 0.25
     */
    shape_preservation?: number;
    /**
     * Skip Ccsr
     * @description If set to true, the image will not be processed by the CCSR model before
     *                 being processed by the creativity model.
     * @default false
     */
    skip_ccsr?: boolean;
}

export interface CreativeUpscalerOutput extends SharedType_0c0 {}

export interface CreatifyAuroraInput {
    /**
     * Audio Guidance Scale
     * @description Guidance scale to be used for audio adherence.
     * @default 2
     */
    audio_guidance_scale?: number;
    /**
     * Audio Url
     * @description The URL of the audio file to be used for video generation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/creatify/aurora/input.wav
     */
    audio_url: string;
    /**
     * Guidance Scale
     * @description Guidance scale to be used for text prompt adherence.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description The URL of the image file to be used for video generation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/creatify/aurora/input_.png
     */
    image_url: string;
    /**
     * Prompt
     * @description A text prompt to guide the video generation process.
     * @example 4K studio interview, medium close-up (shoulders-up crop). Solid light-grey seamless backdrop, uniform soft key-light—no lighting change. Presenter faces lens, steady eye-contact. Hands remain below frame, body perfectly still. Ultra-sharp.
     */
    prompt?: string;
    /**
     * Resolution
     * @description The resolution of the generated video.
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
}

export interface CreatifyAuroraOutput {
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "file_name": "output.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/creatify/aurora/output.mp4"
     *     }
     */
    video: Components.VideoFile_1;
}

export interface ControlnextInput {
    /**
     * Batch Frames
     * @description Number of frames to process in each batch.
     * @default 24
     */
    batch_frames?: number;
    /**
     * Controlnext Cond Scale
     * @description Condition scale for ControlNeXt.
     * @default 1
     */
    controlnext_cond_scale?: number;
    /**
     * Decode Chunk Size
     * @description Chunk size for decoding frames.
     * @default 2
     */
    decode_chunk_size?: number;
    /**
     * Fps
     * @description Frames per second for the output video.
     * @default 7
     */
    fps?: number;
    /**
     * Guidance Scale
     * @description Guidance scale for the diffusion process.
     * @default 3
     */
    guidance_scale?: number;
    /**
     * Height
     * @description Height of the output video.
     * @default 1024
     */
    height?: number;
    /**
     * Image Url
     * @description URL of the reference image.
     * @example https://storage.googleapis.com/falserverless/model_tests/musepose/ref.png
     */
    image_url: string;
    /**
     * Max Frame Num
     * @description Maximum number of frames to process.
     * @default 240
     */
    max_frame_num?: number;
    /**
     * Motion Bucket Id
     * @description Motion bucket ID for the pipeline.
     * @default 127
     */
    motion_bucket_id?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps.
     * @default 25
     */
    num_inference_steps?: number;
    /**
     * Overlap
     * @description Number of overlapping frames between batches.
     * @default 6
     */
    overlap?: number;
    /**
     * Sample Stride
     * @description Stride for sampling frames from the input video.
     * @default 2
     */
    sample_stride?: number;
    /**
     * Video Url
     * @description URL of the input video.
     * @example https://storage.googleapis.com/falserverless/model_tests/musepose/dance.mp4
     */
    video_url: string;
    /**
     * Width
     * @description Width of the output video.
     * @default 576
     */
    width?: number;
}

export interface ControlnextOutput extends SharedType_5a6 {}

export interface Cogview4Input {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default landscape_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want
     *                 in the image. This could be colors, objects, scenery and even the small details
     *                 (e.g. moustache, blurry, low resolution).
     * @default
     * @example
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description The number of images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the generated image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png';
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A vibrant and artistic digital composition featuring colorful splashes of paint in the background, creating an energetic and dynamic effect. The text 'CogView4 on Fal' is elegantly integrated into the scene, standing out with a modern, bold, and slightly futuristic font. The colors are bright and varied, including neon blues, purples, pinks, and oranges, blending seamlessly in a fluid, abstract style. The text appears slightly illuminated, complementing the vivid splashes around it.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface Cogview4Output {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "height": 768,
     *         "content_type": "image/jpeg",
     *         "url": "https://v3.fal.media/files/tiger/rN6_PpE-o8QlSecqFku6h.jpeg",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface Cogvideox5bVideoToVideoInput {
    /**
     * Export Fps
     * @description The target FPS of the video
     * @default 16
     */
    export_fps?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related video to show you.
     * @default 7
     */
    guidance_scale?: number;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. We currently support one lora.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Negative Prompt
     * @description The negative prompt to generate video from
     * @default
     * @example Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example An astronaut stands triumphantly at the peak of a towering mountain. Panorama of rugged peaks and valleys. Very futuristic vibe and animated aesthetic. Highlights of purple and golden colors in the scene. The sky is looks like an animated/cartoonish dream of galaxies, nebulae, stars, planets, moons, but the remainder of the scene is mostly realistic.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
    /**
     * Strength
     * @description The strength to use for Video to Video.  1.0 completely remakes the video while 0.0 preserves the original.
     * @default 0.8
     */
    strength?: number;
    /**
     * Use Rife
     * @description Use RIFE for video interpolation
     * @default true
     */
    use_rife?: boolean;
    /**
     * Video Size
     * @description The size of the generated video.
     * @default {
     *       "height": 480,
     *       "width": 720
     *     }
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Input Video Url
     * @description The video to generate the video from.
     * @example https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/hiker.mp4
     */
    video_url: string;
}

export interface Cogvideox5bVideoToVideoOutput extends SharedType_63d {}

export interface Cogvideox5bImageToVideoInput {
    /**
     * Export Fps
     * @description The target FPS of the video
     * @default 16
     */
    export_fps?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related video to show you.
     * @default 7
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The URL to the image to generate the video from.
     * @example https://d3phaj0sisr2ct.cloudfront.net/research/eugene.jpg
     */
    image_url: string;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. We currently support one lora.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Negative Prompt
     * @description The negative prompt to generate video from
     * @default
     * @example Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A low angle shot of a man walking down a street, illuminated by the neon signs of the bars around him
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
    /**
     * Use Rife
     * @description Use RIFE for video interpolation
     * @default true
     */
    use_rife?: boolean;
    /**
     * Video Size
     * @description The size of the generated video.
     * @default {
     *       "height": 480,
     *       "width": 720
     *     }
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
}

export interface Cogvideox5bImageToVideoOutput extends SharedType_63d {}

export interface Cogvideox5bInput {
    /**
     * Export Fps
     * @description The target FPS of the video
     * @default 16
     */
    export_fps?: number;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related video to show you.
     * @default 7
     */
    guidance_scale?: number;
    /**
     * Loras
     * @description The LoRAs to use for the image generation. We currently support one lora.
     * @default []
     */
    loras?: Components.LoraWeight[];
    /**
     * Negative Prompt
     * @description The negative prompt to generate video from
     * @default
     * @example Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate the video from.
     * @example A garden comes to life as a kaleidoscope of butterflies flutters amidst the blossoms, their delicate wings casting shadows on the petals below. In the background, a grand fountain cascades water with a gentle splendor, its rhythmic sound providing a soothing backdrop. Beneath the cool shade of a mature tree, a solitary wooden chair invites solitude and reflection, its smooth surface worn by the touch of countless visitors seeking a moment of tranquility in nature's embrace.
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same video every time.
     */
    seed?: number;
    /**
     * Use Rife
     * @description Use RIFE for video interpolation
     * @default true
     */
    use_rife?: boolean;
    /**
     * Video Size
     * @description The size of the generated video.
     * @default {
     *       "height": 480,
     *       "width": 720
     *     }
     */
    video_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
}

export interface Cogvideox5bOutput extends SharedType_63d {}

export interface CodeformerInput {
    /**
     * Aligned
     * @description Should faces etc should be aligned.
     * @default false
     */
    aligned?: boolean;
    /**
     * Face Upscale
     * @description Should faces be upscaled
     * @default true
     */
    face_upscale?: boolean;
    /**
     * Fidelity
     * @description Weight of the fidelity factor.
     * @default 0.5
     */
    fidelity?: number;
    /**
     * Image Url
     * @description URL of image to be used for relighting
     * @example https://storage.googleapis.com/falserverless/model_tests/codeformer/codeformer_poor_1.jpeg
     */
    image_url: string;
    /**
     * Only Center Face
     * @description Should only center face be restored
     * @default false
     */
    only_center_face?: boolean;
    /**
     * Seed
     * @description Random seed for reproducible generation.
     */
    seed?: number;
    /**
     * Upscale Factor
     * @description Upscaling factor
     * @default 2
     */
    upscale_factor?: number;
}

export interface CodeformerOutput {
    /**
     * Image
     * @description The generated image file info.
     * @example {
     *       "file_size": 423052,
     *       "height": 512,
     *       "file_name": "36d3ca4791a647678b2ff01a35c87f5a.png",
     *       "content_type": "image/png",
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/codeformer/codeformer_restored_1.jpeg",
     *       "width": 512
     *     }
     */
    image: Components.Image;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
}

export interface ClarityUpscalerInput {
    /**
     * Creativity
     * @description The creativity of the model. The higher the creativity, the more the model will deviate from the prompt.
     *                 Refers to the denoise strength of the sampling.
     * @default 0.35
     */
    creativity?: number;
    /**
     * Enable Safety Checker
     * @description If set to false, the safety checker will be disabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 4
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description The URL of the image to upscale.
     * @example https://storage.googleapis.com/falserverless/gallery/NOCA_Mick-Thompson.resized.resized.jpg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to address details that you don't want in the image.
     * @default (worst quality, low quality, normal quality:2)
     * @example (worst quality, low quality, normal quality:2)
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 18
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @default masterpiece, best quality, highres
     * @example masterpiece, best quality, highres
     */
    prompt?: string;
    /**
     * Resemblance
     * @description The resemblance of the upscaled image to the original image. The higher the resemblance, the more the model will try to keep the original image.
     *                 Refers to the strength of the ControlNet.
     * @default 0.6
     */
    resemblance?: number;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable Diffusion
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Upscale Factor
     * @description The upscale factor
     * @default 2
     */
    upscale_factor?: number;
}

export interface ClarityUpscalerOutput {
    /** @description The URL of the generated image. */
    image: Components.Image_2;
    /**
     * Seed
     * @description The seed used to generate the image.
     */
    seed: number;
    /**
     * Timings
     * @description The timings of the different steps in the workflow.
     */
    timings: {
        [key: string]: number;
    };
}

export interface ChronoEditLoraGalleryUpscalerInput {
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale for the inference.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description The image to upscale.
     */
    image_url: string;
    /**
     * Lora Scale
     * @description The scale factor for the LoRA adapter.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Loras
     * @description Optional additional LoRAs to merge (max 3).
     * @default []
     */
    loras?: Components.ChronoLoraWeight[];
    /**
     * Num Inference Steps
     * @description Number of inference steps for the upscaling pass.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Seed
     * @description The seed for the inference.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description Whether to return the image in sync mode.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Upscale Factor
     * @description Target scale factor for the output resolution.
     * @default 2
     */
    upscale_factor?: number;
}

export interface ChronoEditLoraGalleryUpscalerOutput extends SharedType_95c {}

export interface ChronoEditLoraGalleryPaintbrushInput {
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Classifier-free guidance scale.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description The image to edit.
     */
    image_url: string;
    /**
     * Lora Scale
     * @description The scale factor for the LoRA adapter.
     * @default 1
     */
    lora_scale?: number;
    /**
     * Loras
     * @description Optional additional LoRAs to merge (max 3).
     * @default []
     */
    loras?: Components.ChronoLoraWeight[];
    /**
     * Mask Url
     * @description Optional mask image where black areas indicate regions to sketch/paint.
     */
    mask_url?: string;
    /**
     * Num Inference Steps
     * @description Number of denoising steps to run.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Output Format
     * @description The format of the output image.
     * @default png
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description Describe how to transform the sketched regions.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the output image.
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description The seed for the inference.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description Whether to return the image in sync mode.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Turbo Mode
     * @description Enable turbo mode to use faster inference.
     * @default true
     */
    turbo_mode?: boolean;
}

export interface ChronoEditLoraGalleryPaintbrushOutput extends SharedType_95c {}

export interface ChronoEditLoraInput {
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     * @example true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Enable Temporal Reasoning
     * @description Whether to enable temporal reasoning.
     * @default false
     */
    enable_temporal_reasoning?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale for the inference.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The image to edit.
     * @example https://v3b.fal.media/files/b/zebra/yRvp9rTyDeDGHnbmtcsgK_original-wave.jpg
     */
    image_url: string;
    /**
     * Loras
     * @description Optional additional LoRAs to merge for this request (max 3).
     * @default []
     */
    loras?: Components.ChronoLoraWeight[];
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Number of Temporal Reasoning Steps
     * @description The number of temporal reasoning steps to perform.
     * @default 8
     */
    num_temporal_reasoning_steps?: number;
    /**
     * Output Format
     * @description The format of the output image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to edit the image.
     * @example Add a surfer to the wave in the illustration.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the output image.
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description The seed for the inference.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description Whether to return the image in sync mode.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Turbo Mode
     * @description Enable turbo mode to use for faster inference.
     * @default true
     */
    turbo_mode?: boolean;
}

export interface ChronoEditLoraOutput extends SharedType_95c {}

export interface ChronoEditInput {
    /**
     * Enable Prompt Expansion
     * @description Whether to enable prompt expansion.
     * @default true
     * @example true
     */
    enable_prompt_expansion?: boolean;
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Enable Temporal Reasoning
     * @description Whether to enable temporal reasoning.
     * @default false
     */
    enable_temporal_reasoning?: boolean;
    /**
     * Guidance Scale
     * @description The guidance scale for the inference.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Image URL
     * @description The image to edit.
     * @example https://v3b.fal.media/files/b/zebra/yRvp9rTyDeDGHnbmtcsgK_original-wave.jpg
     */
    image_url: string;
    /**
     * Number of Inference Steps
     * @description The number of inference steps to perform.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Number of Temporal Reasoning Steps
     * @description The number of temporal reasoning steps to perform.
     * @default 8
     */
    num_temporal_reasoning_steps?: number;
    /**
     * Output Format
     * @description The format of the output image.
     * @default jpeg
     * @enum {string}
     */
    output_format?: 'jpeg' | 'png' | 'webp';
    /**
     * Prompt
     * @description The prompt to edit the image.
     * @example Add a surfer to the wave in the illustration.
     */
    prompt: string;
    /**
     * Resolution
     * @description The resolution of the output image.
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description The seed for the inference.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description Whether to return the image in sync mode.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Turbo Mode
     * @description Enable turbo mode to use for faster inference.
     * @default true
     */
    turbo_mode?: boolean;
}

export interface ChronoEditOutput extends SharedType_95c {}

export interface ChatterboxTextToSpeechMultilingualInput {
    /**
     * CFG Scale
     * @description Configuration/pace weight controlling generation guidance (0.0-1.0). Use 0.0 for language transfer to mitigate accent inheritance.
     * @default 0.5
     */
    cfg_scale?: number;
    /**
     * Custom Audio Language
     * @description If using a custom audio URL, specify the language of the audio here. Ignored if voice is not a custom url.
     * @enum {string}
     */
    custom_audio_language?:
        | 'english'
        | 'arabic'
        | 'danish'
        | 'german'
        | 'greek'
        | 'spanish'
        | 'finnish'
        | 'french'
        | 'hebrew'
        | 'hindi'
        | 'italian'
        | 'japanese'
        | 'korean'
        | 'malay'
        | 'dutch'
        | 'norwegian'
        | 'polish'
        | 'portuguese'
        | 'russian'
        | 'swedish'
        | 'swahili'
        | 'turkish'
        | 'chinese';
    /**
     * Exaggeration
     * @description Controls speech expressiveness and emotional intensity (0.25-2.0). 0.5 is neutral, higher values increase expressiveness. Extreme values may be unstable.
     * @default 0.5
     */
    exaggeration?: number;
    /**
     * Seed
     * @description Random seed for reproducible results. Set to 0 for random generation, or provide a specific number for consistent outputs.
     */
    seed?: number;
    /**
     * Temperature
     * @description Controls randomness and variation in generation (0.05-5.0). Higher values create more varied speech patterns.
     * @default 0.8
     */
    temperature?: number;
    /**
     * Text
     * @description The text to be converted to speech (maximum 300 characters). Supports 23 languages including English, French, German, Spanish, Italian, Portuguese, Hindi, Arabic, Chinese, Japanese, Korean, and more.
     * @example Last month, we reached a new milestone with two billion views on our YouTube channel.
     * @example Le mois dernier, nous avons atteint un nouveau jalon avec deux milliards de vues sur notre chaîne YouTube.
     */
    text: string;
    /**
     * Voice
     * @description Language code for synthesis. In case using custom please provide audio url and select custom_audio_language.
     * @default english
     * @example english
     * @example arabic
     * @example danish
     * @example german
     * @example greek
     * @example spanish
     * @example finnish
     * @example french
     * @example hebrew
     * @example hindi
     * @example italian
     * @example japanese
     * @example korean
     * @example malay
     * @example dutch
     * @example norwegian
     * @example polish
     * @example portuguese
     * @example russian
     * @example swedish
     * @example swahili
     * @example turkish
     * @example chinese
     */
    voice?: string;
}

export interface ChatterboxTextToSpeechMultilingualOutput {
    /**
     * Audio
     * @description The generated multilingual speech audio file
     * @example {
     *       "url": "https://v3.fal.media/files/example/multilingual_speech_output.wav"
     *     }
     */
    audio: Components.File;
}

export interface ChatterboxTextToSpeechInput {
    /**
     * Audio Url
     * @description Optional URL to an audio file to use as a reference for the generated speech. If provided, the model will try to match the style and tone of the reference audio.
     * @default https://storage.googleapis.com/chatterbox-demo-samples/prompts/male_rickmorty.mp3
     */
    audio_url?: string;
    /**
     * Cfg
     * @default 0.5
     */
    cfg?: number;
    /**
     * Exaggeration
     * @description Exaggeration factor for the generated speech (0.0 = no exaggeration, 1.0 = maximum exaggeration).
     * @default 0.25
     */
    exaggeration?: number;
    /**
     * Seed
     * @description Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file. Set to 0 for random seed..
     */
    seed?: number;
    /**
     * Temperature
     * @description Temperature for generation (higher = more creative).
     * @default 0.7
     */
    temperature?: number;
    /**
     * Text
     * @description The text to be converted to speech. You can additionally add the following emotive tags: <laugh>, <chuckle>, <sigh>, <cough>, <sniffle>, <groan>, <yawn>, <gasp>
     * @example I just found a hidden treasure in the backyard! Check it out!
     */
    text: string;
}

export interface ChatterboxTextToSpeechOutput extends SharedType_4411 {}

export interface ChatterboxSpeechToSpeechInput {
    /**
     * Source Audio Url
     * @example https://storage.googleapis.com/chatterbox-demo-samples/samples/duff_stewie.wav
     */
    source_audio_url: string;
    /**
     * Target Voice Audio Url
     * @description Optional URL to an audio file to use as a reference for the generated speech. If provided, the model will try to match the style and tone of the reference audio.
     * @example https://v3.fal.media/files/tiger/0XODRhebRLiBdu8MqgZc5_tmpljqsylwu.wav
     */
    target_voice_audio_url?: string;
}

export interface ChatterboxSpeechToSpeechOutput {
    /**
     * Audio
     * @description The generated speech audio
     * @example {
     *       "url": "https://v3.fal.media/files/kangaroo/RQ_pxc7oPdueYqWUqEbPE_tmpjnzvvzx_.wav"
     *     }
     */
    audio: Components.File;
}

export interface ChainOfZoomInput {
    /**
     * Center X
     * @description X coordinate of zoom center (0-1)
     * @default 0.5
     */
    center_x?: number;
    /**
     * Center Y
     * @description Y coordinate of zoom center (0-1)
     * @default 0.5
     */
    center_y?: number;
    /**
     * Image Url
     * @description Input image to zoom into
     * @example https://storage.googleapis.com/falserverless/example_inputs/coz_example_input
     */
    image_url: string;
    /**
     * Scale
     * @description Zoom scale in powers of 2
     * @default 5
     */
    scale?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * User Prompt
     * @description Additional prompt text to guide the zoom enhancement
     * @default
     */
    user_prompt?: string;
}

export interface ChainOfZoomOutput {
    /**
     * Images
     * @description List of intermediate images
     * @example https://storage.googleapis.com/falserverless/example_outputs/coz_example_output_5
     */
    images: Components.Image_1[];
    /**
     * Scale
     * @description Actual linear zoom scale applied
     */
    scale: number;
    /**
     * Zoom Center
     * @description Center coordinates used for zoom
     */
    zoom_center: number[];
}

export interface CcsrInput {
    /**
     * Color Fix Type
     * @description Type of color correction for samples.
     * @default adain
     * @example adain
     * @example wavelet
     * @example none
     * @enum {string}
     */
    color_fix_type?: 'none' | 'wavelet' | 'adain';
    /**
     * Image Url
     * @description The URL or data URI of the image to upscale.
     * @example https://storage.googleapis.com/falserverless/gallery/blue-bird.jpeg
     */
    image_url: string;
    /**
     * Scale
     * @description The scale of the output image. The higher the scale, the bigger the output image will be.
     * @default 2
     */
    scale?: number;
    /**
     * Seed
     * @description Seed for reproducibility. Different seeds will make slightly different results.
     */
    seed?: number;
    /**
     * Steps
     * @description The number of steps to run the model for. The higher the number the better the quality and longer it will take to generate.
     * @default 50
     */
    steps?: number;
    /**
     * T Max
     * @description The ending point of uniform sampling strategy.
     * @default 0.6667
     */
    t_max?: number;
    /**
     * T Min
     * @description The starting point of uniform sampling strategy.
     * @default 0.3333
     */
    t_min?: number;
    /**
     * Tile Diffusion
     * @description If specified, a patch-based sampling strategy will be used for sampling.
     * @default none
     * @example none
     * @example mix
     * @example gaussian
     * @enum {string}
     */
    tile_diffusion?: 'none' | 'mix' | 'gaussian';
    /**
     * Tile Diffusion Size
     * @description Size of patch.
     * @default 1024
     */
    tile_diffusion_size?: number;
    /**
     * Tile Diffusion Stride
     * @description Stride of sliding patch.
     * @default 512
     */
    tile_diffusion_stride?: number;
    /**
     * Tile Vae
     * @description If specified, a patch-based sampling strategy will be used for VAE decoding.
     * @default false
     */
    tile_vae?: boolean;
    /**
     * Tile Vae Decoder Size
     * @description Size of VAE patch.
     * @default 226
     */
    tile_vae_decoder_size?: number;
    /**
     * Tile Vae Encoder Size
     * @description Size of latent image
     * @default 1024
     */
    tile_vae_encoder_size?: number;
}

export interface CcsrOutput {
    /**
     * Image
     * @description The generated image file info.
     */
    image: Components.Image;
    /**
     * Seed
     * @description The seed used for the generation.
     */
    seed: number;
}

export interface CatVtonInput {
    /**
     * Cloth Type
     * @description Type of the Cloth to be tried on.
     *
     *             Options:
     *             upper: Upper body cloth
     *             lower: Lower body cloth
     *             overall: Full body cloth
     *             inner: Inner cloth, like T-shirt inside a jacket
     *             outer: Outer cloth, like a jacket over a T-shirt
     * @example upper
     * @example lower
     * @example overall
     * @example inner
     * @example outer
     * @enum {string}
     */
    cloth_type: 'upper' | 'lower' | 'overall' | 'inner' | 'outer';
    /**
     * Garment Image Url
     * @description Url to the garment image.
     * @example https://storage.googleapis.com/falserverless/catvton/tshirt.jpg
     */
    garment_image_url: string;
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Human Image Url
     * @description Url for the human image.
     * @example https://storage.googleapis.com/falserverless/catvton/man5.jpg
     */
    human_image_url: string;
    /**
     * Image Size
     * @description The size of the generated image.
     * @default portrait_4_3
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Inference Steps
     * @description The number of inference steps to perform.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Seed
     * @description The same seed and the same input given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
}

export interface CatVtonOutput {
    /**
     * Image
     * @description The output image.
     */
    image: Components.Image;
}

export interface CartoonifyInput {
    /**
     * Enable Safety Checker
     * @description Whether to enable the safety checker
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Guidance scale for the generation
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description URL of the image to apply Pixar style to
     * @example https://v3.fal.media/files/tiger/c8VSfX5XtJ3DCzV-4Bxg8_kid_image.png
     */
    image_url: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps
     * @default 28
     */
    num_inference_steps?: number;
    /**
     * Scale
     * @description Scale factor for the Pixar effect
     * @default 1
     */
    scale?: number;
    /**
     * Seed
     * @description The seed for image generation. Same seed with same parameters will generate same image.
     */
    seed?: number;
    /**
     * Use Cfg Zero
     * @description Whether to use CFG zero
     * @default false
     */
    use_cfg_zero?: boolean;
}

export interface CartoonifyOutput extends SharedType_a73 {}

export interface CalligrapherInput {
    /**
     * Auto Mask Generation
     * @description Whether to automatically generate mask from detected text
     * @default false
     */
    auto_mask_generation?: boolean;
    /**
     * Cfg Scale
     * @description Guidance or strength scale for the model
     * @default 1
     */
    cfg_scale?: number;
    /**
     * Image Size
     * @description Target image size for generation
     * @default {
     *       "height": 1024,
     *       "width": 1024
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Mask Image Url
     * @description Base64-encoded mask image (optional if using auto_mask_generation)
     * @example https://storage.googleapis.com/falserverless/calligrapher/test17_mask.png
     */
    mask_image_url?: string;
    /**
     * Num Images
     * @description How many images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps (1-100)
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description Text prompt to inpaint or customize
     * @example The text is 'Rise'
     */
    prompt: string;
    /**
     * Reference Image Url
     * @description Optional base64 reference image for style
     */
    reference_image_url?: string;
    /**
     * Seed
     * @description Random seed for reproducibility
     */
    seed?: number;
    /**
     * Source Image Url
     * @description Base64-encoded source image with drawn mask layers
     * @example https://storage.googleapis.com/falserverless/calligrapher/test17_source.png
     */
    source_image_url: string;
    /**
     * Source Text
     * @description Source text to replace (if empty, masks all detected text)
     * @default
     */
    source_text?: string;
    /**
     * Use Context
     * @description Whether to prepend context reference to the input
     * @default true
     */
    use_context?: boolean;
}

export interface CalligrapherOutput {
    /** Images */
    images: Components.Image[];
}

export interface BytedanceVideoStylizeInput {
    /**
     * Image Url
     * @description URL of the image to make the stylized video from.
     * @example https://v3.fal.media/files/kangaroo/-KmSPIcXeGA3Z_iiH4C75_tmph2ry_0_8.png
     */
    image_url: string;
    /**
     * Style
     * @description The style for your character in the video. Please use a short description.
     * @example Manga style
     */
    style: string;
}

export interface BytedanceVideoStylizeOutput extends SharedType_4411 {}

export interface BytedanceSeedreamV4TextToImageInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Enhance Prompt Mode
     * @description The mode to use for enhancing prompt enhancement. Standard mode provides higher quality results but takes longer to generate. Fast mode provides average quality results but takes less time to generate.
     * @default standard
     * @enum {string}
     */
    enhance_prompt_mode?: 'standard' | 'fast';
    /**
     * Image Size
     * @description The size of the generated image. Total pixels must be between 960x960 and 4096x4096.
     * @default {
     *       "height": 2048,
     *       "width": 2048
     *     }
     * @example {
     *       "height": 4096,
     *       "width": 4096
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
              | 'auto'
              | 'auto_2K'
              | 'auto_4K'
          );
    /**
     * Max Images
     * @description If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`.
     * @default 1
     */
    max_images?: number;
    /**
     * Num Images
     * @description Number of separate model generations to be run with the prompt.
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The text prompt used to generate the image
     * @example A trendy restaurant with a digital menu board displaying "Seedream 4.0 is available on fal" in elegant script, with diners enjoying their meals.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed to control the stochasticity of image generation.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface BytedanceSeedreamV4TextToImageOutput {
    /**
     * Images
     * @description Generated images
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/seedream4_t2i_output.png"
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Seed
     * @description Seed used for generation
     * @example 746406749
     */
    seed: number;
}

export interface BytedanceSeedreamV4EditInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Enhance Prompt Mode
     * @description The mode to use for enhancing prompt enhancement. Standard mode provides higher quality results but takes longer to generate. Fast mode provides average quality results but takes less time to generate.
     * @default standard
     * @enum {string}
     */
    enhance_prompt_mode?: 'standard' | 'fast';
    /**
     * Image Size
     * @description The size of the generated image. The minimum total image area is 921600 pixels. Failing this, the image size will be adjusted to by scaling it up, while maintaining the aspect ratio.
     * @default {
     *       "height": 2048,
     *       "width": 2048
     *     }
     * @example {
     *       "height": 2160,
     *       "width": 3840
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
              | 'auto'
              | 'auto_2K'
              | 'auto_4K'
          );
    /**
     * Image URLs
     * @description List of URLs of input images for editing. Presently, up to 10 image inputs are allowed. If over 10 images are sent, only the last 10 will be used.
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_inputs/seedream4_edit_input_1.png",
     *       "https://storage.googleapis.com/falserverless/example_inputs/seedream4_edit_input_2.png",
     *       "https://storage.googleapis.com/falserverless/example_inputs/seedream4_edit_input_3.png",
     *       "https://storage.googleapis.com/falserverless/example_inputs/seedream4_edit_input_4.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Max Images
     * @description If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`. The total number of images (image inputs + image outputs) must not exceed 15
     * @default 1
     */
    max_images?: number;
    /**
     * Num Images
     * @description Number of separate model generations to be run with the prompt.
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The text prompt used to edit the image
     * @example Dress the model in the clothes and hat. Add a cat to the scene and change the background to a Victorian era building.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed to control the stochasticity of image generation.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface BytedanceSeedreamV4EditOutput {
    /**
     * Images
     * @description Generated images
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/seedream4_edit_output.png"
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Seed
     * @description Seed used for generation
     * @example 746406749
     */
    seed: number;
}

export interface BytedanceSeedreamV45TextToImageInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image. Width and height must be between 1920 and 4096, or total number of pixels must be between 2560*1440 and 4096*4096.
     * @default {
     *       "height": 2048,
     *       "width": 2048
     *     }
     * @example auto_2K
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
              | 'auto_2K'
              | 'auto_4K'
          );
    /**
     * Max Images
     * @description If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`.
     * @default 1
     */
    max_images?: number;
    /**
     * Num Images
     * @description Number of separate model generations to be run with the prompt.
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The text prompt used to generate the image
     * @example A selfie of a cat, with the cat as the protagonist. The setting is twilight at the Eiffel Tower. The cat is happy, holding a piece of baklava in its paw. The photo has a slight motion blur and is slightly overexposed. From a selfie angle, with a bit of motion blur, the overall image presents a sense of calm madness. The text "Seedream 4.5 is on fal" should be written on the picture at the top in clearly visible font and crisp lettering. The image has a 4:3 aspect ratio
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed to control the stochasticity of image generation.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface BytedanceSeedreamV45TextToImageOutput {
    /**
     * Images
     * @description Generated images
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/seedreamv45/seedream_v45_t2i_output.png"
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Seed
     * @description Seed used for generation
     * @example 42
     */
    seed: number;
}

export interface BytedanceSeedreamV45EditInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Size
     * @description The size of the generated image. Width and height must be between 1920 and 4096, or total number of pixels must be between 2560*1440 and 4096*4096.
     * @default {
     *       "height": 2048,
     *       "width": 2048
     *     }
     * @example auto_4K
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
              | 'auto_2K'
              | 'auto_4K'
          );
    /**
     * Image URLs
     * @description List of URLs of input images for editing. Presently, up to 10 image inputs are allowed. If over 10 images are sent, only the last 10 will be used.
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_inputs/seedreamv45/seedream_v45_edit_input_1.png",
     *       "https://storage.googleapis.com/falserverless/example_inputs/seedreamv45/seedream_v45_edit_input_2.png",
     *       "https://storage.googleapis.com/falserverless/example_inputs/seedreamv45/seedream_v45_edit_input_3.png"
     *     ]
     */
    image_urls: string[];
    /**
     * Max Images
     * @description If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`. The total number of images (image inputs + image outputs) must not exceed 15
     * @default 1
     */
    max_images?: number;
    /**
     * Num Images
     * @description Number of separate model generations to be run with the prompt.
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The text prompt used to edit the image
     * @example Replace the product in Figure 1 with that in Figure 2. For the title copy the text in Figure 3 to the top of the screen, the title should have a clear contrast with the background but not be overly eye-catching.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed to control the stochasticity of image generation.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface BytedanceSeedreamV45EditOutput {
    /**
     * Images
     * @description Generated images
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/example_outputs/seedreamv45/seedream_v45_edit_output.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface BytedanceSeedreamV3TextToImageInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Guidance Scale
     * @description Controls how closely the output image aligns with the input prompt. Higher values mean stronger prompt correlation.
     * @default 2.5
     */
    guidance_scale?: number;
    /**
     * Image Size
     * @description Use for finer control over the output image size. Will be used over aspect_ratio, if both are provided. Width and height must be between 512 and 2048.
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The text prompt used to generate the image
     * @example Fisheye lens, the head of a cat, the image shows the effect that the facial features of the cat are distorted due to the shooting method.
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed to control the stochasticity of image generation.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface BytedanceSeedreamV3TextToImageOutput {
    /**
     * Images
     * @description Generated images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/rabbit/EJqemc4hQlHKAtkkfTJqB_a2aaccab7ff84740b6323da580146087.png"
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Seed
     * @description Seed used for generation
     * @example 42
     */
    seed: number;
}

export interface BytedanceSeedanceV1ProTextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '1:1' | '3:4' | '9:16';
    /**
     * Camera Fixed
     * @description Whether to fix the camera position
     * @default false
     */
    camera_fixed?: boolean;
    /**
     * Duration
     * @description Duration of the video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9' | '10' | '11' | '12';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Prompt
     * @description The text prompt used to generate the video
     * @example A bright blue race car speeds along a snowy racetrack. [Low-angle shot] Captures several cars speeding along the racetrack through a harsh snowstorm. [Overhead shot] The camera gradually pulls upward, revealing the full race scene illuminated by storm lights
     */
    prompt: string;
    /**
     * Resolution
     * @description Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality
     * @default 1080p
     * @enum {string}
     */
    resolution?: '480p' | '720p' | '1080p';
    /**
     * Seed
     * @description Random seed to control video generation. Use -1 for random.
     */
    seed?: number;
}

export interface BytedanceSeedanceV1ProTextToVideoOutput {
    /**
     * Seed
     * @description Seed used for generation
     * @example 42
     */
    seed: number;
    /**
     * Video
     * @description Generated video file
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_inputs/seedance_pro_t2v.mp4"
     *     }
     */
    video: Components.File;
}

export interface BytedanceSeedanceV1ProImageToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '1:1' | '3:4' | '9:16' | 'auto';
    /**
     * Camera Fixed
     * @description Whether to fix the camera position
     * @default false
     */
    camera_fixed?: boolean;
    /**
     * Duration
     * @description Duration of the video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9' | '10' | '11' | '12';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Url
     * @description The URL of the image the video ends with. Defaults to None.
     */
    end_image_url?: string;
    /**
     * Image Url
     * @description The URL of the image used to generate video
     * @example https://storage.googleapis.com/falserverless/example_inputs/seedance_pro_i2v_img.jpg
     */
    image_url: string;
    /**
     * Prompt
     * @description The text prompt used to generate the video
     * @example A skier glides over fresh snow, joyously smiling while kicking up large clouds of snow as he turns. Accelerating gradually down the slope, the camera moves smoothly alongside.
     */
    prompt: string;
    /**
     * Resolution
     * @description Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality
     * @default 1080p
     * @enum {string}
     */
    resolution?: '480p' | '720p' | '1080p';
    /**
     * Seed
     * @description Random seed to control video generation. Use -1 for random.
     */
    seed?: number;
}

export interface BytedanceSeedanceV1ProImageToVideoOutput {
    /**
     * Seed
     * @description Seed used for generation
     * @example 42
     */
    seed: number;
    /**
     * Video
     * @description Generated video file
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_inputs/seedance_pro_i2v.mp4"
     *     }
     */
    video: Components.File;
}

export interface BytedanceSeedanceV1ProFastTextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '1:1' | '3:4' | '9:16';
    /**
     * Camera Fixed
     * @description Whether to fix the camera position
     * @default false
     */
    camera_fixed?: boolean;
    /**
     * Duration
     * @description Duration of the video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9' | '10' | '11' | '12';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Prompt
     * @description The text prompt used to generate the video
     * @example Inside a quiet dojo, a martial artist moves with precision and grace. The performance highlights the beauty and discipline inherent in the ancient practice. Each form unfolds clearly, a testament to dedication and skill.
     */
    prompt: string;
    /**
     * Resolution
     * @description Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality
     * @default 1080p
     * @enum {string}
     */
    resolution?: '480p' | '720p' | '1080p';
    /**
     * Seed
     * @description Random seed to control video generation. Use -1 for random.
     */
    seed?: number;
}

export interface BytedanceSeedanceV1ProFastTextToVideoOutput {
    /**
     * Seed
     * @description Seed used for generation
     * @example 42
     */
    seed: number;
    /**
     * Video
     * @description Generated video file
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_inputs/seedance_fast_t2v_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface BytedanceSeedanceV1ProFastImageToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '1:1' | '3:4' | '9:16' | 'auto';
    /**
     * Camera Fixed
     * @description Whether to fix the camera position
     * @default false
     */
    camera_fixed?: boolean;
    /**
     * Duration
     * @description Duration of the video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9' | '10' | '11' | '12';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Url
     * @description The URL of the image used to generate video
     * @example https://storage.googleapis.com/falserverless/example_inputs/seedance_fast_i2v_input.png
     */
    image_url: string;
    /**
     * Prompt
     * @description The text prompt used to generate the video
     * @example Bathed in a stark spotlight, a lone ballet dancer takes center stage. Her movements, precise and graceful, tell a story of passion and dedication against the velvet darkness. The scene evokes a sense of intimacy, highlighting the raw emotion and artistry of her performance.
     */
    prompt: string;
    /**
     * Resolution
     * @description Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality
     * @default 1080p
     * @enum {string}
     */
    resolution?: '480p' | '720p' | '1080p';
    /**
     * Seed
     * @description Random seed to control video generation. Use -1 for random.
     */
    seed?: number;
}

export interface BytedanceSeedanceV1ProFastImageToVideoOutput {
    /**
     * Seed
     * @description Seed used for generation
     * @example 42
     */
    seed: number;
    /**
     * Video
     * @description Generated video file
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_inputs/seedance_fast_i2v_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface BytedanceSeedanceV1LiteTextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '1:1' | '3:4' | '9:16' | '9:21';
    /**
     * Camera Fixed
     * @description Whether to fix the camera position
     * @default false
     */
    camera_fixed?: boolean;
    /**
     * Duration
     * @description Duration of the video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9' | '10' | '11' | '12';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Prompt
     * @description The text prompt used to generate the video
     * @example A little dog is running in the sunshine. The camera follows the dog as it plays in a garden.
     */
    prompt: string;
    /**
     * Resolution
     * @description Video resolution - 480p for faster generation, 720p for higher quality
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p' | '1080p';
    /**
     * Seed
     * @description Random seed to control video generation. Use -1 for random.
     */
    seed?: number;
}

export interface BytedanceSeedanceV1LiteTextToVideoOutput extends SharedType_74f {}

export interface BytedanceSeedanceV1LiteReferenceToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '1:1' | '3:4' | '9:16' | 'auto';
    /**
     * Camera Fixed
     * @description Whether to fix the camera position
     * @default false
     */
    camera_fixed?: boolean;
    /**
     * Duration
     * @description Duration of the video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9' | '10' | '11' | '12';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Prompt
     * @description The text prompt used to generate the video
     * @example The girl catches the puppy and hugs it.
     */
    prompt: string;
    /**
     * Reference Image Urls
     * @description Reference images to generate the video with.
     * @example [
     *       "https://storage.googleapis.com/falserverless/example_inputs/seedance_reference.jpeg",
     *       "https://storage.googleapis.com/falserverless/example_inputs/seedance_reference_2.jpeg"
     *     ]
     */
    reference_image_urls: string[];
    /**
     * Resolution
     * @description Video resolution - 480p for faster generation, 720p for higher quality
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description Random seed to control video generation. Use -1 for random.
     */
    seed?: number;
}

export interface BytedanceSeedanceV1LiteReferenceToVideoOutput {
    /**
     * Seed
     * @description Seed used for generation
     * @example 42
     */
    seed: number;
    /**
     * Video
     * @description Generated video file
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/seedance_reference_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface BytedanceSeedanceV1LiteImageToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default auto
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '1:1' | '3:4' | '9:16' | 'auto';
    /**
     * Camera Fixed
     * @description Whether to fix the camera position
     * @default false
     */
    camera_fixed?: boolean;
    /**
     * Duration
     * @description Duration of the video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9' | '10' | '11' | '12';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Url
     * @description The URL of the image the video ends with. Defaults to None.
     */
    end_image_url?: string;
    /**
     * Image Url
     * @description The URL of the image used to generate video
     * @example https://fal.media/files/koala/f_xmiodPjhiKjdBkFmTu1.png
     */
    image_url: string;
    /**
     * Prompt
     * @description The text prompt used to generate the video
     * @example A little dog is running in the sunshine. The camera follows the dog as it plays in a garden.
     */
    prompt: string;
    /**
     * Resolution
     * @description Video resolution - 480p for faster generation, 720p for higher quality
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p' | '1080p';
    /**
     * Seed
     * @description Random seed to control video generation. Use -1 for random.
     */
    seed?: number;
}

export interface BytedanceSeedanceV1LiteImageToVideoOutput extends SharedType_74f {}

export interface BytedanceSeedanceV15ProTextToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '1:1' | '3:4' | '9:16';
    /**
     * Camera Fixed
     * @description Whether to fix the camera position
     * @default false
     */
    camera_fixed?: boolean;
    /**
     * Duration
     * @description Duration of the video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '4' | '5' | '6' | '7' | '8' | '9' | '10' | '11' | '12';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Prompt
     * @description The text prompt used to generate the video
     * @example Defense attorney declaring "Ladies and gentlemen, reasonable doubt isn't just a phrase, it's the foundation of justice itself", footsteps on marble, jury shifting, courtroom drama, closing argument power.
     */
    prompt: string;
    /**
     * Resolution
     * @description Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p' | '1080p';
    /**
     * Seed
     * @description Random seed to control video generation. Use -1 for random.
     */
    seed?: number;
}

export interface BytedanceSeedanceV15ProTextToVideoOutput {
    /**
     * Seed
     * @description Seed used for generation
     * @example 42
     */
    seed: number;
    /**
     * Video
     * @description Generated video file
     * @example {
     *       "url": "https://v3b.fal.media/files/b/0a87743e/0K5lW0v-iC_BbKo64o0cA_video.mp4"
     *     }
     */
    video: Components.File;
}

export interface BytedanceSeedanceV15ProImageToVideoInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the generated video
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '21:9' | '16:9' | '4:3' | '1:1' | '3:4' | '9:16';
    /**
     * Camera Fixed
     * @description Whether to fix the camera position
     * @default false
     */
    camera_fixed?: boolean;
    /**
     * Duration
     * @description Duration of the video in seconds
     * @default 5
     * @enum {string}
     */
    duration?: '4' | '5' | '6' | '7' | '8' | '9' | '10' | '11' | '12';
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     * @example true
     */
    enable_safety_checker?: boolean;
    /**
     * End Image Url
     * @description The URL of the image the video ends with. Defaults to None.
     */
    end_image_url?: string;
    /**
     * Generate Audio
     * @description Whether to generate audio for the video
     * @default true
     */
    generate_audio?: boolean;
    /**
     * Image Url
     * @description The URL of the image used to generate video
     * @example https://v3b.fal.media/files/b/0a8773cd/REzCWn1BKUVuMFTxR-R3W_image_317.png
     */
    image_url: string;
    /**
     * Prompt
     * @description The text prompt used to generate the video
     * @example A man is crying and he says "I shouldn't have done it. I regret everything"
     */
    prompt: string;
    /**
     * Resolution
     * @description Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '720p' | '1080p';
    /**
     * Seed
     * @description Random seed to control video generation. Use -1 for random.
     */
    seed?: number;
}

export interface BytedanceSeedanceV15ProImageToVideoOutput {
    /**
     * Seed
     * @description Seed used for generation
     * @example 42
     */
    seed: number;
    /**
     * Video
     * @description Generated video file
     * @example {
     *       "url": "https://v3b.fal.media/files/b/0a8773d3/l2fk-fIO_PQFPzbvHkQX1_video.mp4"
     *     }
     */
    video: Components.File;
}

export interface BytedanceSeed3dImageTo3dInput {
    /**
     * Image Url
     * @description URL of the image for the 3D asset generation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/seed3d_input.png
     */
    image_url: string;
}

export interface BytedanceSeed3dImageTo3dOutput {
    /**
     * Model
     * @description The generated 3D model files
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/seed3d_output.zip"
     *     }
     */
    model: Components.File;
    /**
     * Usage Tokens
     * @description The number of tokens used for the 3D model generation
     * @example 30000
     */
    usage_tokens: number;
}

export interface BytedanceOmnihumanV15Input {
    /**
     * Audio Url
     * @description The URL of the audio file to generate the video. Audio must be under 30s long for 1080p generation and under 60s long for 720p generation.
     * @example https://storage.googleapis.com/falserverless/example_inputs/omnihuman_v15_input_audio.mp3
     */
    audio_url: string;
    /**
     * Image Url
     * @description The URL of the image used to generate the video
     * @example https://storage.googleapis.com/falserverless/example_inputs/omnihuman_v15_input_image.png
     */
    image_url: string;
    /**
     * Prompt
     * @description The text prompt used to guide the video generation.
     */
    prompt?: string;
    /**
     * Resolution
     * @description The resolution of the generated video. Defaults to 1080p. 720p generation is faster and higher in quality. 1080p generation is limited to 30s audio and 720p generation is limited to 60s audio.
     * @default 1080p
     * @enum {string}
     */
    resolution?: '720p' | '1080p';
    /**
     * Turbo Mode
     * @description Generate a video at a faster rate with a slight quality trade-off.
     * @default false
     */
    turbo_mode?: boolean;
}

export interface BytedanceOmnihumanV15Output {
    /**
     * Duration
     * @description Duration of audio input/video output as used for billing.
     */
    duration: number;
    /**
     * Video
     * @description Generated video file
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/omnihuman_v15_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface BytedanceOmnihumanInput {
    /**
     * Audio Url
     * @description The URL of the audio file to generate the video. Audio must be under 30s long.
     * @example https://storage.googleapis.com/falserverless/example_inputs/omnihuman_audio.mp3
     */
    audio_url: string;
    /**
     * Image Url
     * @description The URL of the image used to generate the video
     * @example https://storage.googleapis.com/falserverless/example_inputs/omnihuman.png
     */
    image_url: string;
}

export interface BytedanceOmnihumanOutput {
    /**
     * Duration
     * @description Duration of audio input/video output as used for billing.
     */
    duration: number;
    /**
     * Video
     * @description Generated video file
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/omnihuman_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface BytedanceDreaminaV31TextToImageInput {
    /**
     * Enhance Prompt
     * @description Whether to use an LLM to enhance the prompt
     * @default false
     */
    enhance_prompt?: boolean;
    /**
     * Image Size
     * @description The size of the generated image. Width and height must be between 512 and 2048.
     * @default {
     *       "height": 1536,
     *       "width": 2048
     *     }
     */
    image_size?:
        | Components.ImageSize
        | (
              | 'square_hd'
              | 'square'
              | 'portrait_4_3'
              | 'portrait_16_9'
              | 'landscape_4_3'
              | 'landscape_16_9'
          );
    /**
     * Num Images
     * @description Number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The text prompt used to generate the image
     * @example A 25-year-old korean woman selfie, front facing camera, lighting is soft and natural. If background is visible, it's a clean, modern apartment interior. The clothing color is clearly visible and distinct, adding a hint of color contrast
     */
    prompt: string;
    /**
     * Seed
     * @description Random seed to control the stochasticity of image generation.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface BytedanceDreaminaV31TextToImageOutput {
    /**
     * Images
     * @description Generated images
     * @example [
     *       {
     *         "url": "https://v3.fal.media/files/panda/4mddd7PmDvbbBZDs-xnUW_4294a9041c9d46eaa7b98d15ce6300fb.png"
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Seed
     * @description Seed used for generation
     * @example 746406749
     */
    seed: number;
}

export interface BytedanceUpscalerUpscaleVideoInput {
    /**
     * Target Fps
     * @description The target FPS of the video to upscale.
     * @default 30fps
     * @enum {string}
     */
    target_fps?: '30fps' | '60fps';
    /**
     * Target Resolution
     * @description The target resolution of the video to upscale.
     * @default 1080p
     * @enum {string}
     */
    target_resolution?: '1080p' | '2k' | '4k';
    /**
     * Video Url
     * @description The URL of the video to upscale.
     * @example https://storage.googleapis.com/falserverless/example_inputs/bytedance_video_upscaler_input.mp4
     */
    video_url: string;
}

export interface BytedanceUpscalerUpscaleVideoOutput {
    /**
     * Duration
     * @description Duration of audio input/video output as used for billing.
     */
    duration: number;
    /**
     * Video
     * @description Generated video file
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/bytedance_video_upscaler_output.mp4"
     *     }
     */
    video: Components.File;
}

export interface BriaTextToImageHdInput extends SharedType_411 {}

export interface BriaTextToImageHdOutput extends SharedType_a97 {}

export interface BriaTextToImageFastInput {
    /**
     * Aspect Ratio
     * @description The aspect ratio of the image. When a guidance method is being used, the aspect ratio is defined by the guidance image and this parameter is ignored.
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?: '1:1' | '2:3' | '3:2' | '3:4' | '4:3' | '4:5' | '5:4' | '9:16' | '16:9';
    /**
     * Guidance
     * @description Guidance images to use for the generation. Up to 4 guidance methods can be combined during a single inference.
     * @default []
     */
    guidance?: Components.GuidanceInput[];
    /**
     * Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want
     *                 the model to stick to your prompt when looking for a related image to show you.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Medium
     * @description Which medium should be included in your generated images. This parameter is optional.
     * @enum {string}
     */
    medium?: 'photography' | 'art';
    /**
     * Negative Prompt
     * @description The negative prompt you would like to use to generate images.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description How many images you would like to generate. When using any Guidance Method, Value is set to 1.
     * @default 4
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of iterations the model goes through to refine the generated image. This parameter is optional.
     * @default 8
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt you would like to use to generate images.
     * @example A lone figure stands on the edge of a serene cliff at sunset, gazing out over a vast, mystical valley. The figure is clad in flowing robes that ripple in the gentle breeze, silhouetted against the golden and lavender hues of the sky. Below, a cascading waterfall pours into a sparkling river winding through a forest of bioluminescent trees. The scene blends the awe of nature with a touch of otherworldly wonder, inviting reflection and imagination.
     */
    prompt: string;
    /**
     * Prompt Enhancement
     * @description When set to true, enhances the provided prompt by generating additional, more descriptive variations, resulting in more diverse and creative output images.
     * @default false
     */
    prompt_enhancement?: boolean;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface BriaTextToImageFastOutput extends SharedType_a97 {}

export interface BriaTextToImageBaseInput extends SharedType_411 {}

export interface BriaTextToImageBaseOutput extends SharedType_a97 {}

export interface BriaReimagineInput {
    /**
     * Fast
     * @description Whether to use the fast model
     * @default true
     */
    fast?: boolean;
    /**
     * Num Inference Steps
     * @description The number of iterations the model goes through to refine the generated image. This parameter is optional.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Num Results
     * @description How many images you would like to generate. When using any Guidance Method, Value is set to 1.
     * @default 1
     */
    num_results?: number;
    /**
     * Prompt
     * @description The prompt you would like to use to generate images.
     * @example A 2d illustration of a dog in a vibrant park
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Structure Image Url
     * @description The URL of the structure reference image. Use "" to leave empty. Accepted formats are jpeg, jpg, png, webp.
     * @default
     * @example https://storage.googleapis.com/falserverless/bria/bria_reimagine_input.png
     */
    structure_image_url?: string;
    /**
     * Structure Ref Influence
     * @description The influence of the structure reference on the generated image.
     * @default 0.75
     * @example 0.15
     */
    structure_ref_influence?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface BriaReimagineOutput {
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/bria/bria_reimagine_output.png"
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Seed
     * @description Seed value used for generation.
     */
    seed: number;
}

export interface BriaProductShotInput {
    /**
     * Fast
     * @description Whether to use the fast model
     * @default true
     */
    fast?: boolean;
    /**
     * Image Url
     * @description The URL of the product shot to be placed in a lifestyle shot. If both image_url and image_file are provided, image_url will be used. Accepted formats are jpeg, jpg, png, webp. Maximum file size 12MB.
     * @example https://storage.googleapis.com/falserverless/bria/bria_product_fg.jpg
     */
    image_url: string;
    /**
     * Manual Placement Selection
     * @description If you've selected placement_type=manual_placement, you should use this parameter to specify which placements/positions you would like to use from the list. You can select more than one placement in one request.
     * @default bottom_center
     * @enum {string}
     */
    manual_placement_selection?:
        | 'upper_left'
        | 'upper_right'
        | 'bottom_left'
        | 'bottom_right'
        | 'right_center'
        | 'left_center'
        | 'upper_center'
        | 'bottom_center'
        | 'center_vertical'
        | 'center_horizontal';
    /**
     * Num Results
     * @description The number of lifestyle product shots you would like to generate. You will get num_results x 10 results when placement_type=automatic and according to the number of required placements x num_results if placement_type=manual_placement.
     * @default 1
     */
    num_results?: number;
    /**
     * Optimize Description
     * @description Whether to optimize the scene description
     * @default true
     */
    optimize_description?: boolean;
    /**
     * Original Quality
     * @description This flag is only relevant when placement_type=original. If true, the output image retains the original input image's size; otherwise, the image is scaled to 1 megapixel (1MP) while preserving its aspect ratio.
     * @default false
     */
    original_quality?: boolean;
    /**
     * Padding Values
     * @description The desired padding in pixels around the product, when using placement_type=manual_padding. The order of the values is [left, right, top, bottom]. For optimal results, the total number of pixels, including padding, should be around 1,000,000. It is recommended to first use the product cutout API, get the cutout and understand the size of the result, and then define the required padding and use the cutout as an input for this API.
     */
    padding_values?: number[];
    /**
     * Placement Type
     * @description This parameter allows you to control the positioning of the product in the image. Choosing 'original' will preserve the original position of the product in the image. Choosing 'automatic' will generate results with the 10 recommended positions for the product. Choosing 'manual_placement' will allow you to select predefined positions (using the parameter 'manual_placement_selection'). Selecting 'manual_padding' will allow you to control the position and size of the image by defining the desired padding in pixels around the product.
     * @default manual_placement
     * @enum {string}
     */
    placement_type?: 'original' | 'automatic' | 'manual_placement' | 'manual_padding';
    /**
     * Ref Image Url
     * @description The URL of the reference image to be used for generating the new scene or background for the product shot. Use "" to leave empty.Either ref_image_url or scene_description has to be provided but not both. If both ref_image_url and ref_image_file are provided, ref_image_url will be used. Accepted formats are jpeg, jpg, png, webp.
     * @default
     * @example https://storage.googleapis.com/falserverless/bria/bria_product_bg.jpg
     */
    ref_image_url?: string;
    /**
     * Scene Description
     * @description Text description of the new scene or background for the provided product shot. Bria currently supports prompts in English only, excluding special characters.
     * @example on a rock, next to the ocean, dark theme
     */
    scene_description?: string;
    /**
     * Shot Size
     * @description The desired size of the final product shot. For optimal results, the total number of pixels should be around 1,000,000. This parameter is only relevant when placement_type=automatic or placement_type=manual_placement.
     * @default [
     *       1000,
     *       1000
     *     ]
     */
    shot_size?: number[];
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface BriaProductShotOutput {
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/bria/bria_product_res.png"
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface BriaGenfillInput {
    /**
     * Image Url
     * @description Input Image to erase from
     * @example https://storage.googleapis.com/falserverless/bria/bria_genfill_img.png
     */
    image_url: string;
    /**
     * Mask Url
     * @description The URL of the binary mask image that represents the area that will be cleaned.
     * @example https://storage.googleapis.com/falserverless/bria/bria_genfill_mask.png
     */
    mask_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt you would like to use to generate images.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of Images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The prompt you would like to use to generate images.
     * @example A red delicious cherry
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface BriaGenfillOutput {
    /**
     * Images
     * @description Generated Images
     * @example [
     *       {
     *         "file_size": 1064550,
     *         "height": 768,
     *         "file_name": "a0d138e6820c4ad58f1fd3c758f16047.png",
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/bria/bria_genfill_res.png",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image[];
}

export interface BriaExpandInput {
    /**
     * Aspect Ratio
     * @description The desired aspect ratio of the final image. Will be used over original_image_size and original_image_location if provided.
     * @enum {string}
     */
    aspect_ratio?: '1:1' | '2:3' | '3:2' | '3:4' | '4:3' | '4:5' | '5:4' | '9:16' | '16:9';
    /**
     * Canvas Size
     * @description The desired size of the final image, after the expansion. should have an area of less than 5000x5000 pixels.
     * @example [
     *       1200,
     *       674
     *     ]
     */
    canvas_size: number[];
    /**
     * Image Url
     * @description The URL of the input image.
     * @example https://storage.googleapis.com/falserverless/model_tests/orange.png
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt you would like to use to generate images.
     * @default
     */
    negative_prompt?: string;
    /**
     * Original Image Location
     * @description The desired location of the original image, inside the full canvas. Provide the location of the upper left corner of the original image. The location can also be outside the canvas (the original image will be cropped). Will be ignored if aspect_ratio is provided.
     * @example [
     *       301,
     *       -66
     *     ]
     */
    original_image_location?: number[];
    /**
     * Original Image Size
     * @description The desired size of the original image, inside the full canvas. Ensure that the ratio of input image foreground or main subject to the canvas area is greater than 15% to achieve optimal results. Will be ignored if aspect_ratio is provided.
     * @example [
     *       610,
     *       855
     *     ]
     */
    original_image_size?: number[];
    /**
     * Prompt
     * @description Text on which you wish to base the image expansion. This parameter is optional. Bria currently supports prompts in English only, excluding special characters.
     * @default
     */
    prompt?: string;
    /**
     * Seed
     * @description You can choose whether you want your generated expension to be random or predictable. You can recreate the same result in the future by using the seed value of a result from the response. You can exclude this parameter if you are not interested in recreating your results. This parameter is optional.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface BriaExpandOutput {
    /**
     * Image
     * @description The generated image
     * @example {
     *       "file_size": 1471342,
     *       "height": 674,
     *       "file_name": "afa402a35ea742cdb5c3e219b2b19bfb.png",
     *       "content_type": "image/png",
     *       "url": "https://v3.fal.media/files/koala/8np-spgxxG-I1r3cjthRV_afa402a35ea742cdb5c3e219b2b19bfb.png",
     *       "width": 1200
     *     }
     */
    image: Components.Image;
    /**
     * Seed
     * @description Seed value used for generation.
     */
    seed: number;
}

export interface BriaEraserInput {
    /**
     * Image Url
     * @description Input Image to erase from
     * @example https://storage.googleapis.com/falserverless/bria/bria_eraser_img.png
     */
    image_url: string;
    /**
     * Mask Type
     * @description You can use this parameter to specify the type of the input mask from the list. 'manual' opttion should be used in cases in which the mask had been generated by a user (e.g. with a brush tool), and 'automatic' mask type should be used when mask had been generated by an algorithm like 'SAM'.
     * @default manual
     * @enum {string}
     */
    mask_type?: 'manual' | 'automatic';
    /**
     * Mask Url
     * @description The URL of the binary mask image that represents the area that will be cleaned.
     * @example https://storage.googleapis.com/falserverless/bria/bria_eraser_mask.png
     */
    mask_url: string;
    /**
     * Preserve Alpha
     * @description If set to true, attempts to preserve the alpha channel of the input image.
     * @default false
     */
    preserve_alpha?: boolean;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface BriaEraserOutput {
    /**
     * Image
     * @description The generated image
     * @example {
     *       "content_type": "image/png",
     *       "url": "https://storage.googleapis.com/falserverless/bria/bria_eraser_res.png"
     *     }
     */
    image: Components.Image;
}

export interface BriaBackgroundReplaceInput {
    /**
     * Fast
     * @description Whether to use the fast model
     * @default true
     */
    fast?: boolean;
    /**
     * Image Url
     * @description Input Image to erase from
     * @example https://storage.googleapis.com/falserverless/bria/bria_bg_replace_fg.jpg
     */
    image_url: string;
    /**
     * Negative Prompt
     * @description The negative prompt you would like to use to generate images.
     * @default
     */
    negative_prompt?: string;
    /**
     * Num Images
     * @description Number of Images to generate.
     * @default 1
     */
    num_images?: number;
    /**
     * Prompt
     * @description The prompt you would like to use to generate images.
     * @example Man leaning against a wall
     */
    prompt?: string;
    /**
     * Ref Image Url
     * @description The URL of the reference image to be used for generating the new background. Use "" to leave empty. Either ref_image_url or bg_prompt has to be provided but not both. If both ref_image_url and ref_image_file are provided, ref_image_url will be used. Accepted formats are jpeg, jpg, png, webp.
     * @default
     * @example https://storage.googleapis.com/falserverless/bria/bria_bg_replace_bg.jpg
     */
    ref_image_url?: string;
    /**
     * Refine Prompt
     * @description Whether to refine prompt
     * @default true
     */
    refine_prompt?: boolean;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of the model
     *                 will output the same image every time.
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface BriaBackgroundReplaceOutput {
    /**
     * Images
     * @description The generated images
     * @example [
     *       {
     *         "content_type": "image/png",
     *         "url": "https://storage.googleapis.com/falserverless/bria/bria_bg_replace_res.jpg"
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Seed
     * @description Seed value used for generation.
     */
    seed: number;
}

export interface BriaBackgroundRemoveInput {
    /**
     * Image Url
     * @description Input Image to erase from
     * @example https://fal.media/files/panda/K5Rndvzmn1j-OI1VZXDVd.jpeg
     */
    image_url: string;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface BriaBackgroundRemoveOutput {
    /**
     * Image
     * @description The generated image
     * @example {
     *       "file_size": 1076276,
     *       "height": 1024,
     *       "file_name": "070c731993e949d993c10ef6283d335d.png",
     *       "content_type": "image/png",
     *       "url": "https://v3.fal.media/files/tiger/GQEMNjRyxSoza7N8LPPqb_070c731993e949d993c10ef6283d335d.png",
     *       "width": 1024
     *     }
     */
    image: Components.Image;
}

export interface BirefnetV2VideoInput {
    /**
     * Model
     * @description Model to use for background removal.
     *                 The 'General Use (Light)' model is the original model used in the BiRefNet repository.
     *                 The 'General Use (Light 2K)' model is the original model used in the BiRefNet repository but trained with 2K images.
     *                 The 'General Use (Heavy)' model is a slower but more accurate model.
     *                 The 'Matting' model is a model trained specifically for matting images.
     *                 The 'Portrait' model is a model trained specifically for portrait images.
     *                 The 'General Use (Dynamic)' model supports dynamic resolutions from 256x256 to 2304x2304.
     *                 The 'General Use (Light)' model is recommended for most use cases.
     *
     *                 The corresponding models are as follows:
     *                 - 'General Use (Light)': BiRefNet
     *                 - 'General Use (Light 2K)': BiRefNet_lite-2K
     *                 - 'General Use (Heavy)': BiRefNet_lite
     *                 - 'Matting': BiRefNet-matting
     *                 - 'Portrait': BiRefNet-portrait
     *                 - 'General Use (Dynamic)': BiRefNet_dynamic
     * @default General Use (Light)
     * @enum {string}
     */
    model?:
        | 'General Use (Light)'
        | 'General Use (Light 2K)'
        | 'General Use (Heavy)'
        | 'Matting'
        | 'Portrait'
        | 'General Use (Dynamic)';
    /**
     * Operating Resolution
     * @description The resolution to operate on. The higher the resolution, the more accurate the output will be for high res input images. The '2304x2304' option is only available for the 'General Use (Dynamic)' model.
     * @default 1024x1024
     * @enum {string}
     */
    operating_resolution?: '1024x1024' | '2048x2048' | '2304x2304';
    /**
     * Output Mask
     * @description Whether to output the mask used to remove the background
     * @default false
     */
    output_mask?: boolean;
    /**
     * Refine Foreground
     * @description Whether to refine the foreground using the estimated mask
     * @default true
     */
    refine_foreground?: boolean;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Video Output Type
     * @description The output type of the generated video.
     * @default X264 (.mp4)
     * @enum {string}
     */
    video_output_type?: 'X264 (.mp4)' | 'VP9 (.webm)' | 'PRORES4444 (.mov)' | 'GIF (.gif)';
    /**
     * Video Quality
     * @description The quality of the generated video.
     * @default high
     * @enum {string}
     */
    video_quality?: 'low' | 'medium' | 'high' | 'maximum';
    /**
     * Video Url
     * @description URL of the video to remove background from
     * @example https://storage.googleapis.com/falserverless/example_inputs/birefnet-video-input.mp4
     */
    video_url: string;
    /**
     * Video Write Mode
     * @description The write mode of the generated video.
     * @default balanced
     * @enum {string}
     */
    video_write_mode?: 'fast' | 'balanced' | 'small';
}

export interface BirefnetV2VideoOutput {
    /**
     * Mask Video
     * @description Mask used to remove the background
     */
    mask_video?: Components.VideoFile_1;
    /**
     * Video
     * @description Video with background removed
     * @example {
     *       "height": 1080,
     *       "duration": 8,
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/birefnet-video-output.webm",
     *       "fps": 24,
     *       "width": 1920,
     *       "file_name": "birefnet-video-output.webm",
     *       "content_type": "video/webm",
     *       "num_frames": 192
     *     }
     */
    video: Components.VideoFile_1;
}

export interface BirefnetV2Input {
    /**
     * Image Url
     * @description URL of the image to remove background from
     * @example https://storage.googleapis.com/falserverless/example_inputs/birefnet-input.jpeg
     */
    image_url: string;
    /**
     * Model
     * @description Model to use for background removal.
     *                 The 'General Use (Light)' model is the original model used in the BiRefNet repository.
     *                 The 'General Use (Light 2K)' model is the original model used in the BiRefNet repository but trained with 2K images.
     *                 The 'General Use (Heavy)' model is a slower but more accurate model.
     *                 The 'Matting' model is a model trained specifically for matting images.
     *                 The 'Portrait' model is a model trained specifically for portrait images.
     *                 The 'General Use (Dynamic)' model supports dynamic resolutions from 256x256 to 2304x2304.
     *                 The 'General Use (Light)' model is recommended for most use cases.
     *
     *                 The corresponding models are as follows:
     *                 - 'General Use (Light)': BiRefNet
     *                 - 'General Use (Light 2K)': BiRefNet_lite-2K
     *                 - 'General Use (Heavy)': BiRefNet_lite
     *                 - 'Matting': BiRefNet-matting
     *                 - 'Portrait': BiRefNet-portrait
     *                 - 'General Use (Dynamic)': BiRefNet_dynamic
     * @default General Use (Light)
     * @enum {string}
     */
    model?:
        | 'General Use (Light)'
        | 'General Use (Light 2K)'
        | 'General Use (Heavy)'
        | 'Matting'
        | 'Portrait'
        | 'General Use (Dynamic)';
    /**
     * Operating Resolution
     * @description The resolution to operate on. The higher the resolution, the more accurate the output will be for high res input images. The '2304x2304' option is only available for the 'General Use (Dynamic)' model.
     * @default 1024x1024
     * @enum {string}
     */
    operating_resolution?: '1024x1024' | '2048x2048' | '2304x2304';
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'webp' | 'png' | 'gif';
    /**
     * Output Mask
     * @description Whether to output the mask used to remove the background
     * @default false
     */
    output_mask?: boolean;
    /**
     * Refine Foreground
     * @description Whether to refine the foreground using the estimated mask
     * @default true
     */
    refine_foreground?: boolean;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface BirefnetV2Output extends SharedType_f0b {}

export interface BirefnetInput {
    /**
     * Image Url
     * @description URL of the image to remove background from
     * @example https://storage.googleapis.com/falserverless/example_inputs/birefnet-input.jpeg
     */
    image_url: string;
    /**
     * Model
     * @description Model to use for background removal.
     *                 The 'General Use (Light)' model is the original model used in the BiRefNet repository.
     *                 The 'General Use (Heavy)' model is a slower but more accurate model.
     *                 The 'Portrait' model is a model trained specifically for portrait images.
     *                 The 'General Use (Light)' model is recommended for most use cases.
     *
     *                 The corresponding models are as follows:
     *                 - 'General Use (Light)': BiRefNet-DIS_ep580.pth
     *                 - 'General Use (Heavy)': BiRefNet-massive-epoch_240.pth
     *                 - 'Portrait': BiRefNet-portrait-TR_P3M_10k-epoch_120.pth
     * @default General Use (Light)
     * @enum {string}
     */
    model?: 'General Use (Light)' | 'General Use (Heavy)' | 'Portrait';
    /**
     * Operating Resolution
     * @description The resolution to operate on. The higher the resolution, the more accurate the output will be for high res input images.
     * @default 1024x1024
     * @enum {string}
     */
    operating_resolution?: '1024x1024' | '2048x2048';
    /**
     * Output Format
     * @description The format of the output image
     * @default png
     * @enum {string}
     */
    output_format?: 'webp' | 'png' | 'gif';
    /**
     * Output Mask
     * @description Whether to output the mask used to remove the background
     * @default false
     */
    output_mask?: boolean;
    /**
     * Refine Foreground
     * @description Whether to refine the foreground using the estimated mask
     * @default true
     */
    refine_foreground?: boolean;
    /**
     * Sync Mode
     * @description If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
     * @default false
     */
    sync_mode?: boolean;
}

export interface BirefnetOutput extends SharedType_f0b {}

export interface BenV2VideoInput {
    /**
     * Background Color
     * @description Optional RGB values (0-255) for the background color. If not provided, the background will be transparent. For ex: [0, 0, 0]
     */
    background_color?: { [x: string]: any }[];
    /**
     * Seed
     * @description Random seed for reproducible generation.
     */
    seed?: number;
    /**
     * Video Url
     * @description URL of video to be used for background removal.
     * @example https://storage.googleapis.com/falserverless/gallery/Ben2/100063-video-2160.mp4
     */
    video_url: string;
}

export interface BenV2VideoOutput {
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/gallery/Ben2/foreground.mp4"
     *     }
     */
    video: Components.File;
}

export interface BenV2ImageInput {
    /**
     * Image Url
     * @description URL of image to be used for background removal
     * @example https://storage.googleapis.com/falserverless/gallery/Ben2/arduino-uno-board-electronics-hand-600nw-1869855883.webp
     */
    image_url: string;
    /**
     * Seed
     * @description Random seed for reproducible generation.
     */
    seed?: number;
}

export interface BenV2ImageOutput {
    /**
     * Image
     * @description The output image after background removal.
     * @example {
     *       "height": 512,
     *       "file_size": 423052,
     *       "file_name": "zrZNETpI_ul2jonraqpxN_a57c3f3825d9418f8b3d39cde87c3310.png",
     *       "content_type": "image/png",
     *       "url": "https://storage.googleapis.com/falserverless/gallery/Ben2/zrZNETpI_ul2jonraqpxN_a57c3f3825d9418f8b3d39cde87c3310.png",
     *       "width": 512
     *     }
     */
    image: Components.Image;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
}

export interface BagelUnderstandInput {
    /**
     * Image Url
     * @description The image for the query.
     * @example https://storage.googleapis.com/falserverless/bagel/wRhCPSyiKTiLnnWvUpGIl.jpeg
     */
    image_url: string;
    /**
     * Prompt
     * @description The prompt to query the image with.
     * @example What is shown in the image?
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation.
     */
    seed?: number;
}

export interface BagelUnderstandOutput {
    /**
     * Prompt
     * @description The query used for the generation.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used for the generation.
     */
    seed: number;
    /**
     * Text
     * @description The answer to the query.
     */
    text: string;
    /**
     * Timings
     * @description The timings of the generation.
     */
    timings: Record<string, number>;
}

export interface BagelEditInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Image Url
     * @description The image to edit.
     * @example https://storage.googleapis.com/falserverless/bagel/wRhCPSyiKTiLnnWvUpGIl.jpeg
     */
    image_url: string;
    /**
     * Prompt
     * @description The prompt to edit the image with.
     * @example Change the cosmic cloud background of the floating temple to a clear blue sky with a gentle sunrise on the horizon. Keep all temple architecture, figures, and other elements exactly as they are.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation.
     */
    seed?: number;
    /**
     * Use Thought
     * @description Whether to use thought tokens for generation. If set to true, the model will "think" to potentially improve generation quality. Increases generation time and increases the cost by 20%.
     * @default false
     */
    use_thought?: boolean;
}

export interface BagelEditOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The edited images.
     * @example [
     *       {
     *         "file_size": 423052,
     *         "height": 1024,
     *         "file_name": "hQnndOMvGSt2UsYAiV3vs.jpeg",
     *         "content_type": "image/jpeg",
     *         "url": "https://storage.googleapis.com/falserverless/bagel/hQnndOMvGSt2UsYAiV3vs.jpeg",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface BagelInput {
    /**
     * Enable Safety Checker
     * @description If set to true, the safety checker will be enabled.
     * @default true
     */
    enable_safety_checker?: boolean;
    /**
     * Prompt
     * @description The prompt to generate an image from.
     * @example A luminous ancient temple floating among cosmic clouds, with impossible architecture of twisted spires and inverted arches. The structure is half-built from crystalline white marble and half from living bioluminescent coral in vibrant teal and purple. Ethereal light filters through stained glass windows depicting mythological scenes. Tiny cloaked figures with glowing lanterns traverse impossible staircases. In the foreground, a massive ornate door stands slightly ajar, revealing a glimpse of swirling golden energy within. The scene is lit by two moons of different colors, casting overlapping shadows. Cinematic lighting, hyper-detailed textures, 8K resolution.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for the generation.
     */
    seed?: number;
    /**
     * Use Thought
     * @description Whether to use thought tokens for generation. If set to true, the model will "think" to potentially improve generation quality. Increases generation time and increases the cost by 20%.
     * @default false
     */
    use_thought?: boolean;
}

export interface BagelOutput {
    /**
     * Has Nsfw Concepts
     * @description Whether the generated images contain NSFW concepts.
     */
    has_nsfw_concepts: boolean[];
    /**
     * Images
     * @description The generated images.
     * @example [
     *       {
     *         "file_size": 423052,
     *         "height": 1024,
     *         "file_name": "wRhCPSyiKTiLnnWvUpGIl.jpeg",
     *         "content_type": "image/jpeg",
     *         "url": "https://storage.googleapis.com/falserverless/bagel/wRhCPSyiKTiLnnWvUpGIl.jpeg",
     *         "width": 1024
     *       }
     *     ]
     */
    images: Components.Image[];
    /**
     * Prompt
     * @description The prompt used for generating the image.
     */
    prompt: string;
    /**
     * Seed
     * @description Seed of the generated Image. It will be the same value of the one passed in the
     *                 input or the randomly generated that was used in case none was passed.
     */
    seed: number;
    /** Timings */
    timings: {
        [key: string]: number;
    };
}

export interface AutoCaptionInput {
    /**
     * Font Size
     * @description Size of text in generated captions.
     * @default 24
     */
    font_size?: number;
    /**
     * Left Align
     * @description Left-to-right alignment of the text. Can be a string ('left', 'center', 'right') or a float (0.0-1.0)
     * @default center
     */
    left_align?: string | number;
    /**
     * Refresh Interval
     * @description Number of seconds the captions should stay on screen. A higher number will also result in more text being displayed at once.
     * @default 1.5
     */
    refresh_interval?: number;
    /**
     * Stroke Width
     * @description Width of the text strokes in pixels
     * @default 1
     */
    stroke_width?: number;
    /**
     * Top Align
     * @description Top-to-bottom alignment of the text. Can be a string ('top', 'center', 'bottom') or a float (0.0-1.0)
     * @default center
     */
    top_align?: string | number;
    /**
     * Txt Color
     * @description Colour of the text. Can be a RGB tuple, a color name, or an hexadecimal notation.
     * @default white
     */
    txt_color?: string;
    /**
     * Txt Font
     * @description Font for generated captions. Choose one in 'Arial','Standard','Garamond', 'Times New Roman','Georgia', or pass a url to a .ttf file
     * @default Standard
     */
    txt_font?: string;
    /**
     * Video Url
     * @description URL to the .mp4 video with audio. Only videos of size <100MB are allowed.
     */
    video_url: string;
}

export interface AutoCaptionOutput {
    /**
     * Video Url
     * @description URL to the caption .mp4 video.
     */
    video_url: string;
}

export interface AuraSrInput {
    /**
     * Checkpoint
     * @description Checkpoint to use for upscaling. More coming soon.
     * @default v1
     * @example v2
     * @example v1
     * @enum {string}
     */
    checkpoint?: 'v1' | 'v2';
    /**
     * Image URL
     * @description URL of the image to upscale.
     * @example https://fal.media/files/rabbit/JlBgYUyQRS3zxiBu_B4fM.png
     * @example https://fal.media/files/monkey/e6RtJf_ue0vyWzeiEmTby.png
     * @example https://fal.media/files/monkey/A6HGsigx4mmvs-hJVoOZX.png
     */
    image_url: string;
    /**
     * Overlapping Tiles
     * @description Whether to use overlapping tiles for upscaling. Setting this to true helps remove seams but doubles the inference time.
     * @default false
     * @example true
     * @example false
     */
    overlapping_tiles?: boolean;
    /**
     * Upscaling Factor (Xs)
     * @description Upscaling factor. More coming soon.
     * @default 4
     * @example 4
     * @enum {integer}
     */
    upscaling_factor?: 4;
}

export interface AuraSrOutput {
    /**
     * Image
     * @description Upscaled image
     */
    image: Components.Image;
    /**
     * Timings
     * @description Timings for each step in the pipeline.
     */
    timings: {
        [key: string]: number;
    };
}

export interface AuraFlowInput {
    /**
     * Expand Prompt
     * @description Whether to perform prompt expansion (recommended)
     * @default true
     */
    expand_prompt?: boolean;
    /**
     * Guidance Scale
     * @description Classifier free guidance scale
     * @default 3.5
     */
    guidance_scale?: number;
    /**
     * Num Images
     * @description The number of images to generate
     * @default 1
     */
    num_images?: number;
    /**
     * Num Inference Steps
     * @description The number of inference steps to take
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to generate images from
     * @example Close-up portrait of a majestic iguana with vibrant blue-green scales, piercing amber eyes, and orange spiky crest. Intricate textures and details visible on scaly skin. Wrapped in dark hood, giving regal appearance. Dramatic lighting against black background. Hyper-realistic, high-resolution image showcasing the reptile's expressive features and coloration.
     */
    prompt: string;
    /**
     * Seed
     * @description The seed to use for generating images
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated and uploaded
     *                 before returning the response. This will increase the latency of the function but
     *                 it allows you to get the image directly in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
}

export interface AuraFlowOutput {
    /**
     * Images
     * @description The generated images
     */
    images: Components.Image[];
    /**
     * Prompt
     * @description The expanded prompt
     */
    prompt: string;
    /**
     * Seed
     * @description The seed used to generate the images
     */
    seed: number;
}

export interface AudioUnderstandingInput {
    /**
     * Audio Url
     * @description URL of the audio file to analyze
     * @example https://storage.googleapis.com/falserverless/model_tests/audio-understanding/Title_%20Running%20on%20Fal.mp3
     */
    audio_url: string;
    /**
     * Detailed Analysis
     * @description Whether to request a more detailed analysis of the audio
     * @default false
     */
    detailed_analysis?: boolean;
    /**
     * Prompt
     * @description The question or prompt about the audio content.
     * @example What is being discussed in this audio?
     * @example What emotions are expressed in this audio?
     * @example What is the main topic of this conversation?
     */
    prompt: string;
}

export interface AudioUnderstandingOutput {
    /**
     * Output
     * @description The analysis of the audio content based on the prompt
     * @example Based on the audio, this appears to be a business meeting discussing quarterly sales results. The speakers are analyzing performance metrics and discussing strategies for the upcoming quarter. The tone is professional and collaborative, with multiple participants contributing to the discussion.
     */
    output: string;
}

export interface ArbiterImageTextInput {
    /**
     * Inputs
     * @description The inputs to use for the measurement.
     */
    inputs: Components.SemanticImageInput[];
    /**
     * Measurements
     * @description The measurements to use for the measurement.
     */
    measurements: 'clip_score'[];
}

export interface ArbiterImageTextOutput extends SharedType_97f {}

export interface ArbiterImageImageInput {
    /**
     * Inputs
     * @description The inputs to use for the measurement.
     */
    inputs: Components.ReferenceImageInput[];
    /**
     * Measurements
     * @description The measurements to use for the measurement.
     */
    measurements: ('dists' | 'mse' | 'lpips' | 'sdi' | 'ssim')[];
}

export interface ArbiterImageImageOutput extends SharedType_97f {}

export interface ArbiterImageInput {
    /**
     * Inputs
     * @description The inputs to use for the measurement.
     */
    inputs: Components.ImageInput[];
    /**
     * Measurements
     * @description The measurements to use for the measurement.
     */
    measurements: ('arniqa' | 'clip_iqa' | 'musiq' | 'nima' | 'lapvar')[];
}

export interface ArbiterImageOutput extends SharedType_97f {}

export interface AnimatediffSparsectrlLcmInput {
    /**
     * Controlnet Type
     * @description The type of controlnet to use for generating the video. The controlnet determines how the video will be animated.
     * @default scribble
     * @enum {string}
     */
    controlnet_type?: 'scribble' | 'rgb';
    /**
     * Classifier-Free Guidance scale (CFG)
     * @description The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
     * @default 1
     */
    guidance_scale?: number;
    /**
     * Keyframe 0 Image Url
     * @description The URL of the first keyframe to use for the generation.
     * @example https://storage.googleapis.com/falserverless/scribble2/scribble_2_1.png
     */
    keyframe_0_image_url?: string;
    /**
     * Keyframe 0 Index
     * @description The frame index of the first keyframe to use for the generation.
     * @default 0
     * @example 0
     */
    keyframe_0_index?: number;
    /**
     * Keyframe 1 Image Url
     * @description The URL of the second keyframe to use for the generation.
     * @example https://storage.googleapis.com/falserverless/scribble2/scribble_2_2.png
     */
    keyframe_1_image_url?: string;
    /**
     * Keyframe 1 Index
     * @description The frame index of the second keyframe to use for the generation.
     * @default 0
     * @example 8
     */
    keyframe_1_index?: number;
    /**
     * Keyframe 2 Image Url
     * @description The URL of the third keyframe to use for the generation.
     * @example https://storage.googleapis.com/falserverless/scribble2/scribble_2_3.png
     */
    keyframe_2_image_url?: string;
    /**
     * Keyframe 2 Index
     * @description The frame index of the third keyframe to use for the generation.
     * @default 0
     * @example 15
     */
    keyframe_2_index?: number;
    /**
     * Negative Prompt
     * @description The negative prompt to use. Use it to specify what you don't want.
     * @default
     * @example blurry, low resolution, bad, ugly, low quality, pixelated, interpolated, compression artifacts, noisey, grainy
     */
    negative_prompt?: string;
    /**
     * Number of inference steps
     * @description Increasing the amount of steps tells Stable Diffusion that it should take more steps to generate your final result which can increase the amount of detail in your image.
     * @default 4
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description The prompt to use for generating the image. Be as descriptive as possible for best results.
     * @example Drone footage, futuristic city at night, synthwave, vaporware, neon lights, highly detailed, masterpeice, high quality
     */
    prompt: string;
    /**
     * Seed
     * @description The same seed and the same prompt given to the same version of Stable
     *             Diffusion will output the same image every time.
     * @example 42
     */
    seed?: number;
}

export interface AnimatediffSparsectrlLcmOutput {
    /**
     * Seed
     * @description The seed used to generate the video.
     */
    seed: number;
    /**
     * Video
     * @description Generated video file.
     */
    video: Components.File_2;
}

export interface AmtInterpolationFrameInterpolationInput {
    /**
     * Frames
     * @description Frames to interpolate
     * @example [
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/model_tests/amt-interpolation/start.png"
     *       },
     *       {
     *         "url": "https://storage.googleapis.com/falserverless/model_tests/amt-interpolation/end.png"
     *       }
     *     ]
     */
    frames: Components.Frame[];
    /**
     * Output FPS
     * @description Output frames per second
     * @default 24
     */
    output_fps?: number;
    /**
     * Recursive Interpolation Passes
     * @description Number of recursive interpolation passes
     * @default 4
     */
    recursive_interpolation_passes?: number;
}

export interface AmtInterpolationFrameInterpolationOutput extends SharedType_618 {}

export interface AmtInterpolationInput {
    /**
     * Output FPS
     * @description Output frames per second
     * @default 24
     */
    output_fps?: number;
    /**
     * Recursive Interpolation Passes
     * @description Number of recursive interpolation passes
     * @default 2
     */
    recursive_interpolation_passes?: number;
    /**
     * Video URL
     * @description URL of the video to be processed
     * @example https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-vid2vid-input-2.gif
     */
    video_url: string;
}

export interface AmtInterpolationOutput extends SharedType_618 {}

export interface AiAvatarSingleTextInput {
    /**
     * Acceleration
     * @description The acceleration level to use for generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Image URL
     * @description URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://v3.fal.media/files/panda/HuM21CXMf0q7OO2zbvwhV_c4533aada79a495b90e50e32dc9b83a8.png
     */
    image_url: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
     * @default 136
     */
    num_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example An elderly man with a white beard and headphones records audio with a microphone. He appears engaged and expressive, suggesting a podcast or voiceover.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the video to generate. Must be either 480p or 720p.
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     * @default 42
     */
    seed?: number;
    /**
     * Text Input
     * @description The text input to guide video generation.
     * @example Spend more time with people who make you feel alive, and less with things that drain your soul.
     */
    text_input: string;
    /**
     * Voice
     * @description The voice to use for speech generation
     * @example Bill
     * @enum {string}
     */
    voice:
        | 'Aria'
        | 'Roger'
        | 'Sarah'
        | 'Laura'
        | 'Charlie'
        | 'George'
        | 'Callum'
        | 'River'
        | 'Liam'
        | 'Charlotte'
        | 'Alice'
        | 'Matilda'
        | 'Will'
        | 'Jessica'
        | 'Eric'
        | 'Chris'
        | 'Brian'
        | 'Daniel'
        | 'Lily'
        | 'Bill';
}

export interface AiAvatarSingleTextOutput extends SharedType_266 {}

export interface AiAvatarMultiTextInput {
    /**
     * Acceleration
     * @description The acceleration level to use for generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * First Text Input
     * @description The text input to guide video generation.
     * @example Do you know what are we eating?
     */
    first_text_input: string;
    /**
     * Image URL
     * @description URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://v3.fal.media/files/koala/vhkIF86hmgNTBll_lF1xI_3c7476642b19435aa763fe3b49cf99c7.png
     */
    image_url: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
     * @default 191
     */
    num_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example Two kids talking on a lunch.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the video to generate. Must be either 480p or 720p.
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Second Text Input
     * @description The text input to guide video generation.
     * @example I dont know I am eating this because our mother gave it to us. I think it is something called milky pie.
     */
    second_text_input: string;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     * @default 81
     */
    seed?: number;
    /**
     * Voice1
     * @description The first person's voice to use for speech generation
     * @default Sarah
     * @enum {string}
     */
    voice1?:
        | 'Aria'
        | 'Roger'
        | 'Sarah'
        | 'Laura'
        | 'Charlie'
        | 'George'
        | 'Callum'
        | 'River'
        | 'Liam'
        | 'Charlotte'
        | 'Alice'
        | 'Matilda'
        | 'Will'
        | 'Jessica'
        | 'Eric'
        | 'Chris'
        | 'Brian'
        | 'Daniel'
        | 'Lily'
        | 'Bill';
    /**
     * Voice2
     * @description The second person's voice to use for speech generation
     * @default Roger
     * @enum {string}
     */
    voice2?:
        | 'Aria'
        | 'Roger'
        | 'Sarah'
        | 'Laura'
        | 'Charlie'
        | 'George'
        | 'Callum'
        | 'River'
        | 'Liam'
        | 'Charlotte'
        | 'Alice'
        | 'Matilda'
        | 'Will'
        | 'Jessica'
        | 'Eric'
        | 'Chris'
        | 'Brian'
        | 'Daniel'
        | 'Lily'
        | 'Bill';
}

export interface AiAvatarMultiTextOutput {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "file_size": 352679,
     *       "file_name": "30b76b90c2164f9a926527497c20832b.mp4",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3.fal.media/files/zebra/lKMkUvzCqKn-gHC0vyUPP_30b76b90c2164f9a926527497c20832b.mp4"
     *     }
     */
    video: Components.File;
}

export interface AiAvatarMultiInput {
    /**
     * Acceleration
     * @description The acceleration level to use for generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * First Audio URL
     * @description The URL of the Person 1 audio file.
     * @example https://v3.fal.media/files/monkey/1XKPx3Xu-IhNLbuinVSwP_output.mp3
     */
    first_audio_url: string;
    /**
     * Image URL
     * @description URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://v3.fal.media/files/elephant/Q2ZU6q-d-1boGXhpDgWs9_15a22f816fd34cad969b2329946267b3.png
     */
    image_url: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
     * @default 181
     */
    num_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A smiling man and woman wearing headphones sit in front of microphones, appearing to host a podcast. They are engaged in conversation, looking at each other and the camera as they speak. The scene captures a lively and collaborative podcasting session.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the video to generate. Must be either 480p or 720p.
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Second Audio URL
     * @description The URL of the Person 2 audio file.
     * @example https://v3.fal.media/files/zebra/oVKyL8JZ1K2GreeIMxVzm_output.mp3
     */
    second_audio_url?: string;
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     * @default 81
     */
    seed?: number;
    /**
     * Use Only First Audio
     * @description Whether to use only the first audio file.
     * @default false
     */
    use_only_first_audio?: boolean;
}

export interface AiAvatarMultiOutput {
    /**
     * Seed
     * @description The seed used for generation.
     */
    seed: number;
    /**
     * Video
     * @description The generated video file.
     * @example {
     *       "file_size": 704757,
     *       "file_name": "ab27ac57e9464dbea1ef78f7a25469d2.mp4",
     *       "content_type": "application/octet-stream",
     *       "url": "https://v3.fal.media/files/kangaroo/uAF7N-Ow8WwuvbFw8J4Br_ab27ac57e9464dbea1ef78f7a25469d2.mp4"
     *     }
     */
    video: Components.File;
}

export interface AiAvatarInput {
    /**
     * Acceleration
     * @description The acceleration level to use for generation.
     * @default regular
     * @enum {string}
     */
    acceleration?: 'none' | 'regular' | 'high';
    /**
     * Audio URL
     * @description The URL of the audio file.
     * @example https://v3.fal.media/files/penguin/PtiCYda53E9Dav25QmQYI_output.mp3
     */
    audio_url: string;
    /**
     * Image URL
     * @description URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
     * @example https://v3.fal.media/files/koala/gmpc0QevDF9bBsL1EAYVF_1c637094161147559f0910a68275dc34.png
     */
    image_url: string;
    /**
     * Number of Frames
     * @description Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
     * @default 145
     */
    num_frames?: number;
    /**
     * Prompt
     * @description The text prompt to guide video generation.
     * @example A woman with colorful hair talking on a podcast.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the video to generate. Must be either 480p or 720p.
     * @default 480p
     * @enum {string}
     */
    resolution?: '480p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     * @default 42
     */
    seed?: number;
}

export interface AiAvatarOutput extends SharedType_b29 {}

export interface AceStepPromptToAudioInput {
    /**
     * Duration
     * @description The duration of the generated audio in seconds.
     * @default 60
     */
    duration?: number;
    /**
     * Granularity Scale
     * @description Granularity scale for the generation process. Higher values can reduce artifacts.
     * @default 10
     * @example 10
     */
    granularity_scale?: number;
    /**
     * Guidance Interval
     * @description Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)
     * @default 0.5
     * @example 0.5
     */
    guidance_interval?: number;
    /**
     * Guidance Interval Decay
     * @description Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.
     * @default 0
     * @example 0
     */
    guidance_interval_decay?: number;
    /**
     * Guidance Scale
     * @description Guidance scale for the generation.
     * @default 15
     * @example 15
     */
    guidance_scale?: number;
    /**
     * Guidance Type
     * @description Type of CFG to use for the generation process.
     * @default apg
     * @example apg
     * @enum {string}
     */
    guidance_type?: 'cfg' | 'apg' | 'cfg_star';
    /**
     * Instrumental
     * @description Whether to generate an instrumental version of the audio.
     * @default false
     * @example false
     */
    instrumental?: boolean;
    /**
     * Lyric Guidance Scale
     * @description Lyric guidance scale for the generation.
     * @default 1.5
     * @example 1.5
     */
    lyric_guidance_scale?: number;
    /**
     * Minimum Guidance Scale
     * @description Minimum guidance scale for the generation after the decay.
     * @default 3
     * @example 3
     */
    minimum_guidance_scale?: number;
    /**
     * Number Of Steps
     * @description Number of steps to generate the audio.
     * @default 27
     * @example 27
     */
    number_of_steps?: number;
    /**
     * Prompt
     * @description Prompt to control the style of the generated audio. This will be used to generate tags and lyrics.
     * @example A lofi hiphop song with a chill vibe about a sunny day on the boardwalk.
     */
    prompt: string;
    /**
     * Scheduler
     * @description Scheduler to use for the generation process.
     * @default euler
     * @example euler
     * @enum {string}
     */
    scheduler?: 'euler' | 'heun';
    /**
     * Seed
     * @description Random seed for reproducibility. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Tag Guidance Scale
     * @description Tag guidance scale for the generation.
     * @default 5
     * @example 5
     */
    tag_guidance_scale?: number;
}

export interface AceStepPromptToAudioOutput extends SharedType_b1c {}

export interface AceStepAudioToAudioInput {
    /**
     * Audio Url
     * @description URL of the audio file to be outpainted.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ace-step-audio-to-audio.wav
     */
    audio_url: string;
    /**
     * Edit Mode
     * @description Whether to edit the lyrics only or remix the audio.
     * @default remix
     * @example remix
     * @enum {string}
     */
    edit_mode?: 'lyrics' | 'remix';
    /**
     * Granularity Scale
     * @description Granularity scale for the generation process. Higher values can reduce artifacts.
     * @default 10
     * @example 10
     */
    granularity_scale?: number;
    /**
     * Guidance Interval
     * @description Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)
     * @default 0.5
     * @example 0.5
     */
    guidance_interval?: number;
    /**
     * Guidance Interval Decay
     * @description Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.
     * @default 0
     * @example 0
     */
    guidance_interval_decay?: number;
    /**
     * Guidance Scale
     * @description Guidance scale for the generation.
     * @default 15
     * @example 15
     */
    guidance_scale?: number;
    /**
     * Guidance Type
     * @description Type of CFG to use for the generation process.
     * @default apg
     * @example apg
     * @enum {string}
     */
    guidance_type?: 'cfg' | 'apg' | 'cfg_star';
    /**
     * Lyric Guidance Scale
     * @description Lyric guidance scale for the generation.
     * @default 1.5
     * @example 1.5
     */
    lyric_guidance_scale?: number;
    /**
     * Lyrics
     * @description Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song.
     * @default
     */
    lyrics?: string;
    /**
     * Minimum Guidance Scale
     * @description Minimum guidance scale for the generation after the decay.
     * @default 3
     * @example 3
     */
    minimum_guidance_scale?: number;
    /**
     * Number Of Steps
     * @description Number of steps to generate the audio.
     * @default 27
     * @example 27
     */
    number_of_steps?: number;
    /**
     * Original Lyrics
     * @description Original lyrics of the audio file.
     * @default
     * @example
     */
    original_lyrics?: string;
    /**
     * Original Seed
     * @description Original seed of the audio file.
     */
    original_seed?: number;
    /**
     * Original Tags
     * @description Original tags of the audio file.
     * @example lofi, hiphop, drum and bass, trap, chill
     */
    original_tags: string;
    /**
     * Scheduler
     * @description Scheduler to use for the generation process.
     * @default euler
     * @example euler
     * @enum {string}
     */
    scheduler?: 'euler' | 'heun';
    /**
     * Seed
     * @description Random seed for reproducibility. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Tag Guidance Scale
     * @description Tag guidance scale for the generation.
     * @default 5
     * @example 5
     */
    tag_guidance_scale?: number;
    /**
     * Tags
     * @description Comma-separated list of genre tags to control the style of the generated audio.
     * @example lofi, hiphop, drum and bass, trap, chill
     */
    tags: string;
}

export interface AceStepAudioToAudioOutput {
    /**
     * Audio
     * @description The generated audio file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/ace-step-audio-to-audio.wav"
     *     }
     */
    audio: Components.File;
    /**
     * Lyrics
     * @description The lyrics used in the generation process.
     * @example [inst]
     */
    lyrics: string;
    /**
     * Seed
     * @description The random seed used for the generation process.
     * @example 42
     */
    seed: number;
    /**
     * Tags
     * @description The genre tags used in the generation process.
     * @example lofi, hiphop, drum and bass, trap, chill
     */
    tags: string;
}

export interface AceStepAudioOutpaintInput {
    /**
     * Audio Url
     * @description URL of the audio file to be outpainted.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ace-step-audio-to-audio.wav
     */
    audio_url: string;
    /**
     * Extend After Duration
     * @description Duration in seconds to extend the audio from the end.
     * @default 30
     * @example 30
     */
    extend_after_duration?: number;
    /**
     * Extend Before Duration
     * @description Duration in seconds to extend the audio from the start.
     * @default 0
     * @example 0
     */
    extend_before_duration?: number;
    /**
     * Granularity Scale
     * @description Granularity scale for the generation process. Higher values can reduce artifacts.
     * @default 10
     * @example 10
     */
    granularity_scale?: number;
    /**
     * Guidance Interval
     * @description Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)
     * @default 0.5
     * @example 0.5
     */
    guidance_interval?: number;
    /**
     * Guidance Interval Decay
     * @description Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.
     * @default 0
     * @example 0
     */
    guidance_interval_decay?: number;
    /**
     * Guidance Scale
     * @description Guidance scale for the generation.
     * @default 15
     * @example 15
     */
    guidance_scale?: number;
    /**
     * Guidance Type
     * @description Type of CFG to use for the generation process.
     * @default apg
     * @example apg
     * @enum {string}
     */
    guidance_type?: 'cfg' | 'apg' | 'cfg_star';
    /**
     * Lyric Guidance Scale
     * @description Lyric guidance scale for the generation.
     * @default 1.5
     * @example 1.5
     */
    lyric_guidance_scale?: number;
    /**
     * Lyrics
     * @description Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song.
     * @default
     */
    lyrics?: string;
    /**
     * Minimum Guidance Scale
     * @description Minimum guidance scale for the generation after the decay.
     * @default 3
     * @example 3
     */
    minimum_guidance_scale?: number;
    /**
     * Number Of Steps
     * @description Number of steps to generate the audio.
     * @default 27
     * @example 27
     */
    number_of_steps?: number;
    /**
     * Scheduler
     * @description Scheduler to use for the generation process.
     * @default euler
     * @example euler
     * @enum {string}
     */
    scheduler?: 'euler' | 'heun';
    /**
     * Seed
     * @description Random seed for reproducibility. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Tag Guidance Scale
     * @description Tag guidance scale for the generation.
     * @default 5
     * @example 5
     */
    tag_guidance_scale?: number;
    /**
     * Tags
     * @description Comma-separated list of genre tags to control the style of the generated audio.
     * @example lofi, hiphop, drum and bass, trap, chill
     */
    tags: string;
}

export interface AceStepAudioOutpaintOutput extends SharedType_b1c {}

export interface AceStepAudioInpaintInput {
    /**
     * Audio Url
     * @description URL of the audio file to be inpainted.
     * @example https://storage.googleapis.com/falserverless/example_inputs/ace-step-audio-to-audio.wav
     */
    audio_url: string;
    /**
     * End Time
     * @description end time in seconds for the inpainting process.
     * @default 30
     * @example 30
     */
    end_time?: number;
    /**
     * End Time Relative To
     * @description Whether the end time is relative to the start or end of the audio.
     * @default start
     * @example start
     * @enum {string}
     */
    end_time_relative_to?: 'start' | 'end';
    /**
     * Granularity Scale
     * @description Granularity scale for the generation process. Higher values can reduce artifacts.
     * @default 10
     * @example 10
     */
    granularity_scale?: number;
    /**
     * Guidance Interval
     * @description Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)
     * @default 0.5
     * @example 0.5
     */
    guidance_interval?: number;
    /**
     * Guidance Interval Decay
     * @description Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.
     * @default 0
     * @example 0
     */
    guidance_interval_decay?: number;
    /**
     * Guidance Scale
     * @description Guidance scale for the generation.
     * @default 15
     * @example 15
     */
    guidance_scale?: number;
    /**
     * Guidance Type
     * @description Type of CFG to use for the generation process.
     * @default apg
     * @example apg
     * @enum {string}
     */
    guidance_type?: 'cfg' | 'apg' | 'cfg_star';
    /**
     * Lyric Guidance Scale
     * @description Lyric guidance scale for the generation.
     * @default 1.5
     * @example 1.5
     */
    lyric_guidance_scale?: number;
    /**
     * Lyrics
     * @description Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song.
     * @default
     */
    lyrics?: string;
    /**
     * Minimum Guidance Scale
     * @description Minimum guidance scale for the generation after the decay.
     * @default 3
     * @example 3
     */
    minimum_guidance_scale?: number;
    /**
     * Number Of Steps
     * @description Number of steps to generate the audio.
     * @default 27
     * @example 27
     */
    number_of_steps?: number;
    /**
     * Scheduler
     * @description Scheduler to use for the generation process.
     * @default euler
     * @example euler
     * @enum {string}
     */
    scheduler?: 'euler' | 'heun';
    /**
     * Seed
     * @description Random seed for reproducibility. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Start Time
     * @description start time in seconds for the inpainting process.
     * @default 0
     * @example 0
     */
    start_time?: number;
    /**
     * Start Time Relative To
     * @description Whether the start time is relative to the start or end of the audio.
     * @default start
     * @example start
     * @enum {string}
     */
    start_time_relative_to?: 'start' | 'end';
    /**
     * Tag Guidance Scale
     * @description Tag guidance scale for the generation.
     * @default 5
     * @example 5
     */
    tag_guidance_scale?: number;
    /**
     * Tags
     * @description Comma-separated list of genre tags to control the style of the generated audio.
     * @example lofi, hiphop, drum and bass, trap, chill
     */
    tags: string;
    /**
     * Variance
     * @description Variance for the inpainting process. Higher values can lead to more diverse results.
     * @default 0.5
     * @example 0.5
     */
    variance?: number;
}

export interface AceStepAudioInpaintOutput {
    /**
     * Audio
     * @description The generated audio file.
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/ace-step-audio-inpaint.wav"
     *     }
     */
    audio: Components.File;
    /**
     * Lyrics
     * @description The lyrics used in the generation process.
     * @example [inst]
     */
    lyrics: string;
    /**
     * Seed
     * @description The random seed used for the generation process.
     * @example 42
     */
    seed: number;
    /**
     * Tags
     * @description The genre tags used in the generation process.
     * @example lofi, hiphop, drum and bass, trap, chill
     */
    tags: string;
}

export interface AceStepInput {
    /**
     * Duration
     * @description The duration of the generated audio in seconds.
     * @default 60
     */
    duration?: number;
    /**
     * Granularity Scale
     * @description Granularity scale for the generation process. Higher values can reduce artifacts.
     * @default 10
     * @example 10
     */
    granularity_scale?: number;
    /**
     * Guidance Interval
     * @description Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)
     * @default 0.5
     * @example 0.5
     */
    guidance_interval?: number;
    /**
     * Guidance Interval Decay
     * @description Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.
     * @default 0
     * @example 0
     */
    guidance_interval_decay?: number;
    /**
     * Guidance Scale
     * @description Guidance scale for the generation.
     * @default 15
     * @example 15
     */
    guidance_scale?: number;
    /**
     * Guidance Type
     * @description Type of CFG to use for the generation process.
     * @default apg
     * @example apg
     * @enum {string}
     */
    guidance_type?: 'cfg' | 'apg' | 'cfg_star';
    /**
     * Lyric Guidance Scale
     * @description Lyric guidance scale for the generation.
     * @default 1.5
     * @example 1.5
     */
    lyric_guidance_scale?: number;
    /**
     * Lyrics
     * @description Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song.
     * @default
     */
    lyrics?: string;
    /**
     * Minimum Guidance Scale
     * @description Minimum guidance scale for the generation after the decay.
     * @default 3
     * @example 3
     */
    minimum_guidance_scale?: number;
    /**
     * Number Of Steps
     * @description Number of steps to generate the audio.
     * @default 27
     * @example 27
     */
    number_of_steps?: number;
    /**
     * Scheduler
     * @description Scheduler to use for the generation process.
     * @default euler
     * @example euler
     * @enum {string}
     */
    scheduler?: 'euler' | 'heun';
    /**
     * Seed
     * @description Random seed for reproducibility. If not provided, a random seed will be used.
     */
    seed?: number;
    /**
     * Tag Guidance Scale
     * @description Tag guidance scale for the generation.
     * @default 5
     * @example 5
     */
    tag_guidance_scale?: number;
    /**
     * Tags
     * @description Comma-separated list of genre tags to control the style of the generated audio.
     * @example lofi, hiphop, drum and bass, trap, chill
     */
    tags: string;
}

export interface AceStepOutput extends SharedType_b1c {}

export interface DecartLucyRestyleInput {
    /**
     * Enhance Prompt
     * @description Whether to enhance the prompt for better results.
     * @default true
     */
    enhance_prompt?: boolean;
    /**
     * Prompt
     * @description Text description of the desired video content
     * @example Make it psychedelic art style with trippy colors and patterns
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p';
    /**
     * Seed
     * @description Seed for video generation
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the video to be generated
     *                 and uploaded before returning the response. This will increase the
     *                 latency of the function but it allows you to get the video directly
     *                 in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Video Url
     * @description URL of the video to edit
     * @example https://v3b.fal.media/files/b/0a86729a/wIXfP8RYCtKBQAV9HPs7o_output.mp4
     */
    video_url: string;
}

export interface DecartLucyRestyleOutput {
    /**
     * Video
     * @description The generated video
     * @example https://v3b.fal.media/files/b/0a86d478/2JM2_bD0iJOmfcKHUEudv_generated_video.mp4
     */
    video: Components.File;
}

export interface DecartLucyEditProInput {
    /**
     * Enhance Prompt
     * @description Whether to enhance the prompt for better results.
     * @default true
     */
    enhance_prompt?: boolean;
    /**
     * Prompt
     * @description Text description of the desired video content
     * @example Transform the woman's outfit into a regal medieval gown with flowing velvet fabric, intricate gold embroidery, and a jeweled crown, giving her the appearance of a queen.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p';
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the video to be generated
     *                 and uploaded before returning the response. This will increase the
     *                 latency of the function but it allows you to get the video directly
     *                 in the response without going through the CDN.
     * @default true
     */
    sync_mode?: boolean;
    /**
     * Video Url
     * @description URL of the video to edit
     * @example https://v3.fal.media/files/monkey/GI7ArkqpQVk3M6V1C_epr_original.mp4
     */
    video_url: string;
}

export interface DecartLucyEditProOutput extends SharedType_e3b {}

export interface DecartLucyEditFastInput {
    /**
     * Enhance Prompt
     * @description Whether to enhance the prompt for better results.
     * @default true
     */
    enhance_prompt?: boolean;
    /**
     * Prompt
     * @description Text description of the desired video content
     * @example Change her jacket to formal brown jacket
     */
    prompt: string;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the video to be generated
     *                 and uploaded before returning the response. This will increase the
     *                 latency of the function but it allows you to get the video directly
     *                 in the response without going through the CDN.
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Video Url
     * @description URL of the video to edit
     * @example https://v3b.fal.media/files/b/elephant/p8aXM3bMaj_Nzbk5RfEoa_original-lucy-fast-coffee.mp4
     */
    video_url: string;
}

export interface DecartLucyEditFastOutput {
    /**
     * Video
     * @description The generated video
     * @example {
     *       "file_size": 819791,
     *       "file_name": "generated_video.mp4",
     *       "content_type": "video/mp4",
     *       "url": "https://v3b.fal.media/files/b/koala/Q24io3IIZNRvszEBUdb6Z_generated_video.mp4"
     *     }
     */
    video: Components.File;
}

export interface DecartLucyEditDevInput {
    /**
     * Enhance Prompt
     * @description Whether to enhance the prompt for better results.
     * @default true
     */
    enhance_prompt?: boolean;
    /**
     * Prompt
     * @description Text description of the desired video content
     * @example Transform the woman's outfit into a regal medieval gown with flowing velvet fabric, intricate gold embroidery, and a jeweled crown, giving her the appearance of a queen.
     */
    prompt: string;
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the video to be generated
     *                 and uploaded before returning the response. This will increase the
     *                 latency of the function but it allows you to get the video directly
     *                 in the response without going through the CDN.
     * @default true
     */
    sync_mode?: boolean;
    /**
     * Video Url
     * @description URL of the video to edit
     * @example https://v3.fal.media/files/monkey/GI7ArkqpQVk3M6V1C_epr_original.mp4
     */
    video_url: string;
}

export interface DecartLucyEditDevOutput extends SharedType_e3b {}

export interface DecartLucy14bImageToVideoInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video.
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '9:16' | '16:9';
    /**
     * Image Url
     * @description URL of the image to use as the first frame
     * @example https://storage.googleapis.com/falserverless/model_tests/lucy-14b/lucy-14b-art-swirl-image.png
     */
    image_url: string;
    /**
     * Prompt
     * @description Text description of the desired video content
     * @example A cinematic video begins with a woman standing in an art studio, wearing a paint-splattered apron over a white off-shoulder blouse, surrounded by colorful canvases on easels. She gently plays with her hair for a moment, then straightens her head and looks directly at the camera with a warm smile. After holding the smile, she gracefully twirls around in place, her apron flowing slightly with the motion, creating a playful and artistic atmosphere against the backdrop of her vibrant paintings.
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video
     * @default 720p
     * @enum {string}
     */
    resolution?: '720p';
    /**
     * Sync Mode
     * @description If set to true, the function will wait for the image to be generated
     *                 and uploaded before returning the response. This will increase the
     *                 latency of the function but it allows you to get the image directly
     *                 in the response without going through the CDN.
     * @default true
     */
    sync_mode?: boolean;
}

export interface DecartLucy14bImageToVideoOutput {
    /**
     * Video
     * @description The generated MP4 video with H.264 encoding
     * @example {
     *       "url": "https://storage.googleapis.com/falserverless/model_tests/lucy-14b/lucy-14b-art-swirl-video.mp4"
     *     }
     */
    video: Components.File;
}

export interface ClarityaiCrystalVideoUpscalerInput {
    /**
     * Scale Factor
     * @description Scale factor. The scale factor must be chosen such that the upscaled video does not exceed 5K resolution.
     * @default 2
     * @example 2
     */
    scale_factor?: number;
    /**
     * Video Url
     * @description URL to the input video.
     * @example https://storage.googleapis.com/falserverless/example_inputs/crystal_upscaler/video_upscaling/video_in.mp4
     */
    video_url: string;
}

export interface ClarityaiCrystalVideoUpscalerOutput {
    /**
     * Video
     * @description URL to the upscaled video
     * @example {
     *       "height": 2160,
     *       "duration": 13.056527,
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/crystal_upscaler/video_upscaling/video_out.mp4",
     *       "fps": 23.130193905817176,
     *       "width": 4096,
     *       "file_name": "w0VQQvPdwvV2GSCtRTMzh_hDH8SPrB.mp4",
     *       "content_type": "video/mp4",
     *       "num_frames": 302
     *     }
     */
    video: Components.VideoFile_1;
}

export interface ClarityaiCrystalUpscalerInput {
    /**
     * Creativity
     * @description Creativity level for upscaling
     * @default 0
     */
    creativity?: number;
    /**
     * Image Url
     * @description URL to the input image
     * @example https://v3b.fal.media/files/b/zebra/eW3waMFDT-2_7Pq8j3r9d_upscaled.png
     */
    image_url: string;
    /**
     * Scale Factor
     * @description Scale factor
     * @default 2
     */
    scale_factor?: number;
}

export interface ClarityaiCrystalUpscalerOutput {
    /**
     * Images
     * @description List of upscaled images
     * @example [
     *       "https://v3b.fal.media/files/b/penguin/sVriwxLaU3fVTz5B-xBBC_upscaled.png"
     *     ]
     */
    images: Components.Image[];
}

export interface CassetteaiVideoSoundEffectsGeneratorInput {
    /**
     * @description A video file to analyze & re-sound with generated SFX.
     * @example https://v3.fal.media/files/tiger/3NOa3BqrJfr3jJBMqGexs_final_with_sfx.mp4
     * @example https://v3.fal.media/files/rabbit/vkNtbcJ3x7KmzjJZeVWQe_final_with_sfx.mp4
     */
    video_url: Components.Video;
}

export interface CassetteaiVideoSoundEffectsGeneratorOutput {
    /**
     * Please ensure to unmute the video after playing manually to hear the SFX.
     * @description The final video with the newly generated SFX track.
     * @example {
     *       "url": "https://v3.fal.media/files/tiger/3NOa3BqrJfr3jJBMqGexs_final_with_sfx.mp4"
     *     }
     */
    video: Components.File_1;
}

export interface CassetteaiSoundEffectsGeneratorInput {
    /**
     * Duration
     * @description The duration of the generated SFX in seconds.
     * @example 30
     */
    duration: number;
    /**
     * Prompt
     * @description The prompt to generate SFX.
     * @example dog barking in the rain
     */
    prompt: string;
}

export interface CassetteaiSoundEffectsGeneratorOutput {
    /**
     * @description The generated SFX
     * @example {
     *       "url": "https://v3.fal.media/files/panda/FJ56Mbpj1F_MQVuO0UJ9k_generated.wav"
     *     }
     */
    audio_file: Components.File_1;
}

export interface CassetteaiMusicGeneratorInput {
    /**
     * Duration
     * @description The duration of the generated music in seconds.
     * @example 50
     */
    duration: number;
    /**
     * Prompt
     * @description The prompt to generate music from.
     * @example Smooth chill hip-hop beat with mellow piano melodies, deep bass, and soft drums, perfect for a night drive. Key: D Minor, Tempo: 90 BPM.
     */
    prompt: string;
}

export interface CassetteaiMusicGeneratorOutput {
    /**
     * @description The generated music
     * @example {
     *       "url": "https://v3.fal.media/files/panda/T-GP6cbpo1lgL8ll4oKGj_generated.wav"
     *     }
     */
    audio_file: Components.File_1;
}

export interface BytedanceLynxInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio of the generated video (16:9, 9:16, or 1:1)
     * @default 16:9
     * @enum {string}
     */
    aspect_ratio?: '16:9' | '9:16' | '1:1';
    /**
     * Frames Per Second
     * @description Frames per second of the generated video. Must be between 5 to 30.
     * @default 16
     */
    frames_per_second?: number;
    /**
     * Guidance Scale
     * @description Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Guidance Scale 2
     * @description Image guidance scale. Controls how closely the generated video follows the reference image. Higher values increase adherence to the reference image but may decrease quality.
     * @default 2
     */
    guidance_scale_2?: number;
    /**
     * Image Url
     * @description The URL of the subject image to be used for video generation
     * @example https://storage.googleapis.com/falserverless/example_inputs/lynx/example_in.png
     */
    image_url: string;
    /**
     * Ip Scale
     * @description Identity preservation scale. Controls how closely the generated video preserves the subject's identity from the reference image.
     * @default 1
     */
    ip_scale?: number;
    /**
     * Negative Prompt
     * @description Negative prompt to guide what should not appear in the generated video
     * @default Bright tones, overexposed, blurred background, static, subtitles, style, works, paintings, images, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards
     */
    negative_prompt?: string;
    /**
     * Num Frames
     * @description Number of frames in the generated video. Must be between 9 to 100.
     * @default 81
     */
    num_frames?: number;
    /**
     * Num Inference Steps
     * @description Number of inference steps for sampling. Higher values give better quality but take longer.
     * @default 50
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description Text prompt to guide video generation
     * @example A person carves a pumpkin on a porch in the evening. The camera captures their upper body as they draw a face with a marker, carefully cut along the lines, then lift the lid with both hands. Their face lights up with excitement as they peek inside
     */
    prompt: string;
    /**
     * Resolution
     * @description Resolution of the generated video (480p, 580p, or 720p)
     * @default 720p
     * @enum {string}
     */
    resolution?: '480p' | '580p' | '720p';
    /**
     * Seed
     * @description Random seed for reproducibility. If None, a random seed is chosen.
     */
    seed?: number;
    /**
     * Strength
     * @description Reference image scale. Controls the influence of the reference image on the generated video.
     * @default 1
     */
    strength?: number;
}

export interface BytedanceLynxOutput {
    /**
     * Seed
     * @description The seed used for generation
     */
    seed: number;
    /**
     * Video
     * @description The generated video file
     * @example {
     *       "content_type": "video/mp4",
     *       "url": "https://storage.googleapis.com/falserverless/example_outputs/lynx/example_out.mp4"
     *     }
     */
    video: Components.VideoFile_1;
}

export interface BriaVideoIncreaseResolutionInput {
    /**
     * Desired Increase
     * @description desired_increase factor. Options: 2x, 4x.
     * @default 2
     * @enum {string}
     */
    desired_increase?: '2' | '4';
    /**
     * Output Container And Codec
     * @description Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, mov_h265, mov_proresks, mkv_h265, mkv_h264, mkv_vp9, gif.
     * @default webm_vp9
     * @enum {string}
     */
    output_container_and_codec?:
        | 'mp4_h265'
        | 'mp4_h264'
        | 'webm_vp9'
        | 'mov_h265'
        | 'mov_proresks'
        | 'mkv_h265'
        | 'mkv_h264'
        | 'mkv_vp9'
        | 'gif';
    /**
     * Video Url
     * @description Input video to increase resolution. Size should be less than 14142x14142 and duration less than 30s.
     * @example https://bria-datasets.s3.us-east-1.amazonaws.com/video_increase_res/3446608-sd_426_240_25fps.mp4
     */
    video_url: string;
}

export interface BriaVideoIncreaseResolutionOutput extends SharedType_741 {}

export interface BriaVideoErasePromptInput extends SharedType_baf {}

export interface BriaVideoErasePromptOutput extends SharedType_346 {}

export interface BriaVideoEraseMaskInput extends SharedType_81d {}

export interface BriaVideoEraseMaskOutput extends SharedType_346 {}

export interface BriaVideoEraseKeypointsInput extends SharedType_f67 {}

export interface BriaVideoEraseKeypointsOutput extends SharedType_346 {}

export interface BriaVideoBackgroundRemovalInput {
    /**
     * Background Color
     * @description Background color. Options: Transparent, Black, White, Gray, Red, Green, Blue, Yellow, Cyan, Magenta, Orange.
     * @default Black
     * @enum {string}
     */
    background_color?:
        | 'Transparent'
        | 'Black'
        | 'White'
        | 'Gray'
        | 'Red'
        | 'Green'
        | 'Blue'
        | 'Yellow'
        | 'Cyan'
        | 'Magenta'
        | 'Orange';
    /**
     * Output Container And Codec
     * @description Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, mov_h265, mov_proresks, mkv_h265, mkv_h264, mkv_vp9, gif.
     * @default webm_vp9
     * @enum {string}
     */
    output_container_and_codec?:
        | 'mp4_h265'
        | 'mp4_h264'
        | 'webm_vp9'
        | 'mov_h265'
        | 'mov_proresks'
        | 'mkv_h265'
        | 'mkv_h264'
        | 'mkv_vp9'
        | 'gif';
    /**
     * Video Url
     * @description Input video to remove background from. Size should be less than 14142x14142 and duration less than 30s.
     * @example https://bria-datasets.s3.us-east-1.amazonaws.com/rmbg_tests/videos/5586521-uhd_3840_2160_25fps_original.mp4
     */
    video_url: string;
}

export interface BriaVideoBackgroundRemovalOutput extends SharedType_741 {}

export interface BriaTextToImage32Input {
    /**
     * Aspect Ratio
     * @description Aspect ratio. Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?: '1:1' | '2:3' | '3:2' | '3:4' | '4:3' | '4:5' | '5:4' | '9:16' | '16:9';
    /**
     * Guidance Scale
     * @description Guidance scale for text.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Negative Prompt
     * @description Negative prompt for image generation.
     * @default Logo,Watermark,Ugly,Morbid,Extra fingers,Poorly drawn hands,Mutation,Blurry,Extra limbs,Gross proportions,Missing arms,Mutated hands,Long neck,Duplicate,Mutilated,Mutilated hands,Poorly drawn face,Deformed,Bad anatomy,Cloned face,Malformed limbs,Missing legs,Too many fingers
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description Prompt for image generation.
     * @example Oil painting of a fluffy, wide-eyed cat sitting upright, holding a small wooden sign reading “Feed Me.” Rich textures, dramatic brushstrokes, warm tones, and vintage charm.
     */
    prompt: string;
    /**
     * Prompt Enhancer
     * @description Whether to improve the prompt.
     * @default true
     */
    prompt_enhancer?: boolean;
    /**
     * Seed
     * @description Random seed for reproducibility.
     * @default 5555
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If true, returns the image directly in the response (increases latency).
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Truncate Prompt
     * @description Whether to truncate the prompt.
     * @default true
     */
    truncate_prompt?: boolean;
}

export interface BriaTextToImage32Output extends SharedType_dc7 {}

export interface BriaReplaceBackgroundInput {
    /**
     * Image Url
     * @description Reference image (file or URL).
     * @default https://v3b.fal.media/files/b/0a8bea8c/Mztgx0NG3HPdby-4iPqwH_a_coffee_machine_standing_in_the_kitchen.png
     */
    image_url?: string;
    /**
     * Negative Prompt
     * @description Negative prompt for background replacement.
     * @default
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description Prompt for background replacement.
     * @example On a smooth kitchen counter in front of a blue and white patterned ceramic tile wall. A yellow ceramic mug sits to the right. Shot from a straight-on front angle.
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility.
     * @default 4925634
     */
    seed?: number;
    /**
     * Steps Num
     * @description Number of inference steps.
     * @default 30
     */
    steps_num?: number;
    /**
     * Sync Mode
     * @description If true, returns the image directly in the response (increases latency).
     * @default false
     */
    sync_mode?: boolean;
}

export interface BriaReplaceBackgroundOutput {
    /** @description Generated image. */
    image: Components.Image_2;
    /**
     * Images
     * @description Generated images.
     */
    images?: {
        [key: string]: { [x: string]: any } | null;
    }[];
}

export interface BriaReimagine32Input {
    /**
     * Aspect Ratio
     * @description Aspect ratio. Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?: '1:1' | '2:3' | '3:2' | '3:4' | '4:3' | '4:5' | '5:4' | '9:16' | '16:9';
    /**
     * Canny Image Url
     * @description Canny edge control image (file or URL).
     * @default
     * @example https://bria-image-repository.s3.us-east-1.amazonaws.com/BRIA+(1).png
     */
    canny_image_url?: string;
    /**
     * Canny Preprocess
     * @description Canny image preprocess.
     * @default true
     */
    canny_preprocess?: boolean;
    /**
     * Canny Scale
     * @description Canny edge control strength (0.0 to 1.0).
     * @default 0.5
     */
    canny_scale?: number;
    /**
     * Depth Image Url
     * @description Depth control image (file or URL).
     * @default
     * @example https://bria-image-repository.s3.us-east-1.amazonaws.com/BRIA+(1).png
     */
    depth_image_url?: string;
    /**
     * Depth Preprocess
     * @description Depth image preprocess.
     * @default true
     */
    depth_preprocess?: boolean;
    /**
     * Depth Scale
     * @description Depth control strength (0.0 to 1.0).
     * @default 0.5
     */
    depth_scale?: number;
    /**
     * Guidance Scale
     * @description Guidance scale for text.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Negative Prompt
     * @description Negative prompt for image generation.
     * @default Logo,Watermark,Ugly,Morbid,Extra fingers,Poorly drawn hands,Mutation,Blurry,Extra limbs,Gross proportions,Missing arms,Mutated hands,Long neck,Duplicate,Mutilated,Mutilated hands,Poorly drawn face,Deformed,Bad anatomy,Cloned face,Malformed limbs,Missing legs,Too many fingers
     */
    negative_prompt?: string;
    /**
     * Num Inference Steps
     * @description Number of inference steps.
     * @default 30
     */
    num_inference_steps?: number;
    /**
     * Prompt
     * @description Prompt for image generation.
     * @example Delicate, watercolor-style letters infused with shades of blue and green, accompanied by artistic, blooming flowers that blend harmoniously into a light background, giving a serene and artistic touch.
     */
    prompt: string;
    /**
     * Prompt Enhancer
     * @description Whether to improve the prompt.
     * @default true
     */
    prompt_enhancer?: boolean;
    /**
     * Seed
     * @description Random seed for reproducibility.
     * @default 5555
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If true, returns the image directly in the response (increases latency).
     * @default false
     */
    sync_mode?: boolean;
    /**
     * Truncate Prompt
     * @description Whether to truncate the prompt.
     * @default true
     */
    truncate_prompt?: boolean;
}

export interface BriaReimagine32Output extends SharedType_dc7 {}

export interface BriaFiboGenerateStructured_promptInput extends SharedType_0d0 {}

export interface BriaFiboGenerateStructured_promptOutput extends SharedType_4411 {}

export interface BriaFiboGenerateInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio. Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?: '1:1' | '2:3' | '3:2' | '3:4' | '4:3' | '4:5' | '5:4' | '9:16' | '16:9';
    /**
     * Guidance Scale
     * @description Guidance scale for text.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description Reference image (file or URL).
     */
    image_url?: string;
    /**
     * Negative Prompt
     * @description Negative prompt for image generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description Prompt for image generation.
     * @example A hyper-detailed, ultra-fluffy owl sitting in the trees at night, looking directly at the camera with wide, adorable, expressive eyes. Its feathers are soft and voluminous, catching the cool moonlight with subtle silver highlights. The owl’s gaze is curious and full of charm, giving it a whimsical, storybook-like personality.
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility.
     * @default 5555
     */
    seed?: number;
    /**
     * Steps Num
     * @description Number of inference steps.
     * @default 50
     */
    steps_num?: number;
    /** @description The structured prompt to generate an image from. */
    structured_prompt?: Components.StructuredPrompt;
    /**
     * Sync Mode
     * @description If true, returns the image directly in the response (increases latency).
     * @default false
     */
    sync_mode?: boolean;
}

export interface BriaFiboGenerateOutput extends SharedType_720 {}

export interface BriaFiboLiteGenerateStructured_promptLiteInput {
    /**
     * Image Url
     * @description Reference image (file or URL).
     */
    image_url?: string;
    /**
     * Prompt
     * @description Prompt for image generation.
     * @example A hyper-detailed, ultra-fluffy owl sitting in the trees at night, looking directly at the camera with wide, adorable, expressive eyes. Its feathers are soft and voluminous, catching the cool moonlight with subtle silver highlights. The owl's gaze is curious and full of charm, giving it a whimsical, storybook-like personality.
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility.
     * @default 5555
     */
    seed?: number;
    /** @description The structured prompt to generate an image from. */
    structured_prompt?: Components.bria_fibovlm_StructuredPrompt;
}

export interface BriaFiboLiteGenerateStructured_promptLiteOutput extends SharedType_4411 {}

export interface BriaFiboLiteGenerateStructured_promptInput extends SharedType_0d0 {}

export interface BriaFiboLiteGenerateStructured_promptOutput extends SharedType_4411 {}

export interface BriaFiboLiteGenerateInput {
    /**
     * Aspect Ratio
     * @description Aspect ratio. Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9
     * @default 1:1
     * @enum {string}
     */
    aspect_ratio?: '1:1' | '2:3' | '3:2' | '3:4' | '4:3' | '4:5' | '5:4' | '9:16' | '16:9';
    /**
     * Image Url
     * @description Reference image (file or URL).
     */
    image_url?: string;
    /**
     * Prompt
     * @description Prompt for image generation.
     * @example A hyper-detailed, ultra-fluffy owl sitting in the trees at night, looking directly at the camera with wide, adorable, expressive eyes. Its feathers are soft and voluminous, catching the cool moonlight with subtle silver highlights. The owl’s gaze is curious and full of charm, giving it a whimsical, storybook-like personality.
     */
    prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility.
     * @default 5555
     */
    seed?: number;
    /**
     * Steps Num
     * @description Number of inference steps for Fibo Lite.
     * @default 8
     */
    steps_num?: number;
    /** @description The structured prompt to generate an image from. */
    structured_prompt?: Components.StructuredPrompt;
    /**
     * Sync Mode
     * @description If true, returns the image directly in the response (increases latency).
     * @default false
     */
    sync_mode?: boolean;
}

export interface BriaFiboLiteGenerateOutput extends SharedType_720 {}

export interface BriaFiboEditSketch_to_colored_imageInput {
    /**
     * Image Url
     * @description The source image.
     * @example https://bria-datasets.s3.us-east-1.amazonaws.com/Liza/create_a_b_w_sketch_of_a_cat.png
     */
    image_url: string;
}

export interface BriaFiboEditSketch_to_colored_imageOutput extends SharedType_ad1 {}

export interface BriaFiboEditRewrite_textInput {
    /**
     * Image Url
     * @description The source image.
     * @example https://bria-datasets.s3.us-east-1.amazonaws.com/Liza/create_an_image_of_cake__with_text_on_it_saying___Hi_there__.png
     */
    image_url: string;
    /**
     * New Text
     * @description The new text string to appear in the image.
     * @example FIBO Edit!
     */
    new_text: string;
}

export interface BriaFiboEditRewrite_textOutput extends SharedType_ad1 {}

export interface BriaFiboEditRestyleInput {
    /**
     * Image Url
     * @description The source image.
     * @example https://bria-datasets.s3.us-east-1.amazonaws.com/Liza/high_camera_angle_warm_filter.png
     */
    image_url: string;
    /**
     * Style
     * @description Select the desired artistic style for the output image.
     * @example 3D Render
     * @enum {string}
     */
    style:
        | '3D Render'
        | 'Cubism'
        | 'Oil Painting'
        | 'Anime'
        | 'Cartoon'
        | 'Coloring Book'
        | 'Retro Ad'
        | 'Pop Art Halftone'
        | 'Vector Art'
        | 'Story Board'
        | 'Art Nouveau'
        | 'Cross Etching'
        | 'Wood Cut';
}

export interface BriaFiboEditRestyleOutput extends SharedType_ad1 {}

export interface BriaFiboEditRestoreInput {
    /**
     * Image Url
     * @description The source image.
     * @example https://bria-datasets.s3.us-east-1.amazonaws.com/Liza/png+-+2026-01-13T134151.337.png
     */
    image_url: string;
}

export interface BriaFiboEditRestoreOutput extends SharedType_ad1 {}

export interface BriaFiboEditReseasonInput {
    /**
     * Image Url
     * @description The source image.
     * @example https://bria-datasets.s3.us-east-1.amazonaws.com/Liza/create_a_realistic_image_of_a_green_field_in_the_spring__also_add_trees.png
     */
    image_url: string;
    /**
     * Season
     * @description The desired season.
     * @example winter
     * @enum {string}
     */
    season: 'spring' | 'summer' | 'autumn' | 'winter';
}

export interface BriaFiboEditReseasonOutput extends SharedType_ad1 {}

export interface BriaFiboEditReplace_object_by_textInput {
    /**
     * Image Url
     * @description The source image.
     * @example https://bria-datasets.s3.us-east-1.amazonaws.com/Liza/a_bowl_of_fruits__should_have_a_red_apple.png
     */
    image_url: string;
    /**
     * Instruction
     * @description The full natural language command describing what to replace.
     * @example Replace the red apple with a green pear
     */
    instruction: string;
}

export interface BriaFiboEditReplace_object_by_textOutput extends SharedType_ad1 {}

export interface BriaFiboEditRelightInput {
    /**
     * Image Url
     * @description The source image.
     * @example https://bria-datasets.s3.us-east-1.amazonaws.com/Liza/bria_result+-+2026-01-13T095546.173.png
     */
    image_url: string;
    /**
     * Light Direction
     * @description Where the light comes from.
     * @example front
     */
    light_direction: 'front' | 'side' | 'bottom' | 'top-down';
    /**
     * Light Type
     * @description The quality/style/time of day.
     * @example soft overcast daylight lighting
     * @enum {string}
     */
    light_type:
        | 'midday'
        | 'blue hour light'
        | 'low-angle sunlight'
        | 'sunrise light'
        | 'spotlight on subject'
        | 'overcast light'
        | 'soft overcast daylight lighting'
        | 'cloud-filtered lighting'
        | 'fog-diffused lighting'
        | 'moonlight lighting'
        | 'starlight nighttime'
        | 'soft bokeh lighting'
        | 'harsh studio lighting';
}

export interface BriaFiboEditRelightOutput extends SharedType_ad1 {}

export interface BriaFiboEditErase_by_textInput {
    /**
     * Image Url
     * @description The source image.
     * @example https://bria-datasets.s3.us-east-1.amazonaws.com/Liza/an_empty_table_in_living_room.png
     */
    image_url: string;
    /**
     * Object Name
     * @description The name of the object to remove.
     * @example Table
     */
    object_name: string;
}

export interface BriaFiboEditErase_by_textOutput extends SharedType_ad1 {}

export interface BriaFiboEditEditStructured_instructionInput {
    /**
     * Image Url
     * @description Reference image (file or URL).
     * @example https://v3b.fal.media/files/b/0a8b07e8/GYKVk2EVivg_MC3jRRZi3_png%20-%202026-01-13T094835.850%20(3).png
     */
    image_url?: string;
    /**
     * Instruction
     * @description Instruction for image editing.
     * @example change lighting to starlight nighttime
     */
    instruction?: string;
    /**
     * Mask Url
     * @description Reference image mask (file or URL). Optional.
     */
    mask_url?: string;
    /**
     * Seed
     * @description Random seed for reproducibility.
     * @default 5555
     */
    seed?: number;
    /**
     * Sync Mode
     * @description If true, returns the image directly in the response (increases latency).
     * @default false
     */
    sync_mode?: boolean;
}

export interface BriaFiboEditEditStructured_instructionOutput extends SharedType_4411 {}

export interface BriaFiboEditEditInput {
    /**
     * Guidance Scale
     * @description Guidance scale for text.
     * @default 5
     */
    guidance_scale?: number;
    /**
     * Image Url
     * @description Reference image (file or URL).
     * @example https://v3b.fal.media/files/b/0a8b07e8/GYKVk2EVivg_MC3jRRZi3_png%20-%202026-01-13T094835.850%20(3).png
     */
    image_url?: string;
    /**
     * Instruction
     * @description Instruction for image editing.
     * @example change lighting to starlight nighttime
     */
    instruction?: string;
    /**
     * Mask Url
     * @description Mask image (file or URL). Optional
     */
    mask_url?: string;
    /**
     * Negative Prompt
     * @description Negative prompt for image generation.
     * @default
     */
    negative_prompt?: string;
    /**
     * Seed
     * @description Random seed for reproducibility.
     * @default 5555
     */
    seed?: number;
    /**
     * Steps Num
     * @description Number of inference steps.
     * @default 50
     */
    steps_num?: number;
    /** @description The structured prompt to generate an image from. */
    structured_instruction?: Components.StructuredInstruction;
    /**
     * Sync Mode
     * @description If true, returns the image directly in the response (increases latency).
     * @default false
     */
    sync_mode?: boolean;
}

export interface BriaFiboEditEditOutput {
    /** @description Generated image. */
    image: Components.Image_2;
    /**
     * Images
     * @description Generated images.
     * @default []
     * @example [
     *       {
     *         "url": "https://bria-datasets.s3.us-east-1.amazonaws.com/Liza/ewT7wv-jMgkqs7z7xQNNL_e8707c299d034feab7a64d903118098f.png"
     *       }
     *     ]
     */
    images?: Components.Image_2[];
    /**
     * Structured Instruction
     * @description Current instruction.
     */
    structured_instruction: {
        [key: string]: { [x: string]: any } | null;
    };
}

export interface BriaFiboEditColorizeInput {
    /**
     * Color
     * @description Select the color palette or aesthetic for the output image
     * @example contemporary color
     * @enum {string}
     */
    color: 'contemporary color' | 'vivid color' | 'black and white colors' | 'sepia vintage';
    /**
     * Image Url
     * @description The source image.
     * @example https://bria-datasets.s3.us-east-1.amazonaws.com/Liza/png+-+2026-01-13T083840.113.png
     */
    image_url: string;
}

export interface BriaFiboEditColorizeOutput extends SharedType_ad1 {}

export interface BriaFiboEditBlendInput {
    /**
     * Image Url
     * @description The source image.
     * @example https://bria-datasets.s3.us-east-1.amazonaws.com/Liza/shirt.png
     */
    image_url: string;
    /**
     * Instruction
     * @description Instruct what elements you would like to blend in your image.
     * @example Place the art on the shirt, keep the art exactly the same
     */
    instruction: string;
}

export interface BriaFiboEditBlendOutput extends SharedType_ad1 {}

export interface BriaFiboEditAdd_object_by_textInput {
    /**
     * Image Url
     * @description The source image.
     * @example https://bria-datasets.s3.us-east-1.amazonaws.com/Liza/an_empty_table_in_living_room.png
     */
    image_url: string;
    /**
     * Instruction
     * @description The full natural language command describing what to add and where.
     * @example Place a red vase with flowers on the table.
     */
    instruction: string;
}

export interface BriaFiboEditAdd_object_by_textOutput extends SharedType_ad1 {}

export interface BriaBria_video_eraserErasePromptInput extends SharedType_baf {}

export interface BriaBria_video_eraserErasePromptOutput extends SharedType_346 {}

export interface BriaBria_video_eraserEraseMaskInput extends SharedType_81d {}

export interface BriaBria_video_eraserEraseMaskOutput extends SharedType_346 {}

export interface BriaBria_video_eraserEraseKeypointsInput extends SharedType_f67 {}

export interface BriaBria_video_eraserEraseKeypointsOutput extends SharedType_346 {}

export interface BeatovenSoundEffectGenerationInput {
    /**
     * Creativity
     * @description Creativity level - higher values allow more creative interpretation of the prompt
     * @default 16
     * @example 16
     * @example 14
     * @example 10
     */
    creativity?: number;
    /**
     * Duration
     * @description Length of the generated sound effect in seconds
     * @default 5
     * @example 7
     * @example 10
     * @example 20
     * @example 30
     */
    duration?: number;
    /**
     * Negative Prompt
     * @description Describe the types of sounds you don't want to generate in the output, avoid double-negatives, compare with positive prompts
     * @default
     * @example Low-pitched hum
     * @example High-pitched screech, rain, wind
     * @example Thunder, lightning
     * @example traffic, people speaking
     * @example Soft whisper
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description Describe the sound effect you want to generate
     * @example Powerful helicopter takeoff: rapidly building rotor blades whirring and chopping, increasing engine whine, ground vibrations.
     * @example A futuristic spaceship door opening
     * @example A cinematic explosion with debris falling
     * @example Rain falling on a window pane
     * @example Footsteps on gravel
     */
    prompt: string;
    /**
     * Refinement
     * @description Refinement level - Higher values may improve quality but take longer
     * @default 40
     * @example 40
     * @example 70
     * @example 100
     * @example 200
     */
    refinement?: number;
    /**
     * Seed
     * @description Random seed for reproducible results - leave empty for random generation
     */
    seed?: number;
}

export interface BeatovenSoundEffectGenerationOutput {
    /**
     * @description Generated audio file in WAV format
     * @example {
     *       "url": "https://v3b.fal.media/files/b/lion/9Uo4MoD-efg4sjcDyI6Nl_sfx_QHCg3z.wav"
     *     }
     */
    audio: Components.File_1;
    /**
     * Metadata
     * @description Generation metadata including duration, sample rate, and parameters
     */
    metadata: {
        [key: string]: { [x: string]: any } | null;
    };
    /**
     * Prompt
     * @description The processed prompt used for generation
     */
    prompt: string;
}

export interface BeatovenMusicGenerationInput {
    /**
     * Creativity
     * @description Creativity level - higher values allow more creative interpretation of the prompt
     * @default 16
     * @example 16
     * @example 14
     * @example 11
     */
    creativity?: number;
    /**
     * Duration
     * @description Length of the generated music in seconds
     * @default 90
     * @example 90
     * @example 47
     * @example 150
     */
    duration?: number;
    /**
     * Negative Prompt
     * @description Describe what you want to avoid in the music (instruments, styles, moods). Leave blank for none.
     * @default
     * @example noise
     * @example distortion
     * @example heavy drums
     * @example high-hats
     */
    negative_prompt?: string;
    /**
     * Prompt
     * @description Describe the music you want to generate
     * @example Jazz music for a late-night restaurant setting
     * @example A lush, ambient soundscape featuring serene sounds, and a gentle, melancholic piano melody
     * @example Hip-hop music, mellow keys and vinyl crackle
     * @example House music with synthesizers, driving bass and a steady 4/4 beat
     * @example Classical piano melody with emotional depth and gentle strings
     */
    prompt: string;
    /**
     * Refinement
     * @description Refinement level - higher values may improve quality but take longer
     * @default 100
     * @example 100
     * @example 200
     */
    refinement?: number;
    /**
     * Seed
     * @description Random seed for reproducible results - leave empty for random generation
     */
    seed?: number;
}

export interface BeatovenMusicGenerationOutput {
    /**
     * @description Generated audio file in WAV format
     * @example {
     *       "url": "https://v3b.fal.media/files/b/rabbit/DBesSNPP6NwfhwMftene-_music_ZfniDF.wav"
     *     }
     */
    audio: Components.File_1;
    /**
     * Metadata
     * @description Generation metadata including duration, sample rate, and parameters
     */
    metadata: {
        [key: string]: { [x: string]: any } | null;
    };
    /**
     * Prompt
     * @description The processed prompt used for generation
     */
    prompt: string;
}

export interface ArgilAvatarsTextToVideoInput {
    /**
     * Avatar
     * @example Noemie car (UGC)
     * @enum {string}
     */
    avatar:
        | 'Mia outdoor (UGC)'
        | 'Lara (Masterclass)'
        | 'Ines (UGC)'
        | 'Maria (Masterclass)'
        | 'Emma (UGC)'
        | 'Sienna (Masterclass)'
        | 'Elena (UGC)'
        | 'Jasmine (Masterclass)'
        | 'Amara (Masterclass)'
        | 'Ryan podcast (UGC)'
        | 'Tyler (Masterclass)'
        | 'Jayse (Masterclass)'
        | 'Paul (Masterclass)'
        | 'Matteo (UGC)'
        | 'Daniel car (UGC)'
        | 'Dario (Masterclass)'
        | 'Viva (Masterclass)'
        | 'Chen (Masterclass)'
        | 'Alex (Masterclass)'
        | 'Vanessa (UGC)'
        | 'Laurent (UGC)'
        | 'Noemie car (UGC)'
        | 'Brandon (UGC)'
        | 'Byron (Masterclass)'
        | 'Calista (Masterclass)'
        | 'Milo (Masterclass)'
        | 'Fabien (Masterclass)'
        | 'Rose (UGC)';
    /**
     * Remove Background
     * @description Enabling the remove background feature will result in a 50% increase in the price.
     * @default false
     */
    remove_background?: boolean;
    /**
     * Text
     * @example Argil is kinda crazy guys! You just turn a real person into
     *     an avatar that actually talks and moves and it's already reel-ready,
     *     for TikTok, Shorts, whatever. No wasting hours editing, it still looks super pro.
     */
    text: string;
    /**
     * Voice
     * @enum {string}
     */
    voice:
        | 'Rachel'
        | 'Clyde'
        | 'Roger'
        | 'Sarah'
        | 'Laura'
        | 'Thomas'
        | 'Charlie'
        | 'George'
        | 'Callum'
        | 'River'
        | 'Harry'
        | 'Liam'
        | 'Alice'
        | 'Matilda'
        | 'Will'
        | 'Jessica'
        | 'Lilly'
        | 'Bill'
        | 'Oxley'
        | 'Luna';
}

export interface ArgilAvatarsTextToVideoOutput extends SharedType_768 {}

export interface ArgilAvatarsAudioToVideoInput {
    /**
     * Audio Url
     * @example {
     *       "url": "https://argildotai.s3.us-east-1.amazonaws.com/fal-resource/example_fal.mp3"
     *     }
     */
    audio_url: string;
    /**
     * Avatar
     * @example Noemie car (UGC)
     * @enum {string}
     */
    avatar:
        | 'Mia outdoor (UGC)'
        | 'Lara (Masterclass)'
        | 'Ines (UGC)'
        | 'Maria (Masterclass)'
        | 'Emma (UGC)'
        | 'Sienna (Masterclass)'
        | 'Elena (UGC)'
        | 'Jasmine (Masterclass)'
        | 'Amara (Masterclass)'
        | 'Ryan podcast (UGC)'
        | 'Tyler (Masterclass)'
        | 'Jayse (Masterclass)'
        | 'Paul (Masterclass)'
        | 'Matteo (UGC)'
        | 'Daniel car (UGC)'
        | 'Dario (Masterclass)'
        | 'Viva (Masterclass)'
        | 'Chen (Masterclass)'
        | 'Alex (Masterclass)'
        | 'Vanessa (UGC)'
        | 'Laurent (UGC)'
        | 'Noemie car (UGC)'
        | 'Brandon (UGC)'
        | 'Byron (Masterclass)'
        | 'Calista (Masterclass)'
        | 'Milo (Masterclass)'
        | 'Fabien (Masterclass)'
        | 'Rose (UGC)';
    /**
     * Remove Background
     * @description Enabling the remove background feature will result in a 50% increase in the price.
     * @default false
     */
    remove_background?: boolean;
}

export interface ArgilAvatarsAudioToVideoOutput extends SharedType_768 {}

export {};
